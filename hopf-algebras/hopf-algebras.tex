% !TeX program = lualatex
\documentclass[fleqn]{NotesClass}

\strictpagecheck

\usepackage{csquotes}

\usepackage{tikz}
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]
\usetikzlibrary{positioning}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{calc}

\usepackage{tikz-cd}
\AtBeginEnvironment{tikzcd}{\tikzexternaldisable}
\AtEndEnvironment{tikzcd}{\tikzexternalenable}

\usepackage[pdfauthor={Willoughby Seago},pdftitle={Notes from Hopf Algebras Course},pdfkeywords={Hopf Algebra, Bialgebra, Comonoid},pdfsubject={Hopf Algebras}]{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{NotesBoxes}
\usepackage{NotesMaths2}

\setmathfont[range={\int, \oint, \otimes, \oplus, \bigotimes, \bigoplus}]{Latin Modern Math}


% Highlight colour
%\definecolor{highlight}{HTML}{710D78}
%\definecolor{my blue}{HTML}{2A0D77}
%\definecolor{my red}{HTML}{770D38}
%\definecolor{my green}{HTML}{14770D}
%\definecolor{my yellow}{HTML}{E7BB41}

% Title page info
\title{Hopf Algebras}
\author{Willoughby Seago}
\date{January 16th, 2024}
\subtitle{Notes from}
\subsubtitle{University of Glasgow}
\renewcommand{\abstracttext}{These are my notes from the SMSTC course \emph{Hopf Algebras} taught by Dr Andrew Baker. For category theory details I have been referring to Riehl's \emph{Category Theory in Context}. These notes were last updated at \printtime{} on \today{}.}

% Commands
% Maths
\newcommand{\cat}[1]{\symsfup{#1}}
\newcommand{\initial}{\symbf{0}}
\newcommand{\terminal}{\symbf{1}}
\makeatletter
\newcommand{\c@egory}[1]{\symsfup{#1}}
\newcommand{\Set}{\c@egory{Set}}
\newcommand{\SetPt}{\c@egory{Set}_{\bullet}}
\newcommand{\Grp}{\c@egory{Grp}}
\newcommand{\Ab}{\c@egory{Ab}}
\newcommand{\Ring}{\c@egory{Ring}}
\newcommand{\Rng}{\c@egory{Rng}}
\newcommand{\Vect}[1][\field]{\c@egory{Vect}_{#1}}
\newcommand{\RMod}[1][R]{#1\text{-}\c@egory{Mod}}
\newcommand{\ModR}[1][R]{\c@egory{Mod}\text{-}#1}
\newcommand{\Top}{\c@egory{Top}}
\newcommand{\TopPt}{\c@egory{Top}_{\bullet}}
\newcommand{\Man}{\c@egory{Man}}
\newcommand{\Mon}{\c@egory{Mon}}
\newcommand{\hTopPt}{\c@egory{hTop}_{\bullet}}
\newcommand{\Alg}[1][\field]{\c@egory{Alg}_{#1}}
\newcommand{\CAlg}[1][\field]{\c@egory{CAlg}_{#1}}
\newcommand{\Coalg}[1][\field]{\c@egory{Coalg}_{#1}}
\newcommand{\CCoalg}[1][\field]{\c@egory{CCoalg}_{#1}}
\makeatother
\ExplSyntaxOn
% Create LaTeX interface command
\NewDocumentCommand{\cycle}{ O{\,} m }{  % optional arg is separator, mandatory
    %arg is comma separated list
    (
    \willoughby_cycle:nn { #1 } { #2 }
    )
}

\clist_new:N \l_willougbhy_cycle_clist  % Create new clist variable
\cs_new_protected:Npn \willoughby_cycle:nn #1 #2 {  % create LaTeX3 function
    \clist_set:Nn \l_willougbhy_cycle_clist { #2 }  % set clist variable with
    %clist #2 passed by user
    \clist_use:Nn \l_willougbhy_cycle_clist { #1 }  % print list separated by #1
}
\ExplSyntaxOff
\newcommand{\switch}{\symup{T}}
\newcommand{\id}{\symrm{id}}
\newcommand{\isomorphic}{\cong}
\newcommand{\natTrans}{\Rightarrow}
\renewcommand{\field}{\symbb{k}}
\newcommand{\leftadjointto}{\dashv}
\newcommand{\rightadjointto}{\vdash}
\newcommand{\action}{\mathbin{.}}
\DeclareMathOperator{\Hom}{Hom}
\newcommand{\op}{\symrm{op}}
\newcommand{\incl}{\symrm{incl}}
\DeclareMathOperator{\im}{im}
\newcommand{\universalEnveloping}{\symcal{U}}
\DeclareMathOperator{\coker}{coker}
\newcommand{\trans}{\top}
\DeclarePairedDelimiterX{\bracket}[2]{[}{]}{#1, #2}
\newcommand{\tsum}{{\textstyle \sum}}
\DeclareMathOperator{\Span}{span}

% String diagrams
\tikzset{wire/.style = {thick}}
\tikzset{over wire/.style={thick, preaction={draw, background color, line width=#1}}}
\tikzset{over wire/.default={1.5mm}}
\colorlet{background color}{white}
\tikzset{morphism/.style = {draw, rounded corners}}
\tikzset{monoid dot/.style = {circle, draw, fill, inner sep=0pt, minimum width=2mm}}
\tikzset{comonoid dot/.style = {circle, draw, fill=background color, inner sep=0pt, minimum width=2mm}}

\begin{document}
    \frontmatter
    \titlepage
    \innertitlepage{}
    \tableofcontents
    % \listoffigures
    \mainmatter
    \chapter{Category Theory}
    We will make use of a lot of category theory throughout the course.
    It is expected that people are familiar with the basic notions of category theory, some of which we recap here.
    
    \section{Initial and Terminal Objects}
    \begin{dfn}{}{}
        Let \(\cat{C}\) be a category.
        \begin{itemize}
            \item An object, \(t \in \cat{C}\), is \define{terminal}\index{terminal object} if for all \(a \in \cat{C}\) the set \(\cat{C}(a, t)\) contains exactly one element.
            \item An object, \(i \in \cat{C}\), is \define{initial}\index{initial object} if for all \(a \in \cat{C}\) the set \(\cat{C}(i, a)\) contains exactly one element.
        \end{itemize}
        An object that is both initial and terminal is called a \define{null}\index{null object} or \defineindex{zero object}.
    \end{dfn}
    
    \begin{lma}{}{}
        Initial and terminal objects are unique up to unique isomorhpism.
    \end{lma}
    
    \begin{ntn}{}{}
        We denote the initial object in a category by \(\initial\) and the terminal object by \(\terminal\).
    \end{ntn}
    
    \begin{exm}{}{}
        \begin{itemize}
            \item The category of sets, \(\Set\), has \(\initial = \emptyset\) and \(\terminal = \{\bullet\}\) (that is, any singleton set is terminal).
            \item The category of pointe sets, \(\SetPt\), has \(\initial = \terminal = \{\bullet\}\) where \(\{\bullet\}\) is the pointed set with \(\bullet\) as its distinguished element.
            \item The category of groups, \(\Grp\), or abelian groups, \(\Ab\), has \(\initial = \terminal = \{e\}\), that is the trivial group is a null object.
            \item The category of rings\footnote{which we assume are unital}, \(\Ring\), has \(\initial = \integers\), since a ring homomorhpism\footnote{which we assume preserves the unit} \(\varphi \colon \integers \to \reals\) is uniquely determined by \(\varphi(1) = 1_R\) since then \(\varphi(n) = \varphi(n 1) = n \varphi(1) = n 1_R\).
            If we allow \(0 = 1\) then the terminal object is the trivial ring, \(\terminal = \{0\}\).
            If we do not allow \(0 = 1\) then \(\Ring\) has no terminal object\footnote{In this course we generally assume \(0 \ne 1\). I disagree with this choice.}.
            \item The category of non-unital rings, \(\Rng\), has \(\terminal = \{0\}\), and no initial object.
            \item \define{Abelian categories}\index{abelian category} are a special class of categories with certain properties.
            One of these properties is that they have a null object.
            Examples of abelian categories include \(\Ab\), \(\Vect\), and more generally \(\RMod\).
        \end{itemize}
    \end{exm}
    
    \section{Products and Coproducts}
    \begin{dfn}{Product}{}
        Let \(\cat{C}\) be a category.
        A set of morphisms \(\{p_i \colon c \to c_i \mid i \in I\}\), for some indexing set, \(I\), is a \defineindex{product} of the \(p_i\) (although usually we refer to it as a product of the \(c_i\)) if given any set of morphisms \(\{f_i \colon d \to c_i \mid i \in I\}\) there exists a unique morphism \(f \colon d \to c\) such that \(f_i = p_i f\) for all \(i \in I\).
        
        If \(I = \emptyset\) then we define the product to be the terminal object, if it exists.
    \end{dfn}
    
    \begin{dfn}{}{}
        Let \(\cat{C}\) be a category.
        A set of morphisms \(\{j_i \colon c_i \to c \mid i \in I\}\), for some indexing set, \(I\), is a \defineindex{coproduct} of the \(j_i\) if given any set of morphisms \(\{g_i \colon c_i \to d \mid i \in I\}\) there exists a unique morphism \(g \colon c \to d\) such that \(g_i = g j_i\) for all \(i \in I\).
        
        If \(I = \emptyset\) then we define the product to be the initial object, if it exists.
    \end{dfn}
    
    \begin{lma}{}{}
        Products and coproducts are unique up to unique isomorphism.
    \end{lma}
    
    We can express the definition of the (co)product as the existence of a morphism making a certain \(I\)-indexed family of diagrams commute:
    \begin{equation}
        \begin{tikzcd}
            d \arrow[d, dashed, "\exists ! f"'] \arrow[dr, "f_i"]\\
            c \arrow[r, "p_i"'] & c_i
        \end{tikzcd}
        \qquad \left(
            \begin{tikzcd}
                c \arrow[r, "j_i"] \arrow[d, dashed, "g"'] & c_i \arrow[dl, "g_i"]\\
                d
            \end{tikzcd}
        \right).
    \end{equation}
    When we're dealing with binary (co)products (\(I = \{1, 2\}\)) we usually combine the two triangles into the commuting diagram
    \begin{equation}
        \begin{tikzcd}
            & d \arrow[dl, "f_1"'] \arrow[dr, "f_2"] \arrow[d, dashed, "\exists ! f", pos=0.6]\\
            c_1 & c \arrow[l, "p_1"] \arrow[r, "p_2"'] & c_2
        \end{tikzcd}
        \qquad \left(
            \begin{tikzcd}
                c_1 \arrow[r, "j_1"] \arrow[dr, "g_1"'] & c \arrow[d, dashed, "\exists ! g", pos=0.4] & c_2 \arrow[l, "j_2"'] \arrow[dl, "g_2"]\\
                & d
            \end{tikzcd}
        \right).
    \end{equation}
    
    \begin{ntn}{}{}
        The (co)product is typically denoted in terms of the objects, with the projections (inclusions) left implicit.
        We will denote binary products by \(\times\), other notations include \(\symrm{\Pi}\) and \(\otimes\),
        We will denote coproducts by \(+\), other notations include \(\amalg\), \(\oplus\) and \(\sqcup\).
        
        We'll also use \(\prod\) and \(\coprod\) to denote products over arbitrary families.
    \end{ntn}
    
    Note that there is ambiguity in the above notation in that \(I\) is not assumed to be ordered, so \(c_1 \times c_2\) and \(c_2 \times c_1\) are both valid notations for the product of \(c_1\) and \(c_2\).
    Formally these objects may be different, but they will be isomorphic with a unique isomorphism between them so it doesn't really matter.
    
    Products and coproducts are functorial in their variables.
    That is, if we have two products \(\{p_i \colon c \to c_i \mid i \in I\}\) and \(q_i \colon d \to d_i \mid i \in I\) and morphisms \(f_i \colon c_i \to d_i\) then there is a unique morphism \(h \colon c \to d\) such that for all \(i \in I\) the diagram
    \begin{equation}
        \begin{tikzcd}
            c \arrow[r, "h"] \arrow[d, "p_i"'] & d \arrow[d, "q_i"]\\
            c_i \arrow[r, "f_i"'] & d_i
        \end{tikzcd}
    \end{equation}
    commutes.
    We call \(h\) the product of the \(f_i\), and denote it \(h = \prod_{i \in I} f_i\).
    A dual result holds for the coproduct.
    
    Consider the special case of the above where \(d_i = c_{\sigma^{-1}(i)}\) for some bijection \(\sigma \colon I \to I\).
    This uniquely defines an isomorphism \(\switch_\sigma \colon \prod_I c_i \to \prod_I c_{\sigma^{-1}(i)}\).
    The most important case of this is when \(I = \{1, 2\}\) and \(\sigma(0) = 1\) and \(\sigma(1) = 0\), in which case we have the isomorphism
    \begin{equation}
        \switch = \switch_{\cycle{1,2}} \colon c_1 \times c_2 \to c_2 \times c_1.
    \end{equation}
    Note that \(\switch\) actually depends on \(c_1\) and \(c_2\), so we should probably call it \(\switch_{c_1,c_2}\).
    However, for any given objects \(c_1\) and \(c_2\) for which the product \(c_1 \times c_2\) exists there is such a morphism, so we typically drop the objects from the notation letting context inform us of which \(\switch\) we're using.
    The more formal justification for this is that the functor \((c_1, c_2) \mapsto c_1 \times c_2\) is naturally isomorphic to the functor \((c_1, c_2) \mapsto c_2 \times c_1\), and \(\switch_{c_1,c_2}\) is the component of this natural isomorphism at \((c_1, c_2) \in \cat{C} \times \cat{C}\).
    
    Note that the composite
    \begin{equation}
        c_1 \times c_2 \xrightarrow{\switch_{c_1,c_2}} c_2 \times c_1 \xrightarrow{\switch_{c_2,c_1}} c_1 \times c_2
    \end{equation}
    is actually the identity.
    That is,
    \begin{equation}
        \switch_{c_2, c_1} \circ \switch_{c_1,c_2} = \id_{\cat{C}},
    \end{equation}
    or more snappily, \(\switch^2 = \id_{\cat{C}}\).
    
    \begin{exm}{}{}
        \begin{itemize}
            \item In \(\Set\) the product is the Cartesian product, \(\times\), and the coproduct is the disjoin union, \(\sqcup\).
            \item In \(\SetPt\) the product \((X, x) \times (Y, y)\) is the pointed set \((X \times Y, (x, y))\) where \(X \times Y\) is the Cartesian product.
            The coproduct \((X, x) + (Y, y)\) is the pointed set \(((X \sqcap Y)/{\sim}, [x])\) where \(\sim\) is the equivalence relation identifying \(x\) and \(y\) in the disjoint union with each other, and \([x]\) is the equivalence class that \(x\) and \(y\) end up in under this relation.
            More intuitively, the coproduct is just the disjoint union where we identify the base points of the original sets with each other.
            \item In \(\Grp\) the product is the Cartesian product (also called the direct product) of the underlying sets with the operation defined pointwise.
            The coproduct is the free product, \(G * H\), which intuitively consists of words of the form \(g_1h_1g_2h_2 \dotsm g_nh_n\) with \(g_i \in G\) and \(h_i \in H\) (and only \(g_1\) or \(h_n\) allowed to be identities).
            \item In \(\Ring\) the product is the Cartesian product and the coproduct is a free product defined similarly to the free product of groups, but also allowing us to add elements together as well as forming products.
            \item In \(\Ab\) any finite product or coproduct is given by the direct product (also called the direct sum).
            \item In \(\Top\) the product and coproduct are the Cartesian product and disjoint union of the underlying sets equipped with the appropriate topologies which may be characterised as being the coarsest topologies such that the projections and inclusions are continuous.
            \item In an \defineindex{abelian category} finite products and coproducts coincide by definition.
            Thus, in \(\Vect\) or \(\RMod\) all finite products and coproducts are given by the direct sum.
        \end{itemize}
    \end{exm}
    
    \begin{prp}{}{}
        Let \(\cat{C}\) be a category in which all binary products exist and there is a terminal object, \(\terminal\).
        Then all finite products exist and for all \(c \in \cat{C}\) we have
        \begin{equation}
            \terminal \times c \isomorphic c \isomorphic c \times \terminal.
        \end{equation}
        
        Let \(\cat{C}\) be a category in which all binary coproducts exist and there is an initial object, \(\initial\).
        Then all finite coproducts exist and for all \(c \in \cat{C}\) we have
        \begin{equation}
            \initial + c \isomorphic c \isomorphic c + \initial.
        \end{equation}
    \end{prp}
    
    \section{Monoids and Comonoids}
    \textit{I'm going to take a slightly different approach here and not define monoids in an arbitrary category with products and terminal objects, but instead move straight to monoidal categories, which subsume these cases.}
    
    \begin{dfn}{Monoidal Category}{}
        A \defineindex{monoidal category}, \((\cat{C}, \otimes, I, \alpha, \lambda, \rho)\), is a category, \(\cat{C}\), with a functor
        \begin{equation}
            - \otimes - \colon \cat{C} \times \cat{C} \to \cat{C},
        \end{equation}
        object \(I \in \cat{C}\), and natural transformations
        \begin{gather}
            \alpha \colon (- \otimes -) \otimes - \natTrans - \otimes (- \otimes -);\\
            \lambda \colon I \otimes - \natTrans -;\\
            \rho \colon - \otimes I \natTrans -.
        \end{gather}
        This data is subject to the following:
        \begin{itemize}
            \item For all \(a, b, c, d \in \cat{C}\) the diagram
            \begin{equation}
                \begin{tikzcd}
                    & (ab)(cd) \arrow[r, "{\alpha_{a,b,cd}}"] & a(b(cd))\\
                    ((ab)c)d \arrow[ur, "{\alpha_{ab,c,d}}"] \arrow[dr, "{\alpha_{a,b,c} \otimes \id_d}"']\\
                    & (a(bc))d \arrow[r, "{\alpha_{a,bc,d}}"'] & a((bc)d) \arrow[uu, "{\id_a \otimes \alpha_{b,c,d}}"']
                \end{tikzcd}
            \end{equation}
            commutes where we use the shorthand \(ab = a \otimes b\).
            \item For all \(a, b \in \cat{C}\) the diagram
            \begin{equation}
                \begin{tikzcd}
                    (a \otimes I) \otimes b \arrow[rr, "{\alpha_{a,I,b}}"] \arrow[dr, "\rho_a \otimes \id_b"'] && a \otimes (I \otimes b) \arrow[dl, "\id_a \otimes \lambda_b"]\\
                    & a \otimes b
                \end{tikzcd}
            \end{equation}
            commutes.
        \end{itemize}
    \end{dfn}
    
    \begin{dfn}{Braided Monoidal Category}{}
        A \defineindex{braided monoidal category}, \((\cat{C}, \otimes, I, \alpha, \lambda, \rho, \gamma)\), is a monoidal category \((\cat{C}, \otimes, I, \alpha, \lambda, \rho)\) equipped with a natural transformation, \(\gamma\), with components
        \begin{equation}
            \gamma_{a,b} \colon a \otimes b \natTrans b \otimes a.
        \end{equation}
        This is subject to the condition that the diagrams
        \begin{equation}
            \begin{tikzcd}
                & a(bc) \arrow[r, "{\gamma_{a,bc}}"] & (bc)a \arrow[dr, "{\alpha_{b,c,a}}"]\\
                (ab)c \arrow[ur, "{\alpha_{a,b,c}}"] \arrow[dr, "{\gamma_{a,b} \otimes \id_c}"'] &&& b(ca)\\
                & (ba)c \arrow[r, "{\alpha_{b,a,c}}"'] & b(ac) \arrow[ur, "{\id_b \otimes \gamma_{a,c}}"']
            \end{tikzcd}
        \end{equation}
        and
        \begin{equation}
            \begin{tikzcd}
                & (ab)c \arrow[r, "{\gamma_{ab,c}}"] & c(ab) \arrow[dr, "{\alpha_{c,a,b}^{-1}}"]\\
                a(bc) \arrow[ur, "{\alpha_{a,b,c}^{-1}}"] \arrow[dr, "{\id_a \otimes \gamma_{b,c}}"'] &&& (ca)b\\
                & a(cb) \arrow[r, "{\alpha_{a,c,b}^{-1}}"'] & (ac)b \arrow[ur, "{\gamma_{a,c} \otimes \id_b}"']
            \end{tikzcd}
        \end{equation}
        commute.
        Again, writing \(ab = a \otimes b\) as shorthand.
    \end{dfn}
    
    \begin{dfn}{Symmetric Monoidal Category}{}
        A \defineindex{symmetric monoidal category} is a braided monoidal category for which
        \begin{equation}
            \gamma_{b,a} \circ \gamma_{a,b} = \id_{a \otimes b}
        \end{equation}
        for all \(a, b \in \cat{C}\).
    \end{dfn}
    
    The natural isomorphisms in these definitions are called the coherence morhpisms.
    They all model a particular property mirroring a property of a monoid:
    \begin{itemize}
        \item \(\alpha\) is the associator, and it means that the product (which is not necessarily a categorical product) \(\otimes\) is associative up to natural isomorphism.
        \item \(\lambda\) and \(\rho\) are the left and right unitors, and they mean that \(I\) acts as an identity element for the product \(\otimes\), again up to natural isomorphism.
        \item \(\gamma\) (when it exists) is the braiding or symmetry of the category, and it means that the product \(\otimes\) is commutative up to natural isomorphism.
    \end{itemize}
    
    \begin{exm}{}{}
        The following are all monoidal categories, the coherence morphisms are left out of the notation (as is standard) and they can usually be worked out from context.
        \begin{itemize}
            \item \((\Set, \times, \{\bullet\})\):
            The coherence maps are
            \begin{itemize}
                \item \(\alpha_{a,b,c}((x,y),z) = (x,(y,z))\);
                \item \(\lambda_a(\bullet, x) = x\);
                \item \(\rho_a(x, \bullet) = x\);
                \item \(\gamma_{a,b}(x,y) = (y,x)\);
            \end{itemize}
            \item \((\Set, \sqcup, \emptyset)\):
            \item \((\Vect, \otimes, \field)\);
            \item \((\RMod, \otimes_R, R)\);
            \item \((\cat{C}, \times, \terminal)\) where \(\cat{C}\) is any category with all binary products and a terminal object.
        \end{itemize}
        Note that multiple monoidal structures can exist on the same underlying category.
        All of these are symmetric, as most examples in common practice are.
        An example of a non-symmetric monoidal category is \([\cat{C}, \cat{C}]\), the category of endofunctors, \(\cat{C} \to \cat{C}\), with the monoidal product given by composition and the unit object being the identity functor, \(\id_{\cat{C}}\).
    \end{exm}
    
    We are now ready to give the definition of a monoid in a monoidal category in its full glory.
    
    \begin{dfn}{Monoid}{}
        Let \(\cat{C}\) be a monoidal category.
        A \defineindex{monoid}, \((m, \mu, \eta)\), is an object, \(m \in \cat{C}\), equipped with morphisms
        \begin{equation}
            \mu \colon m \otimes m \to m, \qqand \iota \colon I \to m,
        \end{equation}
        called \defineindex{multiplication} and \defineindex{unit} respectively, such that the diagrams
        \begin{equation}
            \begin{tikzcd}
                & m \otimes (m \otimes m) \arrow[r, "\id_m \otimes \mu"] & m \otimes m \arrow[dd, "\mu"]\\
                (m \otimes m) \otimes m \arrow[ur, "{\alpha_{m,m,m}}"] \arrow[dr, "\mu \otimes \id_m"']\\
                & m \otimes m \arrow[r, "\mu"'] & m
            \end{tikzcd}
        \end{equation}
        and
        \begin{equation}
            \begin{tikzcd}
                I \otimes m \arrow[r, "\eta \otimes \id_m"] \arrow[dr, "\lambda_m"'] & m \otimes m \arrow[d, "\mu"'] & m \otimes I \arrow[l, "\id_m \otimes \eta"'] \arrow[dl, "\rho_m"]\\
                & m
            \end{tikzcd}
        \end{equation}
        commute.
        
        If \(\cat{C}\) is symmetric monoidal category with braiding \(\gamma\) then \((m, \mu, \eta)\) is a \defineindex{commutative monoid} if
        \begin{equation}
            \begin{tikzcd}
                m \otimes m \arrow[rr, "\gamma_{m,m}"] \arrow[dr, "\mu"'] && m \otimes m \arrow[dl, "\mu"]\\
                & m
            \end{tikzcd}
        \end{equation}
        commutes, that is, \(\mu \circ \gamma_{m,m} = \mu\).
    \end{dfn}
    
    \begin{exm}{}{}
        \begin{itemize}
            \item A monoid, \((M, \mu, \eta)\), in the monoidal category \(\Set\) (under the Cartesian product, which we assume to be the monoidal structure of \(\Set\) unless stated otherwise) is just a monoid in the usual sense.
            The map \(\mu \colon M \times M \to M\) is just the multiplication map on \(M\), and \(\eta \colon \{\bullet\} \to M\) picks out an element of \(M\), namely \(\eta(\bullet)\).
            
            The first diagram states that the multiplication in \(M\) is associative, which we can see by starting at \((M \times M) \times M\) and tracing both directions around the diagram, taking \(x, y, z \in M\) going the top path gives 
            \begin{align}
                \mu \circ (\id_m \times \mu) \circ \alpha_{m,m,m}((x, y), z) &= \mu \circ (\id_m \times \mu)(x, (y, z))\\
                &= \mu(\id_m (x), \mu(y, z))\\
                &= \mu(x, \mu(y, z))
            \end{align}
            and going along the bottom path gives
            \begin{align}
                \mu \circ (\mu \times \id_m)((x, y), z) &= \mu(\mu(x, y), \id_m(z))\\
                &= \mu(\mu(x, y), z).
            \end{align}
            Writing \(\mu(a, b) = a b\) these two results become \(x(yz)\) and \((xy)z\).
            Demanding that they are equal is thus associativity of multiplication.
            
            The second diagram asserts that the distinguished element, \(e = \eta(\bullet) \in M\), is the identity for this multiplication map.
            The left triangle gives us
            \begin{equation}
                \mu \circ (\eta \times \id_m)(\bullet, x) = \mu(e, x) = ex \qand \lambda_m(\bullet, x) = x,
            \end{equation}
            so imposing equality gives us that \(ex = x\), so \(e\) is a left identity.
            Similarly, the right triangle gives us that \(e\) is a right identity.
            
            The commutativity condition, assuming it holds, states that \(\mu \circ \gamma_{m,m}(x, y) = \mu(y, x) = yx\) and \(\mu(x, y) = xy\) agree, so it's just stating that the multiplication in this monoid is commutative.
            
            \item A monoid in the category of monoids, \((\Mon, \times, \{\bullet\})\), is a commutative monoid, this follows from the Eckmann--Hilton argument\footnote{Suppose that we have a set, \(M\), equipped with two binary operations, \(\circ\) and \(\diamond\) with associated units \(1_\circ\) and \(1_\diamond\). These are compatible if \((a \circ b) \diamond (c \circ d) = (a \diamond c) \circ (b \diamond d)\) (a result known as the interchange law, note that this is true of vertical and horizontal composition of natural transformations). Then using these properties we have \(1_\circ = 1_{\circ} \circ 1_{\circ} = (1_\diamond \diamond 1_\circ) \circ (1_\circ \diamond 1_\diamond) = (1_\diamond \circ 1_\circ) \diamond (1_\circ \circ 1_\diamond) = 1_\diamond \diamond 1_\diamond = 1_\diamond\), so the units coincide and we write \(1 = 1_\circ = 1_\diamond\). Then for \(x, y \in M\) we have \(a \circ b = (1 \diamond a) \circ (b \diamond 1) = (1 \circ b) \diamond (a \circ 1) = b \diamond a = (b \circ 1) \diamond (1 \circ a) = (b \diamond 1) \circ (1 \diamond a) = b \circ a\), thus \(\circ\) is commutative, and from this we see that \(b \circ a = b \diamond a\), so \(\diamond\) and \(\circ\) coincide. Note that one can also prove associativity of these operations: \((ab)c = (ab)(1c) = (a1)(bc) = a(bc)\), where now that we know that the two multiplications are the same we just use juxtaposition.}, which demonstrates that if there are two compatible monoidal structures then they actually agree and must be commutative.
            Similarly, a monoid in the category of groups, \(\Grp\), is an abelian group.
            
            \item A monoid in the category of topological spaces, \((\Top, \times, \{\bullet\})\), is a topological monoid, that is a monoid for which the multiplication and unit maps are continuous.
            A monoid in the category of smooth manifolds, \((\Man, \times, \{\bullet\})\), is a Lie monoid (a Lie group without the condition that inverses exist).
            
            \item A monoid in the category of abelian groups, \((\Ab, \otimes_{\integers}, \integers)\), is a ring, the group operation providing the addition and the monoid multiplication providing the multiplication.
            Distributivity of multiplication over addition is witnessed by the requirement that \(\mu\) is a group homomorphism, so
            \begin{equation}
                \mu(x, y + z) = \mu(x, y) + \mu(x, z)
            \end{equation}
            which gives
            \begin{equation}
                x(y + z) = xy + xz
            \end{equation}
            and similar for distributivity on the right.
            
            \item For a commutative\footnote{commutativity is required for the tensor product of \(R\)-modules to be an \(R\)-module, we could instead look at a category of bimodules for a slightly more general result.} ring, \(R\), a monoid in the category of \(R\)-modules, \((\RMod, \otimes_{R}, R)\), is an \(R\)-algebra.
            The monoid multiplication provides the multiplication of the algebra, which is commutative exactly when this is a commutative monoid.
            Note that \(R = \field\) is a special case of this, in which case the monoidal category is \(\Vect\).
            
            \item A monoid in the monoidal category of endofunctors, \([\cat{C}, \cat{C}]\), is a monad.
        \end{itemize}
    \end{exm}
    
    So far all we've achieved is a somewhat complex unification of many similar algebraic structures.
    The nice thing is that now we've defined monoids in terms of commutative diagrams as well as picking them up and placing them in other categories we can also dualise the definition.
    
    \begin{dfn}{Comonoid}{}
        Let \(\cat{C}\) be a monoidal category.
        A comonoid, \((c, \Delta, \varepsilon)\), is an object, \(c \in \cat{C}\), equipped with morphisms
        \begin{equation}
            \Delta \colon c \to c \otimes c, \qqand \varepsilon \colon c \to I,
        \end{equation}
        called \defineindex{comultiplication} and \defineindex{counit} respectively, such that the diagrams
        \begin{equation}
            \begin{tikzcd}
                & c \otimes (c \otimes c) \arrow[dl, "{\alpha_{c,c,c}^{-1}}"', pos=0.4] &  c \otimes c \arrow[l, "\id_c \otimes \Delta"']\\
                (c \otimes c) \otimes c\\
                & c \otimes c \arrow[ul, "\Delta \otimes \id_c", pos=0.4] & c \arrow[l, "\Delta"] \arrow[uu, "\Delta"']
            \end{tikzcd}
        \end{equation}
        and
        \begin{equation}
            \begin{tikzcd}
                I \otimes c \arrow[dr, "\lambda_c"'] & c \otimes c \arrow[l, "\varepsilon \otimes \id_c"'] \arrow[r, "\id_c \otimes \varepsilon"] & c \otimes I \arrow[dl, "\rho_c"]\\
                & c \arrow[u, "\Delta"]
            \end{tikzcd}
        \end{equation}
        commute.
        
        If \(\cat{C}\) is a symmetric monoidal category with braiding \(\gamma\) then \((c, \Delta, \varepsilon)\) is a \defineindex{cocommutative comonoid} if
        \begin{equation}
            \begin{tikzcd}
                c \otimes c && c \otimes c \arrow[ll, "{\gamma_{c,c}}"']\\
                & c \arrow[ul, "\Delta"] \arrow[ur, "\Delta"']
            \end{tikzcd}
        \end{equation}
        commutes, that is, \(\gamma_{c,c} \circ \Delta = \Delta\).
    \end{dfn}
    
    Comonoids typically aren't as familiar as monoids, but nonetheless they're important, in particular Hopf algebras are both monoids and comonoids (in a compatible way with one other piece of data).
    
    \begin{exm}{}{}
        \begin{itemize}
            \item In \((\Set, \times, \{\bullet\})\) any set, \(X\), has a unique comonoid structure given by declaring that \(\Delta(x) = (x, x) \in X \times X\) and \(\varepsilon(x) = \bullet\) is the unique element of the singleton set, \(I = \{\bullet\}\).
            This makes comonoids particularly boring in \(\Set\), which explains why they tend not to be so familiar.
            Taking \(X = \reals\) and picturing \(\reals \times \reals\) as the plane the image of \(\Delta\) is thus the diagonal line \(y = x\), which is why \(x \mapsto (x, x)\) is sometimes called the diagonal map, and explains the choice of \(\Delta\) (D for Diagonal) as the notation for the comultiplication.
            \item In \((\Set, \sqcup, \emptyset)\) the only comonoid is \(\emptyset\), since to define a comonoid structure on some non-empty set, \(X\), we'd have to have a map \(\varepsilon \colon X \to \emptyset\), and no such maps exist.
            \item In \(\Vect\) any \(\field\)-vector space, \(V\), may be equipped with a comonoid structure as follows.
            First, pick a basis, \(\{e_i\}\).
            Then define comultiplication by \(\Delta(e_i) = \sum_i e_i \otimes e_i\), and the counit by \(e_i \mapsto 1 \in \field\), both extended by linearity to the whole space.
            \item For a commutative ring, \(R\), a comonoid in \((\RMod, \otimes_R, R)\) is called a coalgebra (more on these later).
            A specific example is the polynomial ring \(R[X]\), which is a comonoid when we define the comultiplication to be \(\Delta(X) = X \otimes X\) and the counit to be \(\varepsilon(X) = 1 \in R\).
            \item The monoidal category \((\TopPt, \vee, \{\bullet\})\) of based topological spaces under the wedge product (union and then identify the base points) has the sphere, \(S^n\), as an \emph{almost} comonoid.
            There's a map \(S^n \vee S^n \to S^n\) that almost works as the comultiplication.
            The only problem is that coassociativity only holds up to homotopy.
            We can get a proper comonoid by instead considering \((\hTopPt, \vee, \{\bullet\})\), the monoidal category of based topological spaces up to homotopy.
            \item A comonoid in the monoidal category of endofunctors, \([\cat{C}, \cat{C}]\), is a comonad.
        \end{itemize}
    \end{exm}
    
    \subsection{String Diagrams}
    When working with monoidal categories a common notation is \define{string diagrams}\index{string diagram}.
    These are used to represent expressions like \(f \otimes (g \circ h)\) with \(h \colon a \to b\), \(g \colon d \to e\) and \(h \colon c \to d\).
    The idea is that an object is represented by its identity morphism, which we draw as a string, labelled with its object if necessary:
    \begin{equation}
        \id_a = 
        \tikzsetnextfilename{string-diagram-identity-morphism}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [wire] (0, 0) node [above] {\(a\)} -- (0, -1) node [below] {\(a\)};
        \end{tikzpicture}
    \end{equation}
    An arbitrary morphism, \(f \colon a \to b\), is then denoted with a box, we read the diagram from top to bottom\footnote{conventions vary here, some read bottom to top, some left to right}, with the incoming wire being the domain, \(a\), and the outgoing wire the codomain, \(b\):
    \begin{equation}
        f = 
        \tikzsetnextfilename{string-diagram-morphism}
        \begin{tikzpicture}[baseline=(f.base)]
            \node [morphism] (f) {\(f\)};
            \draw [wire] (f) -- ++ (0, 0.75) node [above] {\(a\)};
            \draw [wire] (f) -- ++ (0, -0.75) node [below] {\(b\)};
        \end{tikzpicture}
    \end{equation}
    Composition of morphisms is then denoted by chaining together strings, so if \(f \colon a \to b\) and \(g \colon b \to c\) we write \(g \circ f\) as
    \begin{equation}
        g \circ f = 
        \tikzsetnextfilename{string-diagram-composition}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node [morphism] (f) {\(f\)};
            \node [morphism, below=0.75 cm of f] (g) {\(g\)};
            \draw [wire] (f) -- ++ (0, 0.75) node [above] {\(a\)};
            \draw [wire] (f) -- (g) node [midway, right] {\(b\)};
            \draw [wire] (g) -- ++ (0, -0.75) node [below] {\(c\)};
        \end{tikzpicture}
    \end{equation}
    The monoidal product is written by just placing the two wires next to each other, so \(a \otimes b\) is represented by the monoidal product of the corresponding morphisms \(\id_a \otimes \id_b\):
    \begin{equation}
        \label{eqn:identity on monoidal product}
        \id_a \otimes \id_b =
        \tikzsetnextfilename{string-diagram-monoidal-product}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [wire] (0, 0) node [above] {\(a\)} -- ++ (0, -1) node [below] {\(a\)};
            \draw [wire] (1, 0) node [above] {\(b\)} -- ++ (0, -1) node [below] {\(b\)};
        \end{tikzpicture}
    \end{equation}
    Note that since \(\otimes\) is functorial we have \(\id_a \otimes \id_b = \id_{a \otimes b}\), and so we can interpret this diagram as
    \begin{equation}
        \id_{a \otimes b} =
        \tikzsetnextfilename{string-diagram-monoidal-product-again}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [wire] (0, 0) -- ++ (0, -1);
            \draw [wire] (1, 0) -- ++ (0, -1);
            \draw [decoration={brace}, decorate] (-0.1, 0.1) -- (1.1, 0.1) node [midway, above] {\(a \otimes b\)};
            \draw [decoration={brace, mirror}, decorate] (-0.1, -1.1) -- (1.1, -1.1) node [midway, below] {\(a \otimes b\)};
        \end{tikzpicture}
    \end{equation}
    
    The unit object of the monoidal structure is usually not drawn, since we can always apply an appropriate unitor isomorphism to remove it.
    In fact, none of the structure morphisms of the monoidal structure are drawn.
    Instead these morphisms are implicit whenever they're needed to make the morphism that the diagram represents type check.
    This is basically like not writing brackets and unit objects when we compute products in a group, even though we could (and maybe formally should) in certain places.
    
    If we have a braided monoidal category then we draw the braiding, a morphism \(\sigma_{a,b} \colon a \otimes b \to b \otimes a\), and its inverse, \(\sigma_{a,b}^{-1} \colon b \otimes a \to a \otimes b\), as crossing wires:
    \begin{equation}
        \sigma_{a,b} = 
        \tikzsetnextfilename{string-diagram-braiding}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [wire, rounded corners] (1, 0) node [above] {\(b\)} -- ++ (0, -0.3) -- ++ (-1, -0.4) -- ++ (0, -0.3) node [below] {\(b\)};
            \draw [over wire, rounded corners] (0, 0) node [above] {\(a\)} -- ++ (0, -0.3) -- ++ (1, -0.4) -- ++ (0, -0.3) node [below] {\(a\)};
        \end{tikzpicture}
        \qquad \sigma_{a,b}^{-1} = 
        \tikzsetnextfilename{string-diagram-inverse-braiding}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [wire, rounded corners] (0, 0) node [above] {\(b\)} -- ++ (0, -0.3) -- ++ (1, -0.4) -- ++ (0, -0.3) node [below] {\(b\)};
            \draw [over wire, rounded corners] (1, 0) node [above] {\(a\)} -- ++ (0, -0.3) -- ++ (-1, -0.4) -- ++ (0, -0.3) node [below] {\(a\)};
        \end{tikzpicture}
    \end{equation}
    The fact that in a \emph{braided} monoidal category we don't, in general, have \(\sigma_{b,a} \circ \sigma_{a,b} = \id_{a \otimes b}\) is expressed by the fact that disentangle the following diagram to get \cref{eqn:identity on monoidal product} without passing one wire through the other, note that we assume the end points of wires are fixed, and we don't allow moves like passing over the end of the wires, imagine everything is constrained to a box with the wires attached to opposite sides of the box:
    \begin{equation}
        \sigma_{b,a} \circ \sigma_{a,b} =
        \tikzsetnextfilename{string-diagram-composite-braidings}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [wire, rounded corners] (1, 0) node [above] {\(b\)} -- ++ (0, -0.3) -- ++ (-1, -0.4) -- ++ (0, -0.3) coordinate (b);
            \draw [over wire, rounded corners] (0, 0) node [above] {\(a\)} -- ++ (0, -0.3) -- ++ (1, -0.4) -- ++ (0, -0.3) coordinate (a);
            \draw [wire, rounded corners] (a) -- ++ (0, -0.3) -- ++ (-1, -0.4) -- ++ (0, -0.3) node [below] {\(a\)};
            \draw [over wire, rounded corners] (b) -- ++ (0, -0.3) -- ++ (1, -0.4) -- ++ (0, -0.3) node [below] {\(b\)};
        \end{tikzpicture}
    \end{equation}
    Conversely, we can disentangle the following to get the identity:
    \begin{equation}
        \sigma_{a,b}^{-1} \circ \sigma_{a,b} =
        \tikzsetnextfilename{string-diagram-composite-braiding-and-inverse}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [wire, rounded corners] (1, 0) node [above] {\(b\)} -- ++ (0, -0.3) -- ++ (-1, -0.4) -- ++ (0, -0.3) coordinate (b);
            \draw [over wire, rounded corners] (0, 0) node [above] {\(a\)} -- ++ (0, -0.3) -- ++ (1, -0.4) -- ++ (0, -0.3) coordinate (a);
            \draw [wire, rounded corners] (b) -- ++ (0, -0.3) -- ++ (1, -0.4) -- ++ (0, -0.3) node [below] {\(b\)};
            \draw [over wire, rounded corners] (a) -- ++ (0, -0.3) -- ++ (-1, -0.4) -- ++ (0, -0.3) node [below] {\(a\)};
        \end{tikzpicture}
        =
        \tikzsetnextfilename{string-diagram-composite-braiding-and-inverse-gives-identity}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [wire] (0, 0) node [above] {\(a\)} -- ++ (0, -2) node [below] {\(a\)};
            \draw [wire] (1, 0) node [above] {\(b\)} -- ++ (0, -2) node [below] {\(b\)};
        \end{tikzpicture}
    \end{equation}
    
    For a symmetric monoidal category we do have that \(\sigma_{a,b}^{-1} = \sigma_{b,a}\).
    This is represented in the string diagrams by allowing strings to pass through each other, and because of this we don't need to draw one string passing over the other, so we draw the symmetry as
    \begin{equation}
        \sigma_{a,b} = 
        \tikzsetnextfilename{string-diagram-symmetry}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [wire, rounded corners] (1, 0) node [above] {\(b\)} -- ++ (0, -0.3) -- ++ (-1, -0.4) -- ++ (0, -0.3) node [below] {\(b\)};
            \draw [wire, rounded corners] (0, 0) node [above] {\(a\)} -- ++ (0, -0.3) -- ++ (1, -0.4) -- ++ (0, -0.3) node [below] {\(a\)};
        \end{tikzpicture}
        = \sigma_{b,a}^{-1}.
    \end{equation}
    Then the fact that \(\sigma_{b,a} \circ \sigma_{a,b} = \id_{a \otimes b}\) is expressed by
    \begin{equation}
        \sigma_{b,a} \circ \sigma_{a,b} =
        \tikzsetnextfilename{string-diagram-composite-symmetries}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [wire, rounded corners] (1, 0) node [above] {\(b\)} -- ++ (0, -0.3) -- ++ (-1, -0.4) -- ++ (0, -0.3) coordinate (b);
            \draw [wire, rounded corners] (0, 0) node [above] {\(a\)} -- ++ (0, -0.3) -- ++ (1, -0.4) -- ++ (0, -0.3) coordinate (a);
            \draw [wire, rounded corners] (b) -- ++ (0, -0.3) -- ++ (1, -0.4) -- ++ (0, -0.3) node [below] {\(b\)};
            \draw [wire, rounded corners] (a) -- ++ (0, -0.3) -- ++ (-1, -0.4) -- ++ (0, -0.3) node [below] {\(a\)};
        \end{tikzpicture}
        =
        \tikzsetnextfilename{string-diagram-composite-symmetries-gives-identity}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [wire] (0, 0) node [above] {\(a\)} -- ++ (0, -2) node [below] {\(a\)};
            \draw [wire] (1, 0) node [above] {\(b\)} -- ++ (0, -2) node [below] {\(b\)};
        \end{tikzpicture}
    \end{equation}
    
    Given a monoid object, \((m, \mu, \eta)\), we have morphisms \(\mu \colon m \otimes m \to m\) and \(\eta \colon I \to m\).
    We represent these as
    \begin{equation}
        \mu = 
        \tikzsetnextfilename{string-diagram-monoid-multiplication-mu}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node [morphism, minimum width=1.5cm] (mu) {\(\mu\)};
            \draw [wire] ($(mu.north) + (-0.5, 0)$) -- ++ (0, 0.75);
            \draw [wire] ($(mu.north) + (0.5, 0)$) -- ++ (0, 0.75);
            \draw [wire] (mu.south) -- ++ (0, -0.75);
        \end{tikzpicture}
    \end{equation}
    Note that all the wires here are \(m\), as we can deduce from the domain and codomain of \(\mu\), so we'll stop labelling wires when it's easy to work out what they are.
    The interpretation of this diagram is that two copies of \(m\) enter from the top, then we apply the monoid multiplication to combine them into one.
    It's common to replace the \(\mu\) box with just a dot, just to make the diagrams slightly less busy:
    \begin{equation}
        \mu =
        \tikzsetnextfilename{string-diagram-monoid-multiplication-dot}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [wire] (0, 0) arc (-180:0:0.5);
            \node [monoid dot] at (0.5, -0.5) {};
            \draw [wire] (0.5, -0.5) -- ++ (0, -0.5);
        \end{tikzpicture}
    \end{equation}
    
    The unit map, \(\eta \colon I \to m\) starts at the monoidal unit and ends at \(m\), so we can draw this as a map
    \begin{equation}
        \eta =
        \tikzsetnextfilename{string-diagram-monoid-unit-eta-unit-object}
        \begin{tikzpicture}[baseline=(eta.base)]
            \node [morphism] (eta) {\(\eta\)};
            \draw [wire] (eta) -- ++ (0, -0.75) node [left] {\(m\)};
            \draw [wire, dashed] (eta) -- ++ (0, 0.75) node [left] {\(I\)};
        \end{tikzpicture}
         \, = \,
        \tikzsetnextfilename{string-diagram-monoid-unit-eta-no-unit-object}
        \begin{tikzpicture}[baseline=(eta.base)]
            \node [morphism] (eta) {\(\eta\)};
            \draw [wire] (eta) -- ++ (0, -0.75);
        \end{tikzpicture}
        \, = \,
        \tikzsetnextfilename{string-diagram-monoid-unit-dot}
        \begin{tikzpicture}[baseline=-0.1cm]
            \node [monoid dot] (eta) {};
            \draw [wire] (eta) -- ++ (0, -0.75);
        \end{tikzpicture}
    \end{equation}
    Here we use a dashed line to denote the unit object, or most the time we just don't draw it.
    Again, most of the time we just replace \(\eta\) with a dot as well.
    
    If instead we have a comonoid object, \((c, \Delta, \varepsilon)\), then we simply flip all of the diagrams upside down, this is just what happens to diagrams when we take duals.
    This gives us the comonoid maps, \(\Delta \colon c \to c \otimes c\) and \(\varepsilon \colon c \to I\), expressed as the string diagrams
    \begin{equation}
        \Delta =
        \tikzsetnextfilename{string-diagram-comonoid-comultiplication-delta}
        \begin{tikzpicture}[baseline=(delta.base)]
            \node [morphism, minimum width=1.5cm] (delta) {\(\Delta\)};
            \draw [wire] (delta.north) -- ++ (0, 0.75);
            \draw [wire] ($(delta.south) + (-0.5, 0)$) -- ++ (0, -0.75);
            \draw [wire] ($(delta.south) + (0.5, 0)$) -- ++ (0, -0.75);
        \end{tikzpicture}
        \, = \,
        \tikzsetnextfilename{string-diagram-comonoid-comultiplication-dot}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw (0, 0) arc (180:0:0.5);
            \draw (0.5, 0.5) -- ++ (0, 0.5);
            \node [comonoid dot] at (0.5, 0.5) {};
        \end{tikzpicture}
    \end{equation}
    and
    \begin{equation}
        \varepsilon =
        \tikzsetnextfilename{string-diagram-comonoid-counit-epsilon-unit-object}
        \begin{tikzpicture}[baseline=(epsilon.base)]
            \node [morphism] (epsilon) {\(\varepsilon\)};
            \draw [wire] (epsilon) -- (0, 0.75) node [left] {\(c\)};
            \draw [wire, dashed] (epsilon) -- (0, -0.75) node [left] {\(I\)};
        \end{tikzpicture}
        \, = \,
        \tikzsetnextfilename{string-diagram-comonoid-counit-epsilon-no-unit-object}
        \begin{tikzpicture}[baseline=(epsilon.base)]
            \node [morphism] (epsilon) {\(\varepsilon\)};
            \draw [wire] (epsilon) -- (0, 0.75);
        \end{tikzpicture}
        \, = \,
        \tikzexternaldisable
        \tikzsetnextfilename{string-diagram-comonoid-counit-dot}
        \begin{tikzpicture}[baseline=-0.1cm]
            \coordinate (epsilon) at (0, 0);
            \draw [wire] (epsilon) -- (0, 0.75);
            \node [comonoid dot] at (epsilon) {};
        \end{tikzpicture}
    \end{equation}
    
    \section{Adjunctions and Monads}
    We now (briefly) introduce two important concepts of category theory.
    The first is adjunction, the idea that often functors come in opposing pairs.
    It's usually too restrictive to require that a pair of functors are inverses\footnote{As a general rule imposing equalities on functors is usually not a good idea.}, and instead the natural requirement is that they compose to something that is naturally isomorphic to the identity.
    This gives rise to the notion of an adjunction.
    There are several equivalent definitions of adjunctions.
    The one we give here is perhaps the most standard.
    
    \begin{dfn}{Adjoint Functors}{}
        \index{adjoint functor}\index{adjunction}
        Let \(\cat{C}\) and \(\cat{D}\) be categories.
        Let \(L \colon \cat{D} \to \cat{C}\) and \(R \colon \cat{C} \to \cat{D}\) be functors.
        We say that \(L\) is \defineindex{left adjoint} to \(R\), or \(R\) is \defineindex{right adjoint} to \(L\), or write \(L \leftadjointto R\) (\(R \rightadjointto L\)) if there is a natural isomorphism
        \begin{equation}
            \cat{C}(L-, -) \isomorphic \cat{D}(-, R-).
        \end{equation}
    \end{dfn}
    
    Note that the functors \(\cat{C}(L-, -)\) and \(\cat{D}(-, R-)\) in this definition have domain \(\cat{D} \times \cat{C}\), and codomain\footnote{assuming locally small categories} \(\Set\).
    
    \begin{lma}{}{}
        Left and right adjoints are unique up to unique natural isomorphism.
    \end{lma}
    
    \begin{exm}{}{}
        \begin{itemize}
            \item A \defineindex{forgetful functor} is a functor \(\cat{C} \to \cat{D}\) which \enquote{forgets} some structure in \(\cat{C}\) to produce an object of \(\cat{D}\).
            For example, there is a functor \(U \colon \Grp \to \Set\) that sends a group to its underlying set, or a functor \(U \colon \Vect \to \Ab\) which forgets about scalar multiplication and sends a vector space to its underlying set of vectors equipped with addition as the group operation.
            Forgetful functors tend to have left adjoints, \(\cat{D} \to \cat{C}\), which are then termed \define{free functors}\index{free functor}.
            The idea of these functors is that they produce an object of \(\cat{C}\) from an object of \(\cat{D}\) in the most general way possible.
            For example, the left adjoint to the forgetful functor \(U \colon \Grp \to \Set\) is the free group functor, \(F \colon \Set \to \Grp\) which constructs the free group on a given set.
            Similarly, the forgetful functor \(U \colon \Vect \to \Set\), which sends a vector space to its underlying set of vectors, is right adjoint to the free vector space functor, \(F \colon \Set \to \Vect\), which sends a set to the vector space for which that set is a basis.
            
            Take this last example, the adjunction means we have an isomorphism
            \begin{equation}
                \Vect(F-, -) \isomorphic \Set(-, U-).
            \end{equation}
            Taking a vector space, \(V\), and set, \(X\), this becomes a bijection
            \begin{equation}
                \Vect(FX, V) \isomorphic \Set(X, UV).
            \end{equation}
            Given a linear map \(FX \to V\) we can restrict to the basis \(X\) and this gives us a function \(X \to UV\).
            Conversely, given a function \(X \to UV\) we can extend this by linearity to a linear map \(FX \to V\).
            This is the bijection in question.
            
            \item Consider the functor \(\Pi \colon \Set \times \Set \to \Set\) which sends a pair of sets, \((X, Y)\), to its Cartesian product, \(X \times Y\).
            This is right adjoint to the diagonal functor \(\Delta \colon \Set \to \Set \times \Set\) which sends \(X\) to \((X, X)\).
            The adjunction here corresponds to an isomorphism
            \begin{equation}
                (\Set \times \Set)(\Delta-, -) \isomorphic \Set(-, \Pi-)
            \end{equation}
            which, given a set, \(X\), and a pair of sets, \((Y, Z)\), gives rise to an isomorphism
            \begin{equation}
                (\Set \times \Set)((X, X), (Y, Z)) \isomorphic \Set(X, Y \times Z).
            \end{equation}
            Morphisms in \(\Set\times \Set\) are just pairs of morphisms in \(\Set\), so this is expressing the fact that a pair of function \((f \colon X \to Y, g \colon X \to Z)\) is really the same as a single function \((f \times g) \colon X \to Y \times Z\).
            Given \((f, g)\) we can construct \(f \times g\) by defining \((f \times g)(x) = (f(x), g(x))\), and given \(f \times g\) we can define \(f\) and \(g\) by defining \(f = \pi_1 \circ (f \times g)\) and \(g = \pi_2 \circ (f \times g)\), where \(\pi_i\) is projection onto the \(i\)th factor.
            This adjunction is then related to the fact that \(\times\) is the product in \(\Set\).
            
            \item Consider a pair of rings, \(R\) and \(S\), and some fixed ring homomorphism \(f \colon R \to S\).
            We can view \(S\) as a right \(R\)-module by \(s \action r = sf(s)\).
            We may consider the following three functors between categories of right modules:
            \begin{itemize}
                \item \(f_! \colon \ModR \to \ModR[S]\) given by \(f_!M = M \otimes_R S\) (extension of scalars);
                \item \(f_* \colon \ModR \to \ModR[S]\) given by \(f_*M = \Hom_R(S, M)\);
                \item \(f^* \colon \ModR[S] \to \ModR\) given by \(f^*N = N_R\) where \(N_R\) in which \(n \action r = n \action f(r)\) with the action on the left being that of \(N_R\) and on the right the action of \(N\).
            \end{itemize}
            These three constructions are actually related by a pair of adjunctions, namely, \(f_!\) is left adjoint to restriction of scalars, \(f^*\), and \(f^*\) is left adjoint to \(f_*\).
            
            \item The inclusion \(\Ab \hookrightarrow \Grp\) is left adjoint to the abelianisation functor, \(\Grp \to \Ab\), defined by \(G \mapsto G/[G, G]\).
            
            \item 
        \end{itemize}
    \end{exm}
    
    We will now give the two most common alternative definitions of adjunctions, which we derive here from our definition.
    Given an adjunction, \(L \leftadjointto R\), we have an isomorhpism
    \begin{equation}
        \cat{C}(L-, -) \isomorphic \cat{D}(-, R-).
    \end{equation}
    To make this a bijection of sets we need an object of \(\cat{D} \times \cat{C}\) to apply these functors to.
    Suppose that we have some object \(x \in \cat{D}\), then we can produce an object of \(\cat{C}\) using \(L\), namely \(Lx \in \cat{C}\).
    We can then apply these functors to \((x, Lx) \in \cat{D} \times \cat{C}\), and we get
    \begin{equation}
        \cat{C}(Lx, Lx) \isomorphic \cat{D}(x, RLx).
    \end{equation}
    The nice thing about this is that we know a special element of \(\cat{C}(Lx, Lx)\), namely \(\id_{Lx}\).
    In fact, looking at where identities map is a good way to study hom-functors in general, and is what underpins the Yoneda lemma.
    Let \(\eta_x \colon x \to RLx\) be the image of \(\id_{Lx}\) under this isomorphism.
    We call \(\eta_x\) a \defineindex{universal morphism}.
    
    Similarly, given \(y \in \cat{C}\) we may produce \(Ry \in \cat{D}\) and then we can apply these functors to \((Ry, y) \in \cat{D} \times \cat{C}\), giving the bijection
    \begin{equation}
        \cat{C}(LRy, y) \isomorphic \cat{D}(Ry, Ry).
    \end{equation}
    Let \(\varepsilon_y \colon LRy \to y\) be the image of \(\id_{Ry}\) under this isomorphism.
    
    It turns out that these universal morphisms satisfy the property that for all morhpisms \(f \colon Ly \to x\) there exists a unique morphism \(g \colon y \to Rx\) such that
    \begin{equation}
        \begin{tikzcd}
            Fy \arrow[dr, "f"] \arrow[d, dashed, "Lg"']\\
            LRx \arrow[r, "\varepsilon_x"'] & x
        \end{tikzcd}
    \end{equation}
    commutes and for all morphisms \(g \colon y \to Rx\) there exists a unique morphism \(f \colon Ly \to x\) such that 
    \begin{equation}
        \begin{tikzcd}
            y \arrow[r, "\eta_y"] \arrow[dr, "g"'] & RLy \arrow[d, dashed, "Rf"]\\
            & Rx
        \end{tikzcd}
    \end{equation}
    commutes.
    
    An assignment of such universal morhpisms for all objects of \(\cat{C}\) and \(\cat{D}\) is actually equivalent to defining the adjunction by taking \(\Phi \colon \cat{C}(L-, -) \natTrans \cat{D}(-, R-)\) to be defined by \(\Phi_{y,x}(f) = Gf \circ \eta_y\) and \(\Phi_{y,x}^{-1}(g) = \varepsilon_x \circ Fg\).
    Thus, one may define an adjunction via universal morphisms.
    
    We can take the universal morphisms, \(\eta_x\) and \(\varepsilon_y\), for all \(x \in \cat{D}\) and \(y \in \cat{C}\), and combine them into natural transformations \(\eta \colon \id_{\cat{D}} \natTrans RL\) and \(\varepsilon \colon LR \natTrans \id_{\cat{C}}\).
    These are called the \defineindex{unit} and \defineindex{counit} of the adjunction, and they make the diagrams
    \begin{equation}
        \begin{tikzcd}
            L \arrow[r, Rightarrow, "L\eta"] \arrow[dr, Rightarrow, "\id_L"'] & LRL \arrow[d, Rightarrow, "\varepsilon L"]\\
            & L
        \end{tikzcd}
        \qqand 
        \begin{tikzcd}
            R \arrow[r, Rightarrow, "\eta R"] \arrow[dr, Rightarrow, "\id_R"'] & RLR \arrow[d, Rightarrow, "R\varepsilon"]\\
            & R
        \end{tikzcd}
    \end{equation}
    commute.
    Specifying a unit and counit such that these diagrams commute is the same as specifying an adjunction.
    Thus, one may define an adjunction via its unit and counit.
    
    In components the diagrams above become
    \begin{equation}
        \begin{tikzcd}
            Ly \arrow[r, "L \eta_y"] \arrow[dr, "\id_{Ly}"'] & LRLy \arrow[d, "\varepsilon_{Ly}"]\\
            & Ly
        \end{tikzcd}
        \qqand
        \begin{tikzcd}
            Rx \arrow[r, "\eta_{Rx}"] \arrow[dr, "\id_{Rx}"']& RLRx \arrow[d, "R\varepsilon_x"]\\
            & Rx \mathrlap{.}
        \end{tikzcd}
    \end{equation}
    Applying \(R\) to the left diagram and \(L\) to the right diagram these become
    \begin{equation}
        \begin{tikzcd}
            RLy \arrow[r, "RL \eta_y"] \arrow[dr, "\id_{RLy}"'] & RLRLy \arrow[d, "R\varepsilon_{Ly}"]\\
            & RLy
        \end{tikzcd}
        \qqand
        \begin{tikzcd}
            LRx \arrow[r, "L\eta_{Rx}"] \arrow[dr, "\id_{LRx}"']& LRLRx \arrow[d, "LR\varepsilon_x"]\\
            & LRx \mathrlap{.}
        \end{tikzcd}
    \end{equation}
    Going back to natural transformations these become
    \begin{equation}
        \begin{tikzcd}
            RL \arrow[r, Rightarrow, "RL \eta"] \arrow[dr, Rightarrow, "\id_{RL}"'] & RLRL \arrow[d, Rightarrow, "R\varepsilon L"]\\
            & RL
        \end{tikzcd}
        \qqand
        \begin{tikzcd}
            LR \arrow[r, Rightarrow, "L\eta R"] \arrow[dr, Rightarrow, "\id_{LR}"']& LRLR \arrow[d, "LR\varepsilon"]\\
            & LR \mathrlap{.}
        \end{tikzcd}
    \end{equation}
    These are exactly the diagrams that must commute for \((RL, R\varepsilon L, \eta)\) and \((LR, L\eta R, \varepsilon^{-1})\) into monads!
    So, every adjunction gives rise to a monad, and conversely every monad gives rise to (typically) multiple adjunctions, but we don't have time for that.
    We should at least define a monad though.
    
    \begin{dfn}{Monad}{}
        A monad is a monoid in a category of endofunctors.
    \end{dfn}
    
    Ok, not the most useful definition immediately, here's what it means if we spell it out.
    We have the monoidal category \([\cat{C}, \cat{C}]\), whose objects are endofunctors, \(\cat{C} \to \cat{C}\), and whose morphisms are natural transformations.
    The monoidal product is composition of endofunctors, and the monoidal unit is the identity functor, \(\id_{\cat{C}}\).
    A monad is a triple, \((T, \mu, \eta)\) consisting of an endofunctor, \(T \colon \cat{C} \to \cat{C}\), and natural transformations \(\mu \colon T^2 \natTrans T\) and \(\eta \colon \id_{\cat{C}} \natTrans T\).
    These are, of course, subject to the fact that the relevant diagrams must commute, but we won't go into those details.
    
    \section{Details of \texorpdfstring{\(\Vect\)}{Vect k}}
    Much of our work will take place in the context of \(\Vect\), which is a symmetric monoidal category under the tensor product.
    In this section we'll look at some results specific to \(\Vect\).
    
    First, \(\Vect\) is an abelian category, which for our purposes simply means that it has coincident initial and terminal objects, specifically the zero vector space, \(0\).
    It also has all finite products and coproducts, which again coincide, and are given by the direct sum.
    Another part of the definition of an abelian category is that it is \(\Ab\)-enriched, meaning that the hom sets are actually abelian groups in a way that is compatible with composition.
    For \(\Vect\) this abelian group structure is simply pointwise addition of linear maps.
    In fact, \(\Vect\) is actually a \defineindex{closed}\footnote{The functor \(- \otimes a\) has a right adjoint for all objects \(a\).} symmetric monoidal category, meaning that it's actually enriched over itself, so the functor
    \begin{equation}
        \hom(-, -) = \Hom_{\field}(-, -) = \Vect(-, -)
    \end{equation}
    takes values in \(\Vect\), with scalar multiplication of linear maps given pointwise.
    
    The additive structure of \(\Vect\) is actually relatively simple.
    One way we can see this is that every short exact sequence
    \begin{equation}
        0 \to U \to V \to W \to 0
    \end{equation}
    splits.
    That is, we always have\footnote{More rigorously, we should say that the short exact sequence above is isomorphic to a short exact sequence \(0 \to U \to U \oplus W \to W \to 0\), where \(U \to U \oplus W\) is inclusion and \(U \oplus W \to W\) is projection. This means that not only is \(V \isomorphic U \oplus W\) but that composing this isomorphism with \(U \to V\) gives inclusion and this isomorphism with projection gives \(V \to W\).} \(V \isomorphic U \oplus W\).
    This is not the case in the more general setting of \(\RMod\) where short exact sequences do not always split.
    
    More concretely, given the above short exact sequence of vector spaces we can always take \(W = U^{\perp}\) with respect to some inner product which we can induce from a basis for \(U\) carried through the image and then extended to a basis for \(V\).
    So the fact that not all \(R\)-modules are free comes into play here.
    
    There are some common adjunctions that one may meet when working with vector spaces, we've already mentioned these, but we state them here for completeness.
    
    \begin{lma}{}{}
        There is an adjunction \(- \otimes V \leftadjointto \hom(V, -)\).
    \end{lma}
    
    \begin{lma}{}{}
        There is an adjunction \(F \leftadjointto U\) where \(F \colon \Set \to \Vect\) is the free vector space functor and \(U \colon \Vect \to \Set\) is the forgetful functor.
    \end{lma}
    
    The free vector space may be defined by
    \begin{equation}
        FX = \{\alpha \colon X \to \field \mid \alpha \text{ is finitely supported}\}.
    \end{equation}
    That is, \(\alpha\) is a function \(X \to \field\) such that \(\alpha(x) = 0\) for all but finitely many \(x \in X\).
    The idea is that \(\alpha(x)\) is the coefficient of the basis vector \(x\) in the expansion of \(\alpha\) in the basis \(X\).
    Note that if \(V\) is a vector space with basis \(B\) then we have that \(V \isomorphic FB\), which says that every vector space is free.
    The same is not true in the more general setting of \(\RMod\), in which finding a basis is not always possible.
    
    The dual space is defined by
    \begin{equation}
        V^* = \hom(V, \field).
    \end{equation}
    For finite dimensional vector spaces we have a (non-canonical) isomorphism \(V \isomorphic V^*\), and a canonical isomorphism \(V \isomorphic V^{**}\).
    For infinite dimensional vector spaces we have an inclusion \(V \hookrightarrow V^*\), but this is not an isomorphism.
    
    Note that for a fixed vector space, \(W\), we have the functors
    \begin{equation}
        W \otimes -, \quad - \otimes W, \quad \hom(W, -),
    \end{equation}
    which are all functors \(\Vect \to \Vect\), and the functor \(\hom(-, W) \colon \Vect^{\op} \to \Vect\).
    These are all \defineindex{exact} meaning that they preserve short exact sequences, that is, if
    \begin{equation}
        0 \to V' \to V \to V'' \to 0
    \end{equation}
    is a short exact sequence then \(F \colon \Vect \to \Vect\) is an exact functor if
    \begin{equation}
        0 \to FV' \to FV \to FV'' \to 0
    \end{equation}
    is a short exact sequence, and \(G \colon \Vect^{\op} \to \Vect\) is an exact functor if
    \begin{equation}
        0 \to GV'' \to GV \to GV' \to 0
    \end{equation}
    is a short exact sequence.
    
    
    \chapter{Algebras}
    In this chapter we introduce the notion of an algebra, its dual, and both at the same time.
    This takes us most of the way to a definition of Hopf algebras, which require just one more component, but we leave that for the next chapter.
    
    We will look at algebras over a field, \(\field\), meaning we work with \(\Vect\), but most of this works over an arbitrary commutative ring, \(R\), replacing \(\Vect\) with \(\RMod\).
    
    \section{Algebras}
    Most of the definitions we give in this section should be of familiar objects.
    However, we endeavour to give our definitions in terms of commutative diagrams, since this is what we need in order to dualise these ideas later.
    
    \begin{dfn}{Algebra}{}
        An \defineindex{algebra}, \((A, \mu, \eta)\), is a monoid in \(\Vect\).
    \end{dfn}
    
    This is a rather abstract definition but it's exactly the same definition that one would give when first meeting algebras over a field.
    In particular, \(A\) is a vector space, and we're equipping it with a linear map \(\mu \colon A \otimes A \to A\) which we take to be multiplication, so \(\mu(a \otimes b) = ab\).
    Note that a first course may define multiplication as a bilinear map \(A \times A \to A\), but the universal property of the tensor product means that this is exactly the same as a linear map \(A \otimes A \to A\).
    Our definition also comes with a unit map, \(\eta \colon \field \to A\), which is defined entirely by the value of \(\eta(1)\) and linearity.
    Of course, \(\eta(1)\) is then exactly the unit of the multiplication, which we will also call \(1\) or \(1_A\).
    
    Consider the pentagon diagram in the definition of a monoid.
    Chasing an arbitrary element \((a \otimes b) \otimes c \in (A \otimes A) \otimes A\) around this diagram we get
    \begin{equation}
        \begin{tikzcd}
            & a \otimes (b \otimes c) \arrow[r, mapsto, "\id_A \otimes \mu"] & a \otimes bc \arrow[dd, mapsto, "\mu"]\\
            (a \otimes b) \otimes c \arrow[ur, mapsto, "{\alpha_{A,A,A}}"] \arrow[dr, mapsto, "\mu \otimes \id_A"']\\
            & ab \otimes c \arrow[r, mapsto, "\mu"'] & (ab)c = a(bc).
        \end{tikzcd}
    \end{equation}
    So, this diagram exactly tells us that multiplication is associative.
    Considering the left-hand triangle of the second diagram for \(\alpha \in \field\) and \(a \in A\) we have
    \begin{equation}
        \begin{tikzcd}
            \alpha \otimes a \arrow[r, mapsto, "\eta \otimes \id_A"] \arrow[dr, mapsto, "\lambda_A"'] & \eta(\alpha) \otimes a = \alpha 1_A \otimes a \arrow[d, mapsto, "\mu"]\\
            & \alpha a
        \end{tikzcd}
    \end{equation}
    which commutes because \(\lambda_A(\alpha \otimes a) \coloneqq \alpha a\) where  on the right the multiplication is scalar multiplication in \(A\).
    
    An alternative way to package this same information is that an algebra is a ring, \(A\), equipped with a ring homomorphism \(\eta \colon \field \to A\), whose image is contained in the centre of \(A\).
    That \(\eta(\field) \subseteq Z(A)\) is needed so that \(\eta\) specifies an action of \(\field\) on \(A\) by \(\alpha \action a = \eta(\alpha)a\).
    
    Since \(\field\) is a field the unit, \(\eta \colon \field \to A\), is an \emph{injective} ring homomorphism.
    We will identify the image of \(\eta\) with \(\field\), so that we can view \(\field\) as a subring of \(A\).
    For example, if we work with a matrix algebra then we identify \(\alpha \in \field\) with \(\alpha I_n\) where \(I_n\) is the identity matrix of appropriate size.
    
    \begin{exm}{}{}
        \begin{itemize}
            \item The trivial algebra is \((\field, \mu, \eta)\) with \(\mu\) being the isomorphism \(\field \otimes \field \to \field\) given by \(\mu(\alpha \otimes \beta) = \alpha \beta\), and \(\eta\) being the identity.
            \item Any other associative algebra that you care to name, matrices, \(\reals\), \(\complex\), \(\quaternions\), \(\field[X]\), \(\universalEnveloping(\lie{g})\), and so on.
        \end{itemize}
    \end{exm}
    
    \begin{dfn}{Algebra Homomorphism}{}
        An \defineindex{algebra homomorphism}, \((A, \mu, \eta) \to (A', \mu', \eta')\), is a linear map \(\theta \colon A \to A'\) such that the following diagrams commute:
        \begin{equation}
            \begin{tikzcd}
                A \otimes A \arrow[r, "\theta \otimes \theta"] \arrow[d, "\mu"'] & A' \otimes A' \arrow[d, "\mu'"]\\
                A \arrow[r, "\theta"'] & A'
            \end{tikzcd}
            \qqand
            \begin{tikzcd}[column sep=small]
                & \field \arrow[dl, "\eta"'] \arrow[dr, "\eta'"]\\
                A \arrow[rr, "\theta"'] && A' \mathrlap{.}
            \end{tikzcd}
        \end{equation}
    \end{dfn}
    
    Of course, the first diagram commuting gives
    \begin{equation}
        \begin{tikzcd}
            a \otimes b \arrow[r, mapsto, "\theta \otimes \theta"] \arrow[d, mapsto, "\mu"'] & \theta(a) \otimes \theta(a) \arrow[d, mapsto, "\mu'"]\\
            ab \arrow[r, mapsto, "\theta"'] & \theta(ab) = \theta(a)\theta(b)\mathrlap{,}
        \end{tikzcd}
    \end{equation}
    which is just says that as well as the linear map \(\theta\) preserving the additive structure it also preserves the multiplicative structure, and the second diagram commuting gives
    \begin{equation}
        \begin{tikzcd}[column sep=small]
            & 1 \arrow[dl, mapsto, "\eta"'] \arrow[dr, mapsto, "\eta'"]\\
            1_A \arrow[rr, mapsto, "\theta"'] && \theta(1_A) = 1_{A'} \mathrlap{,}
        \end{tikzcd}
    \end{equation}
    which just says that \(\theta\) preserves the multiplicative unit.
    Thus, an algebra homomorphism is exactly a ring homomorphism that preserves the linear structure.
    
    \begin{dfn}{Ideal}{}
        Let \((A, \mu, \eta)\) be an algebra.
        An \defineindex{ideal}, \(I\), is a subspace of \(A\) such that there is a commutative diagram
        \begin{equation}
            \begin{tikzcd}
                A \otimes I \arrow[r, "\id_A \otimes \incl"] \arrow[d, dashed] & A \otimes A \arrow[d, "\mu"'] & I \otimes A \arrow[l, "\incl \otimes \id_A"] \arrow[d, dashed]\\
                I \arrow[r, "\incl"'] & A & I \arrow[l, "\incl"]
            \end{tikzcd}
        \end{equation}
        where \(\incl \colon I \hookrightarrow A\) is inclusion of the subspace.
    \end{dfn}
    
    The idea is that the two dashed arrows are simply applying the multiplication map to (linear combinations of) elements of the form \(a \otimes i\) (or \(i \otimes a\)) with \(a \in A\) and \(i \in I\), to produce an element of \(I\), which we can then include in \(A\), and this should be the same as just performing the multiplication in \(A\).
    
    Consider the exact sequence of vector spaces
    \begin{equation}
        0 \to \field \to A \to \coker \eta \to 0.
    \end{equation}
    Here \(\coker \eta\) is just \(A / \im \eta \).
    This splits to give \(A \isomorphic \field \oplus \coker \eta = \field \oplus (A / \im \eta)\).
    This makes sense based on dimensions, \(\field\) is one-dimensional, \(A/\im \eta\) is then \((\dim A - 1)\)-dimensional, and so their sum has \(\dim A\) dimensions.
    This splitting however depends on how we identify \(\im \eta\) with elements of \(A\), and there is not, in general, a unique way to do this.
    Essentially, this identification comes down to a basis choice for \(A\) (extending the basis \(\{1_A\}\) for \(\field \subseteq A\) to a full basis of \(A\)).
    
    To get around this non-canonicity we sometimes look at \define{augmented algebras}\index{augmented algebra}.
    These are equipped with an extra homomorphism of \(\field\)-algebras, \(\varepsilon \colon A \to \field\).
    Then \(\ker \varepsilon\) is an ideal of \(A\) and we have orthogonal short exact sequences
    \begin{equation}
        \begin{tikzcd}
            & & 0 \arrow[d]\\
            & & \field \arrow[d, "\eta"'] \arrow[dr, equal]\\
            0 \arrow[r] & \ker \varepsilon \arrow[r] \arrow[dr, dashed] & A \arrow[r, "\varepsilon"'] \arrow[d] & \field \arrow[r] & 0\\
            & & \coker \eta \arrow[d]\\
            & & 0
        \end{tikzcd}
    \end{equation}
    The composite \(\ker \varepsilon \to A \to \coker \eta\) is an isomorphism.
    This gives us the canonical (for a fixed choice of \(\varepsilon\)) decomposition
    \begin{equation}
        A \isomorphic \field \oplus \ker \varepsilon \isomorphic \field \oplus \coker \eta.
    \end{equation}
    
    \begin{dfn}{Opposite Algebra}{}
        Given an algebra \((A, \mu, \eta)\) the \defineindex{opposite algebra} is \((A, \mu \circ \switch, \eta)\).
        We will denote this by \(A^{\op}\).
    \end{dfn}
    
    That is, the opposite algebra has the same underlying vector space, \(A\), but the order of multiplication is swapped.
    Writing \(a^{\op}\) to denote \(a \in A\) viewed as an element of \(A^{\op}\) we have
    \begin{equation}
        a^{\op} b^{\op} = \mu^{\op}(a \otimes b) = \mu \circ \switch (a \otimes b) = \mu(b \otimes a) = ba = (ba)^{\op}.
    \end{equation}
    The identity map \(A \to A^{\op}\) is an algebra homomorphism if and only if \(A\) is commutative.
    There is not always an isomorphism \(A \to A^{\op}\), but sometimes there is.
    
    \begin{exm}{}{}
        Consider the matrix algebra \(\matrices{n}{\field}\).
        Then the transpose provides an isomorphism
        \begin{align}
            (-)^{\trans} \colon \matrices{n}{\field} &\to \matrices{n}{\field}^{\op}
        \end{align}
        since \((X Y)^{\trans} = Y^{\trans} X^{\trans}\).
        
        More generally, if \(A\) is any algebra with specified isomorphism \(\varphi \colon A \to A^{\op}\) then we can define \enquote{conjugate transpose} as an isomorphism
        \begin{equation}
            (-)^{\dagger} \colon \matrices{n}{A} \to \matrices{n}{A^{\op}}^{\op}
        \end{equation}
        by defining \(X^{\dagger} = (\varphi(x_{j,i}))\) when \(X = (x_{i,j})\).
        The most obvious example being that \(A = \complex\) (viewed as an \(\reals\)-algebra) and \(\varphi(z) = \overbar{z}\) is the complex conjugate.
    \end{exm}
    
    \begin{exm}{}{}
        Consider the ring of polynomials, \(\field[X_1, \dotsc, X_n]\).
        This is a commutative \(\field\)-algebra.
        Taking an ideal, \(I \subseteq \field[X_1, \dotsc, X_n]\), we can form the quotient \(\field[X_1, \dotsc, X_n]/I\), which is again a commutative algebra with the multiplication operation defined by
        \begin{equation}
            (f(\vv{X}) + I) (g(\vv{X}) + I) = f(\vv{X})g(\vv{X}) + I.
        \end{equation}
        The quotient map \(\field[X_1, \dotsc, X_n] \to \field[X_1, \dotsc, X_n] / I\) given by \(f(\vv{X}) \mapsto f(\vv{X}) + I\) is an algebra homomorphism.
        
        We can also take the ring of non-commutative polynomials, \(\field\langle X_1, \dotsc, X_n \rangle\).
        This, and its quotients, are \(\field\)-algebras.
        In fact, we can actually take \(I\) to be the ideal of \(\field\langle X_1, \dotsc, X_n\rangle\) generated by all elements of the form \(X_iX_j - X_jX_i\) and then \(\field\langle X_1, \dotsc, X_n\rangle / I \isomorphic \field[X_1, \dotsc, X_n]\).
    \end{exm}
    
    \begin{exm}{Weyl Algebra}{}
        Fix some nonzero \(h \in \field\), then we can form the \defineindex{Weyl algebra}
        \begin{equation}
            W_h \coloneqq \frac{\field\langle X, Y \rangle}{(XY - YX - h)}.
        \end{equation}
        This can be identified with the ring of formal differential operators on \(\field[X]\) by taking \(Y = h\diff{}{X}\).
        One can prove inductively that
        \begin{equation}
            \bracket{Y}{X^n} = h n X^{n-1},
        \end{equation}
        so \(Y\) really does behave like a derivative.
        
        Taking \(\field = \complex\), \(h = i \hbar\), \(X = x\) and \(Y = i\hbar p\) we get the algebra of position and momentum variables from quantum mechanics.
    \end{exm}
    
    Given two algebras, \((A_1, \mu_1, \eta_1)\) and \((A_2, \mu_2, \eta_2)\), their tensor product is the algebra \((A_1 \otimes A_2, \mu, \eta)\) where \(\mu\) is the composite
    \begin{equation}
        A_1 \otimes A_2 \otimes A_1 \otimes A_2 \xrightarrow{\id_{A_1} \circ \switch \circ \id_{A_2}} A_1 \otimes A_1 \otimes A_2 \otimes A_2 \xrightarrow{\mu_1 \otimes \mu_2} A_1 \otimes A_2.
    \end{equation}
    We're being a bit sloppy with the associators here, there should really be brackets and then reassociating happening, but then the composite in question would be
    \begin{align}
        (A_1 \otimes A_2) \otimes (A_1 \otimes A_2) &\xrightarrow{\alpha_{A_1,A_2,A_1\otimes A_2}} A_1 \otimes (A_2 \otimes (A_1 \otimes A_2))\\
        &\xrightarrow{\id_{A_1} \otimes \alpha^{-1}_{A_2,A_1,A_2}} A_1 \otimes ((A_2 \otimes A_1) \otimes A_2)\\
        &\xrightarrow{\id_{A_1} \otimes (\gamma_{A_2,A_1} \otimes \id_{A_2})} A_1 \otimes ((A_1 \otimes A_2) \otimes A_2)\\
        &\xrightarrow{\id_{A_1} \otimes \alpha_{A_1,A_2,A_2}} A_1 \otimes (A_1 \otimes (A_2 \otimes A_2))\\
        &\xrightarrow{\alpha^{-1}_{A_1,A_1,A_2\otimes A_2}} (A_1 \otimes A_1) \otimes (A_2 \otimes A_2)\\
        &\xrightarrow{\mu_1 \otimes \mu_2} A_1 \otimes A_2,
    \end{align}
    so you can see why we don't include associators normally.
    The unit is given by the composite
    \begin{equation}
        \field \xrightarrow{\isomorphic} \field \otimes \field \xrightarrow{\eta_1 \otimes \eta_2} A_1 \otimes A_2.
    \end{equation}
    Note that the choice of isomorphism \(\field \to \field \otimes \field\) doesn't matter, all of these isomorphisms are of the form \(1 \mapsto \alpha \otimes \alpha^{-1}\) for some \(\alpha \in \field^{\times}\), and so
    \begin{multline}
        \eta(1) = (\eta_1 \otimes \eta_2)(\alpha \otimes \alpha^{-1}) = \eta_1(\alpha) \otimes \eta_2(\alpha^{-1})\\
        = \alpha \eta_1(1) \otimes \alpha^{-1}\eta_2(1) = (\alpha \alpha^{-1}) (\eta_1(1) \otimes \eta_2(1)) = \eta_1(1) \otimes \eta_2(1)
    \end{multline}
    which is independent of the choice of \(\alpha\).
    
    All of this is fairly abstract, but it's just saying that given basic tensors \(a_1 \otimes a_2, b_1 \otimes b_2 \in A_1 \otimes A_2\) their product is
    \begin{equation}
        (a_1 \otimes a_2)(b_1 \otimes b_2) = a_1b_1 \otimes a_2b_2.
    \end{equation}
    
    \begin{exm}{}{}
        Given an algebra, \(A\), its \defineindex{enveloping algebra} (not to be confused with a \emph{universal enveloping algebra}) is \(A^{\symrm{e}} \coloneqq A \otimes A^{\op}\), on which the product is
        \begin{equation}
            (a_1 \otimes a_2)(b_1 \otimes b_2) = a_1b_1 \otimes b_2a_1
        \end{equation}
        with all products on the right performed in \(A\).
        This has applications to the study of bimodules over \(A\) and to Hochschild (co)homology.
    \end{exm}
    
    In string diagrams the tensor product has the multiplication
    \begin{equation}
        \tikzexternaldisable
        \tikzsetnextfilename{string-diagram-tensor-product-multiplication}
        \begin{tikzpicture}[baseline=(mu.base)]
            \node [morphism, minimum width=1.5cm] (mu) {\(\mu\)};
            \draw [wire] ($(mu.north) - (0.5, 0)$) -- ++ (0, 0.75) node [above] {\tiny\(A_1 \otimes A_2\)};
            \draw [wire] ($(mu.north) + (0.5, 0)$) -- ++ (0, 0.75) node [above] {\tiny\(A_1 \otimes A_2\)};
            \draw [wire] (mu.south) -- ++ (0, -0.75) node [below] {\tiny\(A_1 \otimes A_2\)};
            \node [right=1cm of mu.base] {\(=\)};
            \draw [wire] (1.8, 0.5) coordinate (start) node [above] {\tiny\(A_1\)} arc (-180:0:0.5) node [above, xshift=0.1em] {\tiny\(A_1\)};
            \draw [wire] ($(start) + (0.7, 0)$) node [above, xshift=-0.1em] {\tiny\(A_2\)} arc (-180:0:0.5) node [above] {\tiny\(A_2\)};
            \node [monoid dot] at ($(start) + (0.5, -0.5)$) {};
            \node [monoid dot] at ($(start) + (1.2, -0.5)$) {};
            \draw [wire] ($(start) + (0.5, -0.5)$) -- ++ (0, -0.5) node [below] {\tiny\(A_1\)};
            \draw [wire] ($(start) + (1.2, -0.5)$) -- ++ (0, -0.5) node [below] {\tiny\(A_2\)};
        \end{tikzpicture}
    \end{equation}
    
    \begin{thm}{}{}
        Algebras and commutative algebras form the symmetric monoidal categories \(\Alg\) and \(\CAlg\) under the tensor product with the trivial algebra as the unit object.
        
        In \(\Alg\) and \(\CAlg\) the Cartesian product is the categorical product.
        
        In \(\CAlg\) the tensor product is the categorical coproduct and \(\field\) is initial.
    \end{thm}
    
    \section{Coalgebras}
    We can dualise many of these ideas to get new structures.
    
    \begin{dfn}{Coalgebra}{}
        A \defineindex{coalgebra}, \((C, \Delta, \varepsilon)\), is a comonoid in \(\Vect\).
    \end{dfn}
    
    \begin{exm}{}{}
        The trivial coalgebra over \(\field\) is \((\field, \Delta, \varepsilon)\) with \(\Delta\) given by one of the isomorphisms \(\field \to \field \otimes \field\) (any choice will give an isomorphic coalgebra), so on basic tensors we can take this to be \(\alpha \mapsto \alpha \otimes 1 = 1 \otimes \alpha\), and we can actually distribute the overall factor of \(\alpha\) across the two elements in the tensor product, so most generically for some fixed \(\beta\) we have \(\beta \alpha \otimes \beta^{-1}\), all of these are equal by linearity of the tensor product.
        The counit in this case is the identity map \(\field \to \field\).
    \end{exm}
    
    \begin{dfn}{Coalgebra Homomorphism}{}
        A \defineindex{coalgebra homomorphism}, \((C, \Delta, \varepsilon) \to (C', \Delta', \varepsilon')\), is a linear map \(\theta \colon C \to C'\) such that the following diagrams commute:
        \begin{equation}
            \begin{tikzcd}
                C \otimes C \arrow[r, "\theta \otimes \theta"] & C' \otimes C'\\
                C \arrow[u, "\Delta"] \arrow[r, "\theta"'] & C' \arrow[u, "\Delta'"']
            \end{tikzcd}
            \qqand
            \begin{tikzcd}[column sep=small]
                & \field\\
                C \arrow[ur, "\varepsilon"] \arrow[rr, "\theta"'] && C' \mathrlap{.} \arrow[ul, "\varepsilon'"']
            \end{tikzcd}
        \end{equation}
    \end{dfn}
    
    \begin{dfn}{Coideal}{}
        Let \((C, \Delta, \varepsilon)\) be a colagebra.
        A \defineindex{coideal}, \(J\), is a subspace of \(A\) if the coproduct restricts to a map \(J \to C \otimes J + J \otimes C\) (where \(+\) is the sum of subspaces, so this is the subset \(\{c \otimes j \mid c \in C, j \in J\} \cup \{j \otimes c \mid j \in J, c \in C\}\) of \(C \otimes C\)) such that \(\varepsilon(J) = \{0\}\) and there is a commutative diagram
        \begin{equation}
            \begin{tikzcd}
                J \arrow[d, "\incl"] \arrow[r] & C \otimes J + J \otimes C \arrow[d, "\incl"]\\
                C \arrow[r, "\Delta"] \arrow[d] & C \otimes C \arrow[d]\\
                C/J \arrow[r, dashed] & C/J \otimes C/J.
            \end{tikzcd}
        \end{equation}
    \end{dfn}
    
    This construction is such that the image of \(\theta\) is a subcoalgebra (a coalgebra with an inclusion homomorphism \(\im \theta \hookrightarrow C'\)) isomorphic to the quotient \(C/\ker \theta\), with the comultiplication of this coalgebra being the map \(\overbar{\Delta} \colon C/J \to C/J \otimes C/J\) making up the bottom of the diagram in the definition.
    
    A \defineindex{coaugmented coalgebra} is a coalgebra equipped with an extra homomorhpism of \(\field\)-coalgebras, \(\eta \colon \field \to C\).
    Then \(\coker \eta\) is a subspace of \(C\) and we have orthogonal short exact sequences
    \begin{equation}
        \begin{tikzcd}
            & & 0\arrow[d]\\
            & & \ker \varepsilon \arrow[d] \arrow[dr, dashed, "\isomorphic"]\\
            0 \arrow[r] & \field \arrow[r, "\eta"] \arrow[dr, equal] & C \arrow[r] \arrow[d, "\varepsilon"] & \coker \eta \arrow[r] & 0\\
            & & \field \arrow[d] \\
            & & 0
        \end{tikzcd}
    \end{equation}
    giving a canonical decomposition of vector spaces
    \begin{equation}
        C \isomorphic \field \oplus \ker \varepsilon \isomorphic \field \oplus \coker \eta.
    \end{equation}
    
    \begin{dfn}{Opposite Coalgebra}{}
        Given a coalgebra \((C, \Delta, \varepsilon)\) the \defineindex{opposite coalgebra} is \((C, \switch \circ \Delta, \varepsilon)\).
        We will denote this by \(C^{\op}\).
    \end{dfn}
    
    The interpretation of the opposite coalgebra is slightly more complicated than the interpretation of the opposite algebra.
    Suppose that \(c_i'\) and \(c_i''\) are such that
    \begin{equation}
        \Delta(c) = \sum_i c_i' \otimes c_i''.
    \end{equation}
    Then in the opposite algebra we have
    \begin{equation}
        \Delta^{\op}(c^{\op}) = \switch(\Delta(c)) = \switch\left( \tsum_i c_i' \otimes c_i'' \right) = \tsum_i c_i'' \otimes c_i'.
    \end{equation}
    
    \begin{ntn}{Sweedler Notation}{}
        Sometimes in coalgebra calculations it is useful to use the notation
        \begin{equation}
            \Delta(c) = \sum c_{(1)} \otimes c_{(2)} = \sum c_1 \otimes c_2.
        \end{equation}
        We'll use the brackets, but not everyone does.
        The idea here is that \(c_{(1)}\) and \(c_{(2)}\) are some elements of \(C\) which run over some set that we are summing over.
        We use the indices \((1)\) and \((2)\) to keep track of the order.
        Note that in general a choice of \(c_{(1)}\) and \(c_{(2)}\) is not unique for a given \(c\), for example, we could always rescale \(c_{(1)}\) by some factor, \(\alpha\), and \(c_{(2)}\) by \(\alpha^{-1}\), but there may be less trivial ways to consider different sets.
        It is also common to drop the summation symbol entirely (cf.\@ the Einstein summation convention).
        The advantage to this notation is that it can make for much faster calculations than the huge commutative diagrams we're forced to use otherwise.
        
        For example, in this notation the coassociativity condition (which is a pentagon diagram) becomes the condition that going one way around the diagram we get
        \begin{align}
            (\Delta \otimes \id_C) \circ \Delta(c) &= (\Delta \otimes \id_C)\left( \tsum c_{(1)} \otimes c_{(2)} \right)\\
            &= \tsum \Delta(c_{(1)}) \otimes \id_C(c_{(2)})\\
            &= \tsum \tsum (c_{(1)})_{(1)} \otimes (c_{(1)})_{(2)} \otimes c_{(2)}
        \end{align}
        and going the other way around we get
        \begin{align}
            (\id_C \otimes \Delta) \circ \Delta(c) &= (\id_C \otimes \Delta) \left( \tsum c_{(1)} \otimes c_{(2)} \right)\\
            &= \tsum \id_C(c_{(1)}) \otimes \Delta(c_{(2)})\\
            &= \tsum c_{(1)} \otimes \tsum (c_{(2)})_{(1)} \otimes (c_{(2)})_{(1)},
        \end{align}
        and we demand that these are equal.
        This is \dots better(?) than commutative diagrams?
        
        The counit conditions become that
        \begin{align}
            \lambda_C \circ (\varepsilon \otimes \id_C) \circ \Delta(c) &= \lambda_C \circ (\varepsilon \otimes \id_C) \left( \tsum c_{(1)} \otimes c_{(2)} \right)\\
            &= \lambda_C \left( \tsum \varepsilon(c_{(1)}) \otimes \id_C(c_{(2)}) \right)\\
            &= \tsum \varepsilon(c_{(1)}) c_{(2)}
        \end{align}
        must be \(c\), and similarly
        \begin{align}
            \rho_C \circ (\id_C \otimes \varepsilon) \circ \Delta(c) &= \rho_C \circ (\id_C \otimes \varepsilon) \left( \tsum c_{(1)} \otimes c_{(2)} \right)\\
            &= \rho_C \left( \tsum \id_C(c_{(1)}) \otimes \varepsilon(c_{(2)}) \right)\\
            &= \tsum c_{(1)} \varepsilon(c_{(2)})\\
            &= \tsum \varepsilon(c_{(2)}) c_{(1)}
        \end{align}
        must also be \(c\).
        In the last line we follow the convention of always writing scalars on the left.
    \end{ntn}
    
    Given two coalgebras, \((C_1, \Delta_1, \varepsilon_1)\) and \((C_2, \Delta_2, \varepsilon_2)\), their tensor product is the coalgebra \((C_1 \otimes C_2, \Delta, \varepsilon)\) where the comultiplication is the composite
    \begin{equation}
        C_1 \otimes C_2 \xrightarrow{\Delta_1 \otimes \Delta_2} C_1 \otimes C_1 \otimes C_2 \otimes C_2 \xrightarrow{\id_{C_1} \otimes \switch \otimes \id_{C_2}} (C_1 \otimes C_2) \otimes (C_1 \otimes C_2)
    \end{equation}
    and the counit is the composite
    \begin{equation}
        C_1 \otimes C_2 \xrightarrow{\varepsilon_1 \otimes \varepsilon_2} \field \otimes \field \xrightarrow{\isomorphic} \field.
    \end{equation}
    So, given \(c \in C_1\) and \(c' \in C_2\) with the comultiplications
    \begin{equation}
        \Delta_1(c) = \tsum c_{(1)} \otimes c_{(2)}, \qand \Delta_2(c') = \tsum c_{(1)}' \otimes c_{(2)}'
    \end{equation}
    the comultiplication on \(c \otimes c'\) is
    \begin{equation}
        \Delta(c \otimes c') = \tsum (c_{(1)} \otimes c'_{(1)}) \otimes (c_{(2)} \otimes c'_{(2)}) \in (C_1 \otimes C_2) \otimes (C_1 \otimes C_2).
    \end{equation}
    
    \begin{thm}{}{}
        Coalgebras and cocommutative coalgebras form the symmetric monoidal categories \(\Coalg\) and \(\CCoalg\) under the tensor product with the trivial coalgebra as the unit object.
        
        In \(\Coalg\) and \(\CCoalg\) the Cartesian product is the categorical coproduct.
        
        In \(\CCoalg\) the tensor product is the categorical product and \(\field\) is terminal.
    \end{thm}
    
    \begin{exm}{Divided Powers Coalgebra}{}
        Consider the vector space \(C = \Span_{\field} \{\gamma_0, \gamma_1, \dotsc\}\).
        This is a cocommutative coalgebra when equipped with the comultiplication
        \begin{equation}
            \Delta(\gamma_n) = \sum_{0 \le i \le n} \gamma_i \otimes \gamma_{n-i}
        \end{equation}
        and the counit
        \begin{equation}
            \varepsilon(\gamma_n) = 
            \begin{cases}
                1 & n = 0;\\
                0 & n \ne 1.
            \end{cases}
        \end{equation}
        This is called the \defineindex{divided powers coalgebra} because if we think of \(\gamma_n\) as representing \(X^n/n!\) then the coproduct comes from the binomial expansion applied to
        \begin{equation}
            X^n \mapsto (X \otimes 1 + 1 \otimes X)^n
        \end{equation}
        which, formally, is
        \begin{equation}
            \sum_{i \le 0 \le n} \frac{n!}{(n - i!)i!} (X \otimes 1)^{i}(1 \otimes X)^{n - i} = \sum_{i \le 0 \le n} \frac{n!}{(n - i)! i!} x^i \otimes x^{n-i}
        \end{equation}
        This coalgebra therefore lets us consider things like Taylor series when the notion of \(1/n!\) is not well defined.
    \end{exm}
    
    \begin{exm}{Trigonometric Coalgebra}{}
        Consider the vector space \(C = \Span_{\field}\{s, c\}\).
        This is a cocommutative coalgebra with the comultiplication given by
        \begin{align}
            \Delta(c) &= s \otimes c + c \otimes s,\\
            \Delta(s) &= c \otimes c - s \otimes s,
        \end{align}
        and the counit given by
        \begin{equation}
            \varepsilon(c) = 1, \qqand \varepsilon(s) = 0.
        \end{equation}
        The idea here is that \(c\) and \(s\) represent \(\cos\) and \(\sin\), then \(\varepsilon\) is evaluation at 0, and \(\Delta\) is evaluation at \(a + b\), using the summation formulae to evaluate at \(a \otimes b\) on the right.
    \end{exm}
    
    \begin{exm}{}{}
        Let \(M\) be a finite monoid (just a normal monoid in \(\Set\)).
        Then the vector space \(\Set(M, \field)\) (with pointwise operations) is a coalgebra under the coproduct
        \begin{equation}
            \begin{alignedat}{3}
                \Delta \colon \Set(M, \field) &\to \Set(M \times M, \field) &&\isomorphic \Set(M, \field) \times \Set(M, \field)\\
                f &\mapsto f \circ \mu &&\mapsto \sum_{x, y \in M} f(xy) \delta_x \otimes \delta_y
            \end{alignedat}
        \end{equation}
        where \(\delta_x \in \Set(M, \field)\) is the Kronecker delta given by
        \begin{equation}
            \delta_x(y) = \delta_{xy} = 
            \begin{cases}
                1 & y = x,\\
                0 & y \ne x.
            \end{cases}
        \end{equation}
    \end{exm}
    
    \section{Dualising Between Algebras and Coalgebras}
    The definition of a coalgebra is dual to that of an algebra.
    We can use this to turn any coalgebra into an algebra.
    Going the other way, turning an algebra into a coalgebra, only works some of the time.
    
    Given a coalgebra, \((C, \Delta, \varepsilon)\), the dual space \(C^* \hom(C, \field)\), can be equipped with structure, \((C^*, \mu, \eta)\) by defining \(\mu\) to be the composite
    \begin{equation}
        C^* \otimes C^* \hookrightarrow (C \otimes C)^* \xrightarrow{\Delta^*} C^*
    \end{equation}
    where \(\Delta^* \colon (C \otimes C)^* \to C^*\) is the dual of \(\Delta\) (that is, the image of the morphism \(\Delta\) under the dual functor \((-)^* \colon \Vect^{\op} \to \Vect\), specifically \(\Delta^*(\omega) = \omega \circ \Delta\) where \(\omega \colon C \otimes C \to \field\) is linear).
    
    We can write this in Sweedler notation, if \(c \in C\) is such that
    \begin{equation}
        \Delta(c) = \sum c_{(1)} \otimes c_{(2)}
    \end{equation}
    and \(\alpha, \beta \in C^*\) then we can define \(\mu(\alpha \otimes \beta)\) to be the map sending \(c\) to
    \begin{equation}
        \mu(\alpha \otimes \beta)(c) = \sum \alpha(c_{(2)}) \beta(c_{(1)}).
    \end{equation}
    Note the swapping of the order here.
    This is because we follow the common algebra convention of identifying
    \begin{equation}
        (V \otimes U)^* \hookrightarrow U^* \otimes V^*,
    \end{equation}
    so \((-)^*\) reverses the order of elements of a tensor product.
    Note that other fields (notably topology related fields) follow the opposite convention.
    Note also that this is always an inclusion, and is an isomorphism only when \(V\) and \(U\) are finite dimensional.
    
    Defining the unit is simpler, we just take \(\eta = \varepsilon^* \colon \field \to C^*\), identifying that \(\field^* \isomorphic \field\) as a linear map \(\field \to \field\) is just multiplication by some constant.
    This definition makes the unit
    \begin{equation}
        \eta(t) = \varepsilon^*(t) = t \varepsilon.
    \end{equation}
    This last line is just the composite of the map that multiplies by \(t\) followed by \(\varepsilon\), and then using linearity we can simplify this to \(t \varepsilon\).
    
    We can demonstrate that the resulting structure is indeed an algebra using some calculations.
    For example, associativity follows by considering \(c \in C\) with \(\Delta(c) = \sum c_{(1)} \otimes c_{(2)}\) and \(\alpha, \beta, \gamma \in C^*\), then
    \begin{align}
        \mu(\mu(\alpha, \beta), \gamma)(c) &= \sum \mu(\alpha, \beta)(c_{(2)}) \gamma(c_{(1)})\\
        &= \sum \alpha((c_{(2)})_{(2)}) \beta((c_{(2)})_{(1)}) \gamma(c_{(1)})\\
        &= \sum \alpha(c_{(2)}) \beta((c_{(1)})_{(2)}) \gamma((c_{(1)})_{(1)})\\
        &= \sum \alpha(c_{(2)}) \mu(\beta, \gamma)(c_{(1)})\\
        &= \mu(\alpha, \mu(\beta, \gamma))(c)
    \end{align}
    where in the middle we've used the coassociativity of the comultiplication.
    Alternatively, this fact can be proven by constructing some large commutative diagram.
    Similarly, one can show that \(\eta\) really is a unit, for example
    \begin{align}
        \mu \circ (\eta \otimes \id_{C^*})(t, \alpha)(c) &= \mu(t\varepsilon, \alpha)(c)\\
        &= \sum t\varepsilon(c_{(2)}) \alpha(c_{(1)})\\
        &= t \alpha\left( \tsum \varepsilon(c_{(2)}) c_{(1)} \right)\\
        &= t \alpha(c)\\
        &= \lambda_{C^*}(t\varepsilon \otimes \alpha)(c)
    \end{align}
    where we've used the fact that \(t \varepsilon(c_{(2)})\) is a scalar and so we can use linearity to bring it, and the sum, inside \(\alpha\), then we've used the counit law, and finally rewritten things with a unitor.
    This then expresses the fact that the left unit law holds.
    
    Finally, note that if \(C\) is cocommutative then \(C^*\) will be commutative:
    \begin{align}
        \mu(\alpha, \beta)(c) &= \sum \alpha(c_{(2)}) \beta(c_{(1)})\\
        &= \sum \alpha(c_{(1)}) \beta(c_{(2)})\\
        &= \sum \beta(c_{(2)}) \alpha(c_{(1)})\\
        &= \mu(\beta, \alpha)(c)
    \end{align}
    where cocommutativity is what allows us to swap the \(1\) and \(2\) indices going to the second line.
    
    \begin{prp}{}{}
        Given a coalgebra, \((C, \Delta, \varepsilon)\), there is a dual algebra, \((C^*, \mu, \eta)\) with maps defined as above.
        Further, \(C^*\) is commutative if and only if \(C\) is cocommutative.
        
        Further, this construction gives a functor \(\Coalg^{\op} \to \Alg\), meaning any coalgebra homomorphism \(\theta \colon (C, \Delta, \varepsilon) \to (C', \Delta', \varepsilon')\) induces an algebra homomorphism \(\theta^* \colon (C^{\prime *}, \mu', \varepsilon') \to (C^*, \mu, \varepsilon)\).
    \end{prp}
    
    If \((A, \mu, \eta)\) is a finite dimensional algebra then we can dualise it similarly to get a coalgebra.
    However, if \(A\) is infinite dimensional then this doesn't work.
    Fortunately, there are a couple of ways to fix this.
    One involves topological vector spaces and completing things.
    The other, simpler, fix is to use the restricted dual instead.
    
    \begin{dfn}{Restricted Dual}{}
        Let \(V\) be a vector space over \(\field\).
        The \defineindex{restricted dual}, \(V^{\circ}\), is the vector space
        \begin{equation}
            V^{\circ} \coloneqq \{\alpha \in V^* \mid \text{ there exists some cofinite subspace} U \subseteq V \text{ such that } U \subseteq \ker \alpha\}.
        \end{equation}
        Let \(V\) be an algebra over \(\field\).
        The \defineindex{restricted dual}, \(A^{\circ}\), is the vector space
        \begin{equation}
            A^{\circ} \coloneqq \{\alpha \in A^* \mid \text{ there exists some cofinite ideal} I \subseteq A \text{ such that } I \subseteq \ker \alpha\}.
        \end{equation}
    \end{dfn}
    
    The idea is that \(\alpha \in A^*\) is in \(A^{\circ}\) if and only if it factors through a finite dimensional quotient algebra, \(A/I\).
    
    % Appdendix
    %	\appendixpage
    %	\begin{appendices}
    %	
    %	\end{appendices}
    
    \backmatter
    \renewcommand{\glossaryname}{Acronyms}
    \printglossary[acronym]
    \printindex
\end{document}