% !TeX program = lualatex
\documentclass[fleqn]{NotesClass}

\strictpagecheck

\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{ytableau}
\usepackage{siunitx}
\usepackage{subcaption}

\let\oldwidehat=\widehat
\AtBeginDocument{\let\widehat=\oldwidehat}

\usepackage{tikz}
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]

\usepackage{tikz-cd}
\AtBeginEnvironment{tikzcd}{\tikzexternaldisable}
\AtEndEnvironment{tikzcd}{\tikzexternalenable}

\usetikzlibrary{arrows.meta}
\usetikzlibrary{braids}
\usetikzlibrary{hobby}
\usetikzlibrary{calc}

\usepackage[pdfauthor={Willoughby Seago},pdftitle={Notes from Representation Theory Course},pdfkeywords={representation theory},pdfsubject={Representation Theory}]{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{NotesBoxes}
\usepackage{NotesMaths2}

\setmathfont[range={\int, \oint, \otimes, \oplus, \bigotimes, \bigoplus}]{Latin Modern Math}


% Highlight colour
%\definecolor{highlight}{HTML}{710D78}
%\definecolor{my blue}{HTML}{2A0D77}
%\definecolor{my red}{HTML}{770D38}
%\definecolor{my green}{HTML}{14770D}
%\definecolor{my yellow}{HTML}{E7BB41}

% Title page info
\title{Representation Theory}
\author{Willoughby Seago}
\date{January 13th, 2024}
\subtitle{Notes from}
\subsubtitle{University of Glasgow}
\renewcommand{\abstracttext}{These are my notes from the SMSTC course \emph{Lie Theory} taught by Prof Christian Korff. These notes were last updated at \printtime{} on \today{}.}

% Commands
% Maths
\renewcommand{\field}{\symbb{k}}
\newcommand{\id}{\symrm{id}}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Hom}{Hom}
\newcommand{\action}{\mathbin{.}}
\newcommand{\op}{\symrm{op}}
\makeatletter
\newcommand{\c@egory}[1]{\symsfup{#1}}
\newcommand{\Vect}[1][\field]{#1\text{-}\c@egory{Vect}}
\newcommand{\AMod}[1][A]{#1\text{-}\c@egory{Mod}}
\newcommand{\ModA}[1][A]{\c@egory{Mod}\text{-}#1}
\newcommand{\Mod}[1]{#1\text{-}\c@egory{Mod}}
\newcommand{\Ab}{\c@egory{Ab}}
\newcommand{\Rep}{\c@egory{Rep}}
\newcommand{\Set}{\c@egory{Set}}
\newcommand{\Alg}[1][\field]{{#1}\text{-}\c@egory{Alg}}
\newcommand{\Lie}[1][\field]{{#1}\text{-}\c@egory{Lie}}
\makeatother
\DeclareMathOperator{\im}{im}
\newcommand{\isomorphic}{\cong}
\DeclareMathOperator{\Span}{span}
\ExplSyntaxOn
% Create LaTeX interface command
\NewDocumentCommand{\cycle}{ O{\,} m }{  % optional arg is separator, mandatory
    %arg is comma separated list
    (
    \willoughby_cycle:nn { #1 } { #2 }
    )
}

\clist_new:N \l_willougbhy_cycle_clist  % Create new clist variable
\cs_new_protected:Npn \willoughby_cycle:nn #1 #2 {  % create LaTeX3 function
    \clist_set:Nn \l_willougbhy_cycle_clist { #2 }  % set clist variable with
    %clist #2 passed by user
    \clist_use:Nn \l_willougbhy_cycle_clist { #1 }  % print list separated by #1
}
\ExplSyntaxOff
\newcommand{\universalEnveloping}{\symcal{U}}
\DeclarePairedDelimiterX{\bracket}[2]{[}{]}{#1, #2}
\DeclareMathOperator{\Mat}{Mat}
\RenewDocumentCommand{\matrices}{s o m m}{
    \IfBooleanTF{#1}{
        \IfNoValueTF{#2}{
            \Mat_{#3}\left( #4 \right)
        }{
            \Mat_{#2 \times #3}\left( #4 \right)
        }
    }{
        \IfNoValueTF{#2}{
            \Mat_{#3}(#4)
        }{
            \Mat_{#2 \times #3}(#4)
        }
    }
}
\newcommand{\trans}{\top}
\DeclareMathOperator{\Rad}{Rad}
\DeclareMathOperator{\Irr}{Irr}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Char}{char}
\newcommand{\classFunctions}{\symcal{X}}
\newcommand{\conjugacyClasses}{\symcal{C}}
\DeclareMathOperator{\Func}{Func}
\newcommand{\partition}{\vdash}
\DeclarePairedDelimiterX{\innerprod}[2]{\langle}{\rangle}{#1, #2}
\DeclareMathOperator{\frobeniusSchur}{FS}
\DeclareMathOperator{\standardYoungTableaux}{SYT}
\DeclareMathOperator{\semistandardYoungTableaux}{SSYT}
\newcommand{\normalsub}{\mathrel{\lhd}}
\newcommand{\algNumbers}{\overline{\rationals}}
\newcommand{\algIntegers}{\overline{\integers}}
\newcommand{\Res}{\symrm{Res}}
\newcommand{\Ind}{\symrm{Ind}}
\newcommand{\one}{\symbb{1}}
\newcommand{\rowGroup}{R}
\newcommand{\columnGroup}{C}
\newcommand{\intterobang}{\mathchoice{!\mkern-6.1mu?}{!\mkern-6.2mu?}{!\mkern-6.8mu?}{!\mkern-6.8mu?}}
\DeclareMathOperator{\proj}{proj}
\newcommand{\ch}{\symrm{ch}}
\newcommand{\Gr}{\symrm{Gr}}
\renewcommand{\dd}{\,\symrm{d}}
\newcommand{\ad}{\symrm{ad}}
\DeclareMathOperator{\gr}{gr}
\DeclarePairedDelimiterX{\rootProd}[2]{(}{)}{#1 , #2}
\newcommand{\dynkin}[2]{\symrm{#1}_{#2}}
\newcommand{\purebraid}{\symcal{PB}}
\newcommand{\braid}{\symcal{B}}
\DeclareMathOperator{\Homeo}{Homeo}
\newcommand{\universalRmatrix}{\symcal{R}}
\newcommand{\temperleyLieb}{\symrm{TL}}

%\includeonly{parts/algebra-reps, parts/group-reps, parts/Sn-reps}

\begin{document}
    \frontmatter
    \titlepage
    \innertitlepage{tikz-external/dynkin-E8}
    \tableofcontents
    % \listoffigures
    \mainmatter
    \include{parts/algebra-reps}
    \include{parts/group-reps}
    \include{parts/Sn-reps}
    
    \part{Other Topics in Representation Theory}
    \chapter{Lie Algebras}
    In this section we give a rapid, relatively proof free, tour of the representation theory of Lie algebras.
    We refer the reader to other sources for details, such as my lecture notes \url{https://github.com/WilloughbySeago/phd-courses-notes/tree/main/lie-theory}.
    
    \section{Lie Algebras}
    \begin{dfn}{Lie Algebra}{}
        A \defineindex{Lie algebra}, \(\lie{g}\), is a \(\field\)-vector space equipped with a linear map \(\lie{g} \otimes \lie{g} \to \lie{g}\) called the \defineindex{Lie bracket} subject to the following:
        \begin{itemize}
            \item \define{alternativity}\index{alternating}: \(\bracket{x}{x} = 0\) for all \(x \in \lie{g}\);
            \item \defineindex{Jacobi identity}: \(\bracket{x}{\bracket{y}{z}} + \bracket{y}{\bracket{z}{x}} + \bracket{z}{\bracket{x}{y}} = 0\) for all \(x, y, z \in \lie{g}\).
        \end{itemize}
    \end{dfn}
    
    Note that more commonly the definition is given as a bilinear map \(\lie{g} \times \lie{g} \to \lie{g}\).
    The universal property of the tensor product means that these are equivalent.
    For fields of characteristic other than 2 the first relation is usually replaced with antisymmetry, \(\bracket{x}{y} = -\bracket{y}{x}\) for all \(x, y \in \lie{g}\).
    With our definition using the tensor product we can pass to the quotient \(\Lambda^2\lie{g}\) and we see that \(\bracket{-}{-}\) induces a map \(\bracket{-}{-} \colon \Lambda^2\lie{g} \to \lie{g}\) which trivially is such that \(\bracket{x}{x} = 0\) since \(x \otimes x\) maps to zero in \(\Lambda^2 \lie{g}\).
    
    \begin{dfn}{}{}
        Let \(\lie{g}\) and \(\lie{g}'\) be Lie algebras over the same field, \(\field\).
        A morphism of Lie algebras, \(\varphi \colon \lie{g} \to \lie{g}'\) is a linear map which preserves the Lie bracket, that is
        \begin{equation}
            \varphi(\bracket{x}{y}) = \bracket{\varphi(x)}{\varphi(y)}
        \end{equation}
        where the bracket on the left is that of \(\lie{g}\) and on the right it's that of \(\lie{g}'\).
    \end{dfn}
    
    \begin{exm}{}{}
        \begin{itemize}
            \item Let \(A\) be an associative algebra, then we can make this into a Lie algebra by defining the bracket \(\bracket{a}{b} = ab - ba\).
            A special case of this is \(A = \End V\) for some vector space, \(V\).
            Then we call the corresponding Lie algebra \(\generalLinearLie(V)\), or if \(\dim V = n\) we call it \(\generalLinearLie_n\) (note that as vector spaces \(\generalLinearLie(V)\) is exactly \(A = \End V\), the name change just reflects a shifting view point from associative algebras to Lie algebras).
            \item Any vector space, \(V\), can be made into a Lie algebra by defining \(\bracket{x}{y} = 0\) for all \(x, y \in V\).
            Such a Lie algebra is called \define{abelian}\index{abelian Lie algebra}.
            The idea is that the commutator vanishing means that multiplication is commutative, an idea that only makes sense if \(\bracket{-}{-}\) really is the commutator, like in the previous example.
        \end{itemize}
    \end{exm}
     
    \begin{dfn}{Lie Subalgebra}{}
        Let \(\lie{g}\) be a Lie algebra over \(\field\).
        A Lie subalgebra, \(\lie{h}\), is a Lie algebra over \(\field\) equipped with an injective Lie algebra morphism \(\lie{h} \hookrightarrow \lie{g}\).
    \end{dfn}
    
    An almost identical definition is that a Lie subalgebra is a subspace, \(\lie{h} \subseteq \lie{g}\) such that \(\lie{h}\) is a Lie algebra in its own right (with the same bracket as \(\lie{g}\)).
    One can then show that this is true so long as the \(\lie{h}\) is closed under the Lie bracket.
    That is, \(\bracket{\lie{h}}{\lie{h}}\) is a subset of \(\lie{h}\).
    Note that in general if \(U\) and \(V\) are subspaces of \(\lie{g}\) then \(\bracket{U}{V}\) is defined to be the span of all \(\bracket{u}{v}\) with \(u \in U\) and \(v \in V\).
    Similarly, if \(x \in \lie{g}\) then \(\bracket{x}{U}\) is the span of all \(\bracket{x}{y}\) with \(y \in \lie{g}\).
    
    The only subtle difference between these two definitions is that the existence of a monomorphism \(\lie{h} \hookrightarrow \lie{g}\) only implies that \(\lie{h}\) is isomorphic to a subalgebra of \(\lie{g}\) with the second definition, but we'll only consider things up to isomorphism most the time so this is really the definition we want.
    
    \begin{exm}{}{}
        \begin{itemize}
            \item Let \(\lie{g}\) be any Lie algebra.
            Any one-dimensional subspace, \(\lie{l}\), is an abelian subalgebra, since if \(l, l' \in \lie{l}\) then \(l = \lambda l'\) for some \(\lambda \in \field\), and so \(\bracket{l}{l'} = \bracket{kl'}{l'} = k\bracket{l'}{l'} = 0\) and \(0 \in \lie{l}\).
            \item The \define{centre}\index{centre!of a Lie algebra} of a Lie algebra, \(\lie{g}\), is the abelian subalgebra
            \begin{equation}
                \lie{z}(\lie{g}) \coloneq \{x \in \lie{g} \mid [x, \lie{g}] = 0\} \subseteq \lie{g}.
            \end{equation}
            \item For \(V\) a finite-dimensional vector space of dimension \(n\) we know that \(\generalLinearLie_n = \End V\) is a Lie algebra.
            Fixing a basis the elements of \(\generalLinearLie_n\) are just all \(n \times n\) matrices with entries in \(\field\).
            There is a subalgebra, \(\specialLinearLie_n \subset \generalLinearLie_n\), consisting of only the matrices with zero trace.
            This follows because we have
            \begin{equation}
                \tr(\bracket{x}{y}) = \tr(xy) - \tr(yx) = 0.
            \end{equation}
            This holds for all \(x, y \in \generalLinearLie_n\), not just for the traceless case, and so this turns out to be a special case of another construction, called the derived subalgebra, \(\lie{g}' \coloneq \bracket{\lie{g}}{\lie{g}}\).
        \end{itemize}
    \end{exm}
    
    \begin{dfn}{Ideal}{}
        Let \(\lie{g}\) be a Lie algebra.
        A Lie subalgebra, \(\lie{i} \subseteq \lie{g}\), is an \define{ideal}\index{ideal!of a Lie algebra} if \(\bracket{\lie{i}}{\lie{g}} \subseteq \lie{i}\).
    \end{dfn}
    
    Compare this to the definition of a subalgebra, which only requires that \(\bracket{\lie{i}}{\mathcolor{highlight}{\lie{i}}} \subseteq \lie{i}\).
    Compare this also to the notion of an ideal, \(I\), of a ring, \(R\), which is a subgroup of the additive group such that \(IR \subseteq I\).
    
    The idea is that ideals are to Lie algebras as ideals are to rings, or as normal subgroups are to groups.
    In particular, we have a correspondence between ideals, \(\lie{i} \subseteq g\) and Lie algebra morphisms, \(\varphi \colon \lie{g} \to \lie{h}\) given by \(\lie{i} \leftrightarrow \ker \varphi\) (where the kernel is defined as it is for any linear map).
    We also have that \(\lie{g}/\lie{i}\) is a well defined quotient and a Lie algebra.
    Note that the quotient of any vector space by a subspace is again a vector space, but it's only a Lie algebra again if we quotient by an ideal.
    The bracket of this quotient is defined by \(\bracket{x + \lie{i}}{y + \lie{i}} = \bracket{x}{y} + \lie{i}\).
    
    \begin{dfn}{Derived Subalgebra}{}
        Let \(\lie{g}\) be a Lie algebra, then \(\lie{g}' = \bracket{\lie{g}}{\lie{g}}\) is the \defineindex{derived subalgebra}.
    \end{dfn}
    
    \begin{dfn}{Solvable Lie Algebra}{}
        A Lie algebra, \(\lie{g}\), is solvable if the series
        \begin{equation}
            \lie{g} \supseteq \lie{g}' \supseteq \lie{g}'' \supseteq \dotsb
        \end{equation}
        terminates.
    \end{dfn}
    
    \begin{dfn}{Nilpotent Lie ALgebra}{}
        A Lie algebra, \(\lie{g}\), is solvable if the series
        \begin{equation}
            \lie{g} \supseteq \bracket{\lie{g}}{\lie{g}} \supseteq \bracket{\lie{g}}{\bracket{\lie{g}}{\lie{g}}} \supseteq \dotsb
        \end{equation}
        terminates.
    \end{dfn}
    
    The difference between these two is subtle, one nests brackets on both sides, and the other only on the other side.
    More concretely, the upper triangular matrices form a solvable subalgebra of \(\generalLinearLie_n\) (in fact, this is a maximal solvable subalgebra, also known as a \defineindex{Borel subalgebra}), and the \emph{strictly} upper triangular matrices form a (maximal) nilpotent subalgebra of \(\generalLinearLie_n\).
    
    \begin{dfn}{}{}
        The maximal solvable \emph{ideal} of \(\lie{g}\) is called its \define{radical}\index{radical!of a Lie algebra}, \(\Rad \lie{g}\).
    \end{dfn}
    
    \begin{dfn}{}{}
        A Lie algebra, \(\lie{g}\), is \define{semisimple}\index{semisimple!Lie algebra} if \(\Rad \lie{g} = 0\), that is, if \(\lie{g}\) has no proper solvable ideals.
        Similarly, \(\lie{g}\) is \define{simple}\index{simple!Lie algebra} if it has no proper ideals (solvable or not).
    \end{dfn}
    
    \begin{dfn}{Linear Lie Algebra}{}
        A \defineindex{linear Lie algebra} is any Lie algebra which is isomorphic to a Lie subalgebra of some \(\generalLinearLie(V)\) for \(V\) a finite-dimensional vector space.
    \end{dfn}
    
    Ado's theorem tells us that (over a field of characteristic zero) every finite-dimensional Lie algebra is linear.
    
    \begin{thm}{Ado's Theorem}{}
        Let \(\lie{g}\) be a finite-dimensional Lie algebra over a field of characteristic zero.
        Then \(\lie{g}\) admits a faithful representation \(\lie{g} \hookrightarrow \generalLinearLie(V)\) for some finite-dimensional vector space, \(V\).
        Further, one can choose this representation such that the maximal nilpotent ideal, \(\lie{n} \subseteq \lie{g}\) acts nilpotently on \(V\).
    \end{thm}
    
    There are some special linear Lie algebras.
    Over \(\complex\) these are
    \begin{itemize}
        \item \(\generalLinearLie_n = \{x \in \Mat_n(\complex\}\) (real dimension \(2n^2\))
        \item \(\specialLinearLie_n = \{x \in \Mat_n(\complex) \mid \tr x = 0\}\) (real dimension \(2(n^2 - 1)\));
        \item \(\specialOrthogonalLie_n = \{x \in \Mat_n(\complex) \mid x^{\trans} + x^{\trans} = 0\}\) (real dimension \(n(n - 1)\));
        \item \(\symplecticLie_{2n} = \{x \in \Mat_{2n}(\complex) \mid Jx + x^{\trans}J = 0\}\) where \(J = \begin{pmatrix} 0 & -I_n\\ I_n & 0 \end{pmatrix}\) with \(I_n \in \Mat_n(\complex)\) the identity matrix (real dimension \(2 \binom{2n + 1}{2}\)).
    \end{itemize}
    Over \(\reals\) these are
    \begin{itemize}
        \item \(\generalLinearLie_n = \{x \in \Mat_n(\reals)\}\) (real dimension \(n^2\));
        \item \(\specialOrthogonalLie_n = \{x \in \Mat_n(\reals) \mid \tr x = 0\}\) (real dimension \(n^2 - 1\));
        \item \(\unitaryLie_n = \{x \in \Mat_n(\complex) \mid x + x^* = 0\}\) (real dimension \(n^2\));
        \item \(\specialUnitaryLie_n = \{x \in \Mat_n(\complex) \mid x + x^* = 0 \text{ and } \tr x = 0\}\) (real dimension \(n^2 - 1\));
        \item \(\symplecticLie_{2n} = \{x \in \Mat_n(\quaternions) \mid x + x^* = 0\}\) (real dimension \(2n^2 + n\)).
    \end{itemize}
    
    \section{Representation Theory of Lie Algebras}
    \begin{dfn}{Representation}{}
        A \define{representation}\index{representation!of a Lie algebra}, \(\lie{g}\) (over \(\field\)), is a \(\field\)-vector space, \(V\), equipped with a Lie algebra morphism
        \begin{equation}
            \rho \colon \lie{g} \to \generalLinearLie(V).
        \end{equation}
        
        Equivalently, a \define{\(\lie{g}\)-module}, \(V\), is a vector space equipped with a (left) Lie algebra action of \(\lie{g}\), that is, a map \(\lie{g} \times V \to V\), \((x, v) \mapsto x \action v\) subject to the following:
        \begin{itemize}
            \item Linearity in the first argument: \((\alpha x + \beta y) \action v = \alpha (x \action v) + \beta (y \action v)\) for all \(\alpha, \beta \in \field\), \(x, y \in \lie{g}\) and \(v \in V\);
            \item Linearity in the second argument: \(x \action (\alpha v + \beta w) = \alpha(x \action v) + \beta(x \action w)\) for all \(\alpha, \beta \in \field\), \(x \in \lie{g}\) and \(v, w \in V\);
            \item Respects the bracket: \(\bracket{x}{y} \action v = x \action (y \action v) - y \action (x \action v)\) for all \(x, y \in \lie{g}\) and \(v \in V\).
        \end{itemize}
    \end{dfn}
    
    As with groups and associative algebras the \(\lie{g}\)-module and representation of \(\lie{g}\) carry exactly the same information, and as such which we use is a matter of preference.
    
    \begin{dfn}{Adjoint Representation}{}
        Every Lie algebra, \(\lie{g}\), is a \(\lie{g}\)-module in a canonical way, known as the \defineindex{adjoint representation}
        \begin{equation}
            \begin{aligned}
                \ad \colon \lie{g} &\to \generalLinearLie(\lie{g})\\
                x &\mapsto \ad_x
            \end{aligned}
        \end{equation}
        where \(\ad_x \colon \lie{g} \to \lie{g}\) is defined by \(\ad_x(y) = \bracket{x}{y}\) for all \(x, y \in \lie{g}\).
    \end{dfn}
    
    For the adjoint representation to be a representation we need \(\ad\) to be a Lie algebra morphism.
    That is, we need to have \(\ad_{\bracket{x}{y}} = \bracket{\ad_x}{\ad_y}\) for \(x, y \in \lie{g}\).
    It turns out that this is true precisely because the this statement, upon applying both sides of the above to \(z \in \lie{g}\), expands to the Jacobi identity:
    \begin{align}
        \ad_{\bracket{x}{y}}(z) &= \bracket{\bracket{x}{y}}{z}\\
        \bracket{\ad_x}{\ad_y}(z) = (\ad_x \circ \ad_y - \ad_y \circ \ad_x)(z) = \bracket{x}{\bracket{y}{z}} - \bracket{y}{\bracket{x}{z}}.
    \end{align}
    Equality between the two lines above is, after applying the antisymmetry property, exactly the Jacobi identity.
    
    \begin{dfn}{}{}
        Given \(\lie{g}\)-modules \(V\) and \(W\) we can define
        \begin{itemize}
            \item the \define{direct sum}\index{direct sum!of Lie algebra representations}, \(V \oplus W\), which has the action \(x \action (v + w) = x \action v + x \action w\);
            \item the \define{tensor product}\index{tensor product!of Lie algebra representations}, \(V \otimes W\), which has the action \(x \action (v \otimes w) = (x \action v) \otimes w + v \otimes (x \action w)\);
            \item the \define{dual representation}\index{dual representation!of a Lie algebra representation}, \(V^*\), which has the action \(\rho_{V^*}(x) = -\rho_V(x)^*\)
        \end{itemize}
        all for \(x \in \lie{g}\), \(v \in V\), and \(w \in W\).
    \end{dfn}
    
    \section{Universal Enveloping Algebra}
    \begin{dfn}{Universal Enveloping Algebra}{}
        Let \(\lie{g}\) be a Lie algebra.
        An enveloping algebra, \((E, i)\), is an associative unital algebra, \(E\), and an inclusion of vector spaces \(i \colon \lie{g} \hookrightarrow E\) such that
        \begin{equation}
            i(\bracket{x}{y}) = i(x)i(y) - i(y)i(x).
        \end{equation}
        The \defineindex{universal enveloping algebra} is the\footnote{turns out that the universal enveloping algebra both exists, and is unique up to unique isomorphism} enveloping algebra \((U(\lie{g}), \iota)\) such that for any other enveloping algebra, \((E, i)\), there is a unique morphism of associative unital algebras, \(\varphi \colon U(\lie{g}) \to E\) such that \(i = \varphi \circ \iota\).
    \end{dfn}
    
    The definition is a bit terse, the idea is that \(U(\lie{g})\) (dropping \(\iota\) from the notation) is the smallest associative unital algebra containing \(\lie{g}\) in such a way that the bracket of \(\lie{g}\) in \(U(\lie{g})\) really is just the commutator.
    For example, the universal enveloping algebra of \(\generalLinearLie(V)\) is simply \(\End(V)\), which is just \(\generalLinearLie(V)\) but viewed as an associative algebra.
    
    \begin{thm}{}{}
        The universal enveloping algebra exists.
        An explicit construction is as follows.
        Let \(U(\lie{g}) = T(\lie{g})/I\), where \(I\) is the ideal of the tensor algebra, \(T(\lie{g})\), generated by elements of the form
        \begin{equation}
            \bracket{x}{y} - x \otimes y + y \otimes x
        \end{equation}
        for \(x, y \in \lie{g}\).
    \end{thm}
    
    The universal property of the universal enveloping algebra can be characterised as the statement that there is an isomorphism
    \begin{equation}
        \Hom_{\Lie}(\lie{g}, L(A)) \isomorphic \Hom_{\Alg}(U(\lie{g}), A)
    \end{equation}
    where
    \begin{itemize}
        \item \(\Lie\) is the category of Lie algebras and Lie algebra homomorphisms;
        \item \(\lie{g}\) is a Lie algebra
        \item \(A\) is an unital associative algebra;
        \item \(L(A)\) is the Lie algebra given by equipping \(A\) with the commutator;
        \item \(\Alg\) is the category of unital associative algebras and their homomorphisms.
    \end{itemize}
    Simply send the Lie algebra homomorphism \(\varphi \colon \lie{g} \to L(A)\) to the associative algebra homomorphism \(\tilde{\varphi} \colon U(\lie{g}) \to A\) defined by \(\tilde{\varphi}(x) = \varphi(x)\) for \(x \in \lie{g}\) and extended by linearity and the requirement that \(\tilde{\varphi}\) preserves multiplication.
    This works precisely because of the universal property.
    For the inverse, send \(\psi \colon U(\lie{g}) \to A\) to the restriction \(\psi|_{\lie{g}}\).
    
    It turns out that \(L \colon \Alg \to \Lie\) is a functor, if \(f \colon A \to B\) is a morphism of associative algebras then we can define \(L(f) \colon L(A) \to L(B)\) by defining \(L(f)(\bracket{x}{y}) = \bracket{f(x)}{f(y)} = f(x)f(y) - f(y)f(x)\) for \(x, y \in A\).
    That is, we just require that \(L(f)\) is a Lie algebra homomorphism.
    Similarly, \(U \colon \Lie \to \Alg\) is a functor, if \(f \colon \lie{g} \to \lie{h}\) is a morphism of Lie algebras then we can define \(U(f) \colon U(\lie{g}) \to U(\lie{h})\) by defining \(U(f)(xy) = U(f)(x) U(f)(y)\) for \(x, y \in \lie{g}\) and similarly for products of more than two elements, and extended by linearity to all of \(U(\lie{g})\).
    That is, we just require that \(U(f)\) respects the multiplication of the associative algebra.
    Then the above isomorphism happens to be natural, and we thus have that \(L\) is right adjoint to \(U\).
    
    The important thing here is that if we take \(A = \End V\) then we have
    \begin{equation}
        \Hom_{\Lie}(\lie{g}, \generalLinearLie(V)) \isomorphic \Hom_{\Alg}(U(\lie{g}), \End V).
    \end{equation}
    This means that a map \(\lie{g} \to \generalLinearLie(V)\) carries the same data as a map \(U(\lie{g}) \to \End V\).
    We can identify a map of the first type as a Lie algebra representation of \(\lie{g}\), and a map of the second type as a unital associative algebra representation of \(U(\lie{g})\).
    That is, representations of \(\lie{g}\) are \enquote{the same} as representations of \(U(\lie{g})\).
    
    Another way of thinking about this is that \(U(\lie{g})\) is to \(\lie{g}\) as \(\field G\) is to \(G\) for a finite group, \(G\).
    We can study the representation theory of \(\lie{g}\) or \(G\) just by studying the representation theory of the universal enveloping algebra or group algebra.
    
    \begin{prp}{}{}
        The universal enveloping algebra, \(U(\lie{g})\), is a Hopf algebra with the comultiplication
        \begin{equation}
            \Delta(x) = x \otimes 1 + 1 \otimes x,
        \end{equation}
        counit
        \begin{equation}
            \varepsilon(x) = 0,
        \end{equation}
        and antipode
        \begin{equation}
            \chi(x) = -x.
        \end{equation}
    \end{prp}
    
    Compare and contrast this to the group algebra, \(\field G\), which is a Hopf algebra with
    \begin{equation}
        \Delta(g) = g \otimes g, \quad \varepsilon(g) = 1, \qand \chi(g) = g^{-1}.
    \end{equation}
    These are, in some ways, two opposite ends of the scale for how a Hopf algebra can behave.
    
    \begin{dfn}{Filtred Algebra}{}
        Let \(A\) be an associative algebra.
        We say that \(A\) is \define{\(\integers_{\ge 0}\)-filtred}\index{filtred algebra} if we have a chain of subspaces
        \begin{equation}
            0 = F_{-1}A \subseteq F_0A \subseteq F_1A \subseteq \dotsb \subseteq F_nA \subseteq \dotsb
        \end{equation}
        such that \(1 \in F_0 A\),
        \begin{equation}
            \bigcup_{n=0}^{\infty} F_nA = A,
        \end{equation}
        and \(F_iA \cdot F_jA \subseteq F_{i+j} A\).
    \end{dfn}
    
    \begin{dfn}{Degree Filtration}{}
        If \(A\) is an associative algebra generated by \(\{x_\alpha\}\) then we can define a filtration on \(A\) by declaring all \(x_\alpha\) to be of degree \(1\), and defining \(F_nA \coloneq (F_1A)^n\) to be formed of all terms of degree at most \(n\) (note that the degree of \(x_\alpha x_{\alpha'}\) is 2, as is the degree of \(x_\alpha^2\), and so on).
    \end{dfn}
    
    \begin{dfn}{Associated Graded Algebra}{}
        Given a filtred algebra, \(A\), we define the \defineindex{associated graded algebra} to be
        \begin{equation}
            \gr(A) \coloneq \bigoplus_{n=0}^{\infty} F_n(A)/F_{n-1}(A).
        \end{equation}
    \end{dfn}
    
    For the degree filtration the associated graded algebra is
    \begin{equation}
        \gr(A) = \bigoplus_{n=0}^{\infty} A_n
    \end{equation}
    where \(A_n\) is the span of all words of degree exactly \(n\).
    
    If \(\lie{g}\) is a Lie algebra then we can define a degree filtration on \(U(\lie{g})\) by setting the degree of any \(x \in \lie{g}\) to be \(1\).
    Then \(F_nU(\lie{g})\) is the image of \(\bigoplus_{k=0}^n \lie{g}^{\otimes k} \subset T(\lie{g})\) under the quotient map \(T(\lie{g}) \twoheadrightarrow T(\lie{g})/I\).
    Since in \(U(\lie{g})\) we have \(xy - yx = \bracket{x}{y}\) for \(x \in \lie{g}\) and \(y \in U(\lie{g})\) it follows that \(\bracket{F_iU(\lie{g})}{F_jU(\lie{g})} \subseteq F_{i + j - 1}U(\lie{g})\).
    It then follows that when we take \(F_nU(\lie{g}) / F_{n-1}U(\lie{g})\) in \(\gr(U(\lie{g}))\) we are quotenting by (among other things) all commutators of elements of degree less than \(n\).
    This makes \(\gr(U(\lie{g}))\) commutative.
    This in turn means that there is an epimorphism of associative algebras
    \begin{equation}
        S(\lie{g}) \twoheadrightarrow \gr(U(\lie{g})).
    \end{equation}
    This is a statement that \(S(A)\) is universal amongst commutative subalgebras of \(T(A)\), i.e., that any such subalgebra can be recognised by taking \(S(A)\) and applying some quotient to identify certain terms.
    
    \begin{dfn}{PBW Theorem}{}
        The homomorphism \(S(\lie{g}) \to \gr(U(\lie{g}))\) is an isomorphism.
    \end{dfn}
    
    \begin{crl}{}{}
        If \(\{x_i\}\) is a basis of \(\lie{g}\) we can fix an order on the basis.
        Then \(U(\lie{g})\) is spanned by ordered monomials \(\prod_i x_i^{n_i}\) with \(n_i \in \integers_{\ge 0}\).
    \end{crl}
    
    \begin{thm}{PBW Theorem}{}
        The ordered monomials described above are actually linearly independent, and thus form a basis for \(U(\lie{g})\).
    \end{thm}
    
    \begin{exm}{}{}
        Consider \(\specialLinearLie_2(\complex)\).
        This is a three-dimensional Lie algebra with generators \(\{e, h, f\}\).
        If we order them so that \(e < h < f\) then a basis for \(U(\specialLinearLie_2(\complex))\) is \(e^a h^b f^c\) with \(a, b, c \in \integers_{\ge 0}\).
    \end{exm}
    
    \section{Representation Theory of \texorpdfstring{\(\specialLinearLie_2(\complex)\)}{sl2}}
    The representation theory of all finite dimensional semisimple Lie algebras over \(\complex\) is almost entirely controlled by the representation theory of \(\specialLinearLie_2\).
    For this reason we'll now devote some time to the study of \(\specialLinearLie_2\).
    
    Recall that \(\specialLinearLie_2\) (working over \(\complex\)) is defined to consist of all traceless \(2 \times 2\) complex matrices.
    There is a basis for these given by
    \begin{equation}
        e = 
        \begin{pmatrix}
            0 & 1\\
            0 & 0
        \end{pmatrix}
        , \quad 
        h = 
        \begin{pmatrix}
            1 & 0\\
            0 & -1
        \end{pmatrix}
        ,\qand f =
        \begin{pmatrix}
            0 & 0\\
            1 & 0
        \end{pmatrix}
        .
    \end{equation}
    One can check that these satisfy the commutation relations
    \begin{equation}
        \bracket{h}{e} = 2e, \quad \bracket{h}{f} = -2f, \qand \bracket{e}{f} = h.
    \end{equation}
    We can then abstract the definition of \(\specialLinearLie_2\) to be \(\Span_{\complex}\{e, h, f\}\) subject to the above commutation relations, without needing an explicit matrix form.
    
    \begin{lma}{}{lma:weight space decomposition of sl2}
        Let \(V\) be a finite-dimensional representation of \(\specialLinearLie_2\).
        Then we have the decomposition
        \begin{equation}
            V \isomorphic \bigoplus_{\alpha \in \complex} V_\alpha
        \end{equation}
        where \(V_\alpha\) is the \defineindex{weight space}, defined to be the eigenspace
        \begin{equation}
            V_\alpha = \{v \in V \mid h \action v = \alpha v\}.
        \end{equation}
        \begin{proof}
            It is a fact that finite-dimensional \(\specialLinearLie_2\)-representations are completely reducible.
            Thus, we may assume without loss of generality that \(V\) is irreducible, since if it isn't we can decompose it into a sum of irreducibles and then treat each of these separately.
            
            Let \(W\) be the subspace of eigenvectors of \(h\).
            It is then sufficient to show that \(W = V\).
            To do this we show that \(W\) is a subrepresentation, that is, it's closed under \(h\), \(e\), and \(f\).
            Then irreducibility will imply that \(W = V\).
            
            By definition \(h\) acts as a scalar on \(W\), so \(W\) is closed under \(h\).
            For \(e\) let \(v \in W\) be an eigenvector of \(h\), that is \(hv = \alpha v\).
            Then a direct computation gives
            \begin{align}
                he \action v &= (\bracket{h}{e} + eh) \action v\\
                &= (2e + eh) \action v\\
                &= 2e \action v + eh \action v\\
                &= 2e \action v + \alpha e\action v\\
                &= (\alpha + 2) e \action v.
            \end{align}
            Thus, \(e \action v\) is again an eigenvector of \(h\), with eigenvalue \(\alpha + 2\).
            Similarly, one can show that \(f \action v\) is an eigenvector of \(h\) with eigenvalue \(\alpha - 2\).
            
            Thus, \(W\) is closed under the action of \(e\), \(h\), and \(f\), and thus is a subrepresentation, and so by irreducibility \(W = V\).
            Thus, if \(V\) is not irreducible is a direct sum of irreducibles, each of which is an eigenspace of \(h\) with some given eigenvalue \(\alpha\).
            We may as well sum over all possible eigenvalues, \(\alpha \in \complex\), and simply have \(V_\alpha = 0\) for many terms.
        \end{proof}
    \end{lma}
    
    \begin{exm}{}{}
        The definition of \(\specialLinearLie_2\) in terms of \(2 \times 2\) matrices gives us a natural action of \(\specialLinearLie_2\) on \(\complex^2\).
        Let \(\{e_1, e_2\}\) be the standard basis of \(\complex^2\).
        We have \(he_1 = e_1\) and \(he_2 = -e_2\), so we have two eigenvectors, and the corresponding eigenspaces \(V_1 = \complex e_1\) and \(V_{-1} = \complex e_2\).
        Then we have the following picture:
        \begin{equation}
            \begin{tikzcd}
                V_1 \arrow[loop right, "h"] \arrow[d, bend left, "f"]\\
                V_{-1} \arrow[loop right, "h"] \arrow[u, bend left, "e"]
            \end{tikzcd}
        \end{equation}
        The interpretation of this picture is that \(e\) and \(f\) act to shift the eigenvalue up and down by \(2\).
        Note that applying \(e\) to \(e_1\) gives \(ee_1 = 0\), and likewise, \(fe_2 = 0\).
        Thus, we can add \(0\) to the top and bottom of this picture:
        \begin{equation}
            \begin{tikzcd}
                0 \arrow[d, bend left, "f"]\\
                V_1 \arrow[loop right, "h"] \arrow[d, bend left, "f"] \arrow[u, bend left, "e"]\\
                V_{-1} \arrow[loop right, "h"] \arrow[u, bend left, "e"] \arrow[d, bend left, "f"]\\
                0 \arrow[u, bend left, "e"]
            \end{tikzcd}
        \end{equation}
    \end{exm}
    
    The picture above actually generalises to any finite dimensional representation, we can always draw a picture like the following:
    \begin{equation}
        \begin{tikzcd}
            0 \arrow[d, bend left, "f"]\\
            V_{\alpha + 2k} \arrow[u, "e"] \arrow[loop right, "h"] \arrow[d, bend left, "f"]\\
            \vdots \arrow[d, bend left, "f"] \arrow[u, bend left, "e"]\\
            V_{\alpha + 2} \arrow[loop right, "h"] \arrow[d, bend left, "f"] \arrow[u, "e", bend left]\\
            V_{\alpha} \arrow[loop right, "h"] \arrow[u, bend left, "e"] \arrow[d, bend left, "f"]\\
            V_{\alpha - 2} \arrow[loop right, "h"] \arrow[u, bend left, "e"] \arrow[d, bend left, "f"]\\
            \vdots \arrow[d, bend left, "f"] \arrow[u, bend left, "e"]\\
            V_{\alpha - 2\ell} \arrow[loop right, "h"] \arrow[d, bend left, "f"] \arrow[u, "e", bend left]\\
            0 \arrow[u, bend left, "e"]
        \end{tikzcd}
    \end{equation}
    The fact that we must always eventually get to \(0\) going either up or down is simply due to the fact that \(V\) is finite-dimensional.
    
    \begin{exm}{}{exm:homogeneous polynomials as sl2 rep}
        Consider the vector space \(S^k(\complex^2)\).
        We may identify this with the space of degree \(k\) homogenous polynomials (with coefficients in \(\complex\)).
        For example, for \(S^3(\complex^2)\) we identify \(e_1 \otimes e_1 \otimes e_1\) with \(x^3\), \(e_1 \otimes e_1 \otimes e_2 = e_1 \otimes e_2 \otimes e_1 = e_2 \otimes e_1 \otimes e_1\) with \(x^2y\), and so on.
        Basically, send \(e_1\) to \(x\), \(e_2\) to \(y\), and remember that all tensor products are symmetrised.
        Note then that we can identify \(S(\complex^2)\) and \(\complex[x, y]\) (more generally, \(S(\complex^m)\) and \(\complex[x_1, \dotsc, x_m]\)), an important identification in algebraic geometry.
        
        There is a representation of \(\specialLinearLie_2\) on \(\complex[x, y]\) given by
        \begin{equation}
            e = -y\partial_x, \quad h = -x \partial_x + y \partial_y, \qand f = -x\partial_y.
        \end{equation}
        Note that each operator preserves the total degree of any polynomial (so long as it doesn't send it to zero).
        Thus, we can identify submodules of degree \(k\)-polynomials.
        More generally, the above identification defines an action of \(\specialLinearLie_2\) on smooth functions \(\complex^2 \to \complex\), of which the \(S^k(\complex^2)\) are submodules.
        
        Consider \(S^k(\complex^2)\), which we now identify with the space of degree \(k\) polynomials in \(x\) and \(y\).
        A basis for this space consists of vectors
        \begin{equation}
            v_r = \binom{k}{r} x^r y^{k - r}.
        \end{equation}
        Acting on this with \(h\) we have
        \begin{multline}
            hv_r = (-x \partial_x + y\partial_y) \binom{k}{r}x^r y^{k-r}\\
            = -r\binom{k}{r}x^ry^{k-r} -(k - r)\binom{k}{r}x^ry^{k-r}) = (k - 2r)v_r, 
        \end{multline}
        so \(v_r\) has \(h\)-eigenvalue \(\alpha = k - 2r\).
        We also have
        \begin{equation}
            ev_r = -y\partial_x \binom{k}{r} x^r y^{k-r} = -r \binom{k}{r}x^{r-1} y^{k-r+1} = (r - k - 1) v_{r-1}
        \end{equation}
        and the \(h\)-eigenvalue of \(v_{r-1}\) is \(k - 2(r - 1) = k - 2r + 2 = \alpha + 2\).
        Similarly, we have
        \begin{equation}
            fv_r = -x \partial_y \binom{k}{r} x^r y^{k-r} = -(k-r) \binom{k}{r} x^{r + 1} y^{k - r - 1} = -(1 + r)v_{r + 1}
        \end{equation}
        and the \(h\)-eigenvalue of \(v_{r + 1}\) is \(k -2(r + 1) = k - 2r - 2 = \alpha - 2\).
        Then letting \(V_{k - 2r} = \complex v_{r}\) we have
        \begin{equation}
            \begin{tikzcd}
                V_{k - 2r + 2} \arrow[loop right, "h \sim k-2r"] \arrow[d, bend left, "f \sim -(1 + r)"]\\
                V_{k - 2r} \arrow[loop right, "h \sim k-2r"] \arrow[u, bend left, "e \sim r - k - 1"]
            \end{tikzcd}
        \end{equation}
        Here \(a \sim \lambda\) we mean that \(a\) acts by sending the basis vector of one space to the basis vector of the next multiplied by \(\lambda\).
        
        Let \(V(k) = S^k(\complex^2)\) be this \(\specialLinearLie_2\)-module.
        This is an irreducible module.
        Given any basis vector it lives in one of the \(V_\alpha\), and if we continuously act with \(e\) we eventually get \(v_0\).
        Then \(v_0\) generates this entire module by acting with \(f\) and scalar multiplication.
        Note that \(\dim V(k) = k + 1\), since we have the basis \(\{v_0, \dotsc, v_k\}\).
    \end{exm}
    
    The previous example actually captures all irreducible modules of \(\specialLinearLie_2\), as the following proves.
    The argument basically mirrors the argument above without reference to an explicit structure of polynomials.
    
    \begin{prp}{Classification of Finite Dimensional Irreducible \(\specialLinearLie_2\)-Modules}{}
        Let \(V\) be a \((k + 1)\)-dimensional \(\specialLinearLie_2\)-module.
        Then \(V \isomorphic V(k)\) with \(V(k)\) as defined in \cref{exm:homogeneous polynomials as sl2 rep}.
        \begin{proof}
            By the same argument as in the proof of \cref{lma:weight space decomposition of sl2} we know that the eigenvectors of \(h\) span \(V\) (which we're assuming is irreducible).
            Since \(V\) is finite-dimensional \(h\) has a finite number of eigenvalues, so there must be some \(h\)-eigenvector, \(v_0\), for which we have \(h v_0 = 0\).
            Consider \(f^k v_0\), as we have a finite-dimensional space, and thus finitely many eigenvectors of \(h\), we must have for some \(N\) that \(f^N v_0 = 0\), and suppose \(N\) is the smallest such value.
            If we take \(B = \{v_0, fv_0, \dotsc, f^{N-1}v_0\}\) then this is a submodule of \(V\), and thus is all of \(V\).
            Thus, knowing that \(V\) has dimension \(k + 1\) we know that \(N = k + 1\).
            In particular, \(f^{N-1}v_0 = f^kv_0\) is the last element of this basis.
            
            For what follows it's useful to absorb some scale factor into the basis, define \(v_r = f^r v_0 / r!\) for \(r = 0, \dotsc, k\).
            Then \(\{v_r\}\) is a basis of \(V\).
            
            All that remains is to show that the action of \(e\) and \(f\) on this basis is fully determined.
            Starting with \(e\) we use the fact that \(hv_r = (\alpha_0 - 2r)v_r\) where \(\alpha_0\) is the \(h\)-eigenvalue of \(v_0\).
            We then have
            \begin{align}
                ev_0 &= 0\\
                ev_1 &= efv_0 = \bracket{e}{f}v_0 + fev_0 = hv_0 + 0 = \alpha_0 v_0\\
                ev_2 &= efv_1/2 = \bracket{e}{f}v_1/2 + fev_1/2 = hv_1/2 + \alpha_0fv_0/2\\
                &= (\alpha_0 - 2)v_1/2 + \alpha_0v_1/2 = (\alpha_0 - 1)v_1.
            \end{align}
            We thus make the induction hypothesis that
            \begin{equation}
                ev_n = (\alpha_0 - n + 1)v_{n-1}.
            \end{equation}
            Assuming the equivalent statement for \(v_{n - 1}\) holds we then have
            \begin{align}
                ev_n &= efv_{n-1}/n = \bracket{e}{f}v_{n-1}/n + fev_{n-1}/n\\
                &= hv_{n-1}/n + fev_{n-1}/n\\
                &= (\alpha_0 - 2n + 2)v_{n-2} + (\alpha_0 - n + 2) fv_{n-2}/n\\
                &= (\alpha_0 - 2n + 2)v_{n-1}/n + (n - 1)(\alpha_0 - n + 2)v_{n-1}/n\\
                &= (\alpha_0 - n + 1)v_{n-1}.
            \end{align}
            
            This shows that the structure of \(V\) is entirely determined by \(\alpha_0\), we now show that \(\alpha_0\) is fixed.
            We know that \(fv_k = 0\), and we have
            \begin{align}
                efv_k &= \bracket{e}{f}v_k + fev_k = hv_k + (\alpha_0 - k + 1)fv_{k-1}\\
                &= (\alpha_0 - 2k)v_k + (\alpha_0 - k + 1)k v_k\\
                &= (k + 1)(\alpha_0 - k)v_{k-1}.
            \end{align}
            For this to vanish, given that \(k + 1\), the dimension, is positive (for \(k + 1 = 0\) clearly all zero dimensional \(\specialLinearLie_2\)-modules are isomorphic), and thus \(\alpha_0 = k\) is fixed, and so as soon as we know the dimension of a finite-dimensional irreducible \(\specialLinearLie_2\)-module we know everything about it.
        \end{proof}
    \end{prp}
    
    \begin{dfn}{Weight Vectors}{}
        Let \(V\) be an \(\specialLinearLie_2\)-module.
        We call eigenvectors of \(h\) \define{weight vectors}\index{weight vector}, and the eigenvalue is called its weight.
        If \(v\) is a weight vector and \(ev = 0\) we call \(v\) a \defineindex{highest weight vector}, similarly, if \(fv = 0\) we call \(v\) a \defineindex{lowest weight vector}.
    \end{dfn}
    
    The above proposition then says that any finite-dimensional irreducible \(\specialLinearLie_2\)-module is generated by a highest weight vector, \(v_0\).
    
    \section{Classification of Semisimple Lie Algebras Over \texorpdfstring{\(\complex\)}{C}}
    The steps followed for classifying irreducible finite-dimensional irreducible \(\specialLinearLie_2\)-modules actually generalise remarkably well to classifying not just representations of other Lie algebras, but classifying a whole type of algebra, just by studying the adjoint representations in which these algebras act on themselves.
    
    There were three steps we followed with \(\specialLinearLie_2\).
    First, decompose \(V\) into eigenspaces of \(h\).
    Second, use the commutation relations to determine how \(e\) and \(f\) act on these eigenspaces.
    Finally, use the irreducibility of the module to show that it is generated by a single highest weight vector.
    
    In order to apply this method to other Lie algebras we'll need to generalise some things.
    The main one is that instead of just a single operator, \(h\), we end up with a whole subalgebra of operators, \(\lie{h}\).
    Before we get to this we need a few definitions.
    
    \begin{dfn}{Semisimple and Nilpotent Elements}{}
        Let \(\lie{g}\) be a Lie algebra.
        We say that \(x \in \lie{g}\) is \defineindex{semisimple} if \(\ad_x\) is diagonalisable, and \defineindex{nilpotent} if \(\ad_x\) is nilpotent.
    \end{dfn}
    
    For example, in \(\specialLinearLie_2\) \(h\) is semisimple, since in the adjoint representation, with the ordered basis \(\{e, h, f\}\), we have
    \begin{equation}
        \ad_h = 
        \begin{pmatrix}
            2\\
            & 0\\
            && -2
        \end{pmatrix}
        .
    \end{equation}
    On the other hand, \(e\) and \(f\) are nilpotent, since in the adjoint representation
    \begin{equation}
        \ad_e =
        \begin{pmatrix}
            0 & -2 & 0\\
            0 & 0 & 1\\
            0 & 0 & 0
        \end{pmatrix}
        , \qand \ad_f =
        \begin{pmatrix}
            0 & 0 & 0\\
            -1 & 0 & 0\\
            0 & 2 & 0
        \end{pmatrix}
        ,
    \end{equation}
    both of which have vanishing third power.
    
    An abelian subalgebra, \(\lie{h} \subseteq \lie{g}\) is called \defineindex{toral}\footnote{This name comes from the fact that if \(G\) is a Lie group with Lie algebra \(\lie{g}\) then any toral subgroup, \(H\), will have a Lie algebra isomorphic to \(\lie{h}\). In turn, a toral subgroup is a Lie subgroup of \(G\) which is isomorphic to a torus.} if it consists of only semisimple elements.
    For any toral subalgebra we have the following decomposition:
    \begin{equation}
        \lie{g} = \bigoplus_{\alpha \in \lie{h}^*} \lie{g}_\alpha
    \end{equation}
    where
    \begin{equation}
        \lie{g}_\alpha = \{x \in \lie{g}_\alpha \mid \ad_h(x) = \bracket{h}{x} = \alpha(h)x \text{ for } h \in \lie{h}\}.
    \end{equation}
    This is simply the weight space decomposition of \(\lie{g}\) viewed as an \(\lie{h}\)-module through (restricted) adjoint action.
    
    One can show that
    \begin{equation}
        \bracket{\lie{g}_\alpha}{\lie{g}_\beta} \subseteq \lie{g}_{\alpha + \beta}.
    \end{equation}
    In particular, \(\lie{g}_0\) is a Lie subalgebra, since \(\bracket{\lie{g}_0}{\lie{g}_0} \subseteq \lie{g}_0\), and \(\lie{h} \subseteq \lie{g}_0\).
    
    \begin{dfn}{Cartan Subalgebra}{}
        If \(\lie{g}\) is a Lie algebra with toral subalgebra, \(\lie{h}\), such that, with the notation above, we have \(\lie{g}_0 = \lie{h}\) then we call \(\lie{h}\) a \defineindex{Cartan subalgebra} of \(\lie{g}\).
    \end{dfn}
    
    Note that while Cartan subalgebras aren't unique they are all conjugate, so we typically speak of \emph{the} Cartan subalgebra, when it exists.
    
    When we have a Cartan subalgebra we can change the decomposition to
    \begin{equation}
        \lie{g} = \lie{h} \oplus \bigoplus_{\alpha \in \Delta} \lie{g}_\alpha
    \end{equation}
    where \(\Delta = \{\alpha \in \lie{h}^*\setminus 0 \mid \lie{g}_\alpha \ne 0\}\) is the subset of \(\lie{h}^*\) for which \(\alpha \ne 0\) and \(\lie{g}_\alpha\) is nontrivial.
    We call \(\Delta\) a set of \define{simple roots}\index{simple roots}.
    
    For example, for \(\specialLinearLie_2\) we have the Cartan subalgebra \(\lie{h} = \complex h\).
    In this case we have \(\lie{g}_{2} = \complex e\) and \(\lie{g}_{-2} = \complex f\), and we get the decomposition
    \begin{equation}
        \specialLinearLie_2 = \complex h \oplus \complex e \oplus \complex f.
    \end{equation}
    
    \subsection{Root Systems}
    \begin{dfn}{Reflection}{}
        Let \(E\) be a Euclidean space with inner product \(\rootProd{-}{-} \colon E \otimes E \to \reals\).
        A \defineindex{reflection} is a linear map \(s \colon E \to E\) such that there exists some \(v \in E\) such that \(s(v) = -v\) and the hyperplane \((\reals v)^{\perp}\) is fixed pointwise by \(s\).
        Then we call \(s\) a reflection along \(v\).
    \end{dfn}
    
    Note that given \(v\) the following formula gives a reflection along \(v\):
    \begin{equation}
        s_v(w) = w - 2\frac{\rootProd{v}{w}}{\rootProd{v}{v}} v.
    \end{equation}
    
    \begin{dfn}{Root System}{}
        Let \(E\) be a real Euclidean space with inner product \(\rootProd{-}{-}\).
        A \defineindex{root system}, \(\Phi\), in \(E\) is a finite set of nonzero vectors or \define{roots}\index{root} such that
        \begin{enumerate}
            \item \(\Span_{\reals} \Phi = E\);
            \item if \(\alpha \in \Phi\) then \(c \alpha \in \Phi\) only for \(c = \pm 1\);
            \item \(s_\alpha(\Phi) = \Phi\) for \(\alpha \in \Phi\);
            \item \(2\rootProd{\alpha}{\beta}/\rootProd{\alpha}{\alpha} \in \integers\).
        \end{enumerate}
        Sometimes the second condition isn't required, root systems for which the second condition holds are known as \define{reduced root systems}\index{reduced root system}\index{root system!reduced}.
        
        The \defineindex{rank} of the root system is \(\dim_{\reals} E\).
    \end{dfn}
    
    \begin{dfn}{Positive and Simple Roots}{}
        Given a root system we can make arbitrary choice of a hyperplane containing none of the roots.
        We then choose one side of this hyperplane, again, arbitrarily, and declare roots in this half to be \define{positive}\index{positive root}.
        The \define{simple roots}\index{simple root} are the positive roots which cannot be written as a sum, \(\alpha + \beta\), of two elements of the positive roots, \(\alpha\) and \(\beta\), alternatively, the simple roots are precisely the subset of the positive roots which generate the positive roots through linear combinations with positive integral coefficients.
    \end{dfn}
    
    \begin{ntn}{}{}
        Notation varies here, but we'll call \(\Phi\) the set of roots, \(\Pi\) the set of positive roots and \(\Delta\) the set of simple roots.
    \end{ntn}
    
    It turns out that root systems actually turn up in many different areas of mathematics, but we'll focus on how they're relevant to Lie algebras.
    
    It turns out that, up to scaling, there is only one rank 1 root system.
    For reasons we'll get into later this root system is known as \(\dynkin{A}{1}\).
    This root system is depicted in \cref{fig:root system A1}.
    There are also only four rank 2 root systems, known as \(\dynkin{A}{1} \oplus \dynkin{A}{1}\) (being two orthogonal copies of \(\dynkin{A}{1}\)), \(\dynkin{A}{2}\), \(\dynkin{B}{2}\) (or \(\dynkin{C}{2}\)) and \(\dynkin{G}{2}\).
    These are depicted in \cref{fig:root system rank 2}.
    \Cref{tab:root systems of rank 2} lists the roots, \(\Phi\), positive roots, \(\Pi\), and simple roots, \(\Delta\).
    In all cases we've chosen to label our roots by expressing them in terms of two chosen simple roots, \(\alpha\) and \(\beta\).
    
    \begin{figure}
        \centering
        \tikzsetnextfilename{root-system-A1}
        \begin{tikzpicture}
            \draw [->] (0, 0) -- ++ (2, 0) node [right] {\(\alpha\)};
            \draw [->] (0, 0) -- ++ (-2, 0) node [left] {\(-\alpha\)};
            \fill (0, 0) circle [radius = 0.03cm];
        \end{tikzpicture}
        \caption{The \(\dynkin{A}{1}\) root system, \(\Phi = \{\alpha, -\alpha\}\), with chosen positive roots, \(\Pi = \{\alpha\}\), and simple roots, \(\Delta = \{\alpha\}\).}
        \label{fig:root system A1}
    \end{figure}
    
    \begin{figure}
        \centering
        \begin{subfigure}{0.45\textwidth}
            \centering
            \tikzsetnextfilename{root-system-A1+A1}
            \begin{tikzpicture}
                \draw [->] (0, 0) -- ++ (2, 0) node [right] {\(\alpha\)};
                \draw [->] (0, 0) -- ++ (-2, 0) node [left] {\(-\alpha\)};
                \draw [->] (0, 0) -- ++ (0, 2) node [above] {\(\beta\)};
                \draw [->] (0, 0) -- ++ (0, -2) node [below] {\(\mathllap{-}\beta\)};
                \fill (0, 0) circle [radius = 0.03cm];
            \end{tikzpicture}
            \caption{The \(\dynkin{A}{1} \oplus \dynkin{A}{1}\) root system.}
            \label{fig:root system A1+A1}
        \end{subfigure}
        \begin{subfigure}{0.45\textwidth}
            \centering
            \tikzsetnextfilename{root-system-A2}
            \begin{tikzpicture}
                \draw [->] (0, 0) -- ++ (2, 0) node [right] {\(\alpha\)};
                \draw [->] (0, 0) -- ++ (pi/3 r:2) node [right] {\(\alpha + \beta\)};
                \draw [->] (0, 0) -- ++ (2*pi/3 r:2) node [left] {\(\beta\)};
                \draw [->] (0, 0) -- ++ (-2, 0) node [left] {\(-\alpha\)};
                \draw [->] (0, 0) -- ++ (4*pi/3 r:2) node [left] {\(-\alpha - \beta\)};
                \draw [->] (0, 0) -- ++ (5*pi/3 r:2) node [right] {\(-\beta\)};
                \fill (0, 0) circle [radius = 0.03cm];
            \end{tikzpicture}
            \caption{The \(\dynkin{A}{2}\) root system.}
            \label{fig:root system A2}
        \end{subfigure}
        
        \begin{subfigure}{0.7\textwidth}
            \centering
            \tikzsetnextfilename{root-system-B2}
            \begin{tikzpicture}
                \draw [->] (0, 0) -- ++ (2, 0) node [right] {\(\alpha\)};
                \draw [->] (0, 0) -- ++ (45:{2*sqrt(2)}) node [right] {\(2\alpha + \beta\)};
                \draw [->] (0, 0) -- ++ (0, 2) node [above] {\(\alpha + \beta\)};
                \draw [->] (0, 0) -- ++ (135:{2*sqrt(2)}) node [left] {\(\beta\)};
                \draw [->] (0, 0) -- ++ (-2, 0) node [left] {\(-\alpha\)};
                \draw [->] (0, 0) -- ++ (225:{2*sqrt(2)}) node [left] {\(-2\alpha - \beta\)};
                \draw [->] (0, 0) -- ++ (0, -2) node [below] {\(\mathllap{-}\alpha - \beta\)};
                \draw [->] (0, 0) -- ++ (-45:{2*sqrt(2)}) node [right] {\(-\beta\)};
                \fill (0, 0) circle [radius = 0.03cm];
            \end{tikzpicture}
            \caption{The \(\dynkin{B}{2}\) root system.}
            \label{fig:root system B2}
        \end{subfigure}
        \begin{subfigure}{0.7\textwidth}
            \centering
            \tikzsetnextfilename{root-system-G2}
            \begin{tikzpicture}
                \draw [->] (0, 0) -- ++ (2, 0) node [right] {\(\alpha\)};
                \draw [->] (0, 0) -- ++ (pi/3 r:2) node [above] {\(2\alpha + \beta\)};
                \draw [->] (0, 0) -- ++ (2*pi/3 r:2) node [above] {\(\alpha + \beta\)};
                \draw [->] (0, 0) -- ++ (-2, 0) node [left] {\(-\alpha\)};
                \draw [->] (0, 0) -- ++ (4*pi/3 r:2) node [below] {\(\mathllap{-}2\alpha - \beta\)};
                \draw [->] (0, 0) -- ++ (5*pi/3 r:2) node [below] {\(\mathllap{-}\alpha - \beta\)};
                \begin{scope}[rotate=pi/6 r, scale={sqrt(3)}]
                    \draw [->] (0, 0) -- ++ (2, 0) node [right] {\(3\alpha + \beta\)};
                    \draw [->] (0, 0) -- ++ (pi/3 r:2) node [above] {\(3\alpha + 2\beta\)};
                    \draw [->] (0, 0) -- ++ (2*pi/3 r:2) node [left] {\(\beta\)};
                    \draw [->] (0, 0) -- ++ (-2, 0) node [left] {\(\mathllap{-}3\alpha - \beta\)};
                    \draw [->] (0, 0) -- ++ (4*pi/3 r:2) node [below] {\(\mathllap{-}3\alpha - 2\beta\)};
                    \draw [->] (0, 0) -- ++ (5*pi/3 r:2) node [right] {\(-\beta\)};
                \end{scope}
                \fill (0, 0) circle [radius = 0.03cm];
            \end{tikzpicture}
            \caption{The \(\dynkin{G}{2}\) root system.}
            \label{fig:root system G2}
        \end{subfigure}
        
        \caption{The rank \(2\) root systems.}
        \label{fig:root system rank 2}
    \end{figure}
    
    \begin{table}
        \centering
        \caption[Root systems of rank at most 2]{Information on the root systems of rank at most \(2\). Notice that \(\Phi = \Pi \sqcup (-\Pi)\) and in all cases we have chosen our naming of roots such that \(\Delta = \{\alpha, \beta\}\). Notice that the positive roots, \(\Pi\), are always found in the cone between the simple roots.}
        \label{tab:root systems of rank 2}
        \small
        \begin{tabular}{clll}
            \toprule
            & \(\Phi\) & \(\Pi\) & \(\Delta\) \\ \midrule
            \(\dynkin{A}{1}\) & \(\pm\alpha\) & \(\alpha\) & \(\alpha\)\\
            \(\dynkin{A}{1} \oplus \dynkin{A}{1}\) & \(\pm\alpha\), \(\pm\beta\) & \(\alpha\), \(\beta\) & \(\alpha\), \(\beta\)\\
            \(\dynkin{A}{2}\) & \(\pm \alpha\), \(\pm \beta\), \(\pm(\alpha + \beta)\) & \(\alpha\), \(\beta\), \(\alpha + \beta\) & \(\alpha\), \(\beta\)\\
            \(\dynkin{B}{2}\) & \(\pm\alpha\), \(\pm\beta\), \(\pm(\alpha + \beta)\), \(\pm(2\alpha + \beta)\) & \(\alpha\), \(\beta\), \(\alpha + \beta\), \(2\alpha + \beta\) & \(\alpha\), \(\beta\)\\
            \(\dynkin{G}{2}\) & \(\pm \alpha\), \(\pm\beta\), \(\alpha + \beta\), \(\pm(2\alpha + \beta)\), \(\pm(3\alpha + \beta)\) & \(\alpha\), \(\beta\), \(\alpha + \beta\), \(2\alpha + \beta\), \(3\alpha + \beta\) & \(\alpha\), \(\beta\) \\ \bottomrule
        \end{tabular}
    \end{table}
    
    \subsection{Connection to Semisimple Lie Algebras}
    The reason that these root systems, as abstract subsets of some Euclidean space, are relevant is that given a semisimple Lie algebra the set of simple roots, \(\Delta\), (that is \(\alpha \in \lie{h}^*\) such that \(\lie{g}_\alpha \ne 0\)) is actually the set of simple roots of a corresponding root system.
    
    \begin{thm}{}{}
        Let \(\lie{g}\) be a semisimple Lie algebra over \(\complex\), with Cartan subalgebra \(\lie{h}\).
        Let \(E\) be a Euclidean space such that the complexification of \(E\) is \(\lie{h}^*\).
        Then
        \begin{itemize}
            \item \(\Delta\) forms a reduced root system in \(E\);
            \item Eigenspaces are one-dimensional, \(\lie{g}_\alpha \isomorphic \complex\) for \(\alpha \in \Delta\);
            \item \(\bracket{\lie{g}_\alpha}{\lie{g}_\beta} = \lie{g}_{\alpha + \beta}\).
        \end{itemize}
    \end{thm}
    
    It turns out that these properties are exactly as is required in order for the following result to hold.
    
    \begin{thm}{}{}
        There is a bijection between semisimple Lie algebras over \(\complex\) and reduced root systems.
    \end{thm}
    
    We've constructed the root system from a semisimple Lie algebra.
    Since these objects are in bijection we can construct a semisimple Lie algebra in a unique way from a given root system.
    The process is unfortunately not that insightful, and basically reduces to imposing a bunch of relations on a free Lie algebra according to information encoded in the root system.
    The nice thing about this result is that it turns out to be much simpler to classify all of the finite-rank root systems.
    
    \begin{dfn}{Cartan Matrix}{}
        A (finite-type) \defineindex{Cartan matrix} is an \(n \times n\) matrix, \(A = (a_{ij})_{1 \le i, j \le n}\) such that
        \begin{itemize}
            \item \(a_{ii} = 2\) and \(a_{ij} \in \integers_{\le 0}\) for \(i \ne j\);
            \item \(A\) is symmetrisable (there exists some diagonal matrix, \(D\), such that \(DA\) is a symmetric matrix);
            \item \(A\) is positive (all principle minors of \(A\) are positive).
        \end{itemize}
    We consider two Cartan matrices to be the same if they are equal up to a simultaneous permutation of the rows and columns.
    That is, \(A\) and \(B\) are the same if \(a_{i,j} = b_{\sigma(i),\sigma(j)}\) for some \(\sigma \in S_n\).
    \end{dfn}
    
    \begin{lma}{}{}
        Let \(\Phi\) be a root system with chosen simple roots, \(\Delta = \{\alpha_1, \dotsc, \alpha_n\}\).
        Define a matrix \(A = (a_{ij})_{1 \le i, j \le n}\) by
        \begin{equation}
            a_{ij} \coloneq \frac{2\rootProd{\alpha_i}{\alpha_j}}{\rootProd{\alpha_i}{\alpha_i}}.
        \end{equation}
        This is a Cartan matrix, and is uniquely determined by the root system (up to permutation of the labels of our simple roots).
        Conversely, given a Cartan matrix one can construct a root system with that Cartan matrix.
    \end{lma}
    
    The above result means that classifying Cartan matrices classifies root systems, which in turn classifies semisimple Lie algebras.
    
    We're now ready to state the reverse process, for going from a root system or Cartan matrix to the corresponding semisimple Lie algebra.
    
    \begin{prp}{}{}
        Let \(A = (a_{ij})\) be an \(n \times n\) Cartan matrix.
        Let \(\lie{g}\) be the Lie algebra generated by \(\{e_i, h_i, f_i \mid 1 \le i \le n\}\) subject to the relations
        \begin{itemize}
            \item \(\bracket{h_i}{e_j} = a_{ij}e_j\);
            \item \(\bracket{h_i}{f_j} = -a_{ij}f_j\);
            \item \(\bracket{e_i}{f_j} = \delta_{ij}h_i\);
            \item \(\bracket{h_i}{h_j} = 0\);
            \item \((\ad_{e_i})^{1 - a_{ij}}e_j = 0\);
            \item \((\ad_{f_i})^{1 - a_{ij}}f_i = 0\).
        \end{itemize}
        Then this is a semisimple Lie algebra over \(\complex\) and is uniquely determined by \(A\).
    \end{prp}
    
    The last two relations above are called the \defineindex{Serre relations}.
    
    Note that in the above \(1 - a_{ij}\) is always positive, and \((\ad_{e_i})^{k}\) means the \(k\)-nested bracket with \(e_i\), for example, \((\ad_{e_i})^{3}(x) = \bracket{e_i}{\bracket{e_i}{\bracket{e_i}{x}}}\).
    
    \begin{exm}{\(\specialLinearLie_2\)}{}
        Consider \(\specialLinearLie_2\).
        We will demonstrate here that \(\specialLinearLie_2\) is precisely the semisimple Lie algebra corresponding to \(\dynkin{A}{1}\).
        
        To do so we start with finding the Cartan matrix of \(\dynkin{A}{1}\).
        Since \(\Phi = \{\pm\alpha\}\) and \(\Delta = \{\alpha\}\) this Cartan matrix is just \(1 \times 1\), with the single entry being
        \begin{equation}
            a_{1 1} = \frac{2\rootProd{\alpha}{\alpha}}{\rootProd{\alpha}{\alpha}} = 2.
        \end{equation}
        So, \(A = (2)\), of course the diagonal of the Cartan matrix is, by definition, always 2s, so we didn't actually need this calculation.
        
        Then we can take \(\lie{g}\) to be the Lie algebra generated by \(\{e_1, h_1, f_1\}\) subject to the relations
        \begin{itemize}
            \item \(\bracket{h_1}{e_1} = a_{11}e_1 = 2e_2\);
            \item \(\bracket{h_1}{f_1} = -a_{11}e_1 = -2f_1\);
            \item \(\bracket{e_1}{f_1} = \delta_{11}h_1 = h_1\);
            \item \(\bracket{h_1}{h_1} = 0\).
        \end{itemize}
        The last of these is always true, the first three are exactly the relations on \(\{e, h, f\}\) which we impose on \(\specialLinearLie_2\), so \(\lie{g} \isomorphic \specialLinearLie_2\).
        
        More generally, if we construct a Lie algebra from an arbitrary root system and take the subalgebra generated by \(e_i\), \(h_i\) and \(f_i\) for fixed \(i\) then, since \(a_{ii} = 2\) we always get a copy of \(\specialLinearLie_2\).
    \end{exm}
    
    \begin{exm}{\(\specialLinearLie_3\)}{}
        Let's go one dimension up and consider \(\dynkin{A}{2}\).
        This root system has \(\Phi = \{\pm\alpha, \pm\beta, \pm(\alpha + \beta)\}\) and \(\Delta = \{\alpha, \beta\}\).
        Let \(\alpha_1 = \alpha\) and \(\alpha_2 = \beta\) in what follows.
        Then the Cartan matrix has diagonals 2.
        Looking at the root diagram in \cref{fig:root system A2} the angle between \(\alpha\) and \(\beta\) is \(2\pi/3\), and both roots are the same length.
        Thus, \(\rootProd{\alpha}{\beta} = \rootProd{\alpha_1}{\alpha_2} = \cos(2\pi/3) = -1/2\), and thus
        \begin{equation}
            a_{12} = \frac{2\rootProd{\alpha_1}{\alpha_1}}{\rootProd{\alpha_1}{\alpha_1}} = -1, \qand a_{21} = \frac{2\rootProd{\alpha_2}{\alpha_1}}{\rootProd{\alpha_2}{\alpha_2}} = 1 
        \end{equation}
        having chosen a normalisation such that \(\rootProd{\alpha_1}{\alpha_1} = \rootProd{\alpha_2}{\alpha_2} = 1\).
        The Cartan matrix of \(\dynkin{A}{2}\) is thus
        \begin{equation}
            A = 
            \begin{pmatrix}
                2 & -1\\
                -1 & 2
            \end{pmatrix}
            .
        \end{equation}
        
        The corresponding semisimple Lie algebra is generated by \(\{e_1, e_2, h_1, h_2, f_1, f_2\}\) subject to
        \begin{itemize}
            \item \(\bracket{h_1}{e_1} = 2e_1\), \(\bracket{h_1}{e_2} = -e_2\), \(\bracket{h_2}{e_1} = -e_1\), \(\bracket{h_2}{e_2} = 2e_2\);
            \item \(\bracket{h_1}{f_1} = -2e_1\), \(\bracket{h_1}{f_2} = f_2\), \(\bracket{h_2}{f_1} = f_1\), \(\bracket{h_2}{f_2} = -2f_2\);
            \item \(\bracket{e_1}{f_1} = h_1\), \(\bracket{e_2}{f_2} = h_2\), \(\bracket{e_1}{f_2} = \bracket{e_2}{f_1} = 0\);
            \item \(\bracket{h_1}{h_2} = 0\);
            \item \((\ad_{e_1})^{1 - a_{12}}e_2 = (\ad_{e_1})^2e_2 = \bracket{e_1}{\bracket{e_1}{e_2}} = 0\), \(\bracket{e_2}{\bracket{e_2}{e_1}} = 0\);
            \item \(\bracket{f_1}{\bracket{f_1}{f_2}} = \bracket{f_2}{\bracket{f_2}{f_1}} = 0\).
        \end{itemize}
        This algebra is isomorphic to \(\specialLinearLie_3\).
    \end{exm}
    
    \begin{exm}{\(\specialOrthogonalLie_5\)}{}
        Consider the root system \(\dynkin{B}{3}\), which has \(\Delta = \{\alpha_1, \alpha_2\}\).
        Looking at the root diagram, \cref{fig:root system B2}, we see that if we choose \(\alpha = \alpha_1\) to have length \(1\) then \(\alpha_2 = \beta\) has length \(\sqrt{2}\), and the angle between \(\alpha\) and \(\beta\) is \(3\pi/4\), and \(\cos(3\pi/4) = -\sqrt{2}/2\).
        Thus,
        \begin{align*}
            a_{12} &= \frac{2\rootProd{\alpha_1}{\alpha_2}}{\rootProd{\alpha_1}{\alpha_1}} = \frac{2\norm{\alpha_1}\norm{\alpha_2}\cos(3\pi/4)}{\norm{\alpha_1}^2} = \frac{2 \cdot 1 \cdot \sqrt{2} \cdot (-\sqrt{2}/2)}{1} = -2,\\
            a_{21} &= \frac{2\rootProd{\alpha_2}{\alpha_1}}{\rootProd{\alpha_2}{\alpha_2}} = \frac{2\norm{\alpha_2}\norm{\alpha_1} \cos(3\pi/4)}{\norm{\alpha_2}^2} = \frac{2 \cdot \sqrt{2} \cdot 1 \cdot (-\sqrt{2}/2)}{(\sqrt{2})^2} = -1.
        \end{align*}
        So, the Cartan matrix of \(\dynkin{B}{3}\) is
        \begin{equation}
            A =
            \begin{pmatrix}
                2 & -2\\
                -1 & 2
            \end{pmatrix}
            .
        \end{equation}
        Note that this is symmetrisable:
        \begin{equation}
            D = 
            \begin{pmatrix}
                1 & 0\\
                0 & 2
            \end{pmatrix}
            \implies DA = 
            \begin{pmatrix}
                2 & -2\\
                -2 & 4
            \end{pmatrix}
            .
        \end{equation}
        
        The corresponding Lie algebra is generated by \(\{e_1, e_2, h_1, h_2, f_1, f_2\}\), subject to the relations that
        \begin{itemize}
            \item \(\bracket{h_1}{e_1} = 2e_1\), \(\bracket{h_2}{e_2} = 2e_2\), \(\bracket{h_1}{e_2} = -2e_2\), \(\bracket{h_2}{e_1} = -e_1\);
            \item \(\bracket{h_1}{f_1} = -2f_1\), \(\bracket{h_2}{f_2} = -2f_2\), \(\bracket{h_1}{f_2} = 2f_2\), \(\bracket{h_2}{f_1} = f_1\);
            \item \(\bracket{e_1}{f_1} = h_1\), \(\bracket{e_2}{f_2} = h_2\), \(\bracket{e_1}{f_2} = \bracket{e_2}{f_1} = 0\);
            \item \(\bracket{h_i}{h_j} = 0\) for \(i, j \in \{1, 2\}\);
            \item \((\ad_{e_1})^{1 - a_{12}}e_2 = (\ad_{e_1})^3e_2 = \bracket{e_1}{\bracket{e_1}{\bracket{e_1}{e_2}}} = 0\), \(\bracket{e_2}{\bracket{e_2}{e_1}} = 0\);
            \item \(\bracket{f_1}{\bracket{f_1}{\bracket{f_1}{f_2}}} = \bracket{f_2}{\bracket{f_2}{f_1}} = 0\).
        \end{itemize}
        This Lie algebra is isomorphic to that of \(\specialOrthogonalLie_5\).
    \end{exm}
    
    Notice that in all of these examples, and more generally by inspecting the relations defining \(\lie{g}\), we always have that \(\{e_i, h_i, f_i\}\) (for fixed \(i\)) generates a copy of \(\specialLinearLie_2\).
    These copies of \(\specialLinearLie_2\) are such that the \(e_i\)s and \(f_j\)s of distinct copies don't \enquote{interact} (i.e., they commute).
    The interaction only occurs when \(h_i\)s are involved.
    The \(h_i\)s themselves form a subalgebra, which is exactly the Cartan subalgebra, which we can see from these relations is always abelian.
    
    \subsection{Classification of Cartan Matrices}
    The final part to classifying all finite-dimensional semisimple Lie algebras over \(\complex\) is to classify all finite-type Cartan matrices.
    This has been done.
    The tidiest way to frame this classification is to encode the information of a root system into a labelled graph, and then it turns out that all of the corresponding graphs either fall into one of four families of graphs, or one of five exceptional cases.
    
    First, given an \(n \times n\) Cartan matrix, \(A\), or the corresponding root system, \((\Phi, \Pi, \Delta)\), we can construct a labelled graph as follows:
    \begin{itemize}
        \item The nodes are the simple roots, \(\alpha_i \in \Delta\);
        \item Draw \(a_{ij}a_{ji}\) edges between \(\alpha_i\) and \(\alpha_j\) (\(i \ne j\));
        \item If \(\alpha_i\) is longer than \(\alpha_j\) draw an arrow on the edge pointing towards the shorter root.
    \end{itemize}
    The graph that we get is called the \defineindex{Dynkin diagram} of the root system/Cartan matrix.
    
    \begin{exm}{}{}
        Consider \(\dynkin{A}{2}\), this has two simple roots, \(\alpha_1\) and \(\alpha_2\).
        We have \(a_{12} a_{21} = (-1) (-11) = 1\), and so the corresponding Dynkin diagram is
        \begin{equation}
            \tikzsetnextfilename{dynkin-A2}
            \begin{tikzpicture}
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_1}\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_2}\)};
                \draw (0, 0) -- (1, 0);
            \end{tikzpicture}
        \end{equation}
        
        Now consider \(\dynkin{B}{2}\), this has two simple roots, \(\alpha_1\) and \(\alpha_2\).
        We have \(a_{12} a_{21} = (-2)(-1) = 2\), and \(\alpha_2\) is longer than \(\alpha_1\), so the corresponding Dynkin diagram is
        \begin{equation}
            \tikzsetnextfilename{dynkin-B2}
            \begin{tikzpicture}
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(\alpha_1\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(\alpha_2\)};
                \draw (0, 0.03) -- ++ (1, 0);
                \draw (0, -0.03) -- ++ (1, 0);
                \draw [-{>[length=1.5mm,width=3mm]}, xshift=0.49cm] (0, 0) -- ++ (-0.001, 0);
            \end{tikzpicture}
        \end{equation}
    \end{exm}
    
    This process is invertible, since the Dynkin diagram fully encodes the angles between roots and their relative lengths (well, it encodes which is longer, the actual relative length can then be computed by requiring that the Cartan matrix have integral entries).
    
    \begin{thm}{Classification of Root Systems}{}
        Every (finite-type) \(n \times n\) Cartan matrix and its corresponding root system has a Dynkin diagram which is in one of the following infinite families (all with \(n\) vertices),
        \begin{gather}
            \tikzsetnextfilename{dynkin-An}
            \begin{tikzpicture}[font=\small]
                \node at (-1, 0) {\normalsize\(\dynkin{A}{n}\)};
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_1}\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_2}\)};
                \fill (2, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_3}\)};
                \fill (3, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_4}\)};
                \draw (0, 0) -- (3, 0);
                \draw [dashed] (3, 0) -- ++ (1, 0);
                \fill (4, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_{n-2}}\)};
                \fill (5, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_{n-1}}\)};
                \fill (6, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_n}\)};
                \draw (4, 0) -- (6, 0);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{dynkin-Bn}
            \begin{tikzpicture}[font=\small]
                \node at (-1, 0) {\normalsize\(\dynkin{B}{n}\)};
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_1}\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_2}\)};
                \fill (2, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_3}\)};
                \fill (3, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_4}\)};
                \draw (0, 0) -- (3, 0);
                \draw [dashed] (3, 0) -- ++ (1, 0);
                \fill (4, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_{n-2}}\)};
                \fill (5, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_{n-1}}\)};
                \fill (6, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_n}\)};
                \draw (4, 0) -- (5, 0);
                \draw (5, 0.03) -- ++ (1, 0); 
                \draw (5, -0.03) -- ++ (1, 0);
                \draw [-{>[length=1.5mm,width=3mm]}, xshift=5.52cm] (0, 0) -- ++ (0.001, 0);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{dynkin-Cn}
            \begin{tikzpicture}[font=\small]
                \node at (-1, 0) {\normalsize\(\dynkin{C}{n}\)};
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_1}\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_2}\)};
                \fill (2, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_3}\)};
                \fill (3, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_4}\)};
                \draw (0, 0) -- (3, 0);
                \draw [dashed] (3, 0) -- ++ (1, 0);
                \fill (4, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_{n-2}}\)};
                \fill (5, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_{n-1}}\)};
                \fill (6, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_n}\)};
                \draw (4, 0) -- (5, 0);
                \draw (5, 0.03) -- ++ (1, 0); 
                \draw (5, -0.03) -- ++ (1, 0);
                \draw [-{>[length=1.5mm,width=3mm]}, xshift=5.48cm] (0, 0) -- ++ (-0.001, 0);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{dynkin-Dn}
            \begin{tikzpicture}[font=\small]
                \node at (-1, 0) {\normalsize\(\dynkin{D}{n}\)};
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_1}\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_2}\)};
                \fill (2, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_3}\)};
                \fill (3, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_4}\)};
                \draw (0, 0) -- (3, 0);
                \draw [dashed] (3, 0) -- ++ (1, 0);
                \fill (4, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_{n-3}}\)};
                \fill (5, 0) circle [radius=0.075cm] node [right] {\(\alpha\mathrlap{_{n-2}}\)};
                \fill[xshift=5cm, shift={(45:1)}] coordinate (A) circle [radius=0.075cm] node [right] {\(\alpha\mathrlap{_{n-1}}\)};
                \fill[xshift=5cm, shift={(-45:1)}] coordinate (B) circle [radius=0.075cm] node [right] {\(\alpha\mathrlap{_n}\)};
                \draw (4, 0) -- (5, 0);
                \draw (5, 0) -- (A);
                \draw (5, 0) -- (B);
            \end{tikzpicture}
        \end{gather}
        or is one of the following exceptional cases,
        \begin{gather}
            \tikzsetnextfilename{dynkin-G2}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{G}{2}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \draw (0, 0) -- (1, 0);
                \draw (0, -0.03) -- ++ (1, 0);
                \draw (0, 0.03) -- ++ (1, 0);
                \draw [-{>[length=1.5mm,width=3mm]}, xshift=0.48cm] (0, 0) -- ++ (-0.001, 0);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{dynkin-F4}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{F}{4}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \fill (3, 0) circle [radius=0.075cm];
                \draw (0, 0) -- (1, 0);
                \draw (1, -0.03) -- ++ (1, 0);
                \draw (1, 0.03) -- ++ (1, 0);
                \draw [-{>[length=1.5mm,width=3mm]}, xshift=1.48cm] (0, 0) -- ++ (-0.001, 0);
                \draw (2, 0) -- (3, 0);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{dynkin-E6}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{E}{6}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \fill (3, 0) circle [radius=0.075cm];
                \fill (4, 0) circle [radius=0.075cm];
                \fill (2, 1) circle [radius=0.075cm];
                \draw (0, 0) -- (4, 0);
                \draw (2, 0) -- (2, 1);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{dynkin-E7}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{E}{7}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \fill (3, 0) circle [radius=0.075cm];
                \fill (4, 0) circle [radius=0.075cm];
                \fill (5, 0) circle [radius=0.075cm];
                \fill (2, 1) circle [radius=0.075cm];
                \draw (0, 0) -- (5, 0);
                \draw (2, 0) -- (2, 1);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{dynkin-E8}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{E}{8}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \fill (3, 0) circle [radius=0.075cm];
                \fill (4, 0) circle [radius=0.075cm];
                \fill (5, 0) circle [radius=0.075cm];
                \fill (6, 0) circle [radius=0.075cm];
                \fill (2, 1) circle [radius=0.075cm];
                \draw (0, 0) -- (6, 0);
                \draw (2, 0) -- (2, 1);
            \end{tikzpicture}
        \end{gather}
    \end{thm}
    
    There is much more to be said about Dynkin diagrams and the things that they classify, but this is all we have time for here.
    
    \section{Verma Modules}
    We can use this classification to say something about the representation theory of semisimple Lie algebras over \(\complex\).
    To start with, when \(\lie{g}\) is defined from a root system in terms of the generators \(e_i\), \(h_i\), and \(f_i\) we can make the following definition.
    
    \begin{dfn}{Verma Module}{}
        Let \(\lie{g}\) be a semisimple Lie algebra over \(\complex\) with Cartan subalgebra \(\lie{h}\), and let \(\lambda \in \lie{h}^*\) be a weight.
        Let \(I_\lambda \subseteq U(\lie{g})\) be the left ideal generated by the elements \(h - \lambda(h)1\) for \(h \in \lie{h}\) and \(e_i\) for \(i = 1, \dotsc, r\).
        The \defineindex{Verma module}, \(M_\lambda\), is \(U(\lie{g}) / I_\lambda\).
    \end{dfn}
    
    The idea of this definition is that \(M_\lambda\) is the largest (with respect to inclusion) highest weight representation with highest weight \(\lambda\).
    Recall that by \enquote{highest weight representation} we mean that \(M_\lambda\) is generated (as a \(U(\lie{g})\)-module) by some highest weight vector, \(v\), which is such that \(h \action v = \lambda(h) v\) and \(e_i \action v = 0\).
    Thus, \(M_\lambda\) consists of linear combinations of elements of the form \(f_{i_1} \dotsm f_{i_k} \action v\).
    The only relations imposed amongst these elements are those that are enforced by the commutation relations of the \(f_i\)s.
    As a consequence \(f_i\) need not act nilpotently, and thus \(M_\lambda\) is infinite dimensional.
    
    Let \(\lie{n}_+\) (\(\lie{n}_-\)) denote the subalgebra of \(\lie{g}\) generated by the \(e_i\) (\(f_i\)).
    Then one can show that the Verma module, \(M_\lambda\), is isomorphic to \(U(\lie{g}) \otimes_{U(\lie{h} \oplus \lie{n}_+)} \complex_\lambda\) where \(\complex_\lambda\) is the one-dimensional representation of \(\lie{h} \oplus \lie{n}_+\) in which \(h \in \lie{h}\) acts as \(h \action v = \lambda(h)v\) and \(e \in \lie{n}_+\) acts as \(e \action v = 0\) (define \(\lambda_+ \colon \lie{h} \oplus \lie{n}_+ \to \complex\) by \(\lambda_+(h) = h\) and \(\lambda_+(e) = 0\) and then this is the \enquote{obvious} one-dimensional representation).
    We can identify this construction as inducing \(\complex_\lambda\) up to all of \(\lie{g}\), so
    \begin{equation}
        M_\lambda \isomorphic \Ind^{U(\lie{g})}_{U(\lie{h} \oplus \lie{n}_+)} \complex_\lambda.
    \end{equation}
    This makes sense, the Verma module is such that \(\lie{h} \oplus \lie{n}_+\) acts by highest weight, which is what \(\complex_\lambda\) captures, and then \(\lie{n}_-\) acts freely imposing only the required commutation relations, which is captured by inducing up to \(\lie{g} \isomorphic \lie{n}_- \oplus \lie{h} \oplus \lie{n}_+\).
    
    The Verma module is infinite dimensional, but nevertheless it is still important in the theory of finite dimensional representations of \(\lie{g}\).
    
    \begin{prp}{}{}
        Let \(\lie{g}\) be a semisimple Lie algebra with Cartan subalgebra \(\lie{h}\) and fix a weight \(\lambda \in \lie{h}^*\).
        Let \(\complex\langle f_1, \dotsc, f_n \rangle\) be the free algebra generated by the noncommuting symbols \(f_1, \dotsc, f_n\), and let \(\tilde{M}_\lambda = \complex \langle f_1, \dotsc, f_n \rangle v\) be the free module generated by \(v\).
        There exists an action of \(\lie{g}\) on \(\tilde{M}_\lambda\) such that
        \begin{align}
            f_i \action \left( \prod_{k} f_{j_k} v \right) &= \left( f_i\prod_k f_{jk} \right);\\
            h_i \action \left( \prod_k f_{j_k}v \right) &= \left( \lambda(h_i) - \sum_k a_{i,j_k} \right) \left( \prod_k f_{j_k} v \right);\\
            e_i \action \left( \prod_{k=1}^l f_{j_k} v \right) &= \sum_{k | j_k = i} f_{j_1} \dotsm f_{j_{k-1}} h_i f_{j_{k+1}} \dotsm f_{j_l} v.
        \end{align}
        \begin{proof}
            The \(\lie{g}\)-module defined here is simply the Verma module, the only difference is that we're not imposing any condition on the \(f_i\)s in the monomials in \(\tilde{M}_\lambda\), whereas in \(M_\lambda\) we impose the Serre relations.
        \end{proof}
    \end{prp}
    
    The \defineindex{weight lattice} of \(\lie{g}\) is \(P = \integers \Phi \subset E\), the lattice generated by the roots.
    For example, for \(\dynkin{A}{1}\) the weight lattice is just \(\integers\), for \(\dynkin{A}{1} \oplus \dynkin{A}{1}\) it's \(\integers^2\), for \(\dynkin{A}{2}\) it's a hexagonal lattice, and for \(\dynkin{B}{2}\) it's again a square lattice, \(\integers^2\) (but scaled differently to \(\dynkin{A}{1} \oplus \dynkin{A}{1}\)).
    
    \begin{crl}{}{}
        The Verma module, \(M_\lambda\), has a weight decomposition.
        In this weight decomposition the weight lattice is \(P = \lambda - \integers \Phi\), and the \(\lambda\)-weight eigenspace of \(M_\lambda\) is one-dimensional, further, all weight subspaces are finite dimensional.
    \end{crl}
    
    We are now ready to give the result which links \(M_\lambda\) to the finite-dimensional representations.
    
    \begin{prp}{Universal Property of Verma Modules}{}
        Let \(\lie{g}\) be a semisimple Lie algebra and use notation as above.
        If \(V\) is a \(\lie{g}\)-module and \(v \in V\) is a highest weight vector (\(h \action v = \lambda(h)v\) for \(h \in \lie{h}\) and \(e_i \action v = 0\)) then there exists a unique homomorphism \(\varphi \colon M_\lambda \to V\) such that \(\eta(v_\lambda) = v\) where \(v_\lambda \in M_\lambda\) is the highest weight element of the Verma module \(M_\lambda\).
        In particular, if such a nonzero \(v\) generates \(V\), that is \(V\) is a highest weight representation with weight vector \(v\), then \(V\) is a quotient of \(M_\lambda\).
    \end{prp}
    
    The above result says that \(M_\lambda\) is universal amongst highest weight representations of \(\lie{g}\).
    Any map into any highest weight representation, \(V\), can be achieved by first mapping into \(M_\lambda\), then mapping into \(V\) in a unique way (using \(\varphi\)).
    
    \begin{prp}{}{}
        Every highest weight representation has a weight decomposition into finite-dimensional weight subspaces.
    \end{prp}
    
    So, every highest weight module is a quotient of the Verma module.
    It turns out that only one of these quotients is irreducible.
    
    \begin{prp}{}{}
        For every \(\lambda \in \lie{h}^*\) the Verma module, \(M_\lambda\), has a unique simple quotient, \(L_\lambda\).
        Further, \(L_\lambda\) arises as a quotient of any highest weight \(\lie{g}\)-module with highest weight \(\lambda\).
    \end{prp}
    
    The idea of the above is that as long as we never include \(v_\lambda\) in any submodule of \(M_\lambda\) we never get all of \(M_\lambda\), and so we can sum all proper submodules of \(M_\lambda\), and we know that the result will still be a proper submodule.
    We can then quotient by this sum, and the result is \(L_\lambda\), we've quotiented out all submodules which could appear, and thus \(L_\lambda\) is simple.
    
    \begin{crl}{}{}
        Simple highest weight \(\lie{g}\)-modules (for \(\lie{g}\) a semisimple Lie algebra over \(\complex\)) are classified by their highest weight, \(\lambda \in \lie{h}^*\), by the bijection \(\lambda \mapsto L_\lambda\).
    \end{crl}
    
    
    \chapter{Braids, Knots, and Hecke Algebras}
    \section{The Pure Braid Group}
    We start with a technical definition, assuming the reader is familiar with the notion of a braid group, if not maybe skip the definition and look at the pictures.
    
    \begin{dfn}{Pure Braid Group}{def:pure braid group}
        Let \(M_n = \{(z_1, \dotsc, z_n) \in \complex^n \mid z_i \ne z_j \text{ for } i \ne j\}\), which is a topological space as a subspace of \(\complex^n\).
        The \defineindex{pure braid group}, is the fundamental group, \(\purebraid_n = \pi_1(M_n)\).
    \end{dfn}
    
    A pure braid is then a (homotopy class) continuous function \(\beta \colon [0, 1] \to M_n\) with \(\beta(0) = \beta(1)\), given by \(t \mapsto (\beta_1(t), \dotsc, \beta_n(t))\) where the \(\beta_i\) are continuous functions \([0, 1] \to \complex \setminus \{z_1, \dotsc, \widehat{z_i}, \dotsc, z_n\}\) such that at no \(t \in [0, 1]\) do we have \(\beta_i(t) = \beta_j(t)\).
    
    Let \(\complex_n\) be the \(n\)-punctured complex plane\footnote{We're treating \(\complex\) as a topological space here, the position of the points doesn't matter, we won't use any algebraic properties of this copy of \(\complex\)}.
    Then for a pure braid, \(\beta\), fixing some \(t \in [0, 1]\) we can view \(\beta(t)\) as a choice of \(n\) distinct points in \(\complex\).
    Further, as \(t\) varies these points move around continuously.
    We can draw the whole path by considering \(t \in [0, 1]\) as a third dimension, and considering the positions traced by these points as time goes from \(0\) to \(1\).
    By lining up the punctures we can then project this down onto two dimensions, but keeping track of when a path goes over or under another.
    This gives us the standard picture of a pure braid.
    For example, \cref{fig:pure braid example} shows an element of \(\purebraid_4\).
    
    \begin{figure}
        \centering
        \tikzsetnextfilename{pure-braid-example}
        \begin{tikzpicture}[braid/.cd]
            \pic [thick] at (0, 0) {
                braid={s_1-s_3 s_1 s_2 s_2 s_3}
            };
        \end{tikzpicture}
        \caption[An element of the pure braid group]{An element of the pure braid group, \(\purebraid_4\).}
        \label{fig:pure braid example}
    \end{figure}
    
    The group operation of \(\pi_1(M_n)\) is path concatenation (with rescaling of time so that we still have \(t \in [0, 1]\)).
    The corresponding operation for pure braids is given by taking \(\beta \beta'\) to be given by concatenating the diagram for \(\beta\) below the diagram for \(\beta'\) (reading the braid from the top down we want to do \(\beta'\) first\footnote{The alternative convention gives us a perfectly well defined group, but to match conventions with the symmetric group we want this order.}).
    
    \section{The Braid Group}
    So far we've restricted our pure braids so that if a strand ends at the same puncture it begins at.
    The braid group relaxes this condition.
    
    Let \(M_n\) be as in \cref{def:pure braid group}.
    There is an obvious action of \(S_n\) on \(M_n\) given by permuting elements within a tuple, and this defines an equivalence relation on \(M_n\), in which two tuples are equivalent if they are related by permuting elements.
    Let \(M_n/S_n\) be the quotient of \(M_n\) by this equivalence relation.
    
    \begin{dfn}{Braid Group}{def:braid group}
        The \defineindex{braid group} is \(\braid_n = \pi_1(M_n/S_n)\).
    \end{dfn}
    
    In terms of the pictures of braids the only difference is that we no longer require that braids start and end at the same point.
    See \cref{fig:braid example}.
    The group operation is still concatenation.
    Notice that by tracking where each strand starts and ends we get a permutation, \(w \in S_n\).
    It is always possible to write a braid, \(b \colon [0, 1] \to M_n/S_n\), as a composite, \(b = \beta \circ p\), where \(\beta\) is a pure braid and \(w \in S_n\) is a permutation such that \(\beta(1) = w^{-1}(b(0))\).
    
    \begin{figure}
        \centering
        \tikzsetnextfilename{braid-example}
        \begin{tikzpicture}[braid/.cd]
            \pic [thick] at (0, 0) {
                braid={s_1-s_3 s_1 s_2 s_3 s_3}
            };
        \end{tikzpicture}
        \caption[An element of the braid group]{An element of the braid group, \(\braid_n\). Notice that the strands starting at \(1\), \(2\), \(3\) and \(4\) end at \(1\), \(3\), \(4\), and \(2\) respectively, defining a permutation \(\cycle{2,3,4}\).}
        \label{fig:braid example}
    \end{figure}
    
    \begin{thm}{Artin}{thm:braid group presentation}
        The braid group has the standard presentation
        \begin{equation*}
            \braid_n = \langle \sigma_1, \dotsc, \sigma_{n-1} \mid \sigma_i \sigma_{i + 1} \sigma_i = \sigma_{i + 1} \sigma_i \sigma_{i + 1}, \sigma_i \sigma_j = \sigma_j \sigma_i \text{ for } \abs{i - j} > 1 \rangle.
        \end{equation*}
    \end{thm}
    
    The relationship
    \begin{equation}
        \sigma_i \sigma_{i + 1} \sigma_i = \sigma_{i + 1} \sigma_i \sigma_{i + 1}
    \end{equation}
    is called the \defineindex{braid relation}.
    The identification between this presentation and \(\braid_n\) is pretty simple.
    For simplicity we'll just look at the \(n = 3\) case, but for other values of \(n\) the pictures generalise in the obvious way.
    First, the identity, \(e\), is simply leaving all strands fixed:
    \tikzexternaldisable
    \begin{equation}
        e =\ 
        \tikzsetnextfilename{braid-3-strand-identity} 
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={1}
            };
        \end{tikzpicture}
        \,.
    \end{equation}
    Then \(\sigma_1\) is the braid
    \begin{equation}
        \tikzsetnextfilename{braid-3-strand-sigma1}
        \sigma_1 =\  
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_1}
            };
        \end{tikzpicture}
        \,,
    \end{equation}
    and \(\sigma_1^{-1}\) is given by crossing in the other direction:
    \begin{equation}
        \sigma_1^{-1} =\  
        \tikzsetnextfilename{braid-3-strand-sigma1-inverse}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_1^{-1}}
            };
        \end{tikzpicture}
        \,.
    \end{equation}
    This makes sense, since we then have
    \begin{equation}
        \sigma_1 \sigma_1^{-1} = 
        \tikzsetnextfilename{braid-3-strand-sigma1-sigma1-inverse}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_1 s_1^{-1}}
            };
        \end{tikzpicture}
        \ =\ 
        \tikzsetnextfilename{braid-3-strand-identity-2}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={1 1}
            };
        \end{tikzpicture}
        \ = e.
    \end{equation}
    Similarly, we have
    \begin{equation}
        \sigma_2 =\  
        \tikzsetnextfilename{braid-3-strand-sigma2}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_2}
            };
        \end{tikzpicture}
        \qand \sigma_2^{-1} =\  
        \tikzsetnextfilename{braid-3-strand-sigma2-inverse}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_2^{-1}}
            };
        \end{tikzpicture}
        \,.
    \end{equation}
    So, \(\sigma_i\) means passing the \(i\)th strand over strand \(i + 1\), and \(\sigma_i^{-1}\) means passing the \(i\)th strand \emph{under} strand \(i + 1\).
    
    The braid relation in this case tells us that
    \begin{equation}
        \sigma_1 \sigma_2 \sigma_1 = \sigma_2 \sigma_1 \sigma_2,
    \end{equation}
    which is just the following picture:
    \begin{equation}  
        \tikzsetnextfilename{braid-relation-lhs}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_1 s_2 s_1}
            };
        \end{tikzpicture}
        \ =\ 
        \tikzsetnextfilename{braid-relation-rhs}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_2 s_1 s_2}
            };
        \end{tikzpicture}
        \,.
    \end{equation}
    For \(n = 3\) we never have \(\abs{i - j} > 1\), so let's look at \(n = 4\), where this relation simply tells us that \enquote{sufficiently separated} swaps commute:
    \begin{equation}
        \tikzsetnextfilename{braid-commuting-swaps-lhs}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_1 s_3}
            };
        \end{tikzpicture}
        \ =\ 
        \tikzsetnextfilename{braid-commuting-swaps-lhs}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_3 s_1}
            };
        \end{tikzpicture}
        .
    \end{equation}
    
    So far, we've just been looking at braids and deciding if they're equal if they intuitively give the same picture after rearranging strands without passing them through each other.
    This can be made rigorous as follows.
    
    First, we define the \define{Reidemeister moves}\index{Reidemeister move} of types II and III.
    These are \enquote{local} operations on braids, in that we can apply them to any portion of the diagram without changing the rest of the diagram.
    To represent this we use a dashed circle to \enquote{zoom in} on just a portion of the diagram:
    The Reidemeister move of type II is
    \begin{equation}
        \tikzsetnextfilename{reidemeister-II}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [dashed] (0, 0) circle [radius=2];
            \draw [thick, rounded corners=20] (135:2) -- (1, 0) -- (-135:2);
            \draw [line width=2mm, white, rounded corners=20] (45:2) -- (-1, 0) -- (-45:2);
            \draw [thick, rounded corners=20] (45:2) -- (-1, 0) -- (-45:2);
            \node at (2.5, 0) {\(\xleftrightarrow{\symrm{II}}\)};
            \begin{scope}[xshift=5cm]
                \draw [dashed] (0, 0) circle [radius=2];
                \draw [thick, rounded corners=20] (135:2) -- (-0.5, 0) -- (-135:2);
                \draw [line width=1.5mm, white, rounded corners=20] (45:2) -- (0.5, 0) -- (-45:2);
                \draw [thick, rounded corners=20] (45:2) -- (0.5, 0) -- (-45:2);
            \end{scope}
        \end{tikzpicture}
        \,.
    \end{equation}
    The Reidemeister move of type III is
    \begin{equation}
        \tikzsetnextfilename{reidemeister-III}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [dashed] (0, 0) circle [radius=2];
            \draw [thick] (pi/12 r:2) -- (14*pi/12 r:2);
            \draw [line width=1.5mm, white] (7*pi/12 r:2) -- (17*pi/12 r:2);
            \draw [thick] (7*pi/12 r:2) -- (17*pi/12 r:2);
            \draw [line width=1.5mm, white] (10*pi/12 r:2) -- (23*pi/12 r:2);
            \draw [thick] (10*pi/12 r:2) -- (23*pi/12 r:2);
            \node at (2.5, 0) {\(\xleftrightarrow{\symrm{III}}\)};
            \begin{scope}[xshift=5cm]
                \draw [dashed] (0, 0) circle [radius=2];
                \draw [thick] (13*pi/12 r:2) -- (2*pi/12 r:2);
                \draw [line width=1.5mm, white] (5*pi/12 r:2) -- (19*pi/12 r:2);
                \draw [thick] (5*pi/12 r:2) -- (19*pi/12 r:2);
                \draw [line width=1.5mm, white] (11*pi/12 r:2) -- (22*pi/12 r:2);
                \draw [thick] (11*pi/12 r:2) -- (22*pi/12 r:2);
            \end{scope}
        \end{tikzpicture}
        \,.
    \end{equation}
    These are simply capturing the fact that we want \(\sigma_i^{-1} \sigma_i = e\) and the braid relation.
    
    \begin{remark}{}{}
        The Reidemeister moves first arose in knot theory, in which there is a third Reidemenster move, move number I, which is
        \begin{equation}
            \tikzsetnextfilename{reidemeister-I}
            \begin{tikzpicture}[baseline=(current bounding box)]
                \draw [dashed] (0, 0) circle [radius=1];
                \draw [thick, rounded corners] (0, -1) -- ++ (0, 0.75) -- ++ (0.5, 0.5) -- ++ (0, -0.25) coordinate (A);
                \draw [thick, rounded corners] (A) -- ++ (0, -0.25) -- ++ (-0.5, 0.5) -- ++ (0, 0.75);
                \draw [line width=1mm, azure(web)(azuremist)!45, rounded corners] (0, -1) -- ++ (0, 0.75) -- ++ (0.5, 0.5) -- ++ (0, -0.25) coordinate (A);
                \draw [thick, rounded corners] (0, -1) -- ++ (0, 0.75) -- ++ (0.5, 0.5) -- ++ (0, -0.25) coordinate (A);
                \node at (1.5, 0) {\(\xleftrightarrow{\symrm{I}}\)};
                \begin{scope}[xshift=3cm]
                    \draw [dashed] (0, 0) circle [radius=1];
                    \draw [thick] (0, -1) -- (0, 1);
                \end{scope}
            \end{tikzpicture}
            \,
        \end{equation}
        We don't consider this as strands in braids aren't allowed to loop back up.
    \end{remark}
    
    \begin{prp}{}{}
        Two braids are the same if and only if they are related by an isotopy and a sequence of Reidemeister moves of type II and III.
    \end{prp}
    
    \begin{remark}{}{}
        In physics a braid describes the adiabatic exchange of indistinguishable quasi particles in two dimensions.
        This is important in, for example, the fractional quantum Hall effect.
        This idea has applications to quantum computing.
        Particles whose exchange is governed by the braid group are called \emph{anyons} (cf.\@ bosons and whose exchange is governed by the trivial and antisymmetric representations of the symmetric group).
    \end{remark}
    
    \begin{remark}{}{}
        From a geometric view point \(\braid_n\) is the \enquote{mapping class group} of the punctured disc with \(n\) points.
        We swap the plane to a disc just because it's nicer to work with compact things, and we're not allowing punctures at infinity anyway.
        
        The mapping class group is defined as follows.
        Let \(S\) be a surface, and \(Q \subset S\) a finite set of marked points.
        Denote by \(\Homeo(S, Q)\) the group of homeomorhpisms of \(S\) which fix \(Q\) as a set and fix the boundary pointwise.
        That is, \(\varphi \in \Homeo(S, Q)\) is such that for every marked point, \(p\), there is some marked point \(q\) (not necessarily distinct) such that \(\varphi(p) = q\), and for every boundary point, \(x\) we have \(\varphi(x) = x\).
        The \defineindex{mapping class group} of the marked surface \((S, Q)\) is
        \begin{equation}
            \symrm{Mod}(S, Q) = \Homeo^+(S, Q) / \Homeo_0(S, Q)
        \end{equation}
        where \(\Homeo^+(S, Q)\) denotes the collection of orientation preserving homeomorphisms in \(\Homeo(S, Q)\), and \(\Homeo_0(S, Q)\) denotes the connected component of \(\Homeo(S, Q)\) containing the identity (in the compact-open topology).
        
        This group is also sometimes called the modular group, hence the notation \(\symrm{Mod}(S, Q)\).
        This is because when we take the torus with no marked points the mapping class group ends up being isomorphic to the modular group, \(\specialLinear_2(\integers)\).
        
        When we say that \(\braid_n\) is the mapping class group of the disc with \(n\) punctured points we mean that if \(D\) is this punctured disc and \(Q\) is our set of punctures then there is an isomorphism \(\braid_n \to \symrm{Mod}(D, Q)\) given by \(\sigma_i \mapsto H_i\) where \(\sigma_i\) is a generator of the braid group in the standard presentation and \(H_i\) is the homeomorphism of the \(n\)-punctured disc given by a half twist exchanging the points numbered \(i\) and \(i + 1\).
        See \cref{fig:half twist of the disc}.
    \end{remark}
    
    \begin{figure}
        \centering
        \tikzsetnextfilename{disc-half-twist}
        \begin{tikzpicture}
            \draw (0, 0) circle [radius = 2cm];
            \draw [dashed] (-2, 0) -- ++ (4, 0);
            \fill [highlight] (-0.5, 0) circle [radius=0.1];
            \fill [highlight!40] (0.5, 0) circle [radius=0.1];
            \draw [|->] (2.5, 0) -- ++ (1, 0) node [midway, above] {\(H_i\)};
            \draw (6, 0) circle [radius = 2cm];
            \draw [dashed, use Hobby shortcut] (4, 0) .. (6, 1) .. (7, 0.1) .. (6.5, 0) .. (5.5, 0) .. (5, -0.1) .. (6, -1) .. (8, 0);
            \fill [highlight!40] (5.5, 0) circle [radius=0.1];
            \fill [highlight] (6.5, 0) circle [radius=0.1];
        \end{tikzpicture}
        \caption[Half twist of the disc]{Half twist of the disc exchanging \(i\) and \(i + 1\). The dashed line shows some curve and its image under \(H_i\).}
        \label{fig:half twist of the disc}
    \end{figure}
    
    \section{Coxeter Groups}
    \begin{dfn}{Coxeter System}{}
        Let \(S\) be a finite set.
        A \define{Coxeter matrix}\index{Coxeter!matrix} for \(S\) is a symmetric matrix \(M = (m_{st})_{s,t \in S}\) such that
        \begin{enumerate}
            \item \(m_{ss} = 1\) for all \(s \in S\);
            \item \(m_{st} \in \{2, 3, \dotsc\} \cup \{\infty\}\) for all distinct \(s, t \in S\).
        \end{enumerate}
        We call \((S, M)\) a \define{Coxeter system}\index{Coxeter!system}.
    \end{dfn}
    
    For example, take \(S = \{1, \dotsc, 4\}\) and
    \begin{equation}
        \label{eqn:S5 coxeter matrix}
        M = 
        \begin{pmatrix}
            1 & 3 & 2 & 2\\
            3 & 1 & 3 & 2\\
            2 & 3 & 1 & 3\\
            2 & 2 & 3 & 1
        \end{pmatrix}
        .
    \end{equation}
    
    \begin{dfn}{Coxeter Group}{}
        Given a Coxeter system, \((S, M)\), the corresponding \define{Coxeter group}\index{Coxeter!group}, is the pair \((W, S)\), where \(W\) is the group given by the presentation
        \begin{equation}
            W \coloneq \langle s \in S \mid (st)^{m_{st}} = 1 \forall s, t \in S \rangle.
        \end{equation}
        The \defineindex{rank} of \(W\) is defined to be \(\abs{S}\).
    \end{dfn}
    
    Notice that if we take \(s = t\) then we have \(m_{ss} = 1\), so \((ss)^{m_{ss}} = s^2\), and so in a Coxeter group we always have \(s^2 = 1\), and hence \(s^{-1} = s\) for all generators.
    If \(m_{st} = 2\) then \((st)^2 = 1\), which means that \(st = ts\), since \(s^{-1} = s\) and \(t^{-1} = t\).
    Thus, a \(2\) in the matrix means that the corresponding generators commute.
    More generally, we can always take the equation \((st)^{m_{st}} = stst \dotsm st = 1\), which has \(2m_{st}\) factors, and rearrange to get
    \begin{equation}
        \underbrace{sts \dotsm}_{m_{st} \text{ terms}} = \underbrace{tst \dotsm}_{m_{st} \text{ terms}}.
    \end{equation}
    For example, when \(m_{st} = 3\) this relation is
    \begin{equation}
        sts = tst.
    \end{equation}
    We call this the \defineindex{braid relation}.
    
    Consider the Coxeter system \((\{s_1, s_2, s_3, s_4\}, M)\) with \(M\) given by \cref{eqn:S5 coxeter matrix}.
    The corresponding Coxeter group is
    \begin{equation}
        \label{eqn:symmetric group presentation}
        W = \langle s_1, s_2, s_3, s_4 \mid s_i^2 = 1, s_is_{i+1}s_i = s_{i+1}s_is_{i+1}, s_is_j = s_js_i \text{ for } \abs{i - j} > 1 \rangle.
    \end{equation}
    We can recognise this as a presentation of \(S_5\), where \(s_i = \cycle{i,i+1}\).
    This presentation generalises fully to \(S = \{s_1, \dotsc, s_{n-1}\}\) to give the Coxeter presentation of \(S_n\).
    
    \begin{dfn}{}{}
        Let \((W, S)\) be a Coxeter group.
        Given \(w \in W\) we can write \(w\) as \(s_{i_1} \dotsm s_{i_k}\), with \(s_i \in S\).
        We say that this is a \defineindex{reduced expression} for \(w\) if \(k\) is minimal, and we define \(\ell(w) = k\) to be the \defineindex{length} of \(w\).
    \end{dfn}
    
    \begin{thm}{Matsumoto}{}
        Let \(W\) be a group generated by \(S = \{s_1, \dotsc, s_n\}\) subject to some relations, such that \(s_i^2 = 1\).
        Then the following are equivalent:
        \begin{itemize}
            \item \((W, S)\) is a Coxeter group.
            \item Any two reduced expressions for \(w \in W\) can be transformed into each other by a series of braid relations.
        \end{itemize}
    \end{thm}
    
    Note that two Coxeter groups may be isomorphic as groups, but not as Coxeter groups, since if they have different generating sets and/or relations then we consider them to be different as Coxeter groups, but not as groups.
    The problem of producing a general algorithm to decide if two Coxeter systems produce isomorphic groups is unsolved.
    A related open problem is deciding, given \(W\), what subset, \(S\), and relations can we take to make \((W, S)\) a Coxeter group.
    
    \subsection{Classification of Coxeter Groups}
    \begin{dfn}{Coxeter Diagram}{}
        Let \((S, M)\) be a Coxeter system.
        The corresponding \define{Coxeter diagram}\index{Coxeter!diagram} is constructed as follows:
        \begin{itemize}
            \item The vertices are elements of \(S\);
            \item If \(m_{st} < 3\) there are no edges between \(s\) and \(t\);
            \item If \(m_{st} = 3\) there is an unlabelled edge between \(s\) and \(t\);
            \item If \(m_{st} \ge 4\) there is an edge between \(s\) and \(t\) labelled with \(m_{st}\).
        \end{itemize}
    \end{dfn}
    
    For example, if we take \(M\) as in \cref{eqn:S5 coxeter matrix} then the corresponding Coxeter graph has as vertices \(\{s_1, s_2, s_3, s_4\}\), and there are edges connecting \(s_1\) to \(s_2\), \(s_2\) to \(s_3\), and \(s_3\) to \(s_4\), so the graph is
    \begin{equation}
        \tikzsetnextfilename{coxeter-A4}
        \begin{tikzpicture}[font=\small]
            \fill (0, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_1}\)};
            \fill (1, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_2}\)};
            \fill (2, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_3}\)};
            \fill (3, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_4}\)};
            \draw (0, 0) -- (3, 0);
        \end{tikzpicture}
    \end{equation}
    
    \begin{remark}{}{}
        Notice that this is exactly the Dynkin diagram \(\dynkin{A}{4}\).
        Dynkin diagrams and Coxeter diagrams are very closely related.
        There are some distinctions though: Dynkin diagrams can be directed, Coxeter graphs can be labelled, Dynkin diagrams can have multiple edges between a pair of vertices.
        
        Both Dynkin and Coxeter diagrams encode a matrix, Cartan and Coxeter respectively, which encode some properties of an algebraic structure, a Lie algebra and group respectively.
    \end{remark}
    
    \begin{thm}{Classification of Coxeter Groups}{}
        Every Coxeter system has a Coxeter diagram falling into one of the following infinite families,
        \begin{gather}
            \tikzsetnextfilename{coxeter-An}
            \begin{tikzpicture}[font=\small]
                \node at (-1, 0) {\normalsize\(\dynkin{A}{n}\)};
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_1}\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_2}\)};
                \fill (2, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_3}\)};
                \fill (3, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_4}\)};
                \draw (0, 0) -- (3, 0);
                \draw [dashed] (3, 0) -- ++ (1, 0);
                \fill (4, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_{n-2}}\)};
                \fill (5, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_{n-1}}\)};
                \fill (6, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_n}\)};
                \draw (4, 0) -- (6, 0);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-BnCn}
            \begin{tikzpicture}[font=\small]
                \node at (-1, 0) {\normalsize\(\dynkin{B}{n} = \dynkin{C}{n}\)};
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_1}\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_2}\)};
                \fill (2, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_3}\)};
                \fill (3, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_4}\)};
                \draw (0, 0) -- (3, 0);
                \draw [dashed] (3, 0) -- ++ (1, 0);
                \fill (4, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_{n-2}}\)};
                \fill (5, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_{n-1}}\)};
                \fill (6, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_n}\)};
                \draw (4, 0) -- (6, 0) node [pos=0.75, above] {\(4\)};
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-Dn}
            \begin{tikzpicture}[font=\small]
                \node at (-1, 0) {\normalsize\(\dynkin{D}{n}\)};
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_1}\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_2}\)};
                \fill (2, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_3}\)};
                \fill (3, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_4}\)};
                \draw (0, 0) -- (3, 0);
                \draw [dashed] (3, 0) -- ++ (1, 0);
                \fill (4, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_{n-3}}\)};
                \fill (5, 0) circle [radius=0.075cm] node [right] {\(s\mathrlap{_{n-2}}\)};
                \fill[xshift=5cm, shift={(45:1)}] coordinate (A) circle [radius=0.075cm] node [right] {\(s\mathrlap{_{n-1}}\)};
                \fill[xshift=5cm, shift={(-45:1)}] coordinate (B) circle [radius=0.075cm] node [right] {\(\alpha\mathrlap{_n}\)};
                \draw (4, 0) -- (5, 0);
                \draw (5, 0) -- (A);
                \draw (5, 0) -- (B);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-I2}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{I}{2}(m)\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \draw (0, 0) -- (1, 0) node [midway, above] {\(m\)};
            \end{tikzpicture}
        \end{gather}
        or is one of the following exceptional cases,
        \begin{gather}
            \tikzsetnextfilename{coxeter-G2}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{G}{2}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \draw (0, 0) -- (1, 0) node [midway, above] {\(6\)};
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-H2}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{H}{2}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \draw (0, 0) -- (1, 0) node [midway, above] {\(5\)};
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-H3}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{H}{3}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \draw (0, 0) -- (2, 0) node [pos=0.25, above] {\(5\)};
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-F4}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{F}{4}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \fill (3, 0) circle [radius=0.075cm];
                \draw (0, 0) -- ++ (3, 0) node [midway, above] {\(4\)};
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-E6}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{E}{6}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \fill (3, 0) circle [radius=0.075cm];
                \fill (4, 0) circle [radius=0.075cm];
                \fill (2, 1) circle [radius=0.075cm];
                \draw (0, 0) -- (4, 0);
                \draw (2, 0) -- (2, 1);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-E7}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{E}{7}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \fill (3, 0) circle [radius=0.075cm];
                \fill (4, 0) circle [radius=0.075cm];
                \fill (5, 0) circle [radius=0.075cm];
                \fill (2, 1) circle [radius=0.075cm];
                \draw (0, 0) -- (5, 0);
                \draw (2, 0) -- (2, 1);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-E8}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{E}{8}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \fill (3, 0) circle [radius=0.075cm];
                \fill (4, 0) circle [radius=0.075cm];
                \fill (5, 0) circle [radius=0.075cm];
                \fill (6, 0) circle [radius=0.075cm];
                \fill (2, 1) circle [radius=0.075cm];
                \draw (0, 0) -- (6, 0);
                \draw (2, 0) -- (2, 1);
            \end{tikzpicture}
        \end{gather}
    \end{thm}
    
    Note that \(\dynkin{G}{2} = \dynkin{I}{2}(6)\) and \(\dynkin{H}{2} = \dynkin{I}{2}(5)\).
    It is also common to see \(\dynkin{I}{2}(m)\) written as \(\dynkin{I}{m}\), but this is bad notation as \(\dynkin{I}{2}(m)\) is rank \(2\), not \(m\).
    We have both \(\dynkin{B}{n}\) and \(\dynkin{C}{n}\) because in generalisations (such as affine Coxeter groups) these become different.
    Sometimes we write \(\dynkin{BC}{n}\) for these finite-type diagrams which are the same.
    
    For many of these the corresponding Coxeter group has quite a nice interpretation:
    \begin{itemize}
        \item \(\dynkin{A}{n}\) is the symmetric group \(S_{n + 1}\), it can also be thought of as the symmetries of the \(n\)-simplex.
        \item \(\dynkin{B}{n}\) is the symmetries of the \(n\)-cube, and can be thought of as signed permutations of \(\{-n, \dotsc, n\}\), this group is \(S_n \wr \integers/2\integers = S_n \ltimes (\integers/2\integers)^n\).
        \item \(\dynkin{D}{n}\) is a subgroup of the symmetries of the \(n\)-cube consisting only of signed permutations which fix the number of minus signs.
        \item \(\dynkin{I}{2}(m)\) is the symmetries of the regular \(m\)-gon (that is, the dihedral group of order \(2m\)).
        \item \(\dynkin{A}{3}\), \(\dynkin{B}{3}\), and \(\dynkin{H}{3}\) are symmetries of the platonic solids (including reflections), being the tetrahedron, cube/octahedron, and dodecahedron/icosahedron symmetry groups respectively.
    \end{itemize}
    
    \subsection{Reflection Groups}
    A \defineindex{reflection} of a Euclidean space, \(V\), with inner product \(\innerprod{-}{-}\), is a linear transformation, \(s \colon V \to V\) such that there exists some \(\alpha \in V\) with \(s(\alpha) = -\alpha\) and \(s\) fixes the hyperplane perpendicular to \(\alpha\), \(H_\alpha = (\reals\alpha)^{\perp}\), pointwise.
    A \defineindex{reflection group} is a group which is (isomorphic to) a subgroup of \(\orthogonal(V)\) generated only by reflections.
    It turns out that every Coxeter group can be viewed as a reflection group.
    
    Let \(V\) be a finite-dimensional Euclidean space, and \(\symcal{H}\) a finite collection of hyperplanes.
    Removing these hyperplanes from \(V\) we get \(V \setminus \bigcup_{H \in \symcal{H}}H\).
    We call the connected components of this space \define{alcoves}\index{alcove}.
    For example, if we take \(V = \reals^2\), the plane, then a hyperplane is just a line.
    If we take two non-parallel lines in the plane then they split the plane into four segments, which are our alcoves.
    
    Fix some alcove, \(A\), in \(V\), then define
    \begin{equation}
        S_A = \{s_H \mid s_H \text{ is a reflection in } H \text{ with } H \in \symcal{H} \text{ bounding } A\}.
    \end{equation}
    Then if we take \(W\) to be the group generated by such reflections then \((W, S_A)\) is a Coxeter group.
    
    Conversely, if we're given a Coxeter system, \((W, S)\), then we can define a Euclidean space,
    \begin{equation}
        V = \reals S \isomorphic \bigoplus_{s \in S} \reals \vv{s}
    \end{equation}
    where we're defining a basis vector, \(\vv{s}\), for each \(s \in S\).
    More abstractly, \(V\) is the free vector space on \(S\).
    The inner product on this space is given on this basis by
    \begin{equation}
        \innerprod{\vv{s}}{\vv{t}} = \cos\left( \frac{\pi}{m_{st}} \right).
    \end{equation}
    This is always positive because \(\pi / m_{st} \in [0, \pi]\) (defining \(\pi/\infty = 0\)), and it's symmetric because \(M\) is a symmetric matrix.
    We can then define a reflection in the hyperplane orthogonal to \(\vv{s}\) by
    \begin{equation}
        \sigma_{\vv{s}}(\vv{v}) = \vv{v} - 2\innerprod{\vv{v}}{\vv{s}} \vv{s}
    \end{equation}
    for all \(\vv{v} \in V\).
    
    \begin{prp}{Tits Representation}{}
        With the notation above the map \(W \to \generalLinear(V)\) defined by \(s \mapsto \sigma_{\vv{s}}\) is a faithful representation, and \(\innerprod{-}{-}\) is \(W\)-invariant.
    \end{prp}
    
    \subsection{Generalisations}
    If we remove the requirement that \(s_i^2 = 1\) (equivalently allowing the diagonals to not be \(1\)), then the group that we get is called an \defineindex{Artin group}.
    Matsumoto's theorem then states that two words in a Coxeter group are related only by the relations of the corresponding Artin group.
    Examples of Artin groups include all Coxeter groups, as well as the braid groups.
    Some other examples are \(\langle S \mid st = ts\, \forall s, t \in S\rangle\), which is the free abelian group on \(S\), and \(\langle S \rangle\), the free group on \(S\).
    This suggests the following generalisation of the braid group.
    
    \begin{dfn}{Artin Braid Group}{}
        The \defineindex{Artin braid group} of a Coxeter group, \((W, S)\), is
        \begin{equation}
            \braid_W = \langle \{b_s \mid s \in S\} \mid \underbrace{b_s b_t b_s \dotsm}_{m_{st}} = \underbrace{b_t b_s b_t \dotsm}_{m_{st}} \rangle.
        \end{equation}
    \end{dfn}
    
    Note that the braid group \(\braid_n\) as defined in \cref{def:braid group} is simply \(\braid_{S_n}\), as can be seen by the standard presentation (\cref{thm:braid group presentation}) which mirrors the relations of the \(S_n\) presentation (\cref{eqn:symmetric group presentation}) after removing the condition that \(s_i^2 = 1\).
    
    \begin{prp}{Burau}{}
        Let \(\Lambda = \integers[t, t^{-1}]\).
        There is a group homomorphism
        \begin{equation}
            \rho_n \colon \braid_n \to \generalLinear_n(\Lambda)
        \end{equation}
        given by
        \begin{equation}
            \rho_n(b_i) = 
            \begin{pmatrix}
                1\\
                & \ddots \\
                & & 1 \\
                & & & 1 - t & t\\
                & & & 1 & 0 \\
                & & & & & 1\\
                & & & & & & \ddots\\
                & & & & & & & 1
            \end{pmatrix}
        \end{equation}
        where \(1 - t\) appears in position \((i, i)\).
        This representation of \(\braid_n\) is known as the \defineindex{Burau representation}.
    \end{prp}
    
    For example, if \(n = 3\) then we have
    \begin{equation}
        \rho_3(b_1) = B_1 = 
        \begin{pmatrix}
            1 - t & t & 0\\
            1 & 0 & 0\\
            0 & 0 & 1
        \end{pmatrix}
        , \qand \rho_3(b_2) = B_2 = 
        \begin{pmatrix}
            1 & 0 & 0\\
            0 & 1 - t & t\\
            0 & 1 & 0
        \end{pmatrix}
        .
    \end{equation}
    One can then check that these two matrices satisfy \(B_1B_2B_1 = B_2B_1B_2\).
    
    \begin{remark}{}{}
        For \(n \le 3\) the Burau representation is faithful (as the above shows in the \(n = 3\) case, the \(n = 2\) case is trivially faithful).
        For \(n \ge 5\) the Burau representation is not faithful.
        For \(n = 4\) faithfulness is unknown, but it is known that \(\rho_4\) is a faithful representation if the Jones polynomial detects the unknot, an open question.
        See \cref{sec:knots}.
        
        An interpretation of the Burau representation, due to Jones, is as follows.
        Picture a braid, \(b \in \braid_n\) as a bowling alley where each strand becomes a lane.
        Throw a bowling ball down one lane.
        Upon crossing over a lane below have the ball fall down to the lower lane with probability \(t\) (so it remains in the upper lane with probability \(1 - t\)).
        Then the entry in position \((i, j)\) of \(\rho_n(b)\) is the probability that a ball thrown initially down lane \(i\) ends up in lane \(j\).
    \end{remark}
    
    For \(n > 2\) the Burau representation is not irreducible, it admits a one-dimensional invariant subspace.
    Taking the quotient of \(\Lambda^n\) by this invariant subspace we we get the reduced Burau representation, which is irreducible.
    
    \begin{prp}{}{}
        Let \(\Lambda = \integers[t, t^{-1}]\).
        For \(n\) and integer greater than \(2\) there is a group homomorphism
        \begin{equation}
            \tilde{\rho}_n \colon \braid_n \to \generalLinear_{n-1}(\Lambda)
        \end{equation}
        given by
        \begin{align}
            \tilde{\rho}_n(b_1) &= 
            \begin{pmatrix}
                -t & 0 & 0\\
                1 & 1 & 0\\
                0 & 0 & I_{n-3}
            \end{pmatrix}
            ,\\
            \tilde{\rho}_n(b_i) &=
            \begin{pmatrix}
                I_{i-2} & 0 & 0 & 0 & 0\\
                0 & 1 & t & 0 & 0\\
                0 & 0 & -t & 0 & 0\\
                0 & 0 & 1 & 1 & 0\\
                0 & 0 & 0 & 0 & I_{n-i-2}
            \end{pmatrix}
            \\
            \tilde{\rho}_n(b_{n-1}) &= 
            \begin{pmatrix}
                I_{n-3} & 0 & 0\\
                0 & 1 & t\\
                0 & 0 & -t
            \end{pmatrix} 
        \end{align}
        where \(2 \le i \le n - 2\).
        This representation of \(\braid_n\) is called the \defineindex{reduced Burau representation}\index{Burau representation!reduced}.
    \end{prp}
    
    \begin{lma}{}{}
        Let \(C\) be the \(n \times n\) matrix
        \begin{equation}
            C =
            \begin{pmatrix}
                1 & 1 & 1 & \dots & 1\\
                0 & 1 & 1 & \dots & 1\\
                0 & 0 & 1 & \dots & 1\\
                \vdots & \vdots & \vdots & \ddots & \vdots\\
                0 & 0 & 0 & \dots & 1
            \end{pmatrix}
            .
        \end{equation}
        Then
        \begin{equation}
            C^{-1} \rho_n(b_i)C = 
            \begin{pmatrix}
                \tilde{\rho}_n(b_i) & 0\\
                X_i & 1
            \end{pmatrix}
        \end{equation}
        where \(X_i\) is the row of length \(n - 1\) which is \((0, \dotsc, 0)\) if \(i \ne n - 1\) and \((0, \dotsc, 0, 1)\) for \(i = n - 1\).
    \end{lma}
    
    \section{Knots}
    \label{sec:knots}
    \begin{dfn}{Knot}{}
        A \defineindex{knot} is an embedding of \(S^1\) in \(\reals^3\), considered up to ambient isotopy.
        A \defineindex{link} is an embedding of \(\bigsqcup_{k=1}^n S^1\), up to ambient isotopy.
    \end{dfn}
    
    We draw knots as their projection onto the plane.
    The simplest case is the unknot which \emph{can}\footnote{It's possible also to draw it with crossings.} be drawn with no crossings, in which case it just looks like what we would normally think of as a circle.
    A knot or link is oriented if we assign an orientation to each copy of \(S^1\).
    Similarly, any braid can be oriented by declaring that all strands are oriented downwards.
    
    Given a braid, \(b\), its closure is given by joining the strand starting at position \(i\) to the strand ending at position \(i\) without introducing any new crossings.
    The result is a link.
    This is shown in \cref{fig:closure of a braid}.
    
    \begin{figure}
        \centering
        \tikzsetnextfilename{closing-a-braid}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \node [minimum width=3.2cm, minimum height=2cm, draw] (b) at (0, 0) {\(b\)};
            \draw [thick] ($(b.north) + (-1, 0)$) -- ++ (0, 1);
            \draw [thick] ($(b.north) + (-0.5, 0)$) -- ++ (0, 1);
            \node at ($(b.north) + (0, 0.5)$) {\(\dots\)};
            \draw [thick] ($(b.north) + (1, 0)$) -- ++ (0, 1);
            \draw [thick] ($(b.north) + (0.5, 0)$) -- ++ (0, 1);
            \draw [thick] ($(b.south) + (-1, 0)$) -- ++ (0, -1);
            \draw [thick] ($(b.south) + (-0.5, 0)$) -- ++ (0, -1);
            \node at ($(b.south) + (0, -0.5)$) {\(\dots\)};
            \draw [thick] ($(b.south) + (1, 0)$) -- ++ (0, -1);
            \draw [thick] ($(b.south) + (0.5, 0)$) -- ++ (0, -1);
            \draw [|->] (1.75, 0) -- ++ (2.5, 0) node [midway, above] {Closure};
            \begin{scope}[xshift=6cm]
                \node [minimum width=3.2cm, minimum height=2cm, draw] (b) at (0, 0) {\(b\)};
                \draw [thick] ($(b.north) + (-1, 0)$) -- ++ (0, 1) arc (180:0:2.5) -- ++ (0, -4) arc (0:-180:2.5);
                \draw [thick] ($(b.north) + (-0.5, 0)$) -- ++ (0, 1) arc (180:0:2) -- ++ (0, -4) arc (0:-180:2);
                \node at ($(b.north) + (0, 0.5)$) {\(\dots\)};
                \draw [thick] ($(b.north) + (1, 0)$) -- ++ (0, 1) arc (180:0:0.5) -- ++ (0, -4) arc (0:-180:0.5);
                \draw [thick] ($(b.north) + (0.5, 0)$) -- ++ (0, 1) arc (180:0:1) -- ++ (0, -4) arc (0:-180:1);
                \draw [thick] ($(b.south) + (-1, 0)$) -- ++ (0, -1);
                \draw [thick] ($(b.south) + (-0.5, 0)$) -- ++ (0, -1);
                \node at ($(b.south) + (0, -0.5)$) {\(\dots\)};
                \draw [thick] ($(b.south) + (1, 0)$) -- ++ (0, -1);
                \draw [thick] ($(b.south) + (0.5, 0)$) -- ++ (0, -1);
                \node at (3, 1.5) {\(\dots\)};
                \node at (3, -1.5) {\(\dots\)};
            \end{scope}
        \end{tikzpicture}
        \caption{The closure of a braid, \(b\), to produce a link.}
        \label{fig:closure of a braid}
    \end{figure}
    
    For example, in \(\braid_2\) the closure of \(\sigma_1\) is the unknot,
    \begin{equation}
        \tikzsetnextfilename{braid-closure-of-sigma-1}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box)]
            \pic [thick] (a) {
                braid={s_1}
            };
            \draw [thick] (a-1-s) arc (180:0:1) coordinate (b) -- (b |- a-1-e) arc (0:-180:1);
            \draw [thick] (a-2-s) arc (180:0:0.3) coordinate (b) -- (b |- a-1-e) arc (0:-180:0.3);
            \node (equal) at ($(a-1-s)!0.5!(a-1-e) + (2, 0)$) {\(=\)};
            \draw [thick] ($(equal) + (2, 0)$) circle [radius=1.5];
        \end{tikzpicture}
        \,,
    \end{equation}
    the closure of \(\sigma_1\sigma_1^{-1} = e\) is the two component unlink,
    \begin{equation}
        \tikzsetnextfilename{braid-closure-of-identity}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=2]
            \pic [thick] (a) {
                braid={s_1 s_1^{-1}}
            };
            \draw [thick] (a-1-s) arc (180:0:1) coordinate (b) -- (b |- a-1-e) arc (0:-180:1);
            \draw [thick] (a-2-s) arc (180:0:0.3) coordinate (b) -- (b |- a-1-e) arc (0:-180:0.3);
            \node (equal) at ($(a-1-s)!0.5!(a-1-e) + (2.5, 0)$) {\(=\)};
            \draw [thick] ($(a-1-s) + (3, 0)$) -- ($(a-1-e) + (3, 0)$) arc (-180:0:1) coordinate (c) -- (c |- a-1-s) arc (0:180:1);
            \draw [thick] ($(a-2-s) + (3, 0)$) -- ($(a-2-e) + (3, 0)$) arc (-180:0:0.3) coordinate (c) -- (c |- a-2-s) arc (0:180:0.3);
            \draw [thick] ($(a-1-s) + (4, 0)$) -- ($(a-1-e) + (4, 0)$);
            \node (equal2) at ($(a-1-s)!0.5!(a-1-e) + (5.5, 0)$) {\(=\)};
            \draw [thick] ($(equal2) + (1.5, 0)$) circle [radius=1];
            \draw [thick] ($(equal2) + (4, 0)$) circle [radius=1];
        \end{tikzpicture}
        \,,
    \end{equation}
    and the closure of \(\sigma_1^2\) is the \defineindex{Hopf link},
    \begin{equation}
        \tikzsetnextfilename{braid-closure-of-sigma1-squared}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=2]
            \pic [thick] (a) {
                braid={s_1 s_1}
            };
            \draw [thick] (a-1-s) arc (180:0:1) coordinate (b) -- (b |- a-1-e) arc (0:-180:1);
            \draw [thick] (a-2-s) arc (180:0:0.3) coordinate (b) -- (b |- a-1-e) arc (0:-180:0.3);
            \node (equal) at ($(a-1-s)!0.5!(a-1-e) + (2.5, 0)$) {\(=\)};
            \begin{scope}[shift={($(equal) + (1.5, 0)$)}]
                \draw [thick] (35:1) arc (35:385:1);
                \draw [xshift=1.732cm, thick] (205:1) arc (205:-145:1);
            \end{scope}
        \end{tikzpicture}
        .
    \end{equation}    
    
    \begin{thm}{Alexander}{}
        Any oriented link can be obtained as the closure of an oriented braid.
    \end{thm}
    
    Note that there will be many braids which close to give the same link.
    Thus, there generally multiple ways to take a link and cut it resulting in different braids with the same closure.
    
    In order to consider all links at once it is not sufficient to consider \(\braid_n\) for some fixed number of strands, \(n\).
    Instead we can define the \defineindex{infinite braid group} to be the direct limit
    \begin{equation}
        \braid_{\infty} \coloneq \varinjlim_n \braid_n,
    \end{equation}
    of the directed system given by the obvious inclusions \(\iota \colon \braid_n \to \braid_{n+1}\), adding a strand on the right which doesn't cross any of the \(n-1\) original strands.
    The resulting group can then be thought of as the braid group on an infinite number of strands.
    The way to reason about this is to only consider braids in which a finite number of strands are actually crossing, in which case we can just consider \(\braid_n\) with \(n\) chosen to be sufficiently large\footnote{cf.\@ any element of the ring of symmetric functions can, despite having infinitely many variables, be considered as a polynomial in \enquote{sufficiently many} variables.}.
    
    \begin{dfn}{Markov Moves}{}
        The \defineindex{Markov moves} are certain replacement rules in \(\braid_{\infty}\) which come in two types.
        Consider \(a\) and \(b\) to be elements of \(\braid_{\infty}\), and in particular, suppose that \(b\) has no crossings after string \(n-1\), so can be viewed as an element of \(\braid_n\).
        Type I, conjugation, allows us to replace \(ab\) with \(ba\).
        Type II comes in two subtypes, stabilisation, we can replace \(b\) with \(\iota(b)b_n\) and destabilisation, we can replace \(b\) with \(\iota(b)b_n^{-1}\) where the subscript \(n\) denotes that we start the braid \(b_n\) at strand \(n\).
    \end{dfn}
    
    The ultimate goal of much of knot theory is to compute knot invariants which allow us to distinguish different knots.
    By \enquote{invariants} we mean objects (booleans, numbers, polynomials, vector spaces, etc.) which don't change with an ambient isotopy of the knot, so are the same however we draw the knot.
    By \enquote{distinguish} we mean that these values differ between different knots.
    
    Some knot invariants successfully distinguish all knots, for example the complement of a knot (viewed as a topological space) is a \enquote{complete invariant}.
    Unfortunately, these complete invariants tend to be hard to compute.
    Many more knot invariants fail to distinguish between many distinct knots.
    For example, whether a knot is 3-colourable\footnote{A knot is 3-colourable if each strand of the knot diagram (considering a strand passing under another to be a break in the strand) such that at least two colours are used and at each crossing all three strands are either the same colour or different colours} is an invariant, but only separates knots into one of two classes.
    This is still useful though, it distinguishes the unknot (not 3-colourable) from the trefoil (3-colourable).
    
    Perhaps the most celebrated knots invariants are polynomial invariants.
    The first of these discovered is the Alexander polynomial.
    Conway later showed that a slightly different form of this polynomial admits a description in terms of skein relations.
    Later more knot polynomials, such as the Jones and HOMFLYPT polynomials were found.
    These all have deep connections to representation theory.
    For example, the Jones polynomial can be understood through representations of the quantum group \(U_q(\specialLinearLie_2)\).
    
    \begin{dfn}{Alexander--Conway Polynomial}{}
        Let \(L\) be an oriented link, and let \(b \in \braid_n\) be such that the closure of \(b\) is \(L\).
        Let \(t = q^2\)
        Then the \defineindex{Alexander--Conway polynomial} of \(L\) is the element of \(\integers[t^{\pm 1/2}] = \integers[q^{\pm}]\) given by
        \begin{equation}
            \nabla(L) \coloneq (-1)^{n+1} q^{\deg b} \frac{q - q^{-1}}{q^n - q^{-n}} \det(\tilde{\rho}_n(b) - I_{n-1}).
        \end{equation}
    \end{dfn}
    
    Note that
    \begin{equation}
        [n]_q = \frac{q^n - q^{-n}}{q - q^{-1}}
    \end{equation}
    is the \define{\(\symbf{q}\)-analogue}\index{q-analogue@\(q\)-analogue} \(n\), so there's already some relation to quantum groups occurring here.
    
    \begin{thm}{}{}
        The Alexander--Conway polynomial is uniquely determined on oriented links, \(L\), by the following:
        \begin{enumerate}
            \item \(\nabla(\text{unknot}) = 1\) for either choice of orientation;
            \item \(\nabla(L_+) - \nabla(L_-) = (q^{-1} - q)\nabla(L_0)\), known as the \defineindex{skein relation}.
            Here \(L_+\), \(L_-\), and \(L_0\) are a Conway triple of links which differ only locally by the following:
            \begin{equation}
                \tikzsetnextfilename{skein}
                \begin{tikzpicture}[baseline=(current bounding box), scale=0.6]
                    \node at (-2.75, 0) {\(L_+\):};
                    \draw [dashed] (0, 0) circle [radius=2];
                    \draw [thick, ->] (135:2) -- (-45:2);
                    \draw [line width=1.5mm, black!5] (45:2) -- (-135:2);
                    \draw [thick, ->] (45:2) -- (-135:2);
                    
                    \begin{scope}[xshift=6cm]
                        \node at (-2.75, 0) {\(L_+\):};
                        \draw [thick, ->] (135:2) -- (-45:2);
                        \draw [line width=1.5mm, black!5] (45:2) -- (-135:2);
                        \draw [thick, ->] (45:2) -- (-135:2);
                        \draw [dashed] (0, 0) circle [radius=2];
                    \end{scope}
                    
                    \begin{scope}[xshift=12cm]
                        \node at (-2.75, 0) {\(L_0\):};
                        \draw [dashed] (0, 0) circle [radius=2];
                        \draw [thick, rounded corners=20, ->] (135:2) -- (-0.5, 0) -- (-135:2);
                        \draw [thick, rounded corners=20, ->] (45:2) -- (0.5, 0) -- (-45:2);
                    \end{scope}
                \end{tikzpicture}
            \end{equation}
            Outside of the circle these links will be the same.
        \end{enumerate}
    \end{thm}
    
    The Skein relation gives a way to compute the Alexander--Conway polynomial.
    It will always be possible to use the skein relation to manipulate the polynomial into a combination of polynomials of links for which the Alexander--Conway polynomial is known.
    For example, starting with the two component unlink we can we can perform the following calculation:
    \begin{align}
        \nabla \left(
        \tikzsetnextfilename{alexander-conway-2-link}
        \begin{tikzpicture}[baseline=-0.1cm]
            \draw [thick] (0, 0) circle [radius=0.25];
            \draw [thick] (0.7, 0) circle [radius=0.25];
            \draw [thick, highlight] (45:0.25) arc (45:-45:0.25);
            \draw [thick, highlight, xshift=0.7cm] (135:0.25) arc (135:215:0.25);
            \draw [->] (-0.25, 0.05) -- ++ (0, 0.01);
            \draw [->] (0.95, 0.05) -- ++ (0, 0.01);
        \end{tikzpicture}
        \right) &= \frac{1}{q^{-1} - q} \left[ \nabla \left(
        \tikzsetnextfilename{alexander-conway-figure-8-1}
        \begin{tikzpicture}[baseline=-0.1cm]
            \draw [thick, highlight] (135:0.25) -- ++ (-45:0.5);
            \draw [line width=1.5mm, white] (45:0.25) -- ++ (-135:0.5);
            \draw [thick, highlight] (45:0.25) -- ++ (-135:0.5);
            \draw [thick] (135:0.25) arc (45:325:0.25);
            \draw [thick] (45:0.25) arc (135:-135:0.25);
            \draw [->] (-0.6, 0.05) -- ++ (0, 0.01);
            \draw [->] (0.6, 0.05) -- ++ (0, 0.01);
        \end{tikzpicture}
        \right) - \nabla \left(
        \tikzsetnextfilename{alexander-conway-figure-8-2}
        \begin{tikzpicture}[baseline=-0.1cm]
            \draw [thick, highlight] (45:0.25) -- ++ (-135:0.5);
            \draw [line width=1.5mm, white] (135:0.25) -- ++ (-45:0.5);
            \draw [thick, highlight] (135:0.25) -- ++ (-45:0.5);
            \draw [thick] (135:0.25) arc (45:325:0.25);
            \draw [thick] (45:0.25) arc (135:-135:0.25);
            \draw [->] (-0.6, 0.05) -- ++ (0, 0.01);
            \draw [->] (0.6, 0.05) -- ++ (0, 0.01);
        \end{tikzpicture}
        \right)\right]\\
        &= \frac{1}{q^{-1} - q} \left[
        \nabla \left(
        \tikzsetnextfilename{alexander-conway-unknot-1}
        \begin{tikzpicture}[baseline=-0.1cm]
            \draw [thick] (0, 0) circle [radius=0.25];
            \draw [->] (-0.25, 0.05) -- ++ (0, 0.01);
            \draw [->] (0.25, -0.05) -- ++ (0, -0.01);
        \end{tikzpicture}
        \right)
        - \nabla \left(
        \tikzsetnextfilename{alexander-conway-unknot-2}
        \begin{tikzpicture}[baseline=-0.1cm]
            \draw [thick] (0, 0) circle [radius=0.25];
            \draw [->] (-0.25, 0.05) -- ++ (0, 0.01);
            \draw [->] (0.25, -0.05) -- ++ (0, -0.01);
        \end{tikzpicture}
        \right)
        \right]\\
        &= 0.
    \end{align}
    We can then use this result to compute the Alexander--Conway polynomial of the Hopf link:
    \begin{align}
        \nabla \left(
        \tikzsetnextfilename{alexander-conway-hopf-link}
        \begin{tikzpicture}[baseline=-0.1cm]
            \draw [thick] (40:0.25) arc (40:380:0.25);
            \draw [xshift=1.732*0.25cm, thick] (200:0.25) arc (200:-140:0.25);
            \draw [thick, highlight] (40:0.25) arc (40:60:0.25);
            \draw [thick, highlight] (380:0.25) arc (380:360:0.25);
            \draw [xshift=1.732*0.25cm, thick, highlight] (120:0.25) arc (120:180:0.25);
            \draw [->] (-0.25, 0.05) -- ++ (0, 0.01);
            \draw [->] (0.25+1.732*0.25, 0.05) -- ++ (0, 0.01);
        \end{tikzpicture}
        \right) &=
        \nabla \left(
        \tikzsetnextfilename{alexander-conway-2-link-2}
        \begin{tikzpicture}[baseline=-0.1cm]
            \draw [thick, xshift=1.732*0.25cm] (0, 0) circle [radius=0.25];
            \draw [xshift=1.732*0.25cm, thick, highlight] (120:0.25) arc (120:180:0.25);
            \draw [line width=0.8mm, white] (0, 0) circle [radius=0.25];
            \draw [thick] (0, 0) circle [radius=0.25];
            \draw [->] (-0.25, 0.05) -- ++ (0, 0.01);
            \draw [->] (0.25+1.732*0.25, 0.05) -- ++ (0, 0.01);
            \draw [thick, highlight] (60:0.25) arc (60:0:0.25);
        \end{tikzpicture}
        \right) + (q^{-1} - q) \nabla \left(
        \tikzsetnextfilename{alexander-conway-unknot-twist}
        \begin{tikzpicture}[baseline=0.12cm]
            \draw [thick, rounded corners=1] ({0.1 + 0.2*sqrt(2)/2}, 0) -- ++ (-0.05, 0) -- ++ (135:0.2) -- ++ (-0.05, 0) coordinate (a1);
            \draw [line width=1mm, rounded corners=1, white] (0, 0) -- ++ (0.05, 0) -- ++ (45:0.2) -- ++ (0.05, 0);
            \draw [thick, rounded corners=1] (0, 0) -- ++ (0.05, 0) -- ++ (45:0.2) -- ++ (0.05, 0) coordinate (b1);
            \draw [thick] (a1) arc (270:90:0.05) coordinate (a2) arc (-90:90:0.1) arc (90:270:0.22);
            \draw [thick] (b1) arc (-90:90:0.05) coordinate (b2) arc (270:90:0.1) arc (90:-90:0.22);
            \draw [thick, highlight] (a2) arc (-90:90:0.1);
            \draw [thick, highlight] (b2) arc (270:90:0.1);
            \draw [->] (-0.22, 0.22) -- ++ (0, 0.01);
            \draw [->] (0.46, 0.22) -- ++ (0, 0.01);
        \end{tikzpicture}
        \right)\\
        &= \underbrace{\nabla \left(
            \tikzsetnextfilename{alexander-conway-2-link-3}
            \begin{tikzpicture}[baseline=-0.1cm]
                \draw [thick] (0, 0) circle [radius=0.25];
                \draw [thick] (0.7, 0) circle [radius=0.25];
                \draw [thick, highlight] (45:0.25) arc (45:-45:0.25);
                \draw [thick, highlight, xshift=0.7cm] (135:0.25) arc (135:215:0.25);
                \draw [->] (-0.25, 0.05) -- ++ (0, 0.01);
                \draw [->] (0.95, 0.05) -- ++ (0, 0.01);
            \end{tikzpicture}
            \right)}_{=0}
        + (q^{-1} - q) \underbrace{ \nabla \left(
            \tikzsetnextfilename{alexander-conway-unknot-4}
            \begin{tikzpicture}[baseline=-0.1cm]
                \draw [thick] (0, 0) circle [radius=0.25];
                \draw [->] (-0.25, 0.05) -- ++ (0, 0.01);
                \draw [->] (0.25, -0.05) -- ++ (0, -0.01);
            \end{tikzpicture}
            \right)}_{=1}\\
        &= q^{-1} - q.
    \end{align}
    
    \section{Iwahori--Hecke Algebra}
    In this section we define the Iwahori--Hecke algebra (often just called the Hecke algebra).
    Recall that a given root system, \(\Phi\), has a corresponding Euclidean space, \(E = \Span_{\reals}\Phi\).
    The \defineindex{Weyl group} of this root system, \(W\), is the subgroup of \(\orthogonal(E)\) generated by reflections in the hyperplanes orthogonal to the roots, that is, \(W\) is generated by the \(s_\alpha\).
    Weyl groups are Coxeter groups, but not every Coxeter group is a Weyl group.
    In fact, because root systems are classified by Dynkin diagrams so are Weyl groups, so there are Weyl groups of types \(\dynkin{A}{n}\), \(\dynkin{B}{n}\), \(\dynkin{C}{n}\), \(\dynkin{D}{n}\), \(\dynkin{E}{6}\), \(\dynkin{E}{7}\), \(\dynkin{E}{8}\), \(\dynkin{G}{2}\), and \(\dynkin{F}{4}\).
    
    It is possible to define the Iwahori--Hecke algebra for a general Weyl group, but we will only define it for the type \(\dynkin{A}{n}\) case.
    
    \begin{dfn}{}{}
        The type \(\dynkin{A}{n}\) \defineindex{Iwahori--Hecke algebra}, \(H_n\), is the unital associative \(\rationals(q)\)-algebra with generators \(T_1, \dotsc, T_{n-1}\) and relations
        \begin{enumerate}
            \item \(T_iT_{i+1}T_i = T_{i+1}T_iT_{i+1}\);
            \item \(T_iT_j = T_jT_i\) for \(\abs{i - j} \ge 2\);
            \item \((T_i - q)(T_i + q^{-1}) = 0\).
        \end{enumerate}
    \end{dfn}
    
    \begin{wrn}
        Conventions vary quite a lot here, for example, it's common to swap \(q\) and \(q^{-1}\), and other authors use \(q^{1/2}\) instead of \(q\).
    \end{wrn}
    
    \begin{remark}{}{}
        We have chosen to make the definition here over the field \(\rationals(q)\).
        This is the field of rational functions\footnote{that is, \(f \in \rationals(q)\) is a ratio \(f(q) = g(q)/h(q)\) with polynomials \(g(q), h(q) \in \rationals[q]\) and \(h(q)\) not identically zero.} in \(q\) with coefficients in \(\rationals\).
        It is entirely possible to make the same definition with \(\complex(q)\) instead.
        It is also common to make this definition over \(\rationals\) (or \(\complex\)) and just think of \(q\) as being a chosen value of \(\rationals\) (or \(\complex\)).
        This has the advantage of being slightly less notation, but there are some subtleties that creep in, mostly about when certain denominators vanish.
        We can recover this version by simply picking a value of \(q\) at which to evaluate our rational functions.
    \end{remark}
    
    Notice that the first and second relations are shared by the braid group, so we can define \(H_n\) as the quotient
    \begin{equation}
        H_n = \rationals(q)\braid_n / \langle (T_i - q)(T_i + q^{-1}) \rangle
    \end{equation}
    where \(\rationals(q)\braid_n\) is the group algebra of \(\braid_n\) over the field, \(\rationals(q)\), of rational functions in \(q\) with coefficients in \(\rationals\).
    
    Rearranging the third relation we get
    \begin{equation}
        1 = T_i(T_i - q + q^{-1}).
    \end{equation}
    This means that \(T_i^{-1}\) exists, and is equal to \(T_i + q - q^{-1}\).
    Note that whenever we write \(q\) we're really thinking of it as \(q1\) where \(1\) is the unit of \(H_n\).
    We then have the relation
    \begin{equation}
        T_i - T_i^{-1} = (q - q^{-1}) 1.
    \end{equation}
    This can be understood as being the skein relation
    \begin{equation}
        \tikzsetnextfilename{hecke-skein}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [thick, ->] (0, 1) -- (1, 0);
            \draw [line width=1.5mm, white] (1, 1) -- (0, 0);
            \draw [thick, ->] (1, 1) -- (0, 0);
            \node at (1.4, 0.5) {\(-\)};
            \begin{scope}[xshift=1.8cm]
                \draw [thick, ->] (1, 1) -- (0, 0);
                \draw [line width=1.5mm, white] (0, 1) -- (1, 0);
                \draw [thick, ->] (0, 1) -- (1, 0);
                \node [right] at (1.1, 0.5) {\({}= (q - q^{-1})\)};
            \end{scope}
            \begin{scope}[xshift=5cm]
                \draw [thick, rounded corners=10] (0, 1) -- (0.4, 0.5) -- (0, 0);
                \draw [thick, rounded corners=10] (1, 1) -- (0.6, 0.5) -- (1, 0);
                \draw [thick, ->] (0, 0) -- ++ (-0.01, -0.01);
                \draw [thick, ->] (1, 0) -- ++ (0.01, -0.01);
            \end{scope}
        \end{tikzpicture}
    \end{equation}
    using the obvious notation for the generators \(T_i\) and identity inherited from \(\braid_n\) in the quotient.
    
    Let \(H_n^{\times}\) be the group of units of \(H_n\).
    Note that \(H_n^{\times}\) is more complicated than \(H_n \setminus 0\), since we also have to account for inverses of sums of generators, as well as products of generators (any product of generators having an inverse as each generator has a multiplicative inverse).
    The map \(\braid_n \to H_n^{\times}\), sending generators to generators, \(\sigma_i \mapsto T_i\), is injective exactly when the Burau representation is, so it is for \(n = 3\), not for \(n \ge 5\), and it's an open problem for \(n = 4\).
    
    The idea motivating the definition of \(H_n\) is that if we \enquote{set \(q = 1\)} we recover\footnote{Here \(S_n\) is playing the role of the Weyl group of type \(\dynkin{A}{n}\).} \(S_n\).
    More formally, we have
    \begin{equation}
        H_n / \langle q - 1 \rangle \isomorphic \rationals S_n
    \end{equation}
    with the isomorphism simply being \(T_i \mapsto s_i\).
    Thus, we can interpret \(H_n\) as a \define{\(\symbf{q}\)-deformation}\index{deformation}\index{q-deformation@\(q\)-deformation} of \(S_n\) (or rather its group algebra).
    
    Much of the representation theory of \(S_n\) lifts to \(H_n\).
    The Hecke algebra also has the advantage that certain results in representation theory actually end up being easier to view in \(H_n\), and then remain true after setting \(q = 1\).
    
    This idea is very similar to that of the quantum group, \(U_q(\specialLinearLie_n)\), and indeed the Schur--Weyl duality of the commuting actions \(\specialLinearLie_n \curvearrowright V^{\otimes r} \curvearrowleft S_r\) lifts to a quantum version, \(U_q(\specialLinearLie_n) \curvearrowright V^{\otimes r} \curvearrowleft H_r\).
    
    \begin{prp}{}{}
        Let \(w = s_{i_1} \dotsc s_{i_r}\) be a reduced expression for \(w \in S_n\), and let \(T_w = T_{i_1} \dotsm T_{i_r}\).
        Then \(\{T_w \mid w \in S_n\}\) is a basis of \(H_n\).
    \end{prp}
    
    We won't prove this result, but the key step is to show that for any two reduced expressions of \(w\) the resulting \(T_w\) is the same.
    The proof of this part uses the skein relation to turn one expression into the other.
    
    \subsection{Representations of Iwahori--Hecke Algebras in Type \texorpdfstring{\(\dynkin{A}{{}}\)}{A}}
    \subsubsection{Irreducible Representations}
    For \(\lambda\) a partition of \(n\) let \(S_\lambda = S_{\lambda_1} \times \dotsb \times S_{\lambda_k}\) be the corresponding row Young subgroup.
    Inspired by the representation theory of the symmetric group we define an analogue of the symmetriser,
    \begin{equation}
        a_\lambda = \sum_{w \in S_n} q^{\ell(w)} T_w.
    \end{equation}
    Then we can define a left \(H_n\)-module, \(M_\lambda \coloneq H_n a_\lambda\).
    For the case of the \(S_n\) we had the decomposition
    \begin{equation}
        M_\lambda = V_\lambda \oplus \bigoplus_{\mu > \lambda} K_{\mu\lambda}V_\mu
    \end{equation}
    where the \(V_\mu\) are the irreducible Specht modules and \(K_{\mu\lambda}\) are the Kostka numbers.
    We take this as inspiration to define the Specht modules for the Hecke algebra.
    Let \(I_\lambda\) be the two-sided ideal generated by \(a_\mu\) for \(\mu > \lambda\).
    Then the Hecke-algebra Specht module is
    \begin{equation}
        V_\lambda \coloneq M_\lambda / (M_\lambda \cap I_\lambda).
    \end{equation}
    The construction is such that the quotient by \(I_\lambda\) kills the \(\bigoplus K_{\mu\lambda} V_\mu\) part of the sum in the \(q = 1\) case leaving us with the symmetric-group Specht modules.
    Note that if \(\lambda \ne \mu\) then \(V_\lambda = V_\mu\) only if these module are trivial.
    
    \begin{thm}{}{}
        Every simple \(H_n\)-module is of the form
        \begin{equation}
            \tilde{V}_\lambda = V_\lambda / \Rad V_\lambda
        \end{equation}
        for some \(\lambda\) a partition of \(n\).
    \end{thm}
    
    The above theorem says that every irreducible representation of the Hecke algebra is the unique simple quotient of some Specht module.
    We can characterise the nontrivial simple \(H_n\)-modules in the following by placing a condition on how fast the rows of the Young diagram are allowed to decrease in length.
    This characterisation only works after we specialise, setting \(q\) to be some complex number.
    For example, if we take \(q = i\) then \(t = q^2 = -1\) and \(1 + (-1) = 0\), so in the following theorem \(e = 2\).
    
    \begin{thm}{}{}
        Let \(e\) be the smallest positive integer such that \(1 + t + \dotsb + t^{e-1} = 0\), with \(t = q^2\), and \(e = \infty\) if no such integer exists.
        Then \(\tilde{V}_\lambda\) is nontrivial if and only if \(\lambda_i - \lambda_{i+1} < e\) for all \(i \ge 1\).
    \end{thm}
    
    \subsubsection{Seminormal Representations}
    Recall that for the symmetric group, \(S_n\), we had Young's seminormal form.
    To construct this fix a partition, \(\lambda\), of \(n\).
    We take a space spanned by \(v_T\) where \(T\) is a standard \(\lambda\)-tableau.
    For the generators, \(s_i \in S_n\), we define
    \begin{equation}
        v_{s_i \action T} = 
        \begin{cases}
            v_{s_i \action T} & \text{if } s_i \action T \text{ is a standard \(\lambda\)-tableau},\\
            0 & \text{otherwise},
        \end{cases}
    \end{equation}
    where on the right \(s_i \action T\) means \(s_i\) acting on the boxes of \(T\) according to their labels.
    We also define the content of a box to be \(C_T(k) = j - i\) when \(T(i, j) = k\), that is the box in position \((i, j)\) is the one labelled \(k\).
    We can then define the action of \(S_n\) on this space by
    \begin{equation}
        s_i \action v_T = \frac{1}{C_T(i + 1) - C_T(i)}v_T + \left( 1 + \frac{1}{C_T(i + 1) - C_T(i)} \right) v_{s_i \action T}.
    \end{equation}
    The benefit of this definition is that the Jucys--Murphy elements, \(L_j \coloneq \sum_{1 \le i < j} \cycle{i,j}\), act as a scalar:
    \begin{equation}
        L_j \action v_T = c_T(j)v_T.
    \end{equation}
    
    In order to perform a similar construction for the Hecke algebra we need to complexify, performing a basis change we get the complex Hecke algebra,
    \begin{equation}
        H_n^{\complex} \coloneq H_n \otimes_{\rationals} \complex.
    \end{equation}
    Alternatively, we can define this just as we defined the rational Hecke algebra, but replacing \(\rationals(q)\) with \(\complex(q)\).
    The analogue of the Jucys--Murphy elements is then
    \begin{equation}
        L_j = \sum_{1 \le i < j} T_{\cycle{i,j}}.
    \end{equation}
    
    \begin{lma}{}{}
        Let \(M_j = T_{j-1} \dotsm T_2 T_1^2 T_2 \dotsm T_{j-1}\).
        Then
        \begin{equation}
            L_j = \frac{M_j - 1}{q - q^{-1}}.
        \end{equation}
    \end{lma}
    
    The corresponding representation space is
    \begin{equation}
        V_\lambda = \Span_{\complex(q)} \{v_T \mid T \text{ is a standard \(\lambda\)-tableau}\}.
    \end{equation}
    
    \begin{thm}{}{}
        Under the action
        \begin{equation}
            T_i \action v_T = \frac{q - q^{-1}}{1 - q^{C_T(i) - C_T(i + 1)}} v_T + \left( q^{-1} + \frac{q - q^{-1}}{1 - q^{C_T(i) - C_T(i + 1)}} \right) v_{s_i \action T}.
        \end{equation}
        \(V_\lambda\) is an \(H_n^{\complex}\)-module.
    \end{thm}
    
    \begin{lma}{}{}
        The Jucys-Murphy elements act diagonally on \(V_\lambda\), specifically,
        \begin{equation}
            M_j \action v_T = q^{2C_T(j)} v_T
        \end{equation}
        so
        \begin{equation}
            L_j \action v_T = \frac{q^{2C_T(j)} - 1}{q - q^{-1}}v_T.
        \end{equation}
    \end{lma}
    
    \begin{thm}{}{}
        The modules \(V_\lambda\) with \(\lambda\) a partition of \(n\) define a complete set of pairwise non-isomorphic simple \(H_n^{\complex}\)-modules.
    \end{thm}
    
    \begin{remark}{}{}
        Write \(H_n(q)\) for \(H_n^{\complex}\) with \(q\) specialised to some value in \(\complex\).
        \begin{itemize}
            \item \(H_n(1) \isomorphic \complex S_n\)
            \item If \(q\) is not a root of unity then \(H_n(q)\) is semisimple, and is isomorphic to \(\complex S_n\) as vector spaces.
            The simple modules, \(V_\lambda^q\), are therefore labelled by partitions of \(n\), with a Gelfand--Tsetlin basis with a \(q\)-deformed version of the \(S_n\) action.
            We also get the decomposition
            \begin{equation}
                H_n(q) \isomorphic \bigoplus_\lambda \End V_\lambda^q.
            \end{equation}
            \item If \(q^{2d} = 1\) then the representation theory of \(H_n(q)\) is similar to the representation theory of \(S_n\) over a field of characteristic \(d\).
            In particular, if \(n < d\) then \(H_n(q)\) is still semisimple.
        \end{itemize}
    \end{remark}
    
    \chapter{Quantum Groups}
    We will now give a \emph{very} brief introduction to quantum groups.
    We assume familiarity with the notion of a Hopf algebra.
    We also use the language of monoidal categories, but these aren't essential to understanding what's going on, they're just motivating.
    
    If you're not comfortable with monoidal categories just consider \(\Vect_{\complex}\).
    This is equipped with a tensor product, which is such that \(V \otimes W\) is a vector space for all vector spaces \(V\) and \(W\) (over \(\complex\)).
    This has the property that it is associative up to isomorphism, \(\alpha_{U,V,W} \colon U \otimes (V \otimes W) \xrightarrow{\sim} (U \otimes V) \otimes W\).
    There is also a unit (up to isomorphism) of the tensor product, which is just \(\complex\) as a vector space over itself, so we have isomorphisms \(\lambda_V \colon \complex \otimes V \xrightarrow{\sim} V\) and \(\rho_V \colon V \otimes \complex \xrightarrow{\sim} V\).
    For each vector space, \(V\), we also have its dual, \(V^* = \Hom(V, \complex)\), which is again a vector space, this property is called rigidity.
    This tensor product is braided (in fact, it's symmetric), meaning we have an explicit isomorphism \(\sigma_{V,W} \colon V \otimes W \xrightarrow{\sim} W \otimes V\).
    
    There are some compatibility conditions on all of these, namely \({-}\otimes{-}\) and \((-)^*\) are functorial, and \(\alpha_{U,V,W}\), \(\lambda_V\), \(\rho_V\), and \(\sigma_{V,W}\) are all component of some natural transformations,
    \begin{itemize}
        \item \(\alpha \colon {-} \otimes ({-} \otimes {-}) \Rightarrow ({-} \otimes {-}) \otimes {-}\);
        \item \(\lambda \colon \complex \otimes {-} \Rightarrow \id\);
        \item \(\rho \colon {-} \otimes \complex \Rightarrow \id\);
        \item \(\sigma \colon {-}\otimes{-} \Rightarrow {-}\otimes{-}\).
    \end{itemize}
    There are some diagrams formed from these maps which must commute.
    
    To get the definition of a monoidal category we just replace vector spaces with the appropriate category.
    
    \section{Quasitriangularity}
    For a general algebra, \(A\), the category \(\AMod\), of \(A\)-modules and \(A\)-module homomorphisms is \enquote{just} a category.
    
    If instead we have a bialgebra, \(B\), then the category \(\AMod[B]\), of \(B\)-modules is a monoidal category.
    The tensor product \(M\) and \(N\) in \(\AMod[B]\) is defined to be the tensor product of the underlying vector spaces, \(M \otimes N\), with the action defined on simple tensors by
    \begin{equation}
        b \action (m \otimes n) = \Delta(b)(m \otimes n).
    \end{equation}
    If in Sweedler notation \(\Delta(b) = \sum b_{(1)} \otimes b_{(2)}\) then this action is given by
    \begin{equation}
        b \action (m \otimes n) = \sum (b_{(1)} \action m) \otimes (b_{(2)} \action n)
    \end{equation}
    where on the right we just have the actions of \(B\) on \(M\) and \(N\) respectively.
    
    If we further add an antipode, \(\chi\), to get a Hopf algebra, \(H\), then the category \(\AMod[H]\), of \(H\)-modules is a \defineindex{rigid monoidal category}.
    That is, every object has a dual, in the case of \(\AMod[H]\) (over \(\complex\)) the dual of \(M\) is the dual module, \(M^* = \Hom(M, \complex)\), with the action defined by
    \begin{equation}
        (h \action f)(m) = f(h \action m),
    \end{equation}
    where on the right we have the action of \(H\) on \(M\).
    
    We see that adding structure, going from an algebra to a bialgebra to a Hopf algebra, adds structure to the category of modules.
    A natural question then is what structure do we need to add to a bialgebra (Hopf algebra) to get a \emph{braided} (rigid) monoidal category?
    We'll assume a Hopf algebra, since that's the most useful case, but most of what we're about to do works with a bialgebra.
    To get a braiding we're looking for an element which acts on the tensor product in the way a braiding would.
    
    \begin{dfn}{Quasitriangular}{}
        Let \(H\) be a Hopf algebra.
        We say that \(H\) is \defineindex{quasitriangular} if there exists some invertible element\footnote{\(H \mathbin{\hat{\otimes}} H\) is some appropriate completion of \(H \otimes H\) to include infinite sums. Outside of this definition we'll often drop this notation, and either have an implicit completion or it won't actually be needed.} \(\universalRmatrix \in H \mathbin{\hat{\otimes}} H\), called the \define{universal \(\symbf{R}\)-matrix}\index{universal R-matrix@universal \(R\)-matrix}, such that
        \begin{itemize}
            \item \(\universalRmatrix \Delta(x) \universalRmatrix^{-1} = \Delta^{\op}(x)\) for all \(x \in H\) (note \(\Delta^{\op} = P \circ \Delta\) where \(P(u \otimes v) = v \otimes u\));
            \item \((\Delta \otimes \id)(\universalRmatrix) = \universalRmatrix_{13}\universalRmatrix_{23}\);
            \item \((\id \otimes \Delta)(\universalRmatrix) = \universalRmatrix_{13}\universalRmatrix_{12}\)
        \end{itemize}
        where subscripts on \(\universalRmatrix\) denote which factors of a tensor product it acts on.
        For example, \(\universalRmatrix_{13}\) is the image of \(\universalRmatrix\) under the map \(H^{\otimes 2} \to H^{\otimes 3}\) given by \(a \otimes b \mapsto a \otimes 1 \otimes b\).
    \end{dfn}
    
    This is a slightly complicated definition, but the key idea is that we're only imposing on \(\universalRmatrix\) the requirements such that when given a tensor product of \(H\)-modules the obvious action of \(\universalRmatrix\) on this tensor product is a braiding.
    Specifically, the braiding is
    \begin{equation}
        \sigma_{U,V}(u \otimes v) = P(\universalRmatrix \action (u \otimes v)) = \sum P(\universalRmatrix_{1} \action u \otimes \universalRmatrix_{2} v) = \sum \universalRmatrix_2 \action v \otimes \universalRmatrix_1 \action u
    \end{equation}
    where in the penultimate equality we're choosing some expansion of \(\universalRmatrix \in H \otimes H\) of the form \(\universalRmatrix = \sum \universalRmatrix_1 \otimes \universalRmatrix_2\) in Sweedler notation.
    
    \begin{exm}{}{}
        Consider a Lie algebra, \(\lie{g}\).
        Then the universal enveloping algebra, \(U(\lie{g})\), is a Hopf algebra with
        \begin{equation}
            \Delta(x) = x \otimes 1 + 1 \otimes x.
        \end{equation}
        We have \(\Delta^{\op} = \Delta\) in this case, and this is trivially quasitriangular taking \(\universalRmatrix = 1 \otimes 1\).
    \end{exm}
    
    One can show that a consequence of the coassociativity of \(H\) is that the universal \(R\)-matrix always satisfies the \defineindex{Yang--Baxter equation}:
    \begin{equation}
        \universalRmatrix_{12}\universalRmatrix_{13}\universalRmatrix_{23} = \universalRmatrix_{23}\universalRmatrix_{13}\universalRmatrix_{12},
    \end{equation}
    which is an equation in \(H \otimes H \otimes H\).
    This comes from requiring that \(\Delta^{\op}\) also makes \(H\) into a Hopf algebra, and then using the definition of \(\universalRmatrix\) to replace \(\Delta^{\op}\) with \(\Delta\) conjugated by \(\universalRmatrix\).
    
    It is useful to introduce the flip operator, \(P \colon H \otimes H \to H \otimes H\), \(P(a \otimes b) = b \otimes a\), and then define \(\check{\universalRmatrix} = P \circ \universalRmatrix\).
    The flip operator also satisfies the Yang--Baxter equation.
    Starting with the Yang--Baxter equation for \(\universalRmatrix\) and acting with \(P_{ij}\), which is just the flip operation acting on the \(i\)th and \(j\)th components it is possible to manipulate the Yang--Baxter equation for \(\universalRmatrix\) into the form of the braid equation for \(\check{\universalRmatrix}\):
    \begin{equation}
        \check{\universalRmatrix}_{23} \check{\universalRmatrix}_{12} \check{\universalRmatrix}_{23} = \check{\universalRmatrix}_{12} \check{\universalRmatrix}_{23} \check{\universalRmatrix}_{12}.
    \end{equation}
    Note that \(\check{\universalRmatrix}\) doesn't (necessarily) satisfy the Yang--Baxter equation\footnote{Often people don't distinguish very well between \(\universalRmatrix\) and \(\check{\universalRmatrix}\) or between the Yang--Baxter and braid equations.}.
    It is \(\check{\universalRmatrix}\) which acts as the braiding in \(\AMod[H]\).
    
    \begin{dfn}{Quantum Group}{}
        A \defineindex{quantum group} is a (not necessarily commutative or cocommutative) quasitriangular Hopf algebra.
    \end{dfn}
    
    \section{Quantum Schur--Weyl Duality}
    Recall that \enquote{classical} Schur--Weyl duality is a statement on the compatibility of the actions of \(S_n\) and \(\generalLinear_m\) on \((\complex^m)^{\otimes n}\).
    Specifically, it says that if \(M = \complex^m\) then we have the decomposition
    \begin{equation}
        M^{\otimes n} \isomorphic_\lambda V_\lambda \otimes L_\lambda
    \end{equation}
    where \(V_\lambda\) and \(L_\lambda\) are simple \(S_n\)-modules and simple \(U(\specialLinearLie_m)\)-modules respectively.
    Further, these modules are related by
    \begin{equation}
        V_\lambda \isomorphic \Hom_{U(\generalLinearLie_m)}(L_\lambda, M^{\otimes n}), \qand L_\lambda \isomorphic \Hom_{\complex S_n}(V_\lambda, M^{\otimes n}).
    \end{equation}
    
    Quantum Schur--Weyl duality is the corresponding statement that we get if we replace \(\complex S_n\) with its deformation, the Hecke algebra, \(H_n\).
    The question then is what is the \enquote{correct} replacement for \(U(\specialLinearLie_m)\).
    The answer is the quantum group \(U_q(\specialLinearLie_m)\).
    The full definition of this is a fairly complicated algebra with generators and relations.
    We won't give these relations here.
    The generators are \(e_i\), \(f_i\), and \(k_i\) for \(i = 1, \dotsc, \operatorname{rank}\lie{g}\).
    This can also be generalised to other Dynkin types, replacing \(k_i\) with \(k_\lambda\) where \(\lambda\) is an element of the weight lattice.
    
    \section{\texorpdfstring{\(U_q(\specialLinearLie_2)\)}{Uq(sl2)}}
    \begin{dfn}{}{}
        
        \textit{Warning: Conventions differ in the exact definitions, usually differing by scaling some elements by some power of \(q\).}
        
        The quantum group, \(U_q(\specialLinearLie_2)\), is the unital associative algebra generated by \(e\), \(f\), \(k\) and \(k^{-1}\) subject to the relations
        \begin{itemize}
            \item \(kk^{-1} = 1 = k^{-1}k\);
            \item \(ke = q^2ek\);
            \item \(kf = q^{-2}fk\);
            \item \(\bracket{e}{f} = \frac{k - k^{-1}}{q - q^{-1}}\).
        \end{itemize}
        The coproduct of this algebra is defined by
        \begin{equation*}
            \Delta(k^{\pm 1}) = k^{\pm 1} \otimes k^{\pm 1}, \quad \Delta(e) = e \otimes k^{-1} + 1 \otimes e, \qand \Delta(f) = f \otimes 1 + k \otimes f,
        \end{equation*}
        the counit is defined by
        \begin{equation}
            \varepsilon(k^{\pm 1}) = 1, \qand \varepsilon(e) = \varepsilon(f) = 0,
        \end{equation}
        and the antipode is defined by
        \begin{equation}
            \chi(k^{\pm 1}) = k^{\mp 1}, \quad \chi(e) = -ek, \qand \chi(f) = -k^{-1}f.
        \end{equation}
        Strictly, \(U_q(\specialLinearLie_2)\) is just a Hopf algebra, it isn't actually quasitriangular, however, if we instead work in a completion of \(U_q(\specialLinearLie_2)\) then we are allowed the element
        \begin{equation}
            \universalRmatrix = \left( \sum_{n=0}^{\infty} q^{n(n + 1)/2} \frac{(1 - q^2)^n}{[n]_q!} e^n \otimes f^n \right) q^{-h \otimes h/2}
        \end{equation}
        which does act as a universal \(R\)-matrix for this completion.
        Here \([n]_q!\) is the \defineindex{quantum factorial}, defined in terms of the \defineindex{quantum integer}
        \begin{equation}
            [n]_q = \frac{q^n - q^{-n}}{q - q^{-1}},
        \end{equation}
        such that \([n]_q! = [n]_q [n - 1]_q \dotsm [1]_q\).
    \end{dfn}
    
    The idea behind these definitions is that when we set \(k = q^{h} = e^{\hbar h}\) and take \(\hbar \to 0\) we recover (at least formally) \(U(\specialLinearLie_2)\).
    
    The fundamental representation of \(U_q(\specialLinearLie_2)\), also known as the vector representation, is \(L_1 \isomorphic \complex^2 = \complex v_0 \oplus \complex v_1\), with the action
    \begin{equation}
        e v_0 = fv_1 = 0, \quad ev_1 = v_0, \quad f v_0 = v_1, \quad kv_0 = qv_0, \qand kv_1 = q^{-1}v_1.
    \end{equation}
    Schur-Weyl duality is then the statement that
    \begin{equation}
        L_1^{\otimes r} = \sum_{m = 0}^r V_m \otimes L_m
    \end{equation}
    where \(L_m\) are more simple \(U_q(\specialLinearLie_2)\)-modules and the \(V_m\) are simple \(\temperleyLieb_n\)-modules, where \(\temperleyLieb_n\) is the \defineindex{Temperley--Lieb} algebra, defined as a quotient of the Hecke algebra,
    \begin{equation}
        \temperleyLieb_n = H_n / \langle 1 + T_i + T_{i+1} + T_i T_{i+1} + T_{i+1} T_i + T_i T_{i+1} T_i \rangle.
    \end{equation}
    
    The Temperley--Lieb algebra can also be given as the unital algebra generated by \(e_i\) for \(i = 1, \dotsc, n-1\) subject to the relations
    \begin{itemize}
        \item \(e_i^2 = \delta e_i\) where \(\delta\) is some fixed complex number;
        \item \(e_ie_{i+1}e_i = e_i\);
        \item \(e_ie_{i-1}e_i = e_i\);
        \item \(e_ie_j = e_je_i\) for \(\abs{i - j} > 2\).
    \end{itemize}
    Graphically, we can represent the generator \(e_i\) as
    \begin{equation}
        \tikzsetnextfilename{temperley-lieb-generator}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [thick] (0, 0) -- ++ (0, -2);
            \draw [thick] (1, 0) -- ++ (0, -2);
            \draw [thick] (1.5, 0) arc (180:360:0.5);
            \draw [thick] (1.5, -2) arc (180:0:0.5);
            \draw [thick] (3, 0) -- ++ (0, -2);
            \draw [thick] (4, 0) -- ++ (0, -2);
            \node at (0.5, -1) {\(\dots\)};
            \node at (3.5, -1) {\(\dots\)};
            \node [above] at (0, 0) {\(\scriptscriptstyle 1\)};
            \node [above] at (1, 0) {\(\scriptscriptstyle i-1\)};
            \node [above] at (1.5, 0) {\(\scriptscriptstyle i\)};
            \node [above] at (2.5, 0) {\(\scriptscriptstyle i+1\)};
            \node [above] at (3, 0) {\(\scriptscriptstyle i+2\)};
            \node [above] at (4, 0) {\(\scriptscriptstyle n\)};
        \end{tikzpicture}
    \end{equation}
    The product of two such elements is their vertical concatenation, which we interpret with the rule that any circle is just the scalar \(\delta\).
    For example, with \(n = 3\) the relation \(e_2^2 = \delta e_2\) becomes
    \begin{equation}
        \tikzsetnextfilename{tempeerley-lieb-generator-square}
        \begin{tikzpicture}[baseline=(current bounding box), scale=0.8]
            \draw [thick] (0, 0) -- ++ (0, -2);
            \draw [thick] (0.5, 0) arc (180:360:0.5);
            \draw [thick] (0.5, -2) arc (180:0:0.5);
            \draw [thick] (2, 0) -- ++ (0, -2);
            \node at (2.5, -1) {\(\cdot\)};
            \begin{scope}[xshift=3cm]
                \draw [thick] (0, 0) -- ++ (0, -2);
                \draw [thick] (0.5, 0) arc (180:360:0.5);
                \draw [thick] (0.5, -2) arc (180:0:0.5);
                \draw [thick] (2, 0) -- ++ (0, -2);
            \end{scope}
            \node at (5.5, -1) {\(=\)};
            \begin{scope}[xshift=6cm, yshift=1cm]
                \draw [thick] (0, 0) -- ++ (0, -4);
                \draw [thick] (0.5, 0) arc (180:360:0.5);
                \draw [thick] (0.5, -4) arc (180:0:0.5);
                \draw [thick] (1, -2) circle [radius = 0.5];
                \draw [thick] (2, 0) -- ++ (0, -4);
            \end{scope}
            \node at (8.5, -1) {\(=\)};
            \node at (9, -1) {\(\sqrt{q}\)};
            \begin{scope}[xshift=9.5cm]
                \draw [thick] (0, 0) -- ++ (0, -2);
                \draw [thick] (0.5, 0) arc (180:360:0.5);
                \draw [thick] (0.5, -2) arc (180:0:0.5);
                \draw [thick] (2, 0) -- ++ (0, -2);
            \end{scope}
        \end{tikzpicture}
    \end{equation}
    
    The representation theory of \(U_q(\specialLinearLie_2)\) is not that different from the representation theory of \(U(\specialLinearLie_2)\).
    In particular, there is a notion of a Verma module, where for the quantum case we replace the highest weight, \(n \in \integers\), with the quantum integer \([n]_q\).
    
    We can often understand a topological object, such as the plane, by looking at certain algebras of functions on the space.
    For the plane, we may look at the algebra of polynomial functions on the plane, which is just \(\complex[x, y]\).
    There is a natural action of \(\specialLinearLie_2\) on this, in which it acts by difference operations, in particular, if we define
    \begin{equation}
        e = x \partial_y, \quad f = y \partial_x, \qand h = x \partial_x - y \partial_y
    \end{equation}
    then we can check that these satisfy the \(\specialLinearLie_2\) commutation relations.
    
    There is a general philosophy of quantisation that to get the equivalent \enquote{quantum} result we should replace commuting variables with non-commuting variables.
    The \defineindex{quantum plane}\footnote{In analogy to the normal plane the quantum plane is really the space for which this is the algebra of (polynomial) functions, but that's not really a space that exists, so we just call this the quantum plane.} is \(\complex_q[x, y] \coloneq \complex \langle x, y\rangle / \langle yx - qxy\rangle\).
    That is, polynomials in \(x\) and \(y\), which no longer commute, but instead \(yx = qxy\).
    Then there is an action of \(U_q(\specialLinearLie_2)\) on this in which we replace the derivatives above with the corresponding \define{quantum derivatives}\index{quantum derivative}, defined by
    \begin{equation}
        \delta_x f(x) = \frac{f(qx) - f(q^{-1}x)}{qx - q^{-1}x}.
    \end{equation}
    Then defining
    \begin{equation}
        e = x \delta_y, \qand f = y \delta_x
    \end{equation}
    and \(k\) acts by
    \begin{equation}
        k \action x = qx, \qand k \action y = q^{-1}y.
    \end{equation}
    
    Working with the universal \(R\)-matrix of \(U_q(\specialLinearLie_2)\) (or rather its completion) is somewhat tricky.
    It's usually better to just compute the \(R\)-matrix, \(R\), for the given representation.
    To do so we appeal to the relations defining \(U_q(\specialLinearLie_2)\).
    We'll do this here for the fundamental representation, for which a useful basis is \(\{v_{-1}, v_1\}\) with the action defined by
    \begin{equation*}
        k^{\pm 1} \action v_i = q^{\pm i} v_i, \quad e \action v_{-1} = v_1, \quad e \action v_1 = 0, \quad f \action v_{-1} = 0, \qand f \action v_1 = v_{-1}.
    \end{equation*}
    We see from this that \(v_1\) is a highest weight vector (it's an eigenvector of \(k\) and annihilated by \(e\)) and \(v_{-1}\) is a lowest weight vector (it's an eigenvector of \(k\) and annihilated by \(f\)).
    The tensor product of highest (lowest) weight vectors is again a highest (lowest) weight vector, and so we have \(\Delta(e)(v_1 \otimes v_1) = \Delta(f)(v_{-1} \otimes v_{-1}) = 0\).
    
    We can act with \(R\) on these, and we should still get zero.
    Note that \(R\) is just the image of \(\universalRmatrix\) induced by the representation map \(U_q(\specialLinearLie_2) \to \End \complex^2\) and the coproduct, so \(R \in \End(\complex^2 \otimes \complex^2) \isomorphic \End(\complex^4)\), and thus we can write \(R\) as
    \begin{equation}
        R =
        \begin{pmatrix}
            a & 0 & 0 & 0\\
            0 & b & c & 0\\
            0 & c' & b' & 0\\
            0 & 0 & 0 & a'
        \end{pmatrix}
    \end{equation}
    for some \(a, b, c, a', b', c' \in \complex\) where we're using the ordered basis \(\{v_1 \otimes v_1, v_1 \otimes v_{-1}, v_{-1} \otimes v_1, v_{-1}\otimes v_{-1}\}\).
    
    We can then do more calculations, such as
    \begin{align}
        R\Delta(e) v_{-1} \otimes v_{-1} &= R(e \otimes k^{-1} + 1 \otimes e)v_{-1} \otimes v_{-1}\\
        &= R(ev_{-1} \otimes k^{-1}v_{-1} + 1v_{-1} \otimes ev_{-1})\\
        &= R(v_1 \otimes qv_{-1} + v_{-1} \otimes v_1)\\
        &= qR(v_1 \otimes v_{-1}) + R(v_{-1} \otimes v_1)\\
        &= qcv_1 \otimes v_{-1} + qb' v_{-1} \otimes v_1 + bv_1 \otimes v_{-1} + c'v_{-1} \otimes v_1\notag
    \end{align}
    and
    \begin{align}
        \Delta^{\op}(e)v_{-1} \otimes v_{-1} &= (k^{-1} \otimes e + e \otimes 1)v_{-1} \otimes v_{-1}\\
        &= k^{-1}v_{-1} \otimes ev_{-1} + e v_{-1} \otimes 1v_{-1}\\
        &= qv_{-1} \otimes v_1 + v_1 \otimes v_{-1}.
    \end{align}
    The axioms of a universal \(R\)-matrix mean that these two results should be equal, and so if we equate coefficients we find that
    \begin{equation}
        qc + b = 1, \qand qb' + c' = q.
    \end{equation}
    Continuing like this we eventually can eliminate all unknowns, and we get
    \begin{equation}
        \check{R} = 
        \begin{pmatrix}
            q & 0 & 0 & 0\\
            0 & 0 & 1 & 0\\
            0 & 1 & q - q^{-1} & 0\\
            0 & 0 & 0 & q
        \end{pmatrix}
        .
    \end{equation}
    
    \begin{lma}{}{}
        Let \(V = \complex^2\).
        The action of \(\braid_n\) on \(V^{\otimes n}\) acting by permutations factors through \(H_n(q)\) to \(\temperleyLieb_n(q)\) where \(\temperleyLieb_n(q)\) is the Temperley--Lieb algebra with \(\delta = q + q^{-1}\).
    \end{lma}
    
    These two results are useful to get knot invariants, first look at the braid given by slicing the knot, then pass through the quotient to the Temperley--Lieb algebra, and we'll get a quantity that should be an invariant.
    For example, the Jones polynomial arises in this way, although it was originally discovered using operator algebras, since the underlying Temperley--Lieb algebra structure was not known at the time of discovery.
    
    
    
    % Appdendix
%	\appendixpage
%	\begin{appendices}
%	    \include{appendix/complexification}
%	\end{appendices}

    \backmatter
    \renewcommand{\glossaryname}{Acronyms}
    \printglossary[acronym]
    \printindex
\end{document}