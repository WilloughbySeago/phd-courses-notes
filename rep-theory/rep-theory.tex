% !TeX program = lualatex
\documentclass[fleqn]{NotesClass}

\strictpagecheck

\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{ytableau}

\usepackage{tikz}
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]

\usepackage{tikz-cd}
\AtBeginEnvironment{tikzcd}{\tikzexternaldisable}
\AtEndEnvironment{tikzcd}{\tikzexternalenable}

\usepackage[pdfauthor={Willoughby Seago},pdftitle={Notes from Representation Theory Course},pdfkeywords={representation theory},pdfsubject={Representation Theory}]{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{NotesBoxes}
\usepackage{NotesMaths2}

\setmathfont[range={\int, \oint, \otimes, \oplus, \bigotimes, \bigoplus}]{Latin Modern Math}


% Highlight colour
%\definecolor{highlight}{HTML}{710D78}
%\definecolor{my blue}{HTML}{2A0D77}
%\definecolor{my red}{HTML}{770D38}
%\definecolor{my green}{HTML}{14770D}
%\definecolor{my yellow}{HTML}{E7BB41}

% Title page info
\title{Representation Theory}
\author{Willoughby Seago}
\date{January 13th, 2024}
\subtitle{Notes from}
\subsubtitle{University of Glasgow}
\renewcommand{\abstracttext}{These are my notes from the SMSTC course \emph{Lie Theory} taught by Prof Christian Korff. These notes were last updated at \printtime{} on \today{}.}

% Commands
% Maths
\renewcommand{\field}{\symbb{k}}
\newcommand{\id}{\symrm{id}}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Hom}{Hom}
\newcommand{\action}{\mathbin{.}}
\newcommand{\op}{\symrm{op}}
\makeatletter
\newcommand{\c@egory}[1]{\symsfup{#1}}
\newcommand{\Vect}[1][\field]{#1\text{-}\c@egory{Vect}}
\newcommand{\AMod}[1][A]{#1\text{-}\c@egory{Mod}}
\newcommand{\ModA}[1][A]{\c@egory{Mod}\text{-}#1}
\newcommand{\Mod}[1]{#1\text{-}\c@egory{Mod}}
\newcommand{\Ab}{\c@egory{Ab}}
\newcommand{\Rep}{\c@egory{Rep}}
\newcommand{\Set}{\c@egory{Set}}
\newcommand{\Alg}[1][\field]{{#1}\text{-}\c@egory{Alg}}
\makeatother
\DeclareMathOperator{\im}{im}
\newcommand{\isomorphic}{\cong}
\DeclareMathOperator{\Span}{span}
\ExplSyntaxOn
% Create LaTeX interface command
\NewDocumentCommand{\cycle}{ O{\,} m }{  % optional arg is separator, mandatory
    %arg is comma separated list
    (
    \willoughby_cycle:nn { #1 } { #2 }
    )
}

\clist_new:N \l_willougbhy_cycle_clist  % Create new clist variable
\cs_new_protected:Npn \willoughby_cycle:nn #1 #2 {  % create LaTeX3 function
    \clist_set:Nn \l_willougbhy_cycle_clist { #2 }  % set clist variable with
    %clist #2 passed by user
    \clist_use:Nn \l_willougbhy_cycle_clist { #1 }  % print list separated by #1
}
\ExplSyntaxOff
\newcommand{\universalEnveloping}{\symcal{U}}
\DeclarePairedDelimiterX{\bracket}[2]{[}{]}{#1, #2}
\DeclareMathOperator{\Mat}{Mat}
\RenewDocumentCommand{\matrices}{s o m m}{
    \IfBooleanTF{#1}{
        \IfNoValueTF{#2}{
            \Mat_{#3}\left( #4 \right)
        }{
            \Mat_{#2 \times #3}\left( #4 \right)
        }
    }{
        \IfNoValueTF{#2}{
            \Mat_{#3}(#4)
        }{
            \Mat_{#2 \times #3}(#4)
        }
    }
}
\newcommand{\trans}{\top}
\DeclareMathOperator{\Rad}{Rad}
\DeclareMathOperator{\Irr}{Irr}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Char}{char}
\newcommand{\classFunctions}{\symcal{X}}
\newcommand{\conjugacyClasses}{\symcal{C}}
\DeclareMathOperator{\Func}{Func}
\newcommand{\partition}{\vdash}
\DeclarePairedDelimiterX{\innerprod}[2]{\langle}{\rangle}{#1, #2}
\DeclareMathOperator{\frobeniusSchur}{FS}
\DeclareMathOperator{\standardYoungTableaux}{SYT}
\newcommand{\normalsub}{\mathrel{\lhd}}
\newcommand{\algNumbers}{\overline{\rationals}}
\newcommand{\algIntegers}{\overline{\integers}}
\newcommand{\Res}{\symrm{Res}}
\newcommand{\Ind}{\symrm{Ind}}
\newcommand{\one}{\symbb{1}}

\begin{document}
    \frontmatter
    \titlepage
    \innertitlepage{}
    \tableofcontents
    % \listoffigures
    \mainmatter
    \chapter{Introduction}
    We fix some standard notation here:
    \begin{itemize}
        \item \(\field\) will denote an algebraically closed field, except for when we explicitly mention that the field needn't be algebraically closed.
        \item \(A\) will denote an associative unital algebra.
        \item Letters like \(V\), \(U\), and \(W\) will denote vector spaces over \(\field\).
        \item Letters like \(M\) and \(N\) will denote modules.
    \end{itemize}
    
    \chapter{Initial Definitions}
    \section{Algebra}
    \begin{dfn}{Algebra}{}
        An \defineindex{algebra} is a \(\field\)-vector space, \(A\), equipped with a bilinear map,
        \begin{align}
            m \colon A \times A &\to A\\
            (a, b) &\mapsto m(a, b) = ab.
        \end{align}
        
        If this map satisfies the condition that
        \begin{equation}
            m(a, m(b, c)) = m(m(a, b), c), \text{ or equivalently } a(bc) = (ab)c,
        \end{equation}
        for all \(a, b, c \in A\) then we call \(A\) an \defineindex{associative algebra}.
        
        If \(A\) posses a distinguished element, \(1 \in A\), such that \(m(1, a) = a = m(a, 1)\), or equivalently \(1a = a = a1\) for all \(a \in A\) then we say that \(A\) is a \defineindex{unital algebra}.
        
        If \(m(a, b) = m(b, a)\), or equivalently \(ab = ba\), for all \(a, b \in A\) then we say that \(A\) is a \defineindex{commutative algebra}.
    \end{dfn}
    
    Whenever we say, otherwise unqualified, \enquote{algebra} we will mean associative unital algebra unless we specify otherwise.
    We will not assume commutativity of a general algebra.
    
    The condition of associativity can be written as a commutative diagram,
    \begin{equation}
        \begin{tikzcd}
            A \times A \times A \arrow[r, "m \times \id_A"] \arrow[d, "\id_A \times m"'] & A \times A \arrow[d, "m"]\\
            A \times A \arrow[r, "m"'] & A\mathrlap{,}
        \end{tikzcd}
    \end{equation}
    
    \begin{remark}{}{}
        This diagram goes part of the way to the more abstract definition that \enquote{an associative unital (commutative) algebra is a (commutative) monoid in the category of vector spaces}.
        This definition is nice because it is both very general and dualises to the notion of a coalgebra.
        See the \textit{Hopf Algebra} notes for more details.
    \end{remark}
    
    \begin{exm}{}{}
        \begin{itemize}
            \item \(A = \field\) is an algebra with the product given by the product in the field;
            \item \(A = \field[x_1, \dotsc, x_n]\), the ring of polynomials in the variables \(x_i\) with coefficients in \(\field\), is an algebra under the addition and multiplication of polynomials.
            \item \(A = \field \langle x_1, \dotsc, x_n \rangle\), the free algebra on \(x_i\), may be considered as the algebra of polynomials in non-commuting variables, \(x_i\).
            \item \(A = \End V\) for \(V\) a \(\field\)-vector space is an algebra with multiplication given by composition of morphisms.
        \end{itemize}
    \end{exm}
    
    \begin{dfn}{Group Algebra}{}
        Let \(G\) be a group.
        The \defineindex{group algebra} or \defineindex{group ring} \(\field G = \field[G]\) is defined to be the set of finite formal linear combinations
        \begin{equation}
            \sum_{g \in G} c_g g
        \end{equation}    
        where \(c_g \in \field\) is nonzero for only finitely many values \(g\).
        Addition is defined by
        \begin{equation}
            \sum_{g \in G} c_g g + \sum_{g \in G} d_g g = \sum_{g \in G} (c_g + d_g) g.
        \end{equation}
        Multiplication is defined by requiring that it distributes over addition and that the product of two terms in the above sums is given by
        \begin{equation}
            (c_g g) (d_h h) = (c_g d_h) (gh)
        \end{equation}
        where multiplication on the left is in \(\field G\), the multiplication \(c_g d_h\) is in \(\field\), and the multiplication \(gh\) is in \(G\).
        
        If we do the same construction replacing \(\field\) with a ring, \(R\), then we get the group ring, \(RG\), which is not an algebra but instead an \(R\)-module.
    \end{dfn}
    
    \begin{dfn}{Algebra Homomorphism}{}
        Let \(A\) and \(B\) be \(\field\)-algebras.
        An \defineindex{algebra homomorphism} is a linear map \(f \colon A \to B\) such that \(f(ab) = f(a)f(b)\) for all \(a, b \in A\).
        
        If \(A\) and \(B\) are unital, with units \(1_A\) and \(1_B\) respectively, then we further require that \(f(1_A) = 1_B\).
        
        We denote by \(\Hom(A, B)\) or \(\Hom_{\field}(A, B)\) the set of all algebra homomorhpisms \(A \to B\).
    \end{dfn}
    
    If \(m_A\) and \(m_B\) denote the multiplication maps of \(A\) and \(B\) respectively then we may think of a homomorphism, \(f\), as a linear map which \enquote{commutes} with the multiplication map, that is \(f \circ m_A = m_B \circ f\).
    
    Alternatively, an algebra, \(A\) is both an abelian group under addition, and a monoid under multiplication, and an algebra homomorhpism is both a group and monoid homomorphism with respect to these structures.
    
    \section{Representations and Modules}
    \label{sec:representaitons and modules}
    There are two competing terminologies in the field, with slightly different notation and emphasis depending on which we use.
    We'll use the more modern notion of modules most of the time, but will occasionally and interchangeably use the notion of representations as well.
    
    \begin{dfn}{Representation}{}
        Let \(V\) be a \(\field\)-vector space and \(A\) a \(\field\)-algebra.
        Any \(\rho \in \Hom(A, \End V)\) is called a \defineindex{representation} of \(A\).
        That is, a representation of \(A\) is an algebra homomorphism \(\rho \colon A \to \End V\).
    \end{dfn}
    
    \begin{dfn}{Module}{}
        Let \(A\) be a \(\field\)-algebra.
        A \define{left \(\symbf{A}\)-module}\index{left A-module@left \(A\)-module}, \(M\), is an abelian group, with the binary operation denoted \(+\), equipped with a \defineindex{left action}
        \begin{align}
            \action \colon A \times M &\to M\\
            (a, m) &\mapsto a \action m
        \end{align}
        such that for all \(a, b \in A\) and \(m, n \in M\) we have\footnote{Note that M1 and M2 simply say that this is a group action on the set \(M\), and M3 and M4 two impose that this group action is compatible with both the group operation and addition in the algebra.}
        \begin{itemize}
            \item[M1] \((ab)\action m = a\action (b\action m)\) (note that \((ab)\) is the product in \(A\));
            \item[M2] \(1 \action m = m\).
            \item[M3] \(a\action(m + n) = a\action m + a\action n\);
            \item[M4] \((a + b)\action m = a\action m + b\action m\);
        \end{itemize}
        
        One can similarly define a \define{right \(\symbf{A}\)-module}\index{right A-module@right \(A\)-module}, \(M\), as an abelian group with a \defineindex{right action}
        \begin{align}
            \action \colon M \times A &\to M\\
            (m, a) &\mapsto m \action a
        \end{align}
        such that for all \(a, b \in A\) and \(m, n \in M\) we have
        \begin{itemize}
            \item[M1] \((m + n) \action a = m \action a + n \action a\);
            \item[M2] \(m \action (a + b) = m \action a + m \action b\);
            \item[M3] \(m \action (ab) = (m \action a) \action b\);
            \item[M4] \(m \action 1 = m\).
        \end{itemize}
        
        A \define{two-sided \(\symbf{A}\)-module}\index{two-sided A-module@two-sided \(A\)-module}\index{\(A\)-module}\index{module} is then an abelian group, \(M\), which is simultaneously a left and right \(A\)-module satisfying
        \begin{equation}
            a \action (m \action b) = (a \action m) \action b
        \end{equation}
        for all \(a, b \in A\) and \(m \in M\).
    \end{dfn}
    
    When it doesn't risk confusion we will write \(a \action m\) as \(am\) and \(m \action a\) as \(ma\).
    
    Note that a module is a generalisation of the notion of a vector space.
    In fact, if \(A = \field\) then a module is exactly a vector space.
    
    More compactly, one can define a right \(A\)-module as a left \(A^{\op}\)-module, where \(A^{\op}\) is the \defineindex{opposite algebra} of \(A\), defined to be the same underlying vector space with multiplication \(*\) defined by \(a * b = ba\), where \(ba\) is the multiplication in \(A\).
    Because of this we will almost never have reason to work with right modules, we can always turn them into a left module over the opposite algebra instead.
    
    Note that if \(A\) is commutative every left \(A\)-module is a right \(A\)-module and vice versa, and also a two-sided module.
    
    Without further clarification the term \enquote{module} will mean
    \begin{itemize}
        \item a left module if \(A\) is not necessarily commutative;
        \item a two sided module if \(A\) is commutative.
    \end{itemize}
    
    A representation of \(A\) and an \(A\)-module carry exactly the same information.
    Given a representation, \(\rho \colon A \to \End V\) we may define a group action on \(V\) by \(a \action v = \rho(a)v\).
    Composition in \(\End V\) is exactly repeated application of this action: \([\rho(a)\rho(b)]v = \rho(a)[\rho(b)v]\) (M1).
    The unit of \(\End V\) is the identity morphism, \(\id_V\), and \(1 \in A\) must map to \(\id_V\), so \(\rho(1)v = \id_V v = v\) (M2).
    Linearity of \(\rho(a)\) means that \(\rho(a)(v + w) = \rho(a)v + \rho(v)w\) (M3).
    Linearity of \(\rho\) means that \(\rho(a + b)v = \rho(a)v + \rho(b)v\) (M4).
    
    Conversely, given an \(A\)-module, \(M\), we can define scalar multiplication by \(\lambda \in \field\) on \(M\) by \(\lambda m = (\lambda 1) m\) where \(\lambda 1\) is scalar multiplication in \(A\).
    This makes \(M\) a vector space, and we may define a morphism \(\rho \colon A \to \End M\) by defining \(\rho(a)\) by \(\rho(a) = a \action m\), which uniquely determines \(\rho(a)\), say by considering the action on some fixed basis of \(M\).
    
    Further, these two constructions are inverse, given a module if we construct the corresponding representation then construct the corresponding module from that we get back the original module, and vice versa.
    This means that the notion of a representation and a module really are the same, and we don't need to distinguish between them.
    We will use whichever terminology and notation is better suited to the problem, which is usually the module terminology and notation.
    
    \begin{prp}{}{}
        Let \(V\) be a \(\field\)-vector space, \(G\) a group, and \(\rho \colon G \to GL(V)\) a group homomorphism.
        We may define a \(\field G\)-module by extending this map linearly, defining
        \begin{equation}
            \left( \sum_{g \in G} c_g g \right) \action v = \sum_{g \in G} c_g \rho(g)v.
        \end{equation}
        Conversely, given a left \(\field G\)-module on \(V\) we may define a group homomorphism \(\rho \colon G \to \generalLinear(V)\) by defining \(\rho(g)\) to be the linear operation \(v \mapsto g \action v\).
        \begin{proof}
            This is just a special case of the equivalence of representations and modules discussed above.
        \end{proof}
    \end{prp}
    
    Note that a \defineindex{group representation} is defined to be a group homomorphism \(\rho \colon G \to \generalLinear(V)\).
    The above result shows that a group representation of \(G\) is exactly the same as an algebra representation of \(\field G\), so we can just study algebras.
    
    \begin{dfn}{Regular Representation}{}
        Let \(V = A\) be an algebra and define \(\rho \colon A \to \End A\) by \(\rho(a)b = ab\).
        This is called the \defineindex{left regular representation}.
        Similarly, the \defineindex{right regular representation} is given by defining \(\rho(a)b = ba\).
    \end{dfn}
    
    \section{Direct Sums}
    The goal of much of representation theory is to classify possible representations.
    To do this we usually decompose representations into smaller parts that can be more easily classified.
    This decomposition is done by the direct sum.
    
    \begin{dfn}{Direct Sum}{}
        Let \(M\) and \(N\) be \(A\)-modules.
        The \defineindex{direct sum}, \(M \oplus N\), is the \(A\)-module given by the direct sum of the underlying abelian groups equipped with the action
        \begin{equation}
            a(m \oplus n) = am \oplus an
        \end{equation}
        for all \(a \in A\), \(m \in M\) and \(n \in N\).
    \end{dfn}
    
    The required properties follow immediately from the definition:
    \begin{itemize}
        \item[M1] \((ab)(m \oplus n) = (ab)m \oplus (ab)n = a(bm) \oplus a(bn) = a(bm \oplus bn) = a(b(m \oplus n))\);
        \item[M2] \(1(m \oplus n) = 1m \oplus 1n = m \oplus n\);
        \item[M3] \(a((m \oplus n) + (m' \oplus n')) = a((m + m') \oplus (n + n')) = a(m + m') \oplus a(n + n') = (am + am') \oplus (an + an') = (am \oplus an) + (am' \oplus an') = a(m \oplus n) + a(m' \oplus n')\);
        \item[M4] \((a + b)(m \oplus n) = (a + b)m \oplus (a + b)n = (am + bm) \oplus (an + bn) = (am \oplus an) + (bm \oplus bn) = a(m \oplus n) + b(m \oplus n)\).
    \end{itemize}
    
    \begin{dfn}{Submodule}{}
        Let \(M\) be a left \(A\)-module.
        An abelian subgroup \(N \trianglelefteq M\) is a \define{\(\symbf{A}\)-submodule}\index{submodule} if \(AN \subseteq N\).
        In this case we say that \(N\) is \defineindex{invariant} under the action of \(A\).
    \end{dfn}
    
    Note that by \(AN\) we mean
    \begin{equation}
        AN = \{an \mid a \in A , n \in N\}.
    \end{equation}
    So \(AN \subseteq N\) means that \(an \in N\) for all \(a \in A\) and \(n \in N\).
    Thus, invariance means that no element of \(N\) leaves \(N\) under the action of \(A\).
    
    \begin{dfn}{Trivial Submodule}{}
        Every \(A\)-module, \(M\), admits two submodules, \(M\) itself and the zero module, \(0\), which contains only \(0\).
        We call these \define{trivial submodules}\index{trivial submodule}.
    \end{dfn}
    
    Note that some texts call only \(0\) the trivial submodule, and make the distinction of a submodule vs a \emph{proper} submodule, the distinction being that \(M\) is not a proper submodule of \(M\).
    Then when we say \enquote{nontrivial submodule} these texts will say \enquote{nontrivial proper submodule}.
    
    \begin{dfn}{Simple Submodule}{}
        Let \(M\) be an \(A\)-module.
        We say that \(M\) is \defineindex{simple} or \defineindex{irreducible} if it contains no nontrivial submodules.
    \end{dfn}
    
    Typically \enquote{simple} is used for modules and \enquote{irreducible} is used more for representations, although irreducible is used for both.
    
    \begin{dfn}{Semisimple}{}
        Let \(M\) be an \(A\)-module.
        Then \(M\) is \defineindex{semisimple} or \defineindex{completely reducible} if it can be written as a direct sum of finitely many simple modules.
    \end{dfn}
    
    That is, \(M\) is semisimple if 
    \begin{equation}
        M = \bigoplus_{i=1}^n N_i = N_1 \oplus \dotsb \oplus N_n
    \end{equation}
    where each \(N_i\) is simple.
    Note that we define the empty sum to be the zero module, so the zero module is considered semisimple (and also simple, since it contains only itself as a submodule).
    
    Again, \enquote{semisimple} is typically used only for modules, and \enquote{completely reducible} is used primarily for representations.
    
    \begin{dfn}{Indecomposable}{}
        Let \(M\) be an \(A\)-module.
        Then \(M\) is \defineindex{indecomposable} if \(M\) cannot be written as a direct sum of nontrivial modules.
    \end{dfn}
    
    The nontrivial requirement here just rules out decompositions of the form\footnote{Note that with our definition of the direct sum this really only holds up to isomorphism, since \(M\) has elements \(m\) whereas \(M \oplus 0\) has elements \((m, 0)\). However, we're yet to define morphisms between modules, and once we do we'll see that \(\oplus\) is the product in the category of modules, and as such is only defined up to isomorphism, so we may as well momentarily take the isomorphism that makes this equality true.} \(M = M \oplus 0\).
    
    Note that every simple (irreducible) module is indecomposable, since if it had a decomposition \(M = N_1 \oplus N_2\) with \(N_i\) nontrivial then their is a canonical copy of each \(N_i\) as a submodule of \(M\).
    The converse does not hold in general, not all indecomposable modules are irreducible.
    It is possible that \(M\) contains a submodule, \(N\), but that there is no submodule \(N'\) such that \(M = N \oplus N'\).
    Contrast this to finite dimensional vector spaces where we can take \(N'\) to be the orthogonal complement (with respect to some inner product) of \(N\) and this direct sum holds.
    We can still form the orthogonal complement of a submodule, but it will not, in general, be a submodule.
    There are, however, many special cases, such as finite dimensional complex representations of (group algebras) finite groups, where the orthogonal complement can be defined in such a way that it is a submodule, and in this case indecomposable and irreducible coincide.
    
    One of the main goals of representation theory is to classify all indecomposable modules of a given algebra.
    This then gives us an understanding of \emph{all} modules over that algebra, since any nonsimple or decomposable module may be realised as a direct sum of these classified indecomposable modules.
    
    \section{Module Homomorphisms}
    \begin{dfn}{Module Homomorphism}{}
        Let \(M\) and \(N\) be \(A\)-modules.
        An \define{\(\symbf{A}\)-module homomorphism}\index{module homomorphism} or \defineindex{intertwiner} is a homomorphism of the underlying abelian groups \(\varphi \colon M \to N\) which \enquote{commutes} with the action of \(A\), by which we mean
        \begin{equation}
            \varphi(a \action m) = a \action \varphi(m)
        \end{equation}
        for all \(a \in A\) and \(m \in M\).
        
        An invertible \(A\)-module homomorphism is called an \defineindex{isomorphism} of \(A\)-modules.
        
        Homomorphisms of right \(A\)-modules may be defined similarly.
    \end{dfn}
    
    \begin{ntn}{}{}
        We write \(\Hom_A(M, N)\) for the set of \(A\)-module homomorphisms \(M \to N\).
        Note that \(\Hom_A(M, N) \subseteq \Hom_{\Ab}(M, N)\) where \(\Hom_{\Ab}(M, N)\) is the set of all homomorphisms \(M \to N\) of the underlying abelian groups.
    \end{ntn}
    
    Note that in \(\varphi(a \action m)\) \(a\) is acting on an element of \(M\), and in \(a \action \varphi(m)\) \(a\) is acting on an element of \(N\), so these are in general different actions.
    Writing \(a \action {}\) for the map \(x \mapsto a \action x\) we can express the condition of commuting action as the commutativity of the diagram
    \begin{equation}
        \begin{tikzcd}
            M \arrow[r, "\varphi"] \arrow[d, "a \action {}"'] & N \arrow[d, "a \action {}"]\\
            M \arrow[r, "\varphi"'] & N
        \end{tikzcd}
    \end{equation}
    for all \(a \in A\).
    
    \begin{lma}{}{}
        Isomorphisms of \(A\)-modules are exactly bijective morphisms of \(A\)-modules.
        \begin{proof}
            Let \(\varphi \colon M \to N\) be a bijective morphism of \(A\)-modules.
            Then the (set-theoretic) inverse, \(\varphi^{-1} \colon N \to M\), exists.
            We claim that this is a morphism of \(A\)-modules.
            This follows by taking \(n \in N\) to be the image of \(m \in M\) under \(\varphi\), giving
            \begin{equation}
                \varphi^{-1}(a \action n) = \varphi^{-1}(a \action \varphi(m)) = \varphi^{-1}(\varphi(a \action m)) = a \action m = a \action \varphi^{-1}(m).
            \end{equation}
            
            Conversely, if \(\varphi \colon M \to N\) is an isomorphism of \(A\)-modules it must necessarily be that \(\varphi^{-1}\) is the (set-theoretic) inverse of the underlying function of \(\varphi\), and so \(\varphi\) must be bijective.
        \end{proof}
    \end{lma}
    
    If we instead talk of representations \((V, \rho)\) and \((W, \sigma)\) then a homomorphism of representations, \(\varphi \colon V \to W\), must satisfy \(\varphi(\rho(a)v) = \sigma(a)\varphi(v)\).
    Further, by linearity of \(\rho\) and \(\sigma\) and the fact that \(\rho(1) = \id_V\) and \(\sigma(1) = \id_W\) we have that for \(\lambda \in \field\)
    \begin{equation}
        \varphi(\lambda m) = \varphi(\rho(1)\lambda m) = \varphi(\rho(\lambda 1) m) = \sigma(\lambda 1)\varphi(m) = \lambda \sigma(1) \varphi(m) = \lambda \varphi(m).
    \end{equation}
    This shows that \(\varphi\) must be a linear map \(\varphi \colon V \to W\).
    In fact, we can \emph{define} a homomorphism of representations to be a linear map \(\varphi \colon M \to N\) satisfying \(\varphi(\rho(a)m) = \sigma(a)\varphi(m)\).
    We will also write \(\Hom_A(V, W)\) for the set of representation morphisms \(V \to W\).
    Note then that \(\Hom_A(V, W) \subseteq \Hom_{\Vect[\field]}(V, W)\) where \(\Hom_{\Vect[\field]}(V, W)\) is the set of linear maps \(V \to W\) of the underlying vector spaces.
    Using the notation \(\Hom_A\) for both modules and representations is justified by the following remark.
    
    \begin{remark}{}{}
        There is a category, \(\AMod\) (\(\ModA\)), with left (right) \(A\)-modules as objects and \(A\)-module homomorphisms as morphisms.
        Similarly, there is a category \(\Rep(A)\) of representations of \(A\) with objects being representations \((V, \rho)\) and morphisms being homomorphisms of representations.
        
        In \cref{sec:representaitons and modules} we showed that we have a mapping \(F \colon \AMod \to \Rep(A)\) constructing a representation from a module, and a mapping \(G \colon \Rep(A) \to \AMod\) constructing a module from a representation.
        In the discussion above we extend this mapping to define a representation homomorphism from a module homomorphism.
        We can also ignore the requirement of linearity with respect to scalar multiplication in the definition of a representation homomorphism to recover a module homomorphism.
        Further, applying either of these constructions to the appropriate identity map just gives the identity, and both constructions preserve composition.
        These operations on homomorphisms are also inverses of each other.
        Thus, \(F\) and \(G\) are functors and we have \(FG = \id_{\Rep(A)}\) and \(GF = \id_{\AMod}\).
        Thus, \(\AMod\) and \(\Rep(A)\) are isomorphic as categories, justifying the fact that we will soon cease to distinguish between them.
    \end{remark}
    
    \begin{lma}{}{}
        The category \(\AMod\) defined above is indeed a category.
        \begin{proof}
            First note that \(\id_M \colon M \to M\) is an \(A\)-module homomorphism for any \(A\)-module, \(M\), since we have
            \begin{equation}
                \id_M(a \action m) = a \action m = a \action \id_M(m).
            \end{equation}
            Now note that if \(\varphi \colon M \to N\) and \(\psi \colon N \to P\) are module homomorphisms then \(\psi \circ \varphi \colon M \to P\) is a module homomorphism since
            \begin{equation*}
                (\psi \circ \varphi)(a \action m) = \psi(\varphi(a \action m)) = \psi(a \action \varphi(m)) = a \action \psi(\varphi(m)) = a \action (\psi \circ \varphi)(m)
            \end{equation*}
            for all \(a \in A\) and \(m \in M\).
            Finally, composition is just composition of the underlying functions, which is associative.
        \end{proof}
    \end{lma}
    
    \section{Schur's Lemma}
    We can now give one of the first results of representation theory.
    It places a restriction on the types of morphisms we can have between modules when one or more of the modules is simple.
    We give the result as a proposition and a corollary, although for historical reasons it's called a lemma.
    The proposition is more general, and the corollary is a special case.
    Both are known as Schur's lemma, with context determining if we use the more general result or the special case.
    
    Before we can prove this result however we need a couple of results about kernels and images of module morphisms.
    
    \begin{lma}{}{}
        Let \(\varphi \colon M \to N\) be a morphism of modules.
        Then \(\ker \varphi\) is a submodule of \(M\) and \(\im \varphi\) is a submodule of \(N\).
        \begin{proof}
            \Step{\(\ker \varphi\)}
            We know that \(\ker \varphi\) is a subgroup of \(M\), so we only need to show that it is invariant under the action of \(A\).
            Take \(m \in \ker \varphi\), that is \(m \in M\) is such that \(\varphi(m) = 0\), and \(a \in A\).
            Then
            \begin{equation}
                \varphi(a \action m) = a \action \varphi(m) = a \action 0.
            \end{equation}
            For arbitrary \(m' \in M\) we have
            \begin{equation}
                a \action 0 = a \action (m' - m') = (a \action m') - (a \action m') = 0
            \end{equation}
            so \(a \action 0 = 0\) for any \(a \in A\), and thus \(\varphi(a \action m) = a \action 0 = 0\), so \(a \action m \in \ker \varphi\).
            
            \Step{\(\im \varphi\)}
            We know that \(\im \varphi\) is a subgroup of \(M\), so we only need to show that it is invariant under the action of \(A\).
            Take \(n \in \im \varphi\) and \(a \in A\).
            There exists some \(m \in M\) such that \(n = \varphi(m)\).
            Then
            \begin{equation}
                a \action n = a \action \varphi(m) = \varphi(a \action m)
            \end{equation}
            and \(a \action m \in M\) so \(a \action n \in \im \varphi\).
        \end{proof}
    \end{lma}
    
    \begin{prp}{Schur's Lemma}{prp:schurs lemma}
        Let \(\field\) be any (not necessarily algebraically closed) field, and let \(A\) be an algebra over \(\field\).
        Let \(M\) and \(N\) be \(A\)-modules and let \(\varphi \colon M \to N\) be a morphism of \(A\)-modules.
        Then
        \begin{enumerate}
            \item if \(M\) is simple either \(\varphi = 0\) or \(\varphi\) is injective;
            \item if \(N\) is simple either \(\varphi = 0\) or \(\varphi\) is surjective.
        \end{enumerate}
        Combined if \(M\) and \(N\) are simple then either \(\varphi = 0\) or \(\varphi\) is an isomorphism.
        \begin{proof}
            \Step{\(M\) Simple}
            Let \(M\) be simple, so its only submodules are \(0\) and \(M\).
            We know that \(\ker \varphi\) is a submodule of \(M\), so there are two cases to consider:
            \begin{itemize}
                \item If \(\ker \varphi = M\) then every element of \(M\) maps to \(0\), so \(\varphi = 0\).
                \item If \(\ker \varphi = 0\) then \(\varphi\) is injective\footnote{We know that for group homomorphisms if the kernel is trivial then the map is injective, and injectivity is a set-theoretic property, so it still holds when we add the extra structure of the \(A\)-action}.
            \end{itemize}
            
            \Step{\(N\) Simple}
            Let \(N\) be simple, so its only submodules are \(0\) and \(N\).
            We know that \(\im \varphi\) is a submodule of \(N\), so there are two cases to consider:
            \begin{itemize}
                \item If \(\im \varphi = 0\) then every element of \(M\) maps to \(0\), so \(\varphi = 0\).
                \item If \(\im \varphi = N\) then \(\varphi\) is surjective.
            \end{itemize}
        \end{proof}
    \end{prp}
    
    \begin{crl}{Schur's Lemma}{crl:schurs lemma}
        Let \(\field\) be an algebraically closed field, and let \(A\) be an algebra over \(\field\).
        Let \(V\) be a finite dimensional representation of \(A\).
        Then any representation homomorphism \(\varphi \colon V \to V\) is a multiple of the identity.
        That is, \(\varphi = \lambda \id_V\) for \(\lambda \in \field\).
        Note that \(\lambda = 0\) subsumes the trivial case.
        \begin{proof}
            Let \(\lambda \in \field\) be an eigenvalue of \(\varphi\) with corresponding eigenvector \(v \in V\).
            Note that eigenvalues exist because
            \begin{enumerate}[label={\alph*)}]
                \item \(V\) is finite dimensional so the determinant may be defined as a polynomial in the entries of some matrix representing \(\varphi\) in a fixed basis; and
                \item \(\field\) is algebraically closed, so this polynomial has roots.
            \end{enumerate}
            Then by definition \(\varphi(v) = \lambda v\) which we can rearrange to \((\varphi - \lambda \id_V) v = 0\).
            Thus, \(v \in \ker(\varphi - \lambda \id_V)\), and since eigenvectors are, by definition, nonzero this means that \(\ker(\varphi - \lambda \id_V) \ne 0\), so \(\varphi - \lambda \id_V\) is not injective, so by Schur's lemma (\cref{prp:schurs lemma}) we must have that \(\varphi - \lambda \id_V = 0\).
            Thus, \(\varphi = \lambda \id_V\).
        \end{proof}
    \end{crl}
    
    \begin{crl}{}{crl:commutative algebra irreps are one dimensiona}
        Let \(A\) be a commutative algebra over an algebraically closed field, \(\field\).
        Then all nontrivial finite dimensional irreducible representations of \(A\) are one dimensional.
        \begin{proof}
            Let \(V\) be a finite dimensional irreducible representation of \(A\).
            For \(a \in A\) define a map \(\varphi_a \colon V \to V\) by \(v \mapsto \varphi_a(v) = a \action v\).
            This is an intertwiner: take \(b \in A\) and \(v \in V\), then we have
            \begin{equation}
                \varphi_a(b \action v) = a \action (b \action v) = (ab) \action v = (ba) \action v = b \action (a \action v) = b \action \varphi_a(v).
            \end{equation}
            Note that this is only true because \(ab = ba\).
            
            By Schur's lemma (\cref{crl:schurs lemma}) there exists some \(\lambda_a \in \field\) such that \(\varphi_a = \lambda_a \id_V\).
            Then \(a \action v = \varphi_a(v) = \lambda_a v\), so every \(a \in A\) acts as scalar multiplication.
            This means that any subspace is invariant, since every subspace is, by definition, invariant under scalar multiplication.
            Thus, the only way that a representation can have no nontrivial invariant subspaces if if it only has trivial subspaces, which is only true if it is one dimensional (zero dimensional being ruled out by the assumption that the representation is nontrivial).
        \end{proof}
    \end{crl}
    
    \begin{exm}{}{}
        Consider \(A = \field[x]\), which is a commutative algebra.
        We can determine all irreducible representations of \(A\).
        
        A representation, \(\rho \colon \field[x] \to \End V\), is fully determined by the value of \(\rho(x)\), since given an arbitrary polynomial, \(f(x) = \sum_{i=1}^{n} a_i x^i\), its action on \(v \in V\) is determined through linearity by
        \begin{equation}
            f(x) \action v = \rho(f(x)) v = \rho\left( \sum_{i=1}^{n} a_i x^i \right) v = \sum_{i=1}^n a_i \rho(x)^i v.
        \end{equation}
        
        Further, by \cref{crl:commutative algebra irreps are one dimensiona} we know that any irreducible representation of \(\field[x]\) is one dimensional, so it must be that \(\rho(v) = \lambda v\) for some \(\lambda \in \field\).
        
        Let \(V_\lambda\) denote the one-dimensional representation in 
        which \(x\) acts as scalar multiplication by \(\lambda\).
        We claim that \(V_\lambda \isomorphic V_{\mu}\) if and only if \(\lambda = \mu\).
        Suppose that \(\varphi \colon V_\lambda \to V_\mu\) is an isomorphism.
        Then \(\varphi(x \action v) = \varphi(\lambda v) = \lambda \varphi(v)\) and \(\varphi(x \action v) = x \action \varphi(v) = \mu \varphi(v)\).
        Thus, \(\lambda = \mu\).
        
        So, we have classified all irreducible representations of \(\field[x]\), they are precisely the one dimensional vector spaces, \(V_\lambda\) for \(\lambda \in \field\) in which \(\rho(x) = \lambda \id_{V_\lambda}\).
        
        This result generalises to polynomials in an arbitrary number of variables, \(\field[x_1, \dotsc, x_n]\).
        Then a representation is fully determined by the values of \(\rho(x_1)\) through \(\rho(x_n)\).
        Thus an irreducible representation is a one dimensional vector space, \(V_{\lambda_1, \dotsc, \lambda_n}\) in which \(\rho(x_i) = \lambda_i \id_{V_{\lambda_1, \dotsc, \lambda_n}}\).
        
        Go back to the case of \(A = \field[x]\).
        For a nontrivial (\(\lambda \ne 0\)) finite dimensional irreducible representation, \(V_\lambda\), instead of starting with the action of \(x\) we can perform a change of variables and work with \(y = x/\lambda\).
        Then we get the representation \(V_1\).
        This means that all finite dimensional irreducible representations of \(\field[x]\) are essentially the same, up to rescaling.
        This also means that they're pretty boring.
        
        Indecomposable representations of \(\field[x]\) are more interesting on the other hand.
        Let \(V\) be a finite dimensional representation.
        We can fix a basis and look at matrices.
        Suppose \(B \in \End V\), then since we work over an algebraically closed field we know that the Jordan normal form of \(B\) exists after a basis change, allowing us to write the matrix of \(B\) as
        \begin{equation}
            B = 
            \begin{pmatrix}
                J_{\lambda_1, n_1} \\
                & J_{\lambda_2, n_2} \\
                & & \ddots \\
                & & & J_{\lambda_k, n_k}
            \end{pmatrix}
        \end{equation}
        where \(J_{\lambda_i, n_i}\) is the \(n_i \times n_i\) Jordan block matrix
        \begin{equation}
            J_{\lambda_i, n_i} = 
            \begin{pmatrix}
                \lambda_i & 1 \\
                & \lambda_i & 1\\
                & & \ddots & \ddots\\
                & & & \lambda_i & 1\\
                & & & & \lambda_i
            \end{pmatrix}
            .
        \end{equation}
        This block diagonal decomposition of \(B\) gives us a corresponding direct sum decomposition of \(V\).
        Each Jordan block cannot be diagonalised (with the exception of the \(1 \times 1\) Jordan blocks which are trivially diagonal).
        Thus we cannot further decompose \(B\) and so we cannot further decompose \(V\).
        The result is that
        \begin{equation}
            V = \bigoplus_{i=1}^{k} V_{\lambda_i, n_i}
        \end{equation}
        where \(V_{\lambda_i, n_i} = \field^{n_i}\) is an \(n_i\)-dimensional vector space upon which the action of \(B\) is given by \(J_{\lambda_i, n_i}\).
        Then taking \(B = \varphi(x)\) defines a representation of \(\field[x]\) on \(V\), and specifically we have the subrepresentations \(V_{\lambda_i, n_i}\) in which \(x\) acts as the Jordan block \(J_{\lambda_i, n_i}\).
    \end{exm}
    
    \section{Ideals and Quotients}
    \begin{dfn}{Ideals}{}
        Let \(A\) be an algebra.
        A subspace, \(I \subseteq A\), such that \(AI \subseteq I\) is called a \defineindex{left ideal}.
        Similarly if \(IA \subseteq I\) then we call \(I\) a \defineindex{right ideal}.
        A \defineindex{two-sided ideal} is simultaneously a left and right ideal.
    \end{dfn}
    
    Note that by \(AI\) we mean \(AI = \{a i \mid a \in A, i \in I\}\), so the condition that \(I\) is a left ideal is that \(ai \in I\) for all \(a \in A\) and \(i \in I\).
    
    \begin{exm}{}{}
        \begin{itemize}
            \item Any algebra, \(A\), always has \(0\) and \(A\) as ideals.
            If these are the only ideals then we call \(A\) \defineindex{simple}.
            \item Any left (right) ideal is a submodule of the left (right) regular representation.
            This is simply identifying that \(A\) is an \(A\)-module with the action being left (right) multiplication and as such the notion of an ideal coincides with that of a submodule.
            Note that the notion of a simple module coincides with the notion of a simple algebra under this identification.
            \item If \(f \colon A \to B\) is an algebra morphism then \(\ker f\) is a two-sided ideal.
            We know that \(\ker f\) is a subspace of \(A\), so just note that if \(a \in \ker f\) then \(f(a) = 0\) and we have
            \begin{equation}
                f(ba) = f(b)f(a) = f(b)0 = 0
            \end{equation}
            and
            \begin{equation}
                f(ab) = f(a)f(b) = 0f(a) = 0
            \end{equation}
            so \(ab\) and \(ba\) are in \(\ker f\).
        \end{itemize}
    \end{exm}
    
    We will say \enquote{ideal} when we mean either a left ideal.
    Note that in the commutative case all left ideals are right ideals and hence two-sided ideals, so we don't need to distinguish the three cases.
    
    \begin{ntn}{}{}
        Let \(A\) be an algebra and \(S \subseteq A\) a subset of \(A\).
        Denote by \(\langle S \rangle\) the two-sided ideal generated by \(S\).
        That is,
        \begin{equation}
            \langle S \rangle = \Span\{asb \mid s \in S, \text{ and } a, b \in A\}.
        \end{equation}
    \end{ntn}
    
    For example, consider \(\field[x]\).
    Then \(\langle x \rangle\) consists of all polynomials that can be factorised as \(xf(x)\) where \(f(x)\) is an arbitrary polynomial, so \(f(x) = \sum_{i=0}^n a_i x^i\).
    Thus, \(x f(x) = \sum_{i=0} a_i x^{i + 1}\), so \(\langle x \rangle\) consists of all polynomials with zero constant term.
    More generally, \(\rangle x - a \rangle\) for \(a \in \field\) consists of all polynomials which factorise as \((x - a)f(x)\) for an arbitrary polynomial \(f(x)\), and thus this is the ideal consisting of all polynomials with \(a\) as a root.
    
    The point of defining ideals is really in order to define quotients.
    In this way ideals are to algebras as normal subgroups are to groups.
    
    \begin{dfn}{Quotient}{}
        Let \(A\) be an algebra and \(I \subseteq A\) an ideal.
        We define the \define{quotient}\index{quotient!algebra} to be the algebra \(A/I\) whose elements are equivalence classes
        \begin{equation}
            [a] = a + I \coloneq \{a' \in A \mid a - a' \in I\}.
        \end{equation}
        Addition and scalar multiplication are defined by
        \begin{equation}
            [a] + [b] = (a + I) + (b + I) = [a + b] = a + b + I
        \end{equation}
        and
        \begin{equation}
            \lambda[a] = [\lambda a]
        \end{equation}
        for \(a, b \in A\) and \(\lambda \in \field\).
    \end{dfn}
    
    \begin{lma}{}{}
        The quotient of an algebra by an ideal is again an algebra.
        \begin{proof}
            Let \(A\) be an algebra and \(I \subseteq A\) an ideal.
            Note that the quotient of a vector space by any subspace is again a vector space, so we need only define a multiplication operation on this vector space.
            We do so by defining
            \begin{equation}
                [a][b] = (a + I)(b + I) \coloneq [ab] = ab + I.
            \end{equation}
            We need to show that this is well-defined and satisfies the properties of multiplication in an algebra.
            
            \Step{Well-Defined}
            Let \(a, a' \in A\) be representatives of the same equivalence class, \([a] = [a']\).
            Then by definition \(a - a' \in I\).
            For \(b \in A\) we then have
            \begin{equation}
                [a][b] = [ab] = [a'b + (a - a')b] = [a'b] = [a'][b].
            \end{equation}
            Here we've used the fact that \(a - a' \in I\) and \(I\) is an ideal so \((a - a')b \in I\), and we can add any element of \(I\) inside an equivalence class without leaving the equivalence class.
            Similarly, one can show that \([a][b] = [a][b']\) whenever \([b] = [b']\).
            Thus, this product is well-defined.
            
            \Step{Algebra}
            Linearity in the first argument follows from a direct calculation using the properties of quotient spaces:
            \begin{multline}
                [(a + \lambda a')b] = [a b + \lambda a' b] = [ab] + \lambda [a' b]\\
                = [a][b] + \lambda [a'][b]= ([a] + \lambda[a'])[b] = [a + \lambda a'][b]
            \end{multline}
            for \(a, a', b \in A\) and \(\lambda \in \field\).
            Linearity in the second argument follows similarly.
            Associativity follows from
            \begin{equation}
                [a]([b][c]) = [a][bc] = [a(bc)] = [(ab)c] = [ab][c] = ([a][b])[c].
            \end{equation}
            Unitality follows from
            \begin{equation}
                [1][a] = [1a] = [a], \qqand [a][1] = [a1] = [a].
            \end{equation}
        \end{proof}
    \end{lma}
    
    \subsection{Generators and Relations}
    One of the most common ways to define an algebra is as a quotient of another algebra by some ideal given in terms of generators.
    The most common starting place is the free algebra, \(\field\langle x_1, \dotsc, x_m \rangle\).
    We can then take \(f_1, \dotsc, f_n \in \field\langle x_1, \dotsc, x_m\rangle\), and form an ideal, \(\langle f_1, \dotsc, f_n \rangle\).
    Then we may form the algebra
    \begin{equation}
        A = \field\langle x_1, \dotsc, x_m \rangle / \langle f_1, \dotsc, f_n \rangle.
    \end{equation}
    Intuitively, elements of this are non-commutative polynomials in the \(x_i\) subject to the constraint that anywhere that we can manipulate the polynomial to be written with \(f_i\) we can set that \(f_i\) equal to zero.
    
    For example, let \(f_{i,j} = x_i x_j - x_j x_i\) for \(i, j = 1, \dotsc, m\).
    Consider the algebra \(A = \field \langle x_1, \dotsc, x_m \rangle / \langle f_{i,j} \rangle\) consists of non-commutative polynomials in \(x_i\) subject to the condition that \(x_i x_j - x_j x_i = 0\), which is to say \(x_i x_j = x_j x_i\), which is exactly the condition that the \(x_i\) \emph{do} commute with each other.
    
    Another example is \(A = \field \langle x_1, \dotsc, x_n \rangle / \langle x_i^2 - e, x_ix_{i+1}x_i - x_{i+1}x_ix_{i+1} \rangle\).
    This sets \(x_i^2 = e\) and \(x_ix_{i+1}x_i = x_{i+1}x_ix_{i+1}\) (called the \defineindex{braid relation}).
    These are exactly the relations defining the symmetric group, \(S_n\), when we interpret \(x_i\) as the transposition \(\cycle{i,i+1}\).
    We're also taking linear combinations of these \(x_i\), so \(A = \field S_n\).
    
    \subsection{Quotient Modules}
    \begin{dfn}{Quotient Module}{}
        Let \(M\) be an \(A\)-module and \(N\) a submodule of \(M\).
        We define the \define{quotient module}\index{quotient!module}, \(M/N\), to be the module consisting of equivalence classes
        \begin{equation}
            [m] = m + N \coloneq \{m' \in M \mid m - m' \in M\}.
        \end{equation}
        Addition in this module is defined by
        \begin{equation}
            [m] + [m'] = [m + m']
        \end{equation}
        for \(m, m' \in M\) and the action of \(A\) is given by
        \begin{equation}
            a \action [m] = [a \action m]
        \end{equation}
        for \(a \in A\) and \(m \in M\).
    \end{dfn}
    
    \begin{lma}{}{}
        The quotient of a module by a submodule is again a module.
        \begin{proof}
            Let \(M\) be an \(A\)-module with \(N \subseteq M\) a submodule.
            Then \(N\) is a subgroup of an abelian group, and so is automatically a normal subgroup.
            Then we know that \(M/N\) is an abelian group also.
            
            Suppose that \([m] = [m']\), that is \(m\) and \(m'\) are representatives of the same equivalence class.
            Then \(m' - m \in N\).
            We then have
            \begin{multline}
                a \action [m] = a \action [m' + (m - m')] = [a \action (m' + (m - m'))]\\
                = [a \action m' + a \action (m - m')] = [a \action m'] = a \action [m'].
            \end{multline}
            Here we've used the fact that \(m' - m \in N\) and \(N\) is a submodule so \(a \action (m' - m) \in N\) as well.
            So, the action of \(a \in A\) on \([m] = [m']\) is well-defined.
            
            It remains to show that the action of \(A\) on \(M/N\) makes it an \(A\)-module:
            \begin{itemize}
                \item[M1] \((ab) \action [m] = [(ab) \action m] = [a \action (b \action m)] = a \action [b \action m] = a \action (b \action [m])\);
                \item[M2] \(1 \action [m] = [1 \action m] = [m]\);
                \item[M3] \(a \action ([m] + [n]) = a \action [m + n] = [a \action (m + n)] = [a \action m + a \action n] = [a \action m] + [a \action n] = a \action [m] + a \action [n]\);
                \item[M4] \((a + b) \action [m] = [(a + b) \action m] = [a \action m + b \action m] = [a \action m] + [b \action m] = a \action [m] + b \action [m]\)
            \end{itemize}
            for all \(a, b \in A\) and \(m, n \in M\).
        \end{proof}
    \end{lma}
    
    \begin{remark}{}{}
        Consider the left regular representation of \(A\).
        As we have mentioned ideals of \(A\) are precisely submodules of the regular representation.
        It follows that \(A/I\) is a left \(A\)-module precisely when \(I\) is a left ideal.
    \end{remark}
    
    \chapter{Tensor Products}
    \section{Tensor Product of Modules}
    We first define the tensor product of \(R\)-modules (\(R\) a ring). 
    This definition can also be applied to \(A\)-modules (\(A\) an algebra) without modification.
    
    \begin{dfn}{Tensor Product}{}
        Let \(R\) be a ring, \(M\) a right \(R\)-module, and \(N\) a left \(R\)-module.
        Then the \define{tensor product}\index{tensor product!of R-modules@of \(R\)-modules}, \(M \otimes_R N\), is the abelian group \begin{equation}
            \frac{F(\{m \otimes n \mid m \in M, n \in N\})}{I}
        \end{equation}
        where \(F(X)\) denotes the free abelian group on the set \(X\) and \(I\) is the normal subgroup generated from all elements of the form
        \begin{itemize}
            \item \((m + m') \otimes n - m \otimes n - m' \otimes n\);
            \item \(m \otimes (n + n') - m \otimes n - m \otimes n'\);
            \item \((m \action r) \otimes n - m \otimes (r \action n)\)
        \end{itemize}
        with \(m, m' \in M\), \(n, n' \in N\) and \(r \in R\).
    \end{dfn}
    
    \begin{wrn}
        The tensor product does not, in general, have the structure of an \(R\)-module.
        It is just an abelian group.
        In a sense the \(R\)-actions of \(M\) and \(N\) are \enquote{used up} in the construction and don't \enquote{survive} to produce a sensible notion of an \(R\)-action on \(M \otimes_R N\).
    \end{wrn}
    
    \begin{ntn}{}{}
        When \(R\) is clear from context we will write \(M \otimes N\) instead of \(M \otimes_R N\).
        Conversely, if needed we'll write \(m \otimes_R n\) for elements of \(M \otimes_R N\) if there are multiple ways to define the tensor product.
    \end{ntn}
    
    Intuitively, \(M \otimes_R N\) consists of sums of elements which we write as\footnote{We should write \([m \otimes n]\) or something similar, since what we actually have is the equivalence class of \(m \otimes n\) in \(F(\{m \otimes n\})/I\).} \(m \otimes n\) with \(m \in M\) and \(n \in N\).
    So, one element of \(M \otimes_R N\) might be
    \begin{equation}
        m_1 \otimes n_1 + m_2 \otimes n_2 + m_3 \otimes n_3
    \end{equation}
    with \(m_i \in M\) and \(n_i \in N\).
    Note that there are no factors of \(R\) here, this is purely an operation in the free group.
    The quotient imposes that in \(M \otimes_R N\) we have the relations
    \begin{align}
        (m + m') \otimes n &= m \otimes n + m' \otimes n;\\
        m \otimes (n + n') &= m \otimes n + m \otimes n';\\
        (m \action r) \otimes n &= m \otimes (r \action n).
    \end{align}
    
    As we mentioned the tensor product of a right and left \(R\)-module is not, in general, an \(R\)-module in any consistent way.
    In order for the tensor product to be a module we need to have some extra module structure present in one of the two modules which then remains after the tensor product is formed.
    Of course, this extra structure must be compatible with the existing structure, and it turns out that the following is exactly the right definition for this purpose.
    
    \begin{dfn}{Bimodule}{}
        Left \(A\) and \(B\) be associative unital \(\field\)-algebras.
        An \define{\(\symbf{(A, B)}\)-bimodule}\index{bimodule} is an abelian group, \(M\), which is both a left \(A\)-module and a right \(B\) module in such a way that
        \begin{equation}
            (a \action m) \action b = a \action (m \action b)
        \end{equation}
        for all \(a \in A\), \(b \in B\), and \(m \in M\).
    \end{dfn}
    
    \begin{exm}{}{}
        Let \(V\) be a \(\field\)-vector space and a left \(A\)-module.
        Then \(V\) is an \((A, \field)\)-bimodule where \(a \action v\) is just the action of \(A\) on \(V\) as an \(A\)-module and \(v \action \lambda = \lambda v\) is just scalar multiplication by elements of \(\field\).
        That this is a bimodule follows because
        \begin{equation}
            a \action (v \action \lambda) = a \action (\lambda v) = \lambda (a \action v) = (a \action v) \action \lambda
        \end{equation}
        having used the fact that the action of \(a\) on \(v\) is \(\field\)-linear.
        
        In fact, we can define a bimodule first (just combining the definitions of a left and right module), then a left \(A\)-module is an \((A, \field)\)-bimodule, and a right \(A\)-module is a \((\field, A)\)-bimodule.
    \end{exm}
    
    \begin{lma}{}{}
        Let \(M\) be an \((A, B)\)-bimodule, and \(N\) a left \(B\)-module.
        Then \(M \otimes_B N\) is a left \(A\)-module with \(a \action (m \otimes n) \coloneqq (a \action m) \otimes n\).
        \begin{proof}
            First note that as an \((A, B)\)-bimodule \(M\) is, in particular, a right \(B\)-module.
            Thus, the tensor product \(M \otimes_B N\) is defined as the quotient of a free abelian group by an ideal, and so is again an abelian group.
            It remains only to show that this abelian group equipped with the action of \(A\) on the first factor is an \(A\)-module.
            
            To do so take an arbitrary element of \(M \otimes_B N\), which is of the form \(\sum_{i \in I} m_i \otimes n_i\) where \(I\) is some finite indexing set, \(m_i \in M\) and \(n_i \in N\).
            We are free to define the action of \(A\) on this element to be
            \begin{equation}
                a \action \left( {\textstyle \sum_{i \in I}} m_i \otimes n_i \right) \coloneqq {\textstyle \sum_{i \in I}} (a \action m_i) \otimes n_i.
            \end{equation}
            Then when \(I\) is a singleton this reduces to \(a \action (m \otimes n) = (a \action m) \otimes n\) as required.
            
            We can now prove that this makes \(M \otimes_B N\) a left \(A\)-module:
            \begin{itemize}
                \item[M1] \((ab) \action \sum_{i} m_i \otimes n_i = \sum_{i} ((ab) \action m_i) \otimes n_i = \sum_{i} (a \action (b \action m_i)) \otimes n_i = a \action \sum_i (b \action m_i) \otimes n_i = a \action \left( b \action \sum_i m_i \otimes n_i \right)\);
                \item[M2] \(1 \action \sum_i m_i \otimes n_i = \sum_i (1 \action m_i) \otimes n_i = \sum_i m_i \action n_i\);
                \item[M3] \(a \action \left( \sum_{i \in I} m_i \otimes n_i + \sum_{j \in J} m_j \otimes n_j \right) = a \action \left( \sum_{i \in I \sqcup J} m_i \otimes n_i \right) = \sum_{i \in I \sqcup J} (a \action m_i) \otimes n_i = \sum_{i \in I} (a \action m_i) \otimes n_i + \sum_{j \in J} (a \action m_j) \otimes n_j\);
                \item[M4] \((a + b) \action \sum_i m_i \otimes n_i = \sum_i ((a + b) \action m_i) \otimes n_i = \sum_i (a \action m_i + b \action m_i) \otimes n_i = \sum_i (a \action m_i) \otimes n_i + (b \action m_i) \otimes n_i = a \action \sum_i m_i \otimes n_i + b \action \sum_i m_i \otimes n_i\). 
            \end{itemize}
        \end{proof}
    \end{lma}
    
    Similarly, if \(M\) is a right \(A\)-module and \(N\) is an \((A, B)\)-bimodule then \(M \otimes_A N\) is a right \(B\)-module with the action given by \((m \otimes n) \action b = m \otimes (n \action b)\).
    
    \begin{exm}{}{}
        Any \(\field\)-vector space, \(V\), is a \((\field, \field)\)-bimodule, defining \(\lambda \action v = \lambda v = v \action \lambda\) for \(\lambda \in \field\) and \(v \in V\).
        If \(U\) is some other vector space then we can form the \(\field\)-module \(V \otimes_{\field} U\), which is of course just the usual tensor product of vector spaces.
        
        In fact, this works for any commutative algebra, \(A\), we can take any \(A\)-module as an \((A, A)\)-bimodule, so if \(M\) and \(N\) are \(A\)-modules then \(M \otimes_A N\) is an \(A\)-module.
    \end{exm}
    
    \subsection{Universal Property}
    The tensor product may also be defined via a universal property.
    
    \begin{lma}{}{}
        Let \(M\) be an right \(A\)-module, and let \(N\) be a left \(A\)-module.
        Then for any abelian group, \(G\), and any group homomorphism \(f \colon M \times N \to G\) satisfying ... there is a unique group homomorhpism \(\overbar{f} \colon M \otimes_A N \to G\) such that \(\overbar{f}(m \otimes n) = f(m, n)\) for all \(m \in M\) and \(n \in N\).
        That is, the diagram
        \begin{equation}
            \begin{tikzcd}
                M \times N \arrow[r, "{-}\otimes{-}"] \arrow[dr, "f"'] & M \otimes_A N \arrow[d, "\exists ! \overbar{f}"]\\
                & G
            \end{tikzcd}
        \end{equation}
        commutes.
        \begin{proof}
            To make this diagram commutes we can define \(\overbar{f}(m \otimes n) = f(m, n)\).
            The fact that \(\overbar{f}\) is a group homomorhpism means that this uniquely defines the value of \(\overbar{f}\) on any element of \(M \otimes_A N\) by
            \begin{equation}
                \overbar{f}\left( {\textstyle \sum_i} m_i \otimes n_i \right) = {\textstyle\sum_i} f(m_i, n_i). \qedhere
            \end{equation}
        \end{proof}
    \end{lma}
    
    Note that \(\Hom_A(M, N)\) inherits the module structure of \(N\) via pointwise operations.
    Let \(M\) be an \((A, B)\)-bimodule, \(N\) a \((B, C)\)-bimodule, and \(P\) an \((A, C)\)-bimodule for three algebras, \(A\), \(B\), and \(C\).
    Then we can form the tensor product \(M \otimes_B N\), which is an \(A\)-module, and we can consider the hom-set \(\Hom_A(M \otimes_B N, P)\), of left \(A\)-module homomorphisms, this is itself an \(A\)-module, and in fact is an \((A, A)\)-bimodule.
    We can also form the hom-set \(\Hom_C(N, P)\) of right \(C\)-module homomorhpisms, which is an left \(A\)-module under pointwise action using the \(A\)-module structure of \(P\).
    Then we can take the hom-set \(\Hom_B(M, \Hom_C(N, P))\), which is an \(A\)-module under pointwise the action.
    Then it turns out that we actually have an isomorphism
    \begin{equation}
        \Hom_A(M \otimes_B N, P) \xrightarrow{\isomorphic} \Hom_B(M, \Hom_C(N, P))
    \end{equation}
    given by sending \(f\) to \(g\) defined by \(g(m)(n) = f(m \otimes n)\).
    This isomorphism is natural in all objects, and thus this is an adjunction.
    
    \section{Tensor Algebra}
    \begin{dfn}{Tensor Algebra}{}
        Let \(V\) be a vector space over \(\field\).
        Then the \defineindex{tensor algebra}, \(TV\), is defined to be
        \begin{equation}
            \bigoplus_{n=0}^{\infty} V^{\otimes n} = \field \oplus V \oplus (V \otimes V) \oplus (V \otimes V \otimes V) \oplus \dotsb.
        \end{equation}
        Multiplication is defined by \(ab = a \otimes b \in V^{\otimes(n = m)}\) for \(a \in V^{\otimes n}\) and \(b \in V^{\otimes m}\), and extended linearly.
    \end{dfn}
    
    \begin{lma}{}{}
        Let \(V\) be an \(n\)-dimensional vector space over \(\field\).
        Then \(TV\) is isomorphic to \(\field\langle x_1, \dotsc, x_n \rangle\), the free algebra on \(n\) indeterminates.
        \begin{proof}
            Pick a basis for \(V\).
            Identify this basis with the \(x_i\).
            Elements of \(TV\) are linear combinations of tensor products of these basis elements, so we can identify them with polynomials in non-commuting variables.
            For example, given the basis \(\{e_i\}\) for \(V\) we have that \(e_1 \otimes e_2 \otimes e_1\) maps to \(x_1x_2x_1\), and \(e_1 \otimes e_2 + e_1 \otimes e_3 \otimes e_2\) maps to \(x_1x_2 + x_1x_3x_2\).
        \end{proof}
    \end{lma}
    
    The nice thing about the tensor algebra is that it gives us a basis free way to work with the free algebra, that is a way that is independent of the choice of generators.
    As it is there is no commutativity imposed on the product in \(TV\), we can impose some commutativity condition by taking quotients.
    
    \begin{dfn}{Quotients of the Tensor Algebra}{}
        Let \(V\) be a vector space over \(\field\).
        We define following quotients:
        \begin{itemize}
            \item \(SV \coloneqq TV/\langle v \otimes w - w \otimes v \rangle\), the \defineindex{symmetric algebra}; and
            \item \(\Lambda V \coloneqq TV/\langle v \otimes w + w \otimes v \rangle\), the \defineindex{exterior algebra}.
        \end{itemize}
        If \(\lie{g} = V\) is a Lie algebra then we may define the quotient \(\universalEnveloping(\lie{g}) \coloneqq TV/\langle v \otimes w - w \otimes v - \bracket{v}{w} \rangle\), the \defineindex{universal enveloping algebra}.
    \end{dfn}
    
    The idea is that for \(SV\) we impose that\footnote{identifying elements with their equivalence class} \(v \otimes w = w \otimes v\), which makes \(SV\) isomorphic to \(\field[x_1, \dotsc, x_n]\) for \(n = \dim V\).
    For \(\Lambda V\) we impose that \(v \otimes w = -w\otimes v\) (usually the product here is written as \(v \wedge w\)).
    Finally, for \(\universalEnveloping(\lie{g})\) we impose that the bracket, \(\bracket{v}{w}\) is exactly the commutator \(v \otimes w - w \otimes v\).
    This last case is nice because it allows us to treat the abstract bracket as if it were a commutator.
    
    Note that the tensor algebra, as well as the quotients \(SV\) and \(\Lambda V\), are graded algebras, meaning that they have decompositions as direct sums:
    \begin{equation}
        SV = \bigoplus_{n = 0}^{\infty} S^nV, \qqand \Lambda V = \bigoplus_{n = 0}^{\infty} \Lambda^n V.
    \end{equation}
    Here \(S^nV\) (\(\Lambda^nV\)) is the \(n\)th (anti)symmetric tensor power of \(V\), that is, it's \(V^{\otimes n}\) modulo the relation that factors (anti)commute.
    Note that \(S^nV\) is isomorphic to the subalgebra of \(\field[x_1, \dotsc, x_n]\) consisting of homogeneous polynomials of degree \(n\).
    
    \chapter{Jacobson's Density Theorem}
    \section{Semisimple Representations}
    Recall that a module is semisimple if it is a direct sum of simple modules, and a simple module is one with no nontrivial submodules.
    
    \begin{exm}{}{}
        Let \(V\) be an \(n\)-dimensional simple \(A\)-module.
        Then \(\End V\) is an \(A\)-module as well, with \(A\) acting by left matrix multiplication (after fixing some basis so that elements of \(\End V\) can be identified with matrices and then identifying elements of \(A\) acting on \(\End V\) with the corresponding linear operator on \(V\)).
        With this construction \(\End V\) is semisimple, in particular
        \begin{equation}
            \End V \isomorphic \underbrace{V \oplus \dotsb \oplus V}_{n \text{ terms}} \eqcolon nV.
        \end{equation}
        This isomorphism is given by fixing some basis, \(\{v_1, \dotsc, v_n\} \subseteq V\), and then defining a linear map \(\End V \to nV\) by \(\varphi \mapsto (\varphi(v_1), \dotsc, \varphi(v_n))\).
        Viewing \(v_i\) as column matrices \(\varphi(v_i)\) is simply the \(i\)th column of the matrix corresponding to \(\varphi\) in this basis.
    \end{exm}
    
    In this example \(\End V\) ends up being a direct sum of a single simple module.
    In the general semisimple case any simple module can appear in the decomposition.
    If we restrict ourselves to finite dimensions then we can get a pretty good handle on which simple modules appear in such a decomposition.
    In particular, any finite-dimensional semisimple module, \(V\), may be decomposed as
    \begin{equation}
        V = \bigoplus_{i \in I} m_i V_i
    \end{equation}
    with \(m_i \in \integers_{\ge 0}\) and \(V_i\) running over all finite dimensional simple modules.
    We call \(m_i\) the \defineindex{multiplicity} of \(V_i\) in \(V\).
    Note that since this decomposition is unique up to the order of the terms.
    
    \begin{lma}{}{}
        Let \(V\) be a finite dimensional semisimple \(A\)-module, with decomposition
        \begin{equation}
            V = \bigoplus_{i \in I} m_i V_i
        \end{equation}
        with \(m_i \in \integers_{\ge 0}\) and \(V_i\) simple.
        Then the multiplicity, \(m_i\), is given by
        \begin{equation}
            m_i = \dim( \Hom_A(V_i, V) ).
        \end{equation}
        \begin{proof}
            We make use of the fact that\footnote{\(\Hom(V_i, -)\) is right adjoint (to \(-\otimes_AV_i\)) and as such preserves colimits}
            \begin{equation}
                \Hom_A(V_i, V' \oplus V'') \isomorphic \Hom_A(V_i, V') \oplus \Hom_A(V_i, V'').
            \end{equation}
            This extends to all finite direct sums.
            
            Note that \(\Hom_A(V_i, V)\) is an \((A, \field)\)-bimodule with the left action \((a \action \varphi)(v) = \varphi(a \action v)\) and right action \((\varphi \action \lambda)(v) = \lambda \varphi(v)\).
            Further, \(V_i\) is a right \(\field\)-module with the action \(v \action \lambda = \lambda v = (\lambda 1_A) \action v\).
            Thus, \(\Hom_A(V_i, V) \otimes_{\field} V_i\) is a left \(A\)-module.
            
            We can define a map
            \begin{equation}
                \label{eqn:map between + hom Vi V x Vi and V}
                \begin{aligned}
                    \psi \colon \bigoplus_{i \in I} \Hom_A(V_i, V) \otimes_{\field} V_i &\to V\\
                    \bigoplus_{i \in I} \varphi_i \otimes v_i &\mapsto \sum_i \varphi_i(v_i).
                \end{aligned}
            \end{equation}
            This is an \(A\)-module isomorphism:
            \begin{align}
                \psi\left( a \action {\textstyle \bigoplus_{i \in I}} \varphi_i \otimes v_i \right) &= \psi\left( {\textstyle \bigoplus_{i\in I}} \varphi_i \otimes (a \action v_i) \right)\\
                &= {\textstyle \sum_{i \in I}} \varphi_i(a \action v_i)\\
                &= {\textstyle \sum_{i \in I}} a \action \varphi_i(v_i)\\
                &= a \action {\textstyle \sum_{i \in I}} \varphi_i(v_i)\\
                &= a \action \psi\left( {\textstyle \bigoplus_{i \in I}} \varphi_i \otimes v_i \right).
            \end{align}
            Linearity is clear from the definition.
            It remains only to show that this map is invertible.
            By linearity it is sufficient to show that the map
            \begin{align}
                \Hom(V_i, V) \otimes V_i &\to V\\
                \varphi_i \otimes v_i &\mapsto \varphi_i(v_i)
            \end{align}
            is an isomorphism.
            Since \(V_i\) is simple Schur's lemma tells us that this map is either zero or surjective.
            It is clearly not zero, since we can simply choose some vector \(v_i\) and some nonzero map \(\varphi_i\) on which \(\varphi_i(v_i) \ne 0\).
            Thus, this map is surjective.
            A surjective linear map between finite dimensional modules is an isomorphism.
            Hence, the map in \cref{eqn:map between + hom Vi V x Vi and V} is an isomorphism.
            
            We then have
            \begin{align}
                \dim V &= \dim\left( {\textstyle \bigoplus_{i \in I}} \Hom_A(V_i, V) \right)\\
                &= {\textstyle \sum_{i \in I}} \dim(\Hom_A(V_i, V)) \dim(V_i)
            \end{align}
            and
            \begin{align}
                \dim V &= \dim\left( {\textstyle \bigoplus_{i \in I}} m_i V_i \right)\\
                &= {\textstyle \sum_{i \in I}} m_i \dim (V_i).
            \end{align}
            Since these are finite sums and this must hold for arbitrary semisimple modules \(V\), including the case where \(V = V_i\) is actually simple, we must have that
            \begin{equation*}
                m_i = \dim(\Hom_A(V_i)). \qedhere
            \end{equation*}
        \end{proof}
    \end{lma}
    
    The decomposition into simple submodules also puts restrictions on the non-simple submodules that we can have.
    First, every submodules of a semisimple module must itself be semisimple, meaning it has its own decomposition into simple modules.
    Further, the simple modules that can appear in the decomposition of the submodule are only the ones that appear in the decomposition of the module.
    Finally, the multiplicity with which these simple modules appear in the submodule must be at most the multiplicity with which they appear in the original module.
    That is, the only way to form a submodule of a semisimple module is to take some subset of the simple modules that appear in the decomposition and take their direct sum.
    
    \begin{prp}{}{prp:submodules of semisimple modules}
        Let \(V\) be a semisimple finite-dimensional \(A\)-module with decomposition
        \begin{equation}
            V = \bigoplus_{i=1}^m n_i V_i
        \end{equation}
        with the \(V_i\) pairwise-nonisomorhpic simple \(A\)-modules.
        Let \(W \subseteq V\) be a submodule.
        Then
        \begin{equation}
            W = \sum_{i=1}^m r_i V_i
        \end{equation}
        with \(0 \le r_i \le n_i\) for all \(i\), and the inclusion \(\varphi \colon W \hookrightarrow V\) decomposes as
        \begin{equation}
            \varphi = \bigoplus_{i=1}^m \varphi_i
        \end{equation}
        where \(\varphi_i \colon r_i V_i \to n_i V_i\) are maps given by \(\varphi_i(v_1, \dotsc, v_{r_i}) = (v_1, \dotsc, v_{r_i}) \action X_i\) where \(X_i \in \matrices[r_i]{n_i}{\field}\) acts on the row vector by right matrix multiplication and has rank \(r_i\).
        \begin{proof}
            The proof is by induction on \(n = \sum_{i=1}^m n_i\).
            For the base case we just have that \(V\) is simple, and so its only submodules are the zero module (the empty direct sum) or \(V\) itself, in which case the statement clearly holds.
            
            Now suppose that this is the case when \(\sum_{i} n_i = n - 1\).
            Fix some submodule, \(W \subseteq V\).
            If \(W = 0\) then we're done, so suppose \(W \ne 0\).
            Fix some simple submodule, \(P \subseteq W\).
            Such a \(P\) exists as a consequence of \cref{lma:every finite dimensional module has a simple submodule}.
            By Schur's lemma \(P\) must be isomorphic to \(V_i\) for some \(i\), and the inclusion \(\varphi|_P \colon P \to V\) factors through \(n_i V_i\) by
            \begin{equation}
                P \xrightarrow{\isomorphic} V_i \hookrightarrow n_i V_i \hookrightarrow V.
            \end{equation}
            Identifying \(P\) with \(V_i\) this map is given by
            \begin{equation}
                v \mapsto (v q_1, \dotsc, v q_{n_i})
            \end{equation}
            with \(q_i \in \field\) not all zero.
            
            The group \(G_i = \generalLinear_{n_i}(\field)\) acts on \(n_i V_i\) by right matrix multiplication.
            We can also act trivially on \(n_j V_j\) for \(j \ne i\).
            Then \(G_i\) acts on \(V\).
            This gives an action of \(G_i\) on the set of submodules of \(V\), and this action preserves the property that we're trying to establish, that under the action of \(g_i \in G_i\) the matrix \(X_i\) goes to \(X_i g_i\) while the matrices \(X_j\) (\(j \ne i\)) are left unchanged.
            Taking \(g_i \in G_i\) such that \((1_1, \dotsc, q_{n_i})g_i = (1, 0, \dotsc, 0)\), which is always possible as \(g_i\) is invertible, we have that \(Wg_i\) contains the first summand, \(V_i\), of \(n_i V_i\).
            Thus, \(Wg_i \isomorphic V_i \oplus W'\) where 
            \begin{equation}
                W' \subseteq n_1 V_1 \oplus \dotsb \oplus (n_i - 1)V_i \oplus \dotsb \oplus n_m V_m
            \end{equation}
            is the kernel of the projection of \(Wg_i\) onto the first summand \(V_i\).
            The inductive hypothesis then holds for this subspace, and so it has a decomposition
            \begin{equation}
                W' \isomorphic \bigoplus_{j=1}^m r_j' V_i
            \end{equation}
            with \(0 \le r_i' \le n_i - 1\) and \(0 \le r_j \le n_j\) for \(j \ne i\), and so taking
            \begin{equation}
                W \isomorphic V_i \oplus W \isomorphic \bigoplus_{j=1}^m r_jV_i
            \end{equation}
            with \(r_i = r_i' + 1\) and \(r_j = r_j'\) we get the desired result.
        \end{proof}
    \end{prp}
    
    \begin{lma}{}{lma:every finite dimensional module has a simple submodule}
        Any nonzero finite dimensional \(A\)-module contains a simple submodule.
        \begin{proof}
            The proof is by induction on dimension.
            Let \(V\) be a finite dimensional nonzero \(A\)-module.
            We start with \(\dim V = 1\).
            Then \(V\) is itself simple, and we are done.
            Suppose then that all \(A\)-modules of dimension at most \(k\) contain a simple submodule.
            Consider the case when \(\dim V = k + 1\).
            If \(V\) is simple we are done.
            If \(V\) is not simple then it contains a proper submodule, \(W\).
            Since \(W\) is a \emph{proper} submodule it has dimension less than \(k + 1\), and thus the induction hypothesis holds.
            Thus, \(W\) has a simple submodule, which is then also a simple submodule of \(V\).
            Then, by induction, the statement holds for all finite dimensional \(A\)-modules.
        \end{proof}
    \end{lma}
    
    \begin{remark}{}{}
        We assumed that \(\field\) was algebraically closed in the use of Schur's lemma above.
        However, this is not required for a modified result to hold.
        If we replace \(\matrices[r_i]{n_i}{\field}\) with \(\matrices[r_i]{n_i}{D_i}\) where \(D_i = \End_A (V_i)\) then the result holds for any field \(\field\).
        The \(D_i\) are division algebras (algebras in which division by any nonzero element is defined).
        When \(\field\) \emph{is} algebraically closed Schur's lemma applies and tells us that the maps \(V_i \to V_i\) are just scalar multiplication, allowing us to identify \(D_i\) with \(\field\) to get the result as stated above.
    \end{remark}
    
    \begin{crl}{}{crl:linearly independent set reaches all of V under action of A}
        Let \(V\) be a finite dimensional simple \(A\)-module.
        Given two subsets \(\{x_1, \dotsc, x_n\}, \{y_1, \dotsc, y_n\} \subseteq V\) with the first being linearly independent there exists some \(a \in A\) such that \(a \action x_i = y_i\).
        \begin{proof}
            The proof is by contradiction, so suppose that this is not the case.
            Then \(W = \{(a \action x_1, \dotsc, a \action x_n) \mid a \in A\}\) must be a proper submodule of \(nV\), that is there is some element of \(V\) we can pick for one of the \(y_i\) such that we cannot reach \((y_1, \dotsc, y_n)\) by the action of \(a\).
            Then since \(V\) is simple we know that \(W = rV\) for some \(r < n\), a strict inequality since we have a \emph{proper} submodule.
            By \cref{prp:submodules of semisimple modules} we know that there is some \(X \in \matrices[r]{n}(\field)\) and some \(u_1, \dotsc, u_r \in V\) such that
            \begin{equation}
                (u_1, \dotsc, u_r) \action X = (x_1, \dotsc, x_n).
            \end{equation}
            To achieve this result we've just considered the \(a = 1\) case to get \((x_1, \dotsc, x_n) \in W = rV\).
            Since \(r < n\) we know that there is some \((z_1, \dotsc, z_n) \in \field^n \setminus \{0\}\) such that \(X \action (z_1, \dotsc, z_n)^{\trans} = 0\), because \(X\) only has rank \(r\).
            Thus, we can consider
            \begin{align}
                0 &= (u_1, \dotsc, u_r) \action X \action (z_1, \dotsc, z_n)^{\trans}\\
                &= (x_1, \dotsc, x_n) \cdot (z_1, \dotsc, z_n)^{\trans}\\
                &= \sum_{i=1}^n z_i x_i.
            \end{align}
            Since the \(x_i\) are linearly independent this means that \(z_i = 0\), a contradiction. 
        \end{proof}
    \end{crl}
    
    \section{Density Theorem}
    We're now ready to start working towards a result known as the density theorem.
    This result says that a certain class of algebras are basically just direct sums of matrix algebras.
    We have to prove some technical results first though.
    
    \begin{thm}{}{thm:representation maps are surjections}
        Let \(V\) be a finite dimensional \(A\)-module.
        \begin{enumerate}
            \item If \(V\) is simple then the associated algebra morphism \(r \colon A \to \End V\) is surjective.
            \item If \(V = \oplus_{i=1}^m V_i\) with the \(V_i\) pairwise nonisomorphic finite dimensional simple \(A\)-modules then
            \begin{equation}
                r = \bigoplus_{i=1}^m r_i \colon A \to \bigoplus_{i=1}^m \End V_i
            \end{equation}
            is surjective.
        \end{enumerate}
        \begin{proof}
            \begin{enumerate}
                \item Fix some basis, \(\{v_1, \dotsc, v_n\} \subseteq V\), and let \(w_i = \varphi(v_i)\) for some \(\varphi \in \End V\).
                Then by \cref{crl:linearly independent set reaches all of V under action of A} there exists some \(a \in A\) such that \(a \action v_i = w_i\), and thus \(r(a) = \varphi\), so \(r\) is surjective.
                \item Let \(B_i\) be the image of \(A\) in \(\End V_i\).
                Notice that \(\End V_i \isomorphic d_i V_i\) where \(d_i = \dim V_i\).
                Let \(B\) be the image of \(A\) in \(\bigoplus_i \End V_i\).
                Then \(B \isomorphic \bigoplus_i B_i \isomorphic \bigoplus_i d_i V_i\), and the first part tells us that \(B_i = \End V_i\) by surjectivity of each representation map, and thus \(B \isomorphic \bigoplus \End V_i\), so \(r\) is surjective.
            \end{enumerate}
        \end{proof}
    \end{thm}
    
    The next result considers what happens when we have an algebra that is a direct sum of matrix algebras.
    Before the proof however we need the following definition.
    
    \begin{dfn}{Dual Module}{}
        Let \(V\) be a left \(A\)-module.
        Then the \defineindex{dual module} is \(V^* = \Hom_{\field}(V, \field)\) with the action defined by \((f \action a)(v) = f(a \action v)\) for all \(f \in V^*\), \(a \in A\), and \(v \in V\).
    \end{dfn}
    
    \begin{thm}{}{thm:reps of matrix algebras}
        Let \(\field\) be a field which is not necessarily algebraically closed.
        Let \(A\) be the \(\field\)-algebra given by
        \begin{equation}
            A = \bigoplus_{i=1}^r \matrices{d_i}{\field}
        \end{equation}
        for some \(d_i \in \naturals\).
        Then
        \begin{enumerate}
            \item the simple \(A\)-modules are \(\field^{d_i}\) with \((X_1, \dotsc, X_r)\) acting by matrix multiplication by \(X_i\); and
            \item any finite dimensional \(A\)-module is semisimple.
        \end{enumerate}
        \begin{proof}
            \begin{enumerate}
                \item Let \(v, w \in \field^{d_i}\) be such that \(v \ne 0\).
                Then there exists some linear map sending \(v\) to \(w\), and hence some matrix \(X \in \matrices{d_i}{\field}\) such that \(Xv = w\).
                Thus, \(V_i = \field^{d_i}\) must be simple since any nonzero subspace containing \(v\) and not \(w\) cannot be a submodule.
                \item Let \(W\) be a finite dimensional left \(A\)-module.
                Consider its dual, \(W^*\), which we can think of as a left \(A^{\op}\)-module.
                The algebra \(A^{\op}\) is given by
                \begin{equation}
                    A^{\op} = \bigoplus_{i} \matrices{d_i}{\field}^{\trans} \isomorphic \bigoplus_i \matrices{d_i}{\field}
                \end{equation}
                and we identify \(a \in A\) with \(a^{\trans} \in A^{\op}\) where \((X_1, \dotsc, X_r)^{\trans} = (X_1^{\trans}, \dotsc, X_r^{\trans})\).
                Really nothing is going on here since we're considering square matrices so taking the transpose changes individual elements but doesn't change the set of all matrices under consideration.
                
                What this lets us do is interpret \(W^*\) as an \(A\)-module with \(a \action f = f \action a^{\trans}\).
                We can fix a basis \(\{f_1, \dotsc, f_n\} \subseteq W^*\), and then define a surjection
                \begin{align}
                    \varphi \colon nA &\twoheadrightarrow W^*\\
                    a_1 \oplus \dotsb \oplus a_n &\mapsto a_1 \action f_1 + \dotsb + a_n \action f_n.
                \end{align}
                This is a surjection by \cref{thm:representation maps are surjections}.
                We can consider the dual map, \(\varphi^* \colon W \hookrightarrow (nA)^* \isomorphic nA\), which will be an injection.
                Further, \(W \isomorphic \im \varphi^* \subseteq nA\) is a submodule of the semisimple module \(nA\) (where \(a \action (b_1 \oplus \dotsb \oplus b_n) = ab_1 \oplus \dotsb \oplus ab_n\)) and we can apply \cref{prp:submodules of semisimple modules} to conclude that \(W\) is semisimple.
            \end{enumerate}
        \end{proof}
    \end{thm}
    
    What we have just shown is that matrix algebras, and their direct sums, have particularly nice properties.
    We understand their simple modules well, they're just \(\field^{d}\) with \(d\) appearing as the number of rows of some matrix, and all finite dimensional modules are semisimple, so all are just some direct sum \(\bigoplus_i \field^{d_i}\).
    The logical next question is when is a given algebra, \(A\), isomorphic to some direct sum of matrix algebras?
    It turns out that there's a simple subspace we can consider that vanishes only when \(A\) is a direct sum of matrix algebras.
    
    \begin{dfn}{Radical}{}
        Let \(A\) be an algebra.
        We call
        \begin{equation}
            \Rad A = \{a \in A \mid a \text{ acts as zero on any simple } A \text{-module}\} \subseteq A
        \end{equation}
        the \defineindex{radical} of \(A\).
    \end{dfn}
    
    \begin{dfn}{Nilpotent Ideal}{}
        Let \(A\) be an algebra.
        We call \(a \in A\) a \define{nilpotent element}\index{nilpotent!element} if there exists some \(k \in \naturals\) such that \(a^k = 0\).
        A \define{nilpotent ideal}\index{nilpotent!ideal} is an ideal in which all elements are nilpotent.
    \end{dfn}
    
    \begin{prp}{}{}
        \begin{enumerate}
            \item \(\Rad A\) is a two-sided ideal.
            \item If \(A\) is finite dimensional then any nilpotent two-sided ideal is contained in \(\Rad A\).
            \item \(\Rad A\) is the largest two-sided nilpotent ideal.
        \end{enumerate}
        \begin{proof}
            \begin{enumerate}
                \item We first show that \(\Rad A\) is a subspace.
                Let \(V\) be a simple \(A\)-module.
                Then if \(a, b \in \Rad A\) we have 
                \begin{equation}
                    (a + b) \action v = a \action v + b \action v = 0 + 0 = 0
                \end{equation}
                for all \(v \in V\), and thus \(\Rad A\) is closed under addition.
                If \(\lambda \in \field\) we also have
                \begin{equation}
                    (\lambda a) \action v = \lambda(a \action v) = \lambda 0 = 0,
                \end{equation}
                and so \(\Rad A\) is closed under scalar multiplication.
                Thus, \(\Rad A\) is a subspace of \(A\).
                
                Let \(a \in \Rad A\) and \(b \in A\).
                Then we know that if \(V\) is a simple \(A\)-module \(a \action v = 0\) for all \(v \in V\).
                We therefore have
                \begin{equation}
                    (ab) \action v = a \action (b \action v) = 0, \qand (ba) \action v = b \action (a \action v) = b \action 0 = 0
                \end{equation}
                since \(b \action v \in V\) so \(a\) acts on it by zero, and \(b\) acts linearly so it sends \(0\) to \(0\).
                Thus, \(ab, ba \in \Rad A\), so \(\Rad A\) is a two-sided ideal.
                \item Let \(V\) be a simple \(A\)-module and \(I\) a nilpotent ideal.
                Fix some nonzero \(v \in V\).
                Then \(I \action v \subseteq V\) is a submodule.
                By simplicity of \(V\) there are two possibilities
                \begin{itemize}
                    \item \(I \action v = V\), and since \(v \in V\) there must be some \(x \in I\) such that \(x \action v = v\), but then we cannot have that \(x^k = 0\) for any \(k \in \naturals\) as we must have \(x^k \action v = v\), so we can't have \(I \action v = V\) if \(I\) is nilpotent;
                    \item \(I \action v = 0\), in which case every element of \(I\) acts as zero on any element of \(V\), and so \(I \subseteq \Rad A\).
                \end{itemize}
                \item Let 
                \begin{equation}
                    0 = A_0 \subseteq A_1 \subseteq A_1 \subseteq \dotsb \subseteq A_n = A
                \end{equation}
                be a filtration of the regular representation of \(A\) such that \(A_{i+1}/A_i\) is simple.
                Such a filtration exists by \cref{lma:filtrations exist}.
                
            \end{enumerate}Let \(x \in \Rad A\), then \(x\) acts on the simple \(A\)-module \(A_{i+1}/A_i\) by zero, and so \(x\) must map any element of \(A_{i+1}\) to some element of \(A_i\), since that will then be sent to zero in the quotient.
            Thus \(x^n\) acts as zero on all of \(A_n = A\), and so \(\Rad A\) is nilpotent.
            By the previous part we also know that \(\Rad A\) contains any nilpotent two-sided ideal, and so \(\Rad A\) is the largest two-sided nilpotent ideal (ordered by inclusion).
        \end{proof}
    \end{prp}
    
    \begin{dfn}{Filtration}{}
        Let \(V\) be an \(A\)-module.
        A finite \defineindex{filtration} of \(V\) is a sequence of submodules
        \begin{equation}
            0 = V_0 \subseteq V_1 \subseteq \dotsb \subseteq V_n = V.
        \end{equation}
    \end{dfn}
    
    \begin{lma}{}{lma:filtrations exist}
        Let \(V\) be a finite dimensional \(A\)-module.
        Then there is a filtration
        \begin{equation}
            0 = V_0 \subseteq V_1 \subseteq \dotsb \subseteq V_n = V
        \end{equation}
        for which \(V_{i+1}/V_i\) is a simple \(A\)-module for all \(i\).
        \begin{proof}
            We induct on \(\dim V\).
            If \(\dim V = 0\) then we have the filtration \(0 = V_0 = V\) and we are done.
            Suppose the result holds for all dimensions less than \(\dim V\).
            If \(V\) is simple then we have the filtration \(0 = V_0 \subseteq V_1 = V\) and \(V/0 \isomorphic V\) is simple, so we're done.
            Suppose then that \(V\) is not simple, and pick some nontrivial submodule \(V_1 \subsetneq V\).
            Take the module \(U = V/V_1\).
            Since \(V_1 \ne 0\) we know that \(\dim (V / V_1) < \dim V\), and so by the induction hypothesis there is a filtration
            \begin{equation}
                0 = U_0 \subseteq U_1 \subseteq \dotsb \subseteq U_{n-1} = U
            \end{equation}
            such that \(U_{i+1}/U_i\) is simple.
            Let \(\pi \colon V \twoheadrightarrow V/V_1\) be the canonical projection.
            For \(i \ge 2\) define \(V_i = \pi^{-1}(U_i)\) to be the preimage of \(U_i\) under this projection.
            Then we have the filtration
            \begin{equation}
                0 = V_0 \subseteq V_1 \subseteq \dotsb \subseteq V_n = V.
            \end{equation}
            
            Note that here we've used the fact that the preimage under a module morphism of a submodule of the codomain is a submodule of the domain, which can be seen as follows: take \(v \in V_i\) and we have some \(u \in U_i\) such that \(\pi(v) = u\), then
            \begin{equation}
                a \action u = a \action \pi(v) = \pi(a \action v) \in U_i
            \end{equation}
            which shows that \(a \action v \in V_i\) also, so \(V_i\) is closed under the action of \(A\), and the preimage of a subspace is again a subspace.
            
            All we have to do now is show that the given filtration has the desired property.
            To see that this is indeed the case consider \(V_{i+1}/V_i = \pi^{-1}(U_{i+1})/\pi^{-1}(U_i) \isomorphic \pi^{-1}(U_{i+1}/U_i)\) which shows that \(V_{i+1}/V_i\) is the preimage of a simple module, and must therefore be simple itself, if it wasn't then the image of any nontrivial submodule of \(V_{i+1}/V_i\) would provide a nontrivial submodule of \(U_{i+1}/U_i\).
         \end{proof}
    \end{lma}
    
    The following result gives us a handle on the number of simple \(A\)-modules in the finite dimensional case.
    It also shows that given any algebra we can always quotient by the radical to get something isomorphic to a direct sum of endomorphism spaces, which is isomorphic to a direct sum of matrix algebras.
    In this way the radical consists of the elements which obstruct our attempt to understand \(A\) as being formed from matrix algebras.
    
    \begin{ntn}{}{}
        We write \(\Irr(A)\) for the set of isomorphism classes of simple \(A\)-modules.
        We further assume that each isomorphism class has some canonical choice of representative, which we'll call \(V_i\), so we can take \(\Irr(A) = \{V_i\}\).
        We assume that sums over the index \(i\) in \(V_i\) run over all simple \(A\)-modules.
    \end{ntn}
    
    \begin{thm}{}{thm:dimension of A geq dim squared of irreps}
        Any finite dimensional algebra, \(A\), has only finitely many simple \(A\)-modules, \(V_i\), (up to isomorphism) and
        \begin{equation}
            \sum_i (\dim V_i)^2 \le \dim A.
        \end{equation}
        Further,
        \begin{equation}
            A / \Rad A \isomorphic \bigoplus_i \End V_i.
        \end{equation}
        \begin{proof}
            Let \(V\) be a simple \(A\)-module and take some \(v \in V\) with \(v \ne 0\).
            Then \(A \action v \ne 0\) since \(1 \in A\) so \(v \in A \action v\).
            Thus, by simplicity we must have that \(A \action v = V\).
            Further, \(V\) is finite dimensional since \(A\) is finite dimensional, and if we could construct infinitely many linearly independent elements by acting on \(v\) with elements of \(A\) those infinitely many elements of \(A\) would be linearly independent in \(A\), a contradiction.
            
            Now let \(\{V_i\} = \Irr(A)\) be the set of simple \(A\)-modules.
            Then by \cref{thm:representation maps are surjections} we have a surjection
            \begin{equation}
                \bigoplus_i \rho_i \colon A \twoheadrightarrow \End V_i.
            \end{equation}
            Thus, we have
            \begin{align}
                \dim \left( {\textstyle\bigoplus_{i}} \End V_i\right) &= {\textstyle\sum_i} \dim( \End V_i)\\
                &= {\textstyle\sum_i} (\dim V_i)^2
            \end{align}
            where we've used the fact that the dimension of a direct sum is the sum of the dimensions, and \(\End V\) has dimension \((\dim V)^2\), which can be seen by fixing a basis for \(V\) and considering elements of \(\End V\) as \((\dim V) \times (\dim V)\) matrices.
            Finally, since the above map is a surjection the dimension is bounded by \(\dim A\), and thus we have
            \begin{equation}
                \sum_i (\dim V_i)^2 \le \dim A
            \end{equation}
            as claimed.
            
            We have that
            \begin{equation}
                \ker\left( {\textstyle \bigoplus_i} \, \rho_i \right) = \Rad A
            \end{equation}
            since by definition elements of this kernel are sent to the zero map when when they act on each simple module, \(V_i\), and this is exactly the definition of said elements being in \(\Rad A\).
            Thus, by the first isomorphism theorem we have that
            \begin{equation*}
                A/\ker\left( {\textstyle \bigoplus_i} \, \rho_i \right) = A/\Rad A \isomorphic \bigoplus_i \End V_i. \qedhere
            \end{equation*}
        \end{proof}
    \end{thm}
    
    We now give a definition of a semisimple algebra.
    Note that several equivalent definitions are in use, and some of these are covered in \cref{prp:equivalent definitions of semisimple algebra}.
    
    \begin{dfn}{Semisimple Algebra}{}
        A finite dimensional algebra, \(A\), is \define{semisimple}\index{semisimple!algebra} if \(\Rad A = 0\).
    \end{dfn}
    
    \begin{prp}{}{prp:equivalent definitions of semisimple algebra}
        Let \(A\) be a finite dimensional algebra, then the following are equivalent:
        \begin{enumerate}[label=(\textsc{\roman*})]
            \item \(A\) is semisimple, that is \(\Rad A = 0\);
            \item \(\dim A = \sum_i (\dim V_i)^2\) where \(V_i\) runs over all simple \(A\)-modules;
            \item \(A \isomorphic \bigoplus_i \matrices{d_i}{\field}\) for some \(d_i \in \naturals\);
            \item Any finite dimensional \(A\)-module is semisimple.
            In particular, the regular representation is semisimple.
        \end{enumerate}
        \begin{proof}
            \Step{(i) \(\implies\) (ii)}
            We have that
            \begin{equation}
                A/\Rad A \isomorphic \bigoplus_i \End V_i
            \end{equation}
            and taking dimensions we have
            \begin{equation}
                \dim(A/\Rad A) = \sum_i (\dim V_i)^2.
            \end{equation}
            If \(A\) is semisimple then \(\Rad A = 0\) and this reduces to the equality
            \begin{equation}
                \dim A = \sum_i (\dim V_i)^2.
            \end{equation}
            
            \Step{(i) \(\implies\) (iii)}
            By \cref{thm:dimension of A geq dim squared of irreps} we know that
            \begin{equation}
                A/\Rad A \isomorphic \bigoplus_i \End V_i
            \end{equation}
            and if \(A\) is semisimple then \(\Rad A = 0\) so this reduces to
            \begin{equation}
                A \isomorphic \bigoplus_i \End V_i.
            \end{equation}
            Fixing some basis for \(V_i\) we may identify elements of \(\End V_i\) with matrices in \(\matrices{d_i}{\field}\) where \(d_i = \dim V_i\).
            Thus, we have
            \begin{equation}
                A \isomorphic \bigoplus_i \matrices{d_i}{\field}.
            \end{equation}
            
            \Step{(iii) \(\implies\) (iv)}
            By the second part of \cref{thm:reps of matrix algebras} we have that any finite dimensional \(A\)-module is semisimple.
            
            \Step{(iv) \(\implies\) (i)}
            Consider the regular representation of \(A\) which decomposes as
            \begin{equation}
                A \isomorphic \bigoplus_i n_i V_i
            \end{equation}
            with \(V_i\) simple and \(n_i \in \integers_{\ge 0}\).
            Take some \(x \in \Rad A\), then by definition \(x\) acts as zero on each \(V_i\) submodule, and so acts as zero on all of \(A\), in particular \(x \action 1 = 0\).
            In the regular representation the action of \(x\) is just multiplication, so \(x \action 1 = x1 = x\), thus we must have \(x = 0\), and hence \(\Rad A = 0\).
        \end{proof}
    \end{prp}
    
    One question that we may ask is how many simple \(A\)-modules are there (up to isomorphism)?
    Of course, if we can find the decomposition \(A \isomorphic \bigoplus_i \End V_i\) then we have answered the question, but we can often answer the question much faster with the following result definition and result.
    
    \begin{dfn}{Centre}{}
        Let \(A\) be an algebra.
        The \defineindex{centre} of \(A\), denoted \(Z(A)\), is the subalgebra
        \begin{equation}
            Z(A) \coloneqq \{a \in A \mid ab = ba \forall b \in A\}.
        \end{equation}
    \end{dfn}
    
    That is, the centre is the subspace consisting of all elements of \(A\) that commute with all other elements of \(A\).
    This is clearly a subspace since if \(a, a' \in Z(A)\) then \((a + \lambda a')b = ab + \lambda a'b = ba + \lambda ba' = b(a + \lambda a')\) for all \(b \in A\) and \(\lambda \in \field\).
    This is in fact a subalgebra since if \(a, a' \in Z(A)\) then \(aa'b = aba' = aa'b\) so \(aa' \in Z(A)\).
    
    \begin{lma}{}{}
        Let \(A\) be a finite dimensional semisimple algebra.
        Then
        \begin{equation}
            \abs{\Irr(A)} = \dim Z(A).
        \end{equation}
        \begin{proof}
            First note that if \(A_1\) and \(A_2\) are algebras then
            \begin{equation}
                Z(A_1 \oplus A_2) = Z(A_1) \oplus Z(A_2),
            \end{equation}
            since if \((a_1, a_2) \in Z(A_1 \oplus A_2)\) then we have
            \begin{equation}
                (a_1, a_2)(b_1, b_2) = (b_1, b_2)(a_1, a_2)
            \end{equation}
            for all \(b_1, b_2 \in A_1 \oplus A_2\), and evaluating the left hand side gives \((a_1b_1, a_2b_2)\) and the right hand side gives \((b_1a_1, b_2a_2)\), so this equality holds if and only if \(a_ib_i = b_ia_i\) for all \(b_i \in A_i\), in other words, if \(a_i \in Z(A_i)\) and thus if and only if \((a_1, a_2) \in Z(A_1) \oplus Z(A_2)\).
            
            Since \(A\) is semisimple we know that \(\Rad A = 0\), and thus
            \begin{equation}
                A/\Rad A = A/0 \isomorphic A \isomorphic \bigoplus_i \End V_i
            \end{equation}
            by \cref{thm:dimension of A geq dim squared of irreps}.
            Thus, we have
            \begin{equation}
                Z(A) = \bigoplus_i Z(\End(V_i)).
            \end{equation}
            Further, since \(V_i\) is a simple module we know by Schur's lemma (\cref{prp:schurs lemma}) that if an element commutes with all other elements then said element is just scalar multiplication, and further any multiplication by a scalar gives such a map, so
            \begin{equation}
                Z(\End V_i) \isomorphic \field.
            \end{equation}
            
            Combining these two results we have
            \begin{equation}
                Z(A) \isomorphic \bigoplus_i \field = \abs{\Irr A} \field
            \end{equation}
            and so
            \begin{equation}
                \dim Z(A) = \abs{\Irr A}
            \end{equation}
            where we've used the fact that the sum is indexed by simple \(A\)-modules, so has exactly as many terms as there are simple \(A\)-modules, and of course, \(\dim \field = 1\).
        \end{proof}
    \end{lma}
    
    Note that if \(A\) is not semisimple then this result no longer holds, since \(A/\Rad A \ncong A\).
    However, given a simple \(A\)-module, \(V\), we know that all elements of \(\Rad A\) act on \(V\) by zero, and thus there is a corresponding \((A/\Rad A)\)-module \(V'\), which has the same underlying space, but now elements of \(A/\Rad A\) act by \([a] \action v = a \action v\) for any representative \(a\) of this equivalence class.
    This gives a well-defined action precisely because elements of \(\Rad A\) act by zero, so if \(a'\) is some other representative then \(a - a' \in \Rad A\) and thus \(0 = (a - a') \action v = a \action v - a' \action v\) and thus \(a \action v = a' \action v\) as required.
    
    In fact, more generally if \(I\) is an ideal of \(A\) and \(V\) is an \(A\)-module on which all elements of \(I\) act as zero then \(A/I\) acts on \(V\) by \([a] \action v = a \action v\).
    This can be quite useful when we define algebras via a quotient, first construct an \(A\)-module, \(V\), then show that the ideal \(I \subseteq A\) acts as zero on \(V\), then we automatically get an \((A/I)\)-module structure for \(V\).
    
    \chapter{Character Theory}
    In this chapter we study character theory.
    The general idea being that for finite dimensional representations we can identify elements of \(A\) with linear maps \(V \to V\) which we can identify with matrices.
    We can then take the trace of these matrices, which is a nice thing to do because the trace is basis independent, despite the identification of elements and matrices requiring us to pick a basis.
    We can then learn a surprising amount just looking at these traces, which we call characters.
    
    \section{Definitions}
    \begin{dfn}{Character}{}
        Let \(A\) be an algebra and \(V\) a finite dimensional \(A\)-module with the corresponding algebra homomorphism \(\rho \colon A \to \End V\).
        Then the \defineindex{character} of \(V\) is the map
        \begin{align}
            \chi_V \colon A &\to \field\\
            a &\mapsto \chi_V(a) = \tr_V \rho(a)
        \end{align}
    \end{dfn}
    
    Note that we write \(\tr_V\) to denote the trace of matrices corresponding to elements of \(\End V\) after fixing some basis.
    We do this because later we'll want to take characters over different modules, and it's helpful to be able to distinguish which space the matrices we're taking the trace of act on.
    When there's no chance of confusion we'll drop the subscript \(V\).
    
    \begin{dfn}{}{}
        Let \(A\) be an algebra with subalgebras \(B, C \subseteq A\).
        Then we denote by \(\bracket{B}{C}\) the subspace
        \begin{equation}
            \bracket{B}{C} = \Span \{\bracket{b}{c} \mid b \in B \text{ and } c \in C\}
        \end{equation}
        where \(\bracket{b}{c} = bc - cb\).
    \end{dfn}
    
    Note that for any \(A\)-module, \(V\), with corresponding character \(\chi_V\), we have \(\bracket{A}{A} \subseteq \ker \chi_V\), since
    \begin{align}
        \chi_V(\bracket{a}{b}) &= \tr(\rho(\bracket{a}{b}))\\
        &= \tr(\rho(a)\rho(b) - \rho(b)\rho(a))\\
        &= \tr(\rho(a)\rho(b)) - \tr(\rho(b)\rho(a))\\
        &= \tr(\rho(a)\rho(b)) - \tr(\rho(a)\rho(b))\\
        &= 0,
    \end{align}
    having used the cyclic property of the trace.
    Thus \(\bracket{a}{b} \in \ker \chi_V\) for all \(a, b \in A\), and since the kernel is a subspace any linear combination of commutators will also vanish under \(\chi_V\), showing that \(\bracket{A}{A} \subseteq \ker \chi_V\).
    
    This tells us that the character also gives a well-defined map
    \begin{equation}
        \tilde{\chi}_V \colon A / \bracket{A}{A} \to \field
    \end{equation}
    defined by
    \begin{equation}
        \tilde{\chi}_V([a]) = \chi_V(a) = \tr_V(\rho(a)).
    \end{equation}
    In fact, it will prove more useful to define the character to be such a map.
    This allows us to view the character as an element of the dual space
    \begin{equation}
        \tilde{\chi}_V \in (A/\bracket{A}{A})^* = \hom_{\field}(A/\bracket{A}{A}, \field).
    \end{equation}
    We will do this, and do not distinguish between \(\chi_V\) and \(\tilde{\chi}_V\) in the notation.
    
    This is a useful thing to do because now the characters live in a vector space, and that lets us do linear-algebra-things to them, like look for a basis of this space.
    
    \begin{thm}{}{}
        Let \(A\) be a finite dimensional algebra.
        The characters of distinct finite-dimensional simple \(A\)-modules are linearly independent in \((A/\bracket{A}{A})^*\).
        Further, if \(A\) is finite dimensional and semisimple then the characters of simple \(A\)-modules provide a basis for \((A/\bracket{A}{A})^*\).
        \begin{proof}
            \Step{Linear Independence}
            Let that \(A\) be a finite dimensional (not necessarily semisimple) algebra.
            Then there is a finite number, \(n\), of simple \(A\)-modules, \(V_i\) for \(i = 1, \dotsc, n\), with corresponding algebra homomorphisms \(\rho_i \colon A \to \End V_i\).
            Then by the density theorem we have a surjection
            \begin{equation}
                \rho_1 \oplus \dotsb \oplus \rho_n \colon A \twoheadrightarrow \End V_1 \oplus \dotsb \oplus \End V_n.
            \end{equation}
            Suppose that
            \begin{equation}
                \sum_i \lambda_i \chi_{V_i} = 0
            \end{equation}
            with \(\lambda_i \in \field\).
            If \(a \in A\) we must therefore have
            \begin{equation}
                \sum_i \lambda_i \chi_{V_i}(a) = 0.
            \end{equation}
            Now take some arbitrary \(M \in \End V_1 \oplus \dotsb \oplus \End V_n\), which we view as a matrix by fixing some basis, which fixes a basis for each \(V_i\).
            We can then identify that \(M = M_1 \oplus \dotsb \oplus M_n\), where each \(M_i \in \End V_i\) is viewed as a matrix through the corresponding fixed basis.
            We can then consider the sum
            \begin{equation}
                \sum_i \lambda_i \tr_{V_i} M_i
            \end{equation}
            where the \(\lambda_i\) are the same coefficients as before.
            By surjectivity of \(\rho_1 \oplus \dotsb \oplus \rho_n\) we know that there is some \(a \in A\) such that \(M = (\rho_1 \oplus \dotsb \oplus \rho_n)(a)\), and thus \(M_i = \rho_i(a)\).
            This then gives that the sum above is
            \begin{equation}
                \sum_i \lambda_i \tr_{V_i}(\rho_i(a)) = \sum_i \lambda_i \chi_{V_i}(a) = 0.
            \end{equation}
            Now, we are free to choose \(M\), and hence \(M_i\), such that \(\tr_{V_i} M_i\) takes on any value in \(\field\), which means that the only way this equation can hold for an arbitrary choice of \(M\) is if \(\lambda_i = 0\) for all \(i = 1, \dotsc, n\).
            Thus, the \(\chi_{V_i}\) are linearly independent.
            
            \Step{Basis}
            Now suppose that \(A\) is a finite dimensional semisimple algebra.
            We have shown that the characters, \(\chi_{V_i}\), corresponding to simple \(A\)-modules, are linearly independent elements of \((A/\bracket{A}{A})^*\).
            We now show that they are also a spanning set of \((A / \bracket{A}{A})^*\).
            
            Since \(A\) is semisimple we have that
            \begin{equation}
                A \isomorphic \bigoplus_{i=1}^n \matrices{d_i}{\field}
            \end{equation}
            where \(d_i = \dim V_i\).
            We have the following well known fact about derived subalgebras of Lie algebras (\cref{lma:derived subalgebra of gln}):
            \begin{equation}
                \bracket{\matrices{d}{\field}}{\matrices{d}{\field}} = \bracket{\generalLinearLie_{d_i}(\field)}{\generalLinearLie_{d_i}(\field)} = (\generalLinearLie_{d_i}(\field))' = \specialLinearLie_d(\field).
            \end{equation}
            The Lie algebra \(\specialLinearLie_d(\field)\) consists precisely of the \(d \times d\) matrices over \(\field\) with zero trace.
            Further, for algebra \(B\) and \(C\), we have
            \begin{equation}
                \bracket{B \oplus C}{B \oplus C} = \bracket{B}{B} \oplus \bracket{C}{C}, 
            \end{equation}
            which follows immediately by linearity.
            Thus, we have
            \begin{equation}
                \bracket{A}{A} \isomorphic \bigoplus_{i=1}^n \specialLinearLie_{d_i}(\field).
            \end{equation}
            It then follows that
            \begin{align}
                A / \bracket{A}{A} &\isomorphic \left( \bigoplus_{i=1}^n \matrices{d_i}{\field} \right) / \left( \bigoplus_{i=1}^n \specialLinearLie_{d_i}(\field) \right)\\
                &= \left( \bigoplus_{i=1}^n \generalLinearLie_{d_i}(\field) \right) / \left( \bigoplus_{i=1}^n \specialLinearLie_{d_i}(\field) \right)\\
                &\isomorphic \bigoplus_{i=1}^n \generalLinearLie_{d_i}(\field)/\specialLinearLie_{d_i}(\field)\\
                &\isomorphic \bigoplus_{i=1}^n \field\\
                &= \field^n.  
            \end{align}
            This shows that we have \(n\)-linearly independent elements, \(\chi_{V_i}\), and \(\dim(A/\bracket{A}{A}) = n\), so these linearly independent elements are actually a basis.
        \end{proof}
    \end{thm}
    
    \begin{lma}{}{lma:derived subalgebra of gln}
        The derived subalgebra of \(\generalLinearLie_n(\field)\) is \(\specialLinearLie_n(\field)\).
        \begin{proof}
            First note that \(\generalLinearLie_n(\field) = \matrices{n}{\field}\) is the (Lie algebra) of \(n \times n\) matrices with entries in \(\field\).
            The elementary matrices, \(E_{ij}\), form a basis of \(\generalLinearLie_n(\field)\).
            Note that \(E_{ij}\), for \(i, j = 1, \dotsc, n\), are matrices which are zero everywhere except in row \(i\) and column \(j\), where they have a \(1\).
            So, it is sufficient to show that the commutator of any two elementary matrices is in \(\specialLinearLie_n(\field)\), and then any linear span of such commutators will be in \(\specialLinearLie_n(\field)\).
            To do this first note that
            \begin{equation}
                E_{ij}E_{kl} = \delta_{jk} E_{il}.
            \end{equation}
            Then we have
            \begin{align}
                \bracket{E_{ij}}{E_{kl}} &= E_{ij}E_{kl} - E_{kl}E_{ij}\\
                &= \delta_{jk}E_{il} - \delta_{li}E_{kj}.
            \end{align}
            Now we consider cases:
            \begin{enumerate}
                \item if \(i \ne l\) and \(j \ne k\) we get \(0\);
                \item if \(l \ne i\) and \(j = k\) we get \(E_{il}\);
                \item if \(l = i\) and \(j \ne k\) we get \(-E_{kj}\);
                \item if \(i = l\) and \(j = k\) we get \(E_{ii} - E_{jj}\).
            \end{enumerate}
            We see that in each case the matrix we get is traceless, specifically in the last case if \(i \ne j\) then the diagonal contains a \(1\) and a \(-1\), and if \(i = j\) then we have zero, and the second and third case have zero on the diagonal since \(i \ne l\) and \(k \ne j\) in these two cases.
            Thus, each matrix we get from \(\bracket{E_{ij}}{E_{kl}}\) is an element of \(\specialLinearLie_n(\field)\).
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{}
        Characters are invariant under isomorphism.
        \begin{proof}
            Let \(V\) and \(W\) be isomorphic finite dimensional \(A\)-modules.
            Then \(V\) and \(W\) are related by an isomorphism, \(V \to W\), but fixing bases for both we can view this isomorphism as a basis change, and the character is independent of basis choice.
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{lma:character and quotient}
        Let \(V\) be a finite dimensional \(A\)-module, and let \(W \subseteq V\) be a submodule.
        Then
        \begin{equation}
            \chi_V = \chi_W + \chi_{V/W}.
        \end{equation}
        \begin{proof}
            Fix a basis for \(W\) and extend this to a basis of \(V\).
            This can be done since \(V = W \oplus V/W\) as vector spaces.
            Then any linear map \(\varphi \colon V \to V\) such that \(\varphi(W) \subseteq W\) decomposes into a linear map \(W \to W\) and a linear map \(V/W \to V/W\).
            Since \(W\) is a submodule \(\rho(a)\) is exactly such a linear map for all \(a \in A\), and thus
            \begin{equation}
                \tr_V \rho(a) = \tr_W \rho(a) + \tr_{V/W} \rho(a),
            \end{equation}
            and so
            \begin{equation*}
                \chi_V = \chi_W + \chi_{V/W}. \qedhere
            \end{equation*}
        \end{proof}
    \end{lma}
    
    \section{Jordan--H\"older and Krull--Schmidt Theorems}
    We can now prove two standard results about filtrations using character theory.
    
    \begin{thm}{Jordan--H\"older}{}
        Let \(V\) be a finite dimensional \(A\)-module with filtrations
        \begin{equation}
            0 = V_0 \subseteq V_1 \subseteq \dotsb \subseteq V_n = V
        \end{equation}
        and
        \begin{equation}
            0 = V_0' \subseteq V_1' \subseteq \dotsb \subseteq V_m' = V
        \end{equation}
        such that \(W_i = V_i/V_{i-1}\) and \(W'_{i} = V_i'/V'_{i-1}\) are simple.
        Then
        \begin{enumerate}
            \item \(n = m\); and
            \item There exists some \(\sigma \in S_n\) such that \(W_i \isomorphic W'_{\sigma(i)}\), that is, the two series give rise to the same simple \(A\)-modules (up to isomorphism), but possibly in different orders.
        \end{enumerate}
        \begin{proof}
            ~
            \begin{wrn}
                This proof holds only in characteristic 0.
                The result does hold in general though, and can be proven in positive characteristic by induction on the dimension of \(V\).
                The problem in characteristic \(p\) is that the coefficients only end up being determined \(\bmod p\).
            \end{wrn}
            
            Consider the character \(\chi_V\).
            Using the first series and \cref{lma:character and quotient} we know that
            \begin{equation}
                \chi_V = \bigoplus_{i=1}^n \chi_{W_i},
            \end{equation}
            and using the second series we know that
            \begin{equation}
                \chi_V = \bigoplus_{i=1}^m \chi_{W'_i}.
            \end{equation}
            Since the characters of the simple \(A\)-modules form a basis of \((A/\bracket{A}{A})^*\) any decomposition such as the above must be unique, and thus we have \(n = m\) and there is some permutation, \(\sigma \in S_n\) such that \(\chi_{W_i} = \chi_{W'_{\sigma(i)}}\), and thus \(W_i \isomorphic W'_{\sigma(i)}\).
        \end{proof}
    \end{thm}
    
    \begin{dfn}{Jordan--H\"older Series}{}
        Given a finite dimensional \(A\)-module, \(V\), admitting a filtration
        \begin{equation}
            0 = V_0 \subseteq V_1 \subseteq \dotsb \subseteq V_n = V
        \end{equation}
        such that \(V_i/V_{i-1}\) are simple we call \(n\) the \defineindex{length} of \(V\), and the set of simple modules \(\{V_i/V_{i-1}\}\) is called the \defineindex{Jordan--H\"older series} of \(V\).
    \end{dfn}
    
    Note that by the Jordan--H\"older theorem the length and Jordan--H\"older series are well-defined, being independent of the choice of filtration, so long as the quotient of successive modules is simple.
    
    The following result holds for finite length modules.
    Note that finite length is a strictly weaker condition than finite dimension, since finite dimension guarantees the existence of 
    
    \begin{thm}{Krull--Schmidt}{}
        Every finite length \(A\)-module, \(V\), is a direct sum of indecomposable modules.
        Further, this decomposition is unique up to isomorphism and permutation of the summands.
        \begin{proof}
            \Step{Existence}
            Let \(V\) be a finite length \(A\)-module.
            We may suppose that \(V = V_1 \oplus V_2\) with \(V_i\) \(A\)-modules, and without loss of generality we assume that \(V_1\) cannot be written as a sum of indecomposables.
            Then we must be able to decompose \(V_1\) again.
            Continuing on we see that this gives rise to an infinite length filtration, contradicting the assumption that \(V\) has finite length.
            
            \Step{Uniqueness}
            We make use of \cref{lma:endomorphisms of fin dim indecomposable are iso or nilpotent}.
            Using this result take two decompositions into indecomposables
            \begin{equation}
                V = V_1 \oplus \dotsb \oplus V_m = V_1' \oplus \dotsb \oplus V_m'.
            \end{equation}
            We will prove that \(V_k \isomorphic V'_k\) for some \(k\).
            Let
            \begin{equation}
                i_k \colon V_k \hookrightarrow V, \qqand i'_k \colon V'_k \hookrightarrow V
            \end{equation}
            be the natural inclusions, and
            \begin{equation}
                p_k \colon V \twoheadrightarrow V_k, \qand p'_k \colon V \twoheadrightarrow V'_k
            \end{equation}
            be the natural projections.
            Then we have the map
            \begin{equation}
                \theta_k \colon p_1 \circ i'_k \circ p'_k \circ i_1 \colon V_1 \to V_1,
            \end{equation}
            which is a composite of module morphisms, so is itself a module morphism.
            We also have that \(\sum_k \theta_k = \id_V\), since summing over all \(k\) the image of \(i'_k \circ p'_k\) in the middle runs over all of \(V\),
            We know that \(\id_V\) is not nilpotent, so by the contrapositive of \cref{lma:endomorphisms of fin dim indecomposable are iso or nilpotent} we know that at least one of the \(\theta_k\)s must be an isomorphism.
            Without loss of generality we assume that \(\theta_1\) is an isomorphism.
            Then we have that
            \begin{equation}
                V_1 = \im(p'_1 \circ i_1) \oplus \ker(p_1 \circ i'_1),
            \end{equation}
            but \(V_1\) is indecomposable, so \(p'_1 \circ i_1 \colon V_1 \to V_1'\) must be an isomorphism.
            We may then consider \(V_2 \oplus \dotsb V_m \isomorphic V'_2 \oplus \dotsb V_m\), and by the same logic we may take \(V_2 \isomorphic V'_2\).
            Repeating this eventually terminates after \(m\) applications.
        \end{proof}
    \end{thm}
    
    
    
    \begin{lma}{}{lma:endomorphisms of fin dim indecomposable are iso or nilpotent}
        Let \(W\) be a finite dimensional indecomposable \(A\)-module.
        Then
        \begin{enumerate}
            \item any module morphism \(\theta \colon W \to W\) is either an isomorphism or nilpotent;
            \item if \(\theta_i \colon W \to W\) for \(i = 1, \dotsc, n\) is a set of nilpotent module morphisms then \(\theta = \sum_i \theta_i\) is also a nilpotent module morphism.
        \end{enumerate}
        \begin{proof}
            We work over an algebraically closed field, thus \(W\) splits into a sum of generalised eigenspaces.
            These are submodules of \(W\).
            Thus, \(\theta\) can have only one eigenvalue, call it \(\lambda\).
            If \(\lambda = 0\) then \(\theta\) is nilpotent, and if \(\lambda \ne 0\) then \(\theta\) is an isomorphism.
            
            We prove that the sum of nilpotents is nilpotent by induction on \(n\).
            For the base case, \(n = 1\), we clearly have that \(\theta = \theta_1\) is nilpotent.
            Suppose then that the hypothesis holds up to \(n\) summands, and that at \(n\) summands \(\theta\) is not nilpotent.
            Then \(\theta\) must be an isomorphism, and thus its inverse exists, and we have \(\id_W = \theta \theta^{-1} = \theta^{-1} \sum_{i=1}^n \theta_i = \sum_{i=1}^n \theta^{-1}\theta_i\).
            Since the morphisms \(\theta^{-1}\theta_i\) are not isomorphisms they are nilpotent, and thus \(\id_W - \theta^{-1}\theta_n = \theta^{-1}\theta_1 + \dotsb + \theta^{-1}\theta_{n-1}\) is an isomorphism, but it's also a sum of \(n - 1\) nilpotents, so it should be nilpotent, a contradiction.
            Thus by induction any such sum of nilpotents is itself nilpotent.
        \end{proof}
    \end{lma}
    
    \section{Tensor Products}
    Let \(A\) and \(B\) be \(\field\)-algebras.
    Then \(A \otimes_{\field} B\) is also a \(\field\)-algebra when equipped with the product
    \begin{equation}
        (a \otimes b)(a' \otimes b') = aa' \otimes bb'
    \end{equation}
    for \(a, a' \in A\) and \(b, b' \in B\).
    
    \begin{thm}{}{}
        Let \(A\) and \(B\) be \(\field\)-algebras.
        Let \(V\) be a simple finite dimensional \(A\)-module, and \(W\) a simple finite dimensional \(B\)-module.
        Then \(V \otimes_{\field} W\) is a simple \((A \otimes_{\field} B)\)-module.
        Further, any finite dimensional simple \((A \otimes_{\field} B)\)-module is of this form with \(V\) and \(W\) unique.
        \begin{proof}
            By the density theorem we have surjections \(A \twoheadrightarrow \End V\) and \(B \twoheadrightarrow \End W\).
            Thus, we have a surjection
            \begin{equation}
                A \otimes B \twoheadrightarrow \End V \otimes \End W \isomorphic \End(V \otimes W).
            \end{equation}
            Thus, \(V \otimes W\) must be simple, as any submodules would only arise as submodules of \(V\) and \(W\).
            
            Now suppose that \(U\) is a simple \((A \otimes B)\)-module, and let \(A'\) and \(B'\) denote the images of \(A\) and \(B\) in \(\End U\).
            Then \(A'\) and \(B'\) are finite dimensional, and we can assume without loss of generality that \(A\) and \(B\) are also finite dimensional.
            By \cref{clm:rad of tensor product} we have that
            \begin{equation}
                \Rad(A \otimes B) = \Rad(A) \otimes B + A \otimes \Rad(B)
            \end{equation}
            and thus, we have
            \begin{equation}
                (A \otimes B)/\Rad(A \otimes B) = A/\Rad(A) \otimes B/\Rad(B).
            \end{equation}
            Since all of the algebras in question are matrix algebras the assertion follows.
        \end{proof}
    \end{thm}
    
    \begin{clm}{}{clm:rad of tensor product}
        For \(\field\)-algebras \(A\) and \(B\) we have
        \begin{equation}
            \Rad(A \otimes B) = \Rad(A) \otimes B + A \otimes \Rad(B).
        \end{equation}
        \begin{proof}
            Consider the simple module \(V \otimes W\), where \(V\) is a simple \(A\)-module and \(W\) is a simple \(B\)-module.
            We know that if \(a \otimes b \in \Rad(A \otimes B)\) then \(a \otimes b\) acts as zero on \(V \otimes W\).
            We also know that if \(v \otimes w \in V \otimes W\) then \(a \otimes b\) acts as
            \begin{equation}
                (a \otimes b) \action (v \otimes w) = (a \action v) \otimes (b \action w).
            \end{equation}
            If this is to vanish then it must be that either \(a \action v = 0\) or \(b \action w = 0\).
            Thus, \(a \in \Rad A\) or \(b \in \Rad B\), and so \(a \otimes b \in \Rad A \otimes B + A \otimes \Rad B\).
            Conversely, clearly any element of this set acts trivially on \(V \otimes W\), and thus we have containment the other way.
        \end{proof}
    \end{clm}
    
    \chapter{Representation Theory of Finite Groups}
    Throughout this chapter \(G\) will be a finite group.
    
    In this chapter we will look at representations of finite groups.
    We have already developed much of the required theory because group representations, \(\rho \colon G \to \generalLinear(V)\), are in one-to-one correspondence with \(\field G\)-modules.
    Note that we write \(G\)-module and \(\Hom_{G}(V, W)\) for \(\field G\)-module and \(\Hom_{\field G}(V, W)\).
    
    \section{Maschke's Theorem}
    \begin{thm}{Maschke}{}
        Let \(\Char \field\) be coprime to \(\abs{G}\).
        Then
        \begin{enumerate}
            \item \(\field G\) is semisimple;
            \item \(\field G \isomorphic \bigoplus_i \End V_i\) with the isomorphism given on the basis by \(g \mapsto \bigoplus_i \rho_i(g)\) where \(\rho_i \colon G \to \generalLinear(V_i)\) are the irreducible representations of \(G\).
        \end{enumerate}
        \begin{proof}
            We know that semisimplicity of \(\field G\) implies that \(\field G\) decomposes as in the second point (\cref{prp:equivalent definitions of semisimple algebra}), so we need only show that \(\field G\) is semisimple.
            
            To prove that \(\field G\) is semisimple it is sufficient to prove that given a \(G\)-module, \(V\), and a \(G\)-submodule \(W \subseteq V\) there is some \(G\)-submodule, \(W'\) such that \(V = W \oplus W'\).
            This will show that any finite-dimensional \(\field G\)-module is semisimple, and hence that \(\field G\) is semisimple by \cref{prp:equivalent definitions of semisimple algebra}.
            
            Given a \(G\)-module, \(V\), and a \(G\)-submodule, \(W\), we always have \emph{as vector spaces} some \(\overbar{W} \subseteq V\) such that \(V = W \oplus \overbar{W}\).
            We will construct from \(\overbar{W}\) a \(G\)-submodule \(W'\) such that \(V = W \oplus W'\).
            
            Let \(p \colon V \twoheadrightarrow W\) be projection onto the subspace \(W\).
            That is, \(p|_W = \id_W\) and \(p|_{\overbar{W}} = 0\).
            We may define
            \begin{equation}
                P = \frac{1}{\abs{G}} \sum_{g \in G} \rho(g) p \rho(g)^{-1}
            \end{equation}
            where \(\rho \colon G \to \generalLinear(V)\) is our representation map.
            Now consider \(W' = \ker P\).
            We claim that \(W'\) is a submodule and \(V = W \oplus W'\).
            
            To verify these we need to show that \(G \action W' \subseteq W'\) and that \(P\) is projection onto \(W\).
            Suppose that \(w \in W'\), that is \(Pw = 0\).
            Then for \(h \in G\) we have
            \begin{align}
                P(h \action w) &= P\rho(h)w\\
                &= \frac{1}{\abs{G}} \sum_{g \in G} \rho(g) p \rho(g)^{-1}\rho(h)w\\
                &= \frac{1}{\abs{G}} \sum_{g \in G} \rho(g) p \rho(g^{-1}h) w\\
                &= \frac{1}{\abs{G}} \sum_{g' \in G} \rho(hg') p \rho(g'^{-1}) w\\
                &= \rho(h) \frac{1}{\abs{G}} \sum_{g' \in G} \rho(g') p \rho(g'^{-1}) w\\
                &= \rho(h) P w\\
                &= 0
            \end{align}
            where we've reparametrised the sum using \(g'^{-1} = g^{-1}h\), so \(g' = h^{-1} g\) and \(g = hg'\).
            This is a common trick when dealing with sums over group elements like this one.
            We have successfully shown that \(h \action w \in \ker P\) if \(w \in \ker P\), and thus \(h \action w \in W'\).
            
            We can now verify that \(P\) is a projection onto \(W\).
            For this we have to show that \(P|_W = \id_W\), and \(P(V) \subseteq W\), which combined imply that \(P^2 = P\).
            For the first if \(w \in W\) consider
            \begin{equation}
                Pw = \frac{1}{\abs{G}} \sum_{g \in G} \rho(g) p \rho(g)^{-1}w.
            \end{equation}
            Since \(W\) is a submodule we know that \(\rho(g)^{-1}w \in W\), then since \(p\) is a projection onto \(W\) we know that \(p\rho(g)^{-1}w = \rho(g)^{-1}w\), and thus \(\rho(g)p\rho(g)^{-1}w = \rho(g)\rho(g)^{-1}w = w\).
            So, the sum reduces to
            \begin{equation}
                Pw = \frac{1}{\abs{G}} \sum_{g \in G} w = \frac{\abs{G}}{\abs{G}} w = w.
            \end{equation}
            Thus, \(P|_W = \id_W\) as claimed.
            Now we can show that \(P(V) \subseteq W\).
            For \(v \in V\) consider
            \begin{equation}
                Pv = \frac{1}{\abs{G}} \sum_{g \in G} \rho(g) p \rho(g)^{-1}v.
            \end{equation}
            By definition \(V\) is closed under the action of \(g\), so \(\rho(g)^{-1}v \in V\), then by definition \(p \rho(g)^{-1} v \in W\), and since \(W\) is a submodule \(\rho(g)p\rho(g)^{-1} v \in W\) for all \(g \in G\).
            Submodules are closed under taking linear combinations, so \(Pv \in W\).
            Thus, \(P\) is a projection onto \(W\), and so we have the decomposition of vector spaces \(V = W \oplus W'\), and we've already shown that \(W'\) is actually a submodule, so this is a decomposition of \(G\)-modules.
        \end{proof}
    \end{thm}
    
    \begin{crl}{}{}
        We have
        \begin{equation}
            \field G \isomorphic \bigoplus_i (\dim V_i) V_i
        \end{equation}
        and
        \begin{equation}
            \abs{G} = \sum_i (\dim V_i)^2.
        \end{equation}
        \begin{proof}
            This is simply Maschke's theorem applied to the regular representation, which is just \(G\) acting on itself by multiplication, where we've used \(\abs{G} = \dim \field G\).
        \end{proof}
    \end{crl}
    
    The converse of Maschke's theorem holds also.
    
    \begin{prp}{}{}
        If \(\field G\) is semisimple then \(\Char \field\) and \(\abs{G}\) are coprime.
        \begin{proof}
            By Maschke's theorem we can write
            \begin{equation}
                \field G \isomorphic \bigoplus_{i=1}^r \End V_i
            \end{equation}
            where the \(V_i\) are simple \(G\)-modules and \(V_1 = \field\) is the trivial representation.
            Then we have
            \begin{equation}
                \field G \isomorphic \field \oplus \bigoplus_{i=2}^r \End V_i \isomorphic \field \oplus \bigoplus_{i=2}^r d_i V_i
            \end{equation}
            with \(d_i = \dim V_i\).
            Schur's lemma then tells us that every homomorphism of \(G\)-modules \(\field \to \field G\) is a scalar multiple of some fixed homomorphism \(\Lambda \colon \field \to \field G\), and every \(G\)-module homomorphism \(\field G \to \field\) is a scalar multiple of some fixed homomorphism \(\varepsilon \colon \field G \to \field\).
            More symbolically, the hom-spaces \(\Hom_{\field G}(\field, \field G)\) and \(\Hom_{\field G}(\field G, \field)\) are one-dimensional with bases \(\Lambda\) and \(\varepsilon\) respectively, so are simply \(\field \Lambda\) and \(\field \varepsilon\).
            We are free to choose these maps to be such that \(\varepsilon(g) = 1\) for all \(g \in G\), and \(\Lambda(1) = \sum_{g \in G} g\).
            Then we have
            \begin{equation}
                \varepsilon(\Lambda(1)) = \varepsilon\left( {\textstyle \sum_{g \in G}} g \right) = \sum_{g \in G} \varepsilon(g) = \sum_{g \in G} 1 = \abs{G}.
            \end{equation}
            Now, if \(\abs{G} = kp\) where \(p = \Char \field\) then \(\abs{G} = 0\) in \(\field G\) and so this sum says that \(\varepsilon \circ \Lambda(1) = 0\), which means that \(\Lambda\) has no left-inverse since \(a \varepsilon \circ \Lambda(1) = 0\) for all \(a \in \field\), which rules out all maps \(\field G \to \field\) (since all are of the form \(a\varepsilon\) for some \(a \in \field\)) as inverses for \(\Lambda\), since these would have to give \(a \varepsilon \circ \Lambda(1) = 1\).
        \end{proof}
    \end{prp}
    
    \begin{exm}{}{}
        Consider \(G = \integers/p\integers\), and \(\field\) a field of characteristic \(p\).
        Clearly, \(\Char \field = p\) and \(\abs{G} = p\) are not coprime.
        
        A consequence of this is that every simple \(\integers/p\integers\)-module over \(\field\) is trivial.
        This follows because in a finite group of order \(p\) we have that \(x^p = 1\), so \(x^p - 1\) acts as zero, but over a field of characteristic \(p\) we have that \(x^p - 1 = (x - 1)^p\), and thus \((x - 1)^p\) acts as zero, so \(x - 1\) acts as \(0\) (as \(0\) is the only element of the group which doesn't act as \(1\) when raised to the power of \(p\)), so \(x\) must act as \(1\).
    \end{exm}
    
    \section{Group Characters}
    \begin{dfn}{Group Character}{}
        Let \(G\) be a group and \(\rho \colon G \to \generalLinear(V)\) a representation on a finite dimensional space, \(V\).
        Then the \defineindex{character} of \(V\) is the map
        \begin{align}
            \chi_V \colon G \to \field\\
            & g \mapsto \chi_V(g) = \tr_V(\rho(g)).
        \end{align}
    \end{dfn}
    Of course, if \(\tilde{\chi}_V \colon \field G \to \field\) is the character of the corresponding representation of the group algebra \(\field G\) then \(\chi_V = \tilde{\chi}_V|_G\), viewing \(G\) as a subset of \(\field G\) in the canonical way (i.e., restricting to the canonical basis).
    
    \begin{dfn}{Class Function}{}
        Let \(G\) be a group.
        A \defineindex{class function} of \(G\) is a map \(f \colon G \to \field\) such that \(f(g) = f(hgh^{-1})\) for all \(g, h \in G\).
        We write
        \begin{equation}
            \classFunctions(G) = \{f \colon G \to \field \mid f(g) = f(hgh^{-1}) \forall g, h \in G\}
        \end{equation}
        for the set of all class functions.
    \end{dfn}
    
    That is, class functions are functions which are invariant under conjugation of their argument.
    Another way of putting this, which explains the name, is that class functions are exactly those functions which are constant on each conjugacy class.
    Because of this we can identify
    \begin{equation}
        \classFunctions(G) \isomorphic_{\Set} \Func(\conjugacyClasses(G), \field)
    \end{equation}
    where \(\conjugacyClasses(G)\) is the set of all conjugacy classes and
    \begin{equation}
        \Func(A, B) = \{f \colon A \to B\} = \Set(A, B).
    \end{equation}
    
    Actually, under pointwise addition and scalar multiplication \(\classFunctions(G)\) is a vector space.
    Further, under mild conditions the irreducible characters provide a basis for this space.
    
    \begin{thm}{}{}
        If \(\Char \field\) and \(\abs{G}\) are coprime then the irreducible characters, \(\chi_{V_i}\), of \(G\) form a basis for \(\classFunctions(G)\).
        \begin{proof}
            From Maschke's theorem we know that \(A = \field G\) is semisimple.
            We have proven that the irreducible algebra characters \(\widetilde{\chi}_{V_i}\) form a basis for \((A/\bracket{A}{A})^*\).
            We then have
            \begin{align}
                (A/\bracket{A}{A})^* &= \{f \in \Hom_{\field}(\field G, \field) \mid gh - hg \in \ker f \forall g, h \in G\} \notag\\
                &= \{f \in \Hom_{\field}(\field G, \field) \mid f(gh) - f(hg) = 0 \forall g, h \in G\} \notag\\
                &= \{f \in \Hom_{\field}(\field G, \field) \mid f(gh) = f(hg) \forall g, h \in G\} \notag\\
                &\isomorphic_{\Vect} \{f \in \Func(G, \field) \mid f(gh) = f(hg) \forall g, h \in G\} \notag\\
                &= \classFunctions(G). \notag
            \end{align}
        \end{proof}
    \end{thm}
    
    \begin{crl}{}{}
        The number of irreducible representations of \(G\) is equal to the number of conjugacy classes:
        \begin{equation}
            \abs{\Irr(G)} = \abs{\conjugacyClasses(G)}.
        \end{equation}
    \end{crl}
    
    \begin{exm}{}{}
        Consider the symmetric group, \(G = S_n\).
        Using cycle notation if we write every element as a product of disjoint cycles then two elements are in the same conjugacy class if and only if they have the same cycle type.
        
        More concretely, take \(S_4\), then the cycle type of \(\cycle{1,2,3,4}\) is \((4)\), the cycle type of \(\cycle{1,2}\cycle{3,4}\) is \((2, 2)\), the cycle type of \(\cycle{1,2,3}\) is \((3, 1)\) (note that \(\cycle{1,2,3} = \cycle{1,2,3}\cycle{4}\), and we have to include all elements of \(\{1, 2, 3, 4\}\)).
        So, for example, \(\cycle{1,2,3}\) and \(\cycle{2,3,4}\) are conjugate, and so are \(\cycle{1,2}\cycle{3,4}\) and \(\cycle{1,3}\cycle{2,4}\).
        
        We can identify conjugacy classes with cycle types, and we can identify cycle types with partitions of \(n\).
        A \defineindex{partition} of \(n\) being a tuple \(\lambda = (\lambda_1, \lambda_2, \dotsc, \lambda_k)\) such that \(\lambda_1 \ge \lambda_2 \ge \dotsb \ge \lambda_k \ge 0\) \(\lambda_1 + \lambda_2 + \dotsb + \lambda_k = n\).
        We write \(\lambda \partition n\) to denote that \(\lambda\) is a partition of \(n\).
        
        A common, and useful notation, for partitions is that of \define{Young diagrams}\index{Young diagram}.
        Here we take a partition, \(\lambda\), and write a row of \(\lambda_i\) boxes in the \(i\)th row (rows counted from the top down).
        For example, \(\cycle{1,2}\cycle{3,4}\) has cycle type \(\lambda = (2, 2)\), and the corresponding Young diagram is
        \begin{equation}
            \lambda = \ydiagram{2,2}\,.
        \end{equation}
        Similarly, \(\cycle{1,2,3}\) has cycle type \(\mu = (3, 1)\), and the corresponding Young diagram is
        \begin{equation}
            \mu = \ydiagram{3,1}\,.
        \end{equation}
        
        So, we have a bijection between
        \begin{itemize}
            \item conjugacy classes of \(S_n\);
            \item partitions of \(n\);
            \item Young diagrams with \(n\) boxes.
        \end{itemize}
        
        It will turn out that Young diagrams, and the related Young tableaux, come up a lot when we start counting things related to the symmetric group.
        
        Later, we will explicitly define the irreducible representation, \(V_\lambda\), of \(S_n\) corresponding to a partition \(\lambda \partition n\).
        % TODO: reference to definition of specht module
    \end{exm}
    
    Note that if \(\Char \field\) divides \(\abs{G}\) then \(\field G\) is not generally semisimple and we typically have \(\abs{\conjugacyClasses(G)} \ge \abs{\Irr(G)}\).
    
    \begin{crl}{}{}
        For a field of characteristic \(0\) two \(G\)-modules, \(V\) and \(W\), are isomorphic if and only if \(\chi_V = \chi_W\).
        \begin{proof}
            Under these conditions \(\field G\) is semisimple, and thus we can decompose both representations as
            \begin{equation}
                V = \bigoplus_i n_i V_i, \qqand W = \bigoplus_i m_i V_i
            \end{equation}
            where \(V_i\) are irreducible representations and \(n_i, m_i \in \integers_{\ge 0}\).
            Then we have
            \begin{align}
                \chi_V(g) &= \tr_V(\rho_V(g))\\
                &= \tr_{\bigoplus_i n_i V_i}(n_i\rho_{V_i}(g))\\
                &= \sum_{i} n_i \tr_{V_i}(\rho_{V_i}(g))\\
                &= \sum_i n_i \chi_{V_i}(g)
            \end{align}
            and similarly
            \begin{equation}
                \chi_W(g) = \sum_i m_i \chi_{V_i}(g).
            \end{equation}
            Since the characters are a basis we have equality between these only if \(n_i = m_i\), and thus both representations have the same decomposition, so are isomorphic.
        \end{proof}
    \end{crl}
    
    There is an isomorphism of vector spaces \(\field G \isomorphic_{\Vect} \Func(G, \field)\) on the basis by identifying \(g\) with \(\delta_g\) for \(g \in G\) where
    \begin{equation}
        \delta_g(h) = \delta_{g,h} = 
        \begin{cases}
            1 & g = h\\
            0 & g \ne h
        \end{cases}
    \end{equation}
    is the \defineindex{Kronecker delta}.
    
    We can define the \defineindex{convolution} product, \(*\), on \(\Func(G, \field)\) by
    \begin{equation}
        (\psi * \varphi)(g) = \sum_{h \in G} \psi(h) \varphi(h^{-1}g).
    \end{equation}
    This product makes \(\Func(G, \field)\) an algebra, and extends the above isomorphism to an isomorphism of algebras, \(\field G \isomorphic_{\Alg} \Func(G, \field)\), since
    \begin{align}
        (\delta_g * \delta_h)(k) &= \sum_{\ell \in G} \delta_g(\ell) \delta_h(\ell^{-1}k)\\
        &= \sum_{\ell \in G} \delta_{g,\ell} \delta_{h,\ell^{-1}k}
    \end{align}
    and terms in this sum vanish except for when \(g = \ell\) and \(h = \ell^{-1}k\), which means that \(h = g^{-1}k\), or \(k = gh\).
    So we only get a nonzero output if \(k = gh\), which means that this convolution is exactly \(\delta_{gh}\), which is of course the same as taking the product of \(g\) and \(h\) in \(\field G\) then mapping to \(\Func(G, \field)\).
    
    \begin{prp}{}{}
        Let
        \begin{equation}
            \vv{c} = \sum_{g \in c} g
        \end{equation}
        where \(c \in \conjugacyClasses(G)\) is some conjugacy class.
        Then \(Z(\field G) = \langle \vv{c} \mid C \in \conjugacyClasses(G) \rangle\) and \(Z(\field G) \isomorphic \classFunctions(G)\).
        \begin{proof}
            We first show that for each conjugacy class, \(c\), \(\vv{c}\) is in \(Z(\field G)\).
            To do so we show that \(\vv{c}\) commutes with all elements of \(G\), so taking \(g \in G\) we have
            \begin{align}
                \vv{c}g &= \sum_{h \in C} hg\\
                &= \sum_{h \in C} ghg^{-1}g\\
                &= \sum_{h \in C} gh\\
                &= g \sum_{h \in C} h\\
                &= g\vv{c}. 
            \end{align}
            Here we've used the fact that conjugation by \(g\) is a permutation on \(c\), and thus changing \(h\) to \(ghg^{-1}\) in the sum doesn't change the sum, it just permutes the terms.
            
            The result follows from \cref{lma:invariant subspace spanned by orbit sums} applied to the special case where \(X = G\) with the action given by conjugation, in which case the invariant subspace is exactly the centre of \(\field G\).
        \end{proof}
    \end{prp}
    
    \begin{lma}{}{lma:invariant subspace spanned by orbit sums}
        Let \(G\) be a finite group acting on a finite set, \(X\).
        The invariant subspace of the free vector space \(\field X\) is spanned by elements of the form \(\vv{o} = \sum_{x \in o} x\) where \(o\) ranges over all orbits of the group action.
        \begin{proof}
            Consider \(\vv{o}\) for some orbit, \(o\), we have
            \begin{equation}
                g \action \vv{o} = \sum_{x \in o} g \action o = \vv{o}.
            \end{equation}
            This follows since acting with \(g\) is just a permutation of the orbit, \(o\), and thus the sum is unchanged, it's just a permutation of the terms in the sum.
            
            Conversely, suppose that \(v = \sum_{x \in X} v_x x\) is invariant under the action of \(G\).
            Then we have
            \begin{equation}
                g \action v = \sum_{x \in X} v_x (g \action x)
            \end{equation}
            and by invariance we demand that this is equal to
            \begin{equation}
                v = \sum_{x \in X} v_x x = \sum_{g^{-1}\action x \in X} v_{g^{-1} \action x} x,
            \end{equation}
            so we can conclude that \(v_x = v_{g^{-1} \action x}\) for all \(g \in G\), and thus \(v_x = v_y\) whenever \(x\) and \(y\) lie in the same orbit.
            Hence, \(v\) is a linear combination of the elements \(\vv{o}\), and so the \(\vv{o}\) are a basis of the invariant subspace of \(\field X\).
        \end{proof}
    \end{lma}
    
    \begin{exm}{Finite Abelian Group}{}
        Let \(G\) be a finite abelian group.
        Since \(G\) is abelian every element of \(G\) is in its own conjugacy class, so
        \begin{equation}
            \abs{\Irr(G)} = \abs{\conjugacyClasses(G)} = \abs{G}.
        \end{equation}
        By the structure theorem we know that
        \begin{equation}
            G \isomorphic \integers_{n_1} \times \dotsb \times \integers_{n_k}
        \end{equation}
        for some \(n_i \in \integers_{\ge 0}\).
        Since \(G\) is abelian Schur's lemma tells us that all representations are one dimensional.
        Further, these irreducible representations form a group under pointwise multiplication:
        \begin{equation}
            (\rho_1 \cdot \rho_2)(g) = \rho_1(g)\rho_2(g).
        \end{equation}
        The identity, \(\varepsilon\), is the trivial representation, \(\varepsilon(g) = 1\).
        The inverse of \(\rho\) is the representation \(g \mapsto 1/\rho(g)\).
        
        Each irreducible representation is a map \(\rho \colon G \to \field^{\times} \isomorphic \generalLinear(\field)\).
        Thus, in this case the representations coincide with the characters.
        
        We call the group \(G^{\vee} \coloneqq (\Irr(G), \cdot)\) the \defineindex{character group} or \defineindex{dual group} of \(G\).
        
        Consider now \(G = \integers_n\) and \(\field = \complex\).
        Then we have the irreducible representation
        \begin{align}
            \rho \colon \integers_n &\to \complex\\
            m &\mapsto \e^{2\pi im/n}
        \end{align}
        and \(\integers_n^{\vee} = \{\rho^k \mid k = 1, \dotsc, n\}\), which clearly gives an isomorphism \(\integers_n^{\vee} \isomorphic \integers_n\).
        
        In fact, for any finite abelian group we have \(G^{\vee} \isomorphic G\), but not uniquely.
        However, we do have a canonical isomorphism \(G \isomorphic (G^{\vee})^{\vee}\) given by \(g \mapsto (\chi \mapsto \chi(g))\).
    \end{exm}
    
    \section{Dual Representations}
    \begin{dfn}{Dual Representation}{}
        Let \(\rho \colon G \to \generalLinear(V)\) be a representation of a finite group on a finite dimensional vector space.
        Then the dual space, \(V^*\), gives rise to a representation, \(\rho^* \colon G \to \generalLinear(V^*)\), with the on \(f \in V^*\) given by
        \begin{equation}
            (g \action f)(v) = (\rho^*(g)f)(v) = f(\rho(g^{-1})v)
        \end{equation}
        for all \(v \in V\).
    \end{dfn}
    
    For \(\field = \complex\) we can further simply this by identifying that \(\rho^*(g) = \overline{\rho(g^{-1})}^{\trans}\).
    That is, \(g\) acts on \(V^*\) by the Hermitian conjugate of the action of \(g^{-1}\) on \(V\).
    
    \begin{lma}{}{}
        We have \(\chi_{V^*}(g) = \chi_{V}(g^{-1})\).
        \begin{proof}
            This follows from a direct calculation:
            \begin{align}
                \chi_{V^*}(g) &= \tr_{V^*}(\rho^*(g))\\
                &= \tr_{V}(\rho(g^{-1}))\\
                &= \chi_V(g^{-1}). \notag\qedhere
            \end{align}
        \end{proof}
    \end{lma}
    
    Note that \(\chi_V(g) = \sum_{i} \lambda_i\) where \(\lambda_i\) are the eigenvalues of \(\rho(g)\).
    We also know that for a finite group we have \(\rho(g)^{\abs{G}} = \rho(g^{\abs{G}}) = \rho(e) = I\), and thus the eigenvalues of \(\rho(g)\) must be roots of unity.
    For \(\field = \complex\) we have \(\chi_{V^*}(g) = \sum_{i} \lambda_i^{-1} = \overline{\chi_V(g)}\), and thus \(V \isomorphic V^*\) as \(G\)-modules if and only if \(\chi_V(g) \in \reals\) for all \(g \in G\).
    
    \section{Tensor Products of Representations}
    \begin{dfn}{}{}
        Let \(\rho_V \colon G \to \generalLinear(V)\) and \(\rho_W \colon G \to \generalLinear(W)\) be representations of \(G\).
        Then there is a representation
        \begin{equation}
            \rho_V \otimes \rho_W \colon G \to \generalLinear(V) \otimes \generalLinear(W) \isomorphic \generalLinear(V \otimes W)
        \end{equation}
        given by
        \begin{equation}
            (\rho_V \otimes \rho_W)(g) = \rho_V(g) \otimes \rho_W(g).
        \end{equation}
    \end{dfn}
    
    Note that the character of a tensor product of representations is given by
    \begin{equation}
        \chi_{V \otimes W}(g) = \chi_V(g) \chi_W(g).
    \end{equation}
    
    \begin{exm}{Schur--Weyl Duality}{}
        Consider the group \(G = \generalLinear(V)\).
        Then \(V^{\otimes n}\) carries a left \(G\)-module structure given on simple tensors by
        \begin{equation}
            g \action (v_{i_1} \otimes \dotsb \otimes v_{i_n}) = (g \action v_{i_1} \otimes \dotsb \otimes g \action v_{i_n})
        \end{equation}
        where \(g \action v_{i_k}\) is the obvious action of \(g \in \generalLinear(V)\) on \(v_{i_k} \in V\).
        
        The space \(V^{\otimes n}\) also naturally carries a right \(S_n\)-module action, given on simply tensors by
        \begin{equation}
            (v_{i_1} \otimes \dotsb \otimes v_{i_n}) \action w = v_{i_{w(1)}} \otimes \dotsb \otimes v_{i_{w(n)}}.
        \end{equation}
        That is, \(w \in S_n\) just permutes the terms in the tensor product.
        
        These two actions are compatible, in a sense they \enquote{commute}, since it doesn't matter if we act with \(g \in \generalLinear(V)\) on \(v_{i_k}\) then rearrange the order of the factors, or if we rearrange the order of the factors then act with \(g\).
        
        The result is that \(V^{\otimes n}\) is a \((\generalLinear(V), S_n)\)-bimodule.
    \end{exm}
    
    \section{Orthogonality of Characters}
    For this section we will work over \(\field = \complex\).
    
    \begin{lma}{}{}
        Let \(G\) be a finite group.
        Then we may define a bilinear form
        \begin{equation}
            \innerprod{-}{-} \colon \classFunctions(G) \times \classFunctions(G) \to \complex
        \end{equation}
        by
        \begin{equation}
            \innerprod{\psi}{\varphi} \coloneqq \frac{1}{\abs{G}} \sum_{g \in G} \psi(g) \overline{\varphi(g)}.
        \end{equation}
        This gives a well-defined Hermitian inner product on \(\classFunctions(G)\).
        \begin{proof}
            Linearity in the first argument and conjugate linearity in the second follow because we defined the inner product as a sum over \(\psi\) and \(\overline{\varphi}\).
            Conjugate symmetry is clear from the definition.
            This is positive definite, for \(\psi \ne 0\) we have
            \begin{equation}
                \innerprod{\psi}{\psi} = \frac{1}{\abs{G}} \sum_{g \in G} \psi(g)\overline{\psi(g)} = \frac{1}{\abs{G}} \sum_{g \in G} \abs{\psi(g)}^2
            \end{equation}
            which is clearly a sum of non-negative terms and so is positive, since at least one term must be nonzero as \(\psi \ne 0\).
        \end{proof}
    \end{lma}
    
    \begin{thm}{}{}
        Let \(V\) and \(W\) be \(G\)-modules, then
        \begin{equation}
            \innerprod{\chi_V}{\chi_W} = \dim(\Hom_G(V, W)).
        \end{equation}
        In particular, if \(V\) and \(W\) are irreducible then
        \begin{equation}
            \innerprod{\chi_V}{\chi_W} =
            \begin{cases}
                1 & V \isomorphic W,\\
                0 & \text{otherwise}.
            \end{cases}
        \end{equation}
        \begin{proof}
            By definition we have
            \begin{align}
                \innerprod{\chi_V}{\chi_W} &= \frac{1}{\abs{G}} \sum_{g \in G} \chi_V(g) \overline{\chi_W(g)}\\
                &= \frac{1}{\abs{G}} \sum_{g \in G} \chi_V(g) \chi_{W^*}(g)\\
                &= \frac{1}{\abs{G}} \sum_{g \in G} \chi_{V \otimes W^*}(g)\\
                &= \frac{1}{\abs{G}} \sum_{g \in G} \tr_{V \otimes W^*}(\rho(g))\\
                &= \tr_{V \otimes W^*}\bigg( \frac{1}{\abs{G}} \sum_{g \in G} \rho(g) \bigg).
            \end{align}
            Now, we can identify that
            \begin{equation}
                P = \frac{1}{\abs{G}} \sum_{g \in G} g \in Z(\complex G).
            \end{equation}
            Thus, what we have above is \(\tr_{V \otimes W^*}(\rho(P))\).
            
            If \(X \in \Irr(G)\) then
            \begin{equation}
                P|_X =
                \begin{cases}
                    \id_X & X \isomorphic \complex,\\
                    0 & \text{otherwise}.
                \end{cases}
            \end{equation}
            Thus, for any representation, \(X\), \(P|_X\) is projection onto \(X^G\), the subspace fixed by the action of \(G\).
            Hence,
            \begin{align}
                \tr_{V \otimes W^*}(\rho(P)) &= \dim(\Hom_G(\complex, V \otimes W^*))\\
                &= \dim(V \otimes W^*)^G\\
                &= \dim \Hom_G(V, W)
            \end{align}
            having used the fact that \(V \otimes W^* \isomorphic \Hom_{\complex}(V, W)\) and \(\Hom_{\complex}(V, W)^G \isomorphic \Hom_G(V, W)\).
        \end{proof}
    \end{thm}
    
    \begin{crl}{}{}
        A \(G\)-module, \(V\), is simple if and only if \(\innerprod{\chi_V}{\chi_V} = 1\).
    \end{crl}
    
    \begin{thm}{}{thm:second orthogonality relation}
        Let \(g, h \in G\), then
        \begin{equation}
            \sum_{X \in \Irr(G)} \chi_X(g) \overline{\chi_X(h)} = 
            \begin{cases}
                \abs{Z_g} & g \text{ conjugate to }h,\\
                0 & \text{otherwise},
            \end{cases}
        \end{equation}
        where \(Z_g = \{h \in G \mid gh = hg\}\) is the centraliser of \(g\) in \(G\).
        \begin{proof}
            We start with the following calculation:
            \begin{align}
                \sum_{X \in \Irr(G)} \chi_X(g) \overline{\chi_X(h)} &= \sum_{X \in \Irr(G)} \chi_X(g) \chi_{X^*}(h)\\
                &= \sum_{X \in \Irr(G)} \tr_X(\rho_X(g)) \tr_{X^*}(\rho_{X^*}(h))\\
                &= \tr_{\bigoplus_{X \in \Irr(G)} X \otimes X^*} (\rho_X(g) \otimes \rho_{X^*}(h))\\
                &= \tr_{\bigoplus_{X \in \Irr(G)} X \otimes X^*} (\rho_X(g) \otimes \rho_X(h^{-1}))\\
                &= \tr_{\bigoplus_{X \in \Irr(G)} \End X} (x \mapsto \rho(g)x\rho(h^{-1}))\\
                &= \tr_{\complex G} (y \mapsto gyh^{-1}).
            \end{align}
            Here we've used the fact that \(X \otimes X^* \isomorphic \End X\), with the isomorphism given by \(A \otimes B \mapsto (x \mapsto AxB^*)\).
            We've then used the fact that
            \begin{equation}
                \complex G \isomorphic \bigoplus_{X \in \Irr(G)} \End X,
            \end{equation}
            since \(\complex G\) is semisimple.
            
            We now consider cases, the first being when \(g\) and \(h\) are not conjugate.
            Suppose that \(g_i\) generate \(G\).
            Then \(gg_ih^{-1} \ne g_i\).
            Thus, the map \(y \mapsto gyh^{-1}\), viewed as a matrix, has no on-diagonal elements, and so has vanishing trace.
            
            If instead \(g\) and \(h\) are conjugate then using the fact that characters are class functions and applying the same logic as above we have
            \begin{align}
                \sum_{X \in \Irr(G)} \chi_X(g) \overline{\chi_X(h)} &= \sum_{X \in \Irr(G)} \chi_X(g) \overline{\chi_X(g)}\\
                &= \tr_{\complex G}(y \mapsto gyg^{-1}).
            \end{align}
            Further, viewing \(y \mapsto gyg^{-1}\) as a matrix we can see that the \((y, y)\) component on the diagonal is \(1\) precisely if \(yg = gy\), and \(0\) otherwise.
            That is, there are precisely as many \(1\)s on the diagonal as elements of \(Z_g\), and so \(\tr_{\complex G}(y \mapsto gyg^{-1}) = \abs{Z_g}\).
        \end{proof}
    \end{thm}
    
    \subsection{Unitary Representations}
    \begin{dfn}{Unitary Representation}{}
        Let \(G\) be a group and consider a complex vector space, \(V\), equipped with an inner product, \(\innerprod{-}{-}\).
        We say that the representation \(\rho \colon G \to \generalLinear(V)\) is \define{unitary}\index{unitary representation} if \(\rho(g)\) is a unitary operator, that is, if
        \begin{equation}
            \innerprod{\rho(g)v}{\rho(g)w} = \innerprod{v}{w}
        \end{equation}
        for all \(g \in G\) and \(v, w \in V\).
        
        Alternatively, a \define{unitary representation} of \(G\) is a homomorphism \(\rho \colon G \to \unitary(V) \subseteq \generalLinear(V)\) where
        \begin{equation}
            \unitary(V) = \{\varphi \in \generalLinear(V) \mid \innerprod{\varphi(v)}{\varphi(u)} = \innerprod{v}{u}\}
        \end{equation}
        is the \defineindex{unitary group}.
    \end{dfn}
    
    Unitary representations are particularly important in quantum mechanics.
    The idea is that \(V\) is a state space, that is \(V\) is the space of possible wave functions, \(\psi\) (or \(\ket{\psi}\)).
    As is standard we restrict to normalised wavefunctions
    To each quantity we may want to measure we associate some element of \(V^*\), which we write as \(\bra{\varphi}\) if the corresponding element of \(V\) is \(\ket{\varphi}\) (note that there is a canonical isomorphism \(V \isomorphic V^*\) because we have the inner product (Riesz representation theorem)).
    Then the probability of being measured to be in the state \(\ket{\varphi}\) when in the state \(\ket{\psi}\) is \(\braket{\varphi}{\psi} = \innerprod{\varphi}{\psi}\).
    
    A unitary representation, \(\rho \colon G \to \unitary(V)\), is then interpreted as a symmetry of our system, since the probabilities that we measure are unaffected by this action.
    
    Consider a complex vector space, \(V\).
    Note that \(V \otimes V\) inherits the inner product \(\innerprod{u_1 \otimes v_1}{u_2 \otimes v_2}_{V \otimes V} = \innerprod{u_1}{v_1}_V \innerprod{u_2}{v_2}_V\).
    Without further knowledge of \(V\) there are two unitary representations of \(S_2\) on \(V \otimes V\), they are \(u \otimes v \mapsto v \otimes u\) and \(u \otimes v \mapsto -v \otimes u\).
    
    The physical interpretation of this is that if \(V\) is the state space of a single particle then \(V \otimes V\) is the state space of two identical particles.
    The two options for \(S_2\) actions then correspond to the two fundamental types of particles.
    If \(u \otimes v \mapsto v \otimes u\) we call the particles \define{bosons}\index{boson}, and if \(u \otimes v \mapsto -v \otimes u\) we call the particles \define{fermions}\index{fermion}.
    
    It turns out that if we're given a finite dimensional complex representation, \(\rho \colon G \to \generalLinear(V)\), of a \emph{finite} group we can always construct a new inner product on \(V\) such that this is a unitary representation.
    
    \begin{thm}{}{}
        Let \(G\) be a finite group and \(V\) a complex finite-dimensional inner product space with inner product \(\innerprod{-}{-}\).
        Let \(\rho \colon G \to \generalLinear(V)\) be a representation of \(G\).
        Then there exists an inner product, \((-,-)\), on \(V\) with respect to which \(\rho\) gives a unitary representation.
        \begin{proof}
            We define an inner product on \(V\) by
            \begin{equation}
                (u, v) = \sum_{g \in G} \innerprod{\rho(g)u}{\rho(g)v}.
            \end{equation}
            That this is linear follows from the fact that the action of \(G\) is linear and \(\innerprod{-}{-}\) is linear.
            The fact that this is positive definite follows because each term in the sum is nonnegative, and for \(u \ne v\) we must have \(\rho(g)u \ne \rho(g)v\) since \(\rho(g)\) is invertible, and thus \(\innerprod{\rho(g)u}{\rho(g)v} \ne 0\) for \(u \ne v\).
            
            That this new inner product is invariant under the action of \(G\) follows from a simple calculation:
            \begin{align}
                (\rho(g)u, \rho(g)v) &= \sum_{h \in G} \innerprod{\rho(h)\rho(g)u}{\rho(h)\rho(g)v}\\
                &= \sum_{h \in G} \innerprod{\rho(hg)u}{\rho(hg)v}\\
                &= \sum_{k \in G} \innerprod{\rho(k)u}{\rho(k)v}\\
                &= (u, v),
            \end{align}
            where we've reindexed the sum with \(k = hg\).
        \end{proof}
    \end{thm}
    
    Another nice property of unitary representations is that since they respect the inner product we get all of the structure of vector spaces that comes with it, including the splitting of short exact sequences, which is just a fancy way of saying that given a vector space, \(V\), with subspace \(W \subseteq V\) we always have the orthogonal complement, \(W' = \{w' \in V \mid \innerprod{w}{w'} = 0 \forall w \in W\}\), which is such that \(V \isomorphic W \oplus W'\).
    
    \begin{thm}{}{}
        Any finite dimensional unitary representation of any group is completely reducible.
        \begin{proof}
            Let \(V\) be a finite dimensional unitary representation of a group, \(G\).
            If \(V\) is irreducible we are done.
            Else, let \(W \subseteq V\) be a subrepresentation.
            Then \(W' = \{w' \in V \mid \innerprod{w}{w'} = 0\}\) is a subrepresentation also since if \(w' \in W'\) then \(\rho(g)w' \in W'\) because for any \(w \in W'\) we have \(\innerprod{w}{\rho(g)w'} = \innerprod{\rho(g)\tilde{w}}{\rho(g)w'} = \innerprod{\tilde{w}}{w'} = 0\) where \(\tilde{w} = \rho(g)^{-1}w\) is an element of \(W\) because \(W\) is closed under the action of \(g^{-1}\).
            Thus, \(W\) and \(W'\) are subrepresentations, and as vector spaces we know that \(V \isomorphic W \oplus W'\).
            
            If either of \(W\) or \(W'\) is not irreducible we may iterate this process.
            Eventually this process will terminate as at each iteration the dimensions of the new spaces are lower than the dimension of the original space, and we started with a finite dimensional space.
        \end{proof}
    \end{thm}
    
    \chapter{Applications of Characters}
    \section{Computing Tensor Products}
    Suppose we have simple \(G\)-modules, \(V\) and \(W\).
    Then the tensor product \(V \otimes W\) is again a \(G\)-module with the action \(g \action (v \otimes w) = (g \action v) \otimes (g \action w)\).
    Assuming that \(\field G\) is semisimple (so \(\Char \field\) and \(\abs{G}\) are coprime) we can decompose \(V \otimes W\) as a direct sum of simple \(G\)-modules:
    \begin{equation}
        V \otimes W = \bigoplus_{U \in \Irr(G)} N^U_{VW} U.
    \end{equation}
    Here the coefficients, \(N^U_{VW}\), are just the multiplicities of \(U\) in this decomposition.
    These are nonnegative integer values.
    
    We can compute the coefficients, \(N^U_{VW}\), using characters.
    First, note that the character of \(V \otimes W\) is \(\chi_{V \otimes W} = \chi_V \chi_W\) and using the above decomposition we have
    \begin{equation}
        \chi_{V \otimes W} = \sum_{U \in \Irr(G)} N^U_{VW} \chi_U.
    \end{equation}
    Taking inner products on both sides and using the orthogonality of irreducible characters we have
    \begin{align}
        \innerprod{\chi_{V \otimes W}}{\chi_U} &= \innerprod*{\sum_{U' \in \Irr(G)} N^{U'}_{VW}\chi_{U'}}{\chi_U}\\
        &= \sum_{U' \in \Irr(G)} N^{U'}_{VW} \innerprod{\chi_{U'}}{\chi_U}\\
        &= \sum_{U' \in \Irr(G)} N^{U'}_{VW} \delta_{U'U}\\
        &= N^U_{VW}.
    \end{align}
    Here \(\delta_{U'U} = 0\) if \(U' \ncong U\) and \(\delta_{U'U} = 1\) if \(U' \isomorphic U\) as \(G\)-modules.
    So, by computing characters we can completely determine the decomposition of \(V \otimes W\) into irreducibles, and since this decomposition is unique (up to order and isomorphism) we have completely determined \(V \otimes W\).
    
    \section{Frobenius--Schur Indicator}
    \subsection{Bilinear Forms and Dual Spaces}
    Suppose \(V\) is a finite dimensional vector space over \(\field\).
    Then we know that \(V \isomorphic V^*\), but there is no canonical choice of isomorphism.
    If we fix some isomorphism \(\delta \colon V \to V^*\) then we can define a nondegenerate bilinear form \(\innerprod{-}{-}_\delta \colon V \times V \to \field\) by
    \begin{equation}
        \innerprod{u}{v}_\delta = \delta(u)(v). 
    \end{equation}
    Conversely, if we have a nondegenerate bilinear form \(\innerprod{-}{-} \colon V \times V \to \field\) then we may define an isomorphism \(\varphi \colon V \to V^*\) by \(u \mapsto \varphi_u\) where \(\varphi_u(v) = \innerprod{u}{v}\).
    
    However, this doesn't \emph{quite} determine a \emph{unique} isomorphism, because we made the arbitrary choice to define \(\varphi_u(v)\) to be \(\innerprod{u}{v}\), rather than \(\innerprod{v}{u}\).
    To fix this we can just assume that \(\innerprod{-}{-}\) is not just a bilinear form, but either a symmetric or antisymmetric bilinear form.
    Then \(\varphi\) is uniquely determined for symmetry, or determined up to a sign for antisymmetry.
    We can always construct a symmetric bilinear form by symmetrising, if \((-,-)\) has no specific symmetry then \(\innerprod{u}{v} = [(u, v) \pm (v, u)]/2\) is symmetric for \(+\) and antisymmetric for \(-\).
    
    This analysis also carries over from the theory of vector spaces to a \(G\)-module, \(M\).
    The dual, \(M^*\), is a \(G\)-module with the action defined by \(g \action f(v) = f(g^{-1} \action v)\).
    The only subtlety being that to get a left action we use \(g^{-1}\) in the action.
    The only change we need to make is that the nondegenerate (anti)symmetric bilinear form needs to be invariant under the action of \(G\).
    That is, we should have \(\innerprod{g \action u}{g \action v} = \innerprod{u}{v}\) for all \(u, v \in M\).
    For example, if \(M\) is equipped with an inner product then \(G\) should act unitarily on \(M\).
    Thus, if \(\innerprod{-}{-}\) is a symmetric \(G\)-invariant bilinear form on \(M\) then we may define an isomorphism \(\varphi \colon M \to M^*\) by \(u \mapsto \varphi_u\) where \(\varphi_u(v) = \innerprod{u}{v}\).
    This is an isomorphism of vector spaces, and it's an isomorphism of \(G\)-modules because
    \begin{equation}
        \varphi(g \action u)(v) = \varphi_{g \action u}(v) = \innerprod{g \action u}{v}
    \end{equation}
    and
    \begin{equation}
        (g \action \varphi(u))(v) = (g \action \varphi_u)(v) = \varphi_u(g^{-1} \action v) = \innerprod{u}{g^{-1} \action v}.
    \end{equation}
    These are equal, to see this simply act on the arguments of the first with \(g^{-1}\), which doesn't change anything as \(\innerprod{-}{-}\) is \(G\)-invariant, and we get
    \begin{equation}
        \innerprod{g \action u}{v} = \innerprod{g^{-1} \action (g \action u)}{g^{-1} \action v} = \innerprod{g^{-1}g \action u}{g^{-1} \action v} = \innerprod{u}{g^{-1} \action v}.
    \end{equation}
    
    The question then becomes when does a given \(G\)-module, \(M\), admit such a nondegenerate (anti)symmetric invariant bilinear form?
    There are three possibilities, which we classify as follows.
    
    \begin{dfn}{}{}
        Let \(G\) be a finite group and \(M\) a \(G\)-module.
        We say that \(M\) is of
        \begin{enumerate}
            \item[(\(-1\))] \defineindex{complex type} if \(M^* \ncong M\) as \(G\)-modules;
            \item[(\(0\))] \defineindex{real type} if \(M\) admits a nondegenerate symmetric invariant bilinear form;
            \item[(\(1\))] \defineindex{quaternionic type} if \(M\) admits a nondegenerate antisymmetric invariant bilinear form.
        \end{enumerate}
    \end{dfn}
    
    This naming convention comes from considering \(\End_{\reals G}M\), for a simple \(G\)-module, \(M\), over \(\reals\).
    This is the space of linear maps \(M \to M\) which commute with the action of \emph{real} linear combinations of group elements.
    It turns out that \(\End_{\reals G}M\) is isomorphic to one of \(\reals\), \(\complex\), or \(\quaternions\), precisely when \(M\) is of real, complex, or quaternionic type.
    
    If instead we consider \(M\) to be a simple \(G\)-module over \(\Mat_{2 \times 2}(\complex)\) then \(\End_{\complex G}M\) is isomorphic to one of \(\complex\), \(\complex \times \complex\), or \(\complex\) when \(M\) is of real, complex, or quaternionic type.
    Note that these endomorphism rings over \(\complex\) are the result of applying the extension of scalars functor, \({-} \otimes_{\reals} \complex\), to the endomorphism rings over \(\reals\).
    
    \subsection{The Frobenius--Schur Indicator}
    \begin{dfn}{Frobenius--Schur Indicator}{}
        Let \(G\) be a finite group and \(M\) a simple \(G\)-module.
        The \defineindex{Frobenius--Schur indicator} is defined to be
        \begin{equation}
            \frobeniusSchur(M) \coloneq \frac{1}{\abs{G}} \sum_{g \in G} \chi_M(g^2)
        \end{equation}
        where \(\chi_M\) is the character of \(M\).
    \end{dfn}
    
    \begin{thm}{Frobenius--Schur}{thm:frobenius schur}
        Let \(G\) be a finite group.
        Then the number of involutions in \(G\), that is, the number of elements of order at most \(2\), is precisely
        \begin{equation}
            \sum_{M \in \Irr(G)} \dim(M) \frobeniusSchur(M).
        \end{equation}
        \begin{proof}
            Consider some representation, \(M\), and some \(A \in \End_{\complex G}M\).
            Let \(\lambda_1, \dotsc, \lambda_n\) be the eigenvaluees of \(A\).
            We consider \(S^2M\) and \(\Lambda^2M\).
            These spaces are both formed as quotients of \(M \otimes M\) by the ideal generated by \(v \otimes w \pm w \otimes v\).
            Since \(A\) acts on \(M \otimes M\) as \(A \otimes A\) and this action factors through the quotient \(A \otimes A\) acts on both of these spaces.
            We have that
            \begin{equation}
                \tr_{S^2 M}(A \otimes A) = \sum_{1 \le i \le j \le n} \lambda_i \lambda_j.
            \end{equation}
            This holds for diagonal matrices when \(\otimes\) is the Kronecker product, which is defined by \(A \otimes B = (a_{ij}B)\) and so for a diagonal matrix the diagonal is just all products \(\lambda_i \lambda_j\).
            Since the trace is invariant under a basis change this result must also hold for diagonalisable matrices.
            Finally, it holds for all matrices by continuity because the diagonalisable matrices are dense in all matrices.
            Similarly, we have
            \begin{equation}
                \tr_{\Lambda^2 M}(A \otimes A) = \sum_{1 \le i < j \le n} \lambda_i \lambda_j,
            \end{equation}
            which again, clearly holds for diagonal matrices with the antisymmetrised Kronecker product, since there \(\lambda_i^2 = 0\).
            Thus, we have
            \begin{equation}
                \tr_{S^2M} (A \otimes A) - \tr_{\Lambda^2 M}(A \otimes A) = \sum_{1 \le i \le n} \lambda_i^2 = \tr_{M} A^2.
            \end{equation}
            
            Thus, for \(g \in G\), we can take \(A\) to be the corresponding action of \(g\) and we get
            \begin{equation}
                \chi_M(g^2) = \chi_{S^2M}(g) - \chi_{\Lambda^2M}(g).
            \end{equation}
            Note that \(g\) is \emph{not} squared on the right because by definition of \(S^2M\) and \(\Lambda^2M\) \(g\) acts as \(g \otimes g\) does on \(M \otimes M\), so the squaring is automatic in the definition of the action.
            
            Then summing this result over \(G\) and dividing by \(\abs{G}\) we get
            \begin{equation}
                \frac{1}{\abs{G}} \sum_{g \in G} \chi_M(g^2) = \frac{1}{\abs{G}} \sum_{g \in G} \chi_{S^2M}(g) - \frac{1}{\abs{G}} \sum_{g \in G} \chi_{\Lambda^2 M}(g).
            \end{equation}
            The left hand side is exactly \(\frobeniusSchur(M)\).
            We have the following vector space decomposition into symmetric and antisymmetric parts:
            \begin{equation}
                M \otimes M \isomorphic S^2M \oplus \Lambda^2M.
            \end{equation}
            In the finite-dimensional case we also have
            \begin{equation}
                M \otimes M \isomorphic M \otimes M^* \isomorphic \End_{\complex} M.
            \end{equation}
            Thus, we have
            \begin{equation}
                S^2M \oplus \Lambda^2M \isomorphic \End_{\complex} M
            \end{equation}
            as vector spaces.
            Denote by \(X^G\) the fixed points of the action of \(G\) on \(X\), that is, \(X^G = \{x \in X \mid g \action x = x\}\).
            This clearly distributes over direct sums, and we have
            \begin{equation}
                (S^2M)^G \oplus (\Lambda^2M)^G \isomorphic (\End_{\complex} M)^G = \End_{\complex G} M
            \end{equation}
            where we have identified in the last equality that an endomorphism is fixed under the action of \(G\) precisely if it commutes with the action of \(G\).
            Taking dimensions we have
            \begin{equation}
                \dim(S^2M)^G + \dim (\Lambda^2 M)^G = \dim (\End_{\complex G}M).
            \end{equation}
            Since \(M\) is simple we know that any \(G\)-module endomorphism of \(M\) is just scalar multiplication, and thus \(\dim(\End_{\complex G} M) \le 1\).
            Since dimensions are integers this leaves us with just two options on the right, either both dimensions are \(0\), or one is \(0\) and the other is \(1\).
            Thus, 
            \begin{equation}
                \dim(S^2M)^G - \dim(\Lambda^2M)^G \in \{-1, 0, 1\}.
            \end{equation}
            Note that the above quantity is the correct way to generalise the Frobenius--Schur indicator to fields other than \(\complex\).
            
            Let \(I\) be the number of involutions of \(G\).
            Then
            \begin{equation}
                I = \sum_{g \in G} [g^2 = 1]
            \end{equation}
            where \([\varphi]\) is the Iverson bracket, \([\varphi] = 1\) if \(\varphi\) is true, and \([\varphi] = 0\) if \(\varphi\) is false.
            The second orthogonality relation (\cref{thm:second orthogonality relation}) tells us that
            \begin{equation}
                [g^2 = 1] = \frac{1}{\abs{G}} \sum_{M \in \Irr(G)} \chi_M(g^2) \overline{\chi_M(1)},
            \end{equation}
            since this result should vanish if \(g^2\) is not conjugate to \(1\) and should be \(\abs{Z_g}\) otherwise.
            Then we note that \(g^2\) is conjugate to the identity if and only if \(g^2\) \emph{is} the identity.
            Further, \(\abs{Z_g} = \abs{G}\) if \(g^2 = 1\).
            Thus, we have that
            \begin{equation}
                I = \frac{1}{\abs{G}}\sum_{g \in G} \sum_{M \in \Irr(G)} \chi_M(g^2) \overline{\chi_M(1)}.
            \end{equation}
            Since \(\chi_M(1) = \dim M\) this simplifies to
            \begin{equation}
                I = \frac{1}{\abs{G}} \sum_{g \in G} \sum_{M \in \Irr(G)} \dim(M) \chi_V(g^2) = \sum_{M \in \Irr(G)} \dim(M) \frobeniusSchur(M).
            \end{equation}
        \end{proof}
    \end{thm}
    
    The proof of the following result is some fairly involved linear algebra, but essentially comes down to the universal property of the tensor/symmetric/exterior product giving a correspondence between bilinear forms and linear maps, and the bilinear forms inherit the (anti)symmetry of the symmetric/exterior product.
    
    \begin{prp}{}{}
        Let \(G\) be a finite group, and \(M\) a simple \(G\)-module.
        Then \(\frobeniusSchur(M)\) is \(-1\) if \(M\) is of complex type, \(0\) if \(M\) is of real type, and \(1\) if \(M\) is of quaternionic type.
    \end{prp}
    
    \begin{exm}{}{}
        This example assumes some knowledge about the basics of representations of \(S_n\), a topic we have yet to cover, so maybe come back later if you're not familiar with these ideas.
        
        It is a fact that \(\frobeniusSchur(M) = 1\) for any simple \(S_n\)-module, that is, all \(S_n\)-modules are of real type.
        Simple \(S_n\)-modules are indexed by standard tableaux of shape \(\lambda\) with \(\lambda\) a partition of \(n\).
        Thus, the number of involutions in \(S_n\) is precisely
        \begin{equation}
            \sum_{\lambda \partition n} \abs{\standardYoungTableaux(\lambda)}
        \end{equation}
        where \(\standardYoungTableaux(\lambda)\) is the set of standard Young tableau of shape \(\lambda\).
    \end{exm}
    
    \section{Burnside's Theorem}
    \subsection{Statement of Theorem}
    The next example of an application of character theory is Burnside's theorem, a result in number theory.
    While Burnside's theorem is relatively easy to state its proof requires some number theory.
    The result is famous for being one of the first results in group theory which was first proven through representation theory.
    
    Before we state the theorem recall the following definition from group theory.
    \begin{dfn}{Solvable Group}{}
        A group, \(G\), is \define{solvable}\index{solvable!group} if there exists a series of nested normal subgroups
        \begin{equation}
            \{1\} = G_1 \normalsub G_2 \normalsub \dotsb \normalsub G_n = G
        \end{equation}
        such that \(G_{i+1}/G_i\) is abelian.
    \end{dfn}
    
    \begin{thm*}{Burnside's Theorem}{}
        Any group, \(G\), of order \(p^aq^b\) with \(p\) and \(q\) primes and \(a, b \in \integers_{\ge 0}\) is solvable.
    \end{thm*}
    
    \subsection{Algebraic Integers}
    \begin{dfn}{Algebraic Integers}{}
        A complex number, \(z \in \complex\), is an
        \begin{itemize}
            \item \define{algebraic number}\index{algebraic!number} if it is a root of some polynomial in \(\rationals[x]\);
            \item \define{algebraic integer}\index{algebraic!integer} if it  is a root of some \emph{monic}\footnote{Recall that a polynomial is \defineindex{monic} if the coefficient of the highest degree term is \(1\).} polynomial in \(\integers[x]\).
        \end{itemize}
        We write \(\algNumbers\) and \(\algIntegers\) for the sets of algebraic numbers and integers respectively.
    \end{dfn}
    
    \begin{exm}{}{}
        \begin{itemize}
            \item \(\integers \subseteq \algIntegers\): \(n \in \integers\) is a root of \(x - n\).
            \item \(\rationals \cap \algIntegers = \integers\): Suppose \(a/b\) is rational and reduced, then any rational polynomial with \(a/b\) as a root has a factor of \(x - a/b\), to get an integer polynomial we have to scale this to \(bx - a\).
            Thus, any integer polynomial with \(a/b\) as a root has a factor of \(b x - a\), which means it cannot be monic, since any monic polynomial factors as \((x - \alpha_1) \dotsm (x - \alpha_m)\) for some roots \(\alpha_i \in \complex\).
            Thus, \(a/b\) is an algebraic integer only if \(b = 1\), in which case \(a/b = a\) is an integer.
        \end{itemize}
    \end{exm}
    
    \begin{lma}{}{}
        \(z \in \complex\) is an algebraic number (integer) if and only if it is an eigenvalue of some \(n \times n\) matrix over \(\rationals\) (\(\integers\)).
        \begin{proof}
            If \(z\) is an algebraic number (integer) then it is a root of the monic polynomial
            \begin{equation}
                p(x) = x^n + a_{n-1}x^{n-1} + \dotsb + a_1x + a_0
            \end{equation}
            where \(a_i \in \rationals\) (\(a_i \in \integers\)).
            Note that we are always free to rescale a rational polynomial to be monic.
            Let
            \begin{equation}
                A = 
                \begin{pmatrix}
                    0 & 0 & \dots & 0 & -a_0\\
                    1 & 0 & \dots & 0 & -a_1\\
                    0 & 1 & \dots & 0 &  -a_2\\
                    \vdots & \vdots & \ddots & \vdots & \vdots\\
                    0 & 0 & \dots & 1 & -a_{n-1}
                \end{pmatrix}
                .
            \end{equation}
            Then the characteristic polynomial of \(A\) is
            \begin{equation}
                -\det(A - x I) = p(x),
            \end{equation}
            and thus \(z\) is an eigenvalue of \(A\).
            
            Conversely, suppose that \(z\) is an eigenvalue of some \(n \times n\) rational (integer) matrix, \(A\).
            Then \(z\) is a root of the characteristic polynomial of \(A\).
            The characteristic polynomial of a matrix over \(\rationals\) (\(\integers\)) is always monic over \(\rationals\) (\(\integers\)), and thus \(z\) is an algebraic number (integer).
        \end{proof}
    \end{lma}
    
    \begin{prp}{}{}
        \begin{itemize}
            \item \(\algIntegers\) is a ring\footnote{Fun Fact\textsuperscript{TM}: The first use of the word \enquote{ring} is attributed to Hilbert, who used it describe \(\algIntegers\), and in particular the way higher powers \enquote{loop back around} to be described in terms of lower powers, which can always be done for elements of \(\algIntegers\) using the polynomial they satisfy to replace higher powers with lower ones.}; and
            \item \(\algNumbers\) is a field.
        \end{itemize}
        \begin{proof}
            \Step{\(\algNumbers\) and \(\algIntegers\) are Rings}
            We will prove that \(\algNumbers\) is a ring, the proof for \(\algIntegers\) is analogous.
            Take \(\alpha, \beta \in \algNumbers\), then there are matrices \(A \in \Mat_m(\rationals)\) and \(B \in \Mat_n(\rationals)\) such that \(\alpha\) and \(\beta\) are eigenvalues of \(A\) and \(B\) respectively.
            Let \(v \in \complex^m\) and \(w \in \complex^n\) be the corresponding eigenvectors.
            Consider \(A \otimes \id_{\complex^n} \pm \id_{\complex^{m}} \otimes B\).
            A calculation shows that \(v \otimes w\) is an eigenvector of this matrix with eigenvalue \(\alpha \pm \beta\):
            \begin{align}
                (A \otimes \id_{\complex^n} &\pm \id_{\complex^{m}} \otimes B)(v \otimes w)\\
                &= (A \otimes \id_{\complex^n})(v \otimes w) \pm (\id_{\complex^m} \otimes B)(v \otimes w)\\
                &= Av \otimes w \pm v \otimes Bw\\
                &= \alpha v \otimes w \pm v \otimes \beta w\\
                &= (\alpha \pm \beta) (v \otimes w).
            \end{align}
            Thus, \(\alpha \pm \beta\) is an eigenvalue of some \((m + n) \times (m + n)\) matrix over \(\rationals\), and hence \(\alpha \pm \beta \in \algNumbers\).
            
            Similarly, \(\alpha\beta\) is an eigenvalue of \(A \otimes B\) with eigenvector \(v \otimes w\):
            \begin{equation}
                (A \otimes B)(v \otimes w) = Av \otimes Bw = \alpha v \otimes \beta w = (\alpha\beta)(v \otimes w).
            \end{equation}
            Thus, \(\alpha\beta\) is an eigenvalue of some \(mn \times mn\) matrix over \(\rationals\), and so \(\alpha\beta \in \algNumbers\).
            
            These results, along with the inherited distributivity law from \(\complex\), prove that \(\algNumbers\) is a ring.
            
            \Step{\(\algNumbers\) is a Field}
            Suppose that \(\alpha \in \algNumbers \setminus \{0\}\).
            Then there exits some matrix, \(A \in \Mat_{m \times m}(\rationals)\) such that \(\alpha\) is a root of \(p(x) = \det(A - xI)\).
            We can multiply this whole equation by \(\alpha^m\) and it follows from properties of determinants that \(\alpha^m p(x) = \det(\alpha A - \alpha x I)\).
            Then, \(\alpha^m p(1/\alpha) = \det(\alpha A - I)\), which vanishes when \(\alpha A\) has eigenvalue \(1\), and since \(\alpha A\) has the same eigenvalues as \(A\) but multiplied by \(\alpha\) this shows that some eigenvalue, \(\beta\), is such that \(\alpha \beta = 1\), in other words, \(\beta = 1/\alpha\), so \(1/\alpha \in \algNumbers\).
            Thus, \(\algNumbers\) contains multiplicative inverses of nonzero elements, and so is a field  (it is clearly commutative and has no zero divisors as it is a subring of \(\complex\)).
        \end{proof}
    \end{prp}
    
    \subsection{Towards a Proof of Burnside's Theorem}
    Many quantities that arise in representation theory are naturally algebraic integers.
    We will use this to restrict the possible values that certain quantities can take, which will be important in our proof of Burnside's theorem.
    
    \begin{lma}{}{}
        Let \(G\) be a finite group and \(M\) a finite-dimensional \(G\)-module.
        Then \(\chi_M(g)\) is an algebraic integer for every \(g \in G\).
        \begin{proof}
            Since \(G\) is finite each \(g \in G\) has finite order, \(n\), and thus the eigenvalues of \(\rho_M(g)\) are \(n\)th roots of unity, and so in \(\algIntegers\) as they satisfy the monic polynomial \(x^n - \alpha - 1 = 0\).
            The trace is the sum of the eigenvalues, and \(\algIntegers\) is a ring, so is closed under addition, and thus \(\chi_M(g) \in \algIntegers\).
        \end{proof}
    \end{lma}
    
    \begin{prp}{}{prp:sums over conjugacy classes act as an algebraic number}
        Let \(G\) be a finite group and consider the set of conjugacy classes, \(\conjugacyClasses(G) = \{[g_1], \dotsc, [g_n]\}\), with chosen representatives.
        Define
        \begin{equation}
            c_i = \sum_{g \in [g_i]} \in \complex G,
        \end{equation}
        then for any simple \(G\)-module, \(M\), we have \(c_i|_M = \lambda_i \id_M\) where
        \begin{equation}
            \lambda_i = \abs{[g_i]} \frac{\chi_M(g_i)}{\chi_M(1)}
        \end{equation}
        are algebraic integers.
        \begin{proof}
            First note that the \(c_i\) are central in \(\complex G\) since
            \begin{align}
                c_i g = \sum_{g' \in [g_i]} g'g = \sum_{g'' \in [g_i]} gg'' = g \sum_{g'' \in [g_i]} g'' = g c_i
            \end{align}
            where we've reindexed the sum with \(g'' = g^-1 g'g\), which doesn't change the value as we're still summing over the whole conjugacy class, just in a different order.
            
            Thus, by Schur's lemma we know that the \(c_i\) act as a scalar on any simple \(G\)-module.
            Call this scalar \(\lambda_i\).
            Consider the group ring, \(\integers G\).
            This is finitely generated (since \(G\) is a finite generating set).
            Thus, each \(c_i\) must satisfy some monic integer polynomial equation, and this carries through to the scalars, \(\lambda_i\), which shows they are algebraic integers.
            Viewing \(c_i\) as an operator on \(M\) we know that \(c_i = \lambda_i \id_M\), and we can take the trace of this to get
            \begin{equation}
                \tr_M c_i = \tr_M (\lambda_i \id_M) = \lambda_i \dim M = \lambda_i \chi_M(1).
            \end{equation}
            We also have
            \begin{equation}
                \tr_M c_i = \sum_{g \in [g_i]} \tr_M \rho_M(g) = \sum_{g \in [g_i]} \chi_M(g) = \abs{[g_i]} \chi_M(g_i)
            \end{equation}
            since the character is constant on conjugacy classes.
            Equating these we get the desired result.
        \end{proof}
    \end{prp}
    
    \begin{thm}{Frobenius Divisibility}{}
        Let \(G\) be a finite group and \(M\) a simple \(G\)-module over \(\complex\).
        Then \(\dim M\) divides \(\abs{G}\).
        \begin{proof}
            With notation as in the statement of \cref{prp:sums over conjugacy classes act as an algebraic number} we claim that
            \begin{equation}
                \sum_i \lambda_i \overline{\chi_M(g_i)} \in \algIntegers
            \end{equation}
            where the sum is over all conjugacy classes.
            Since \(\algIntegers\) is a ring and \cref{prp:sums over conjugacy classes act as an algebraic number} shows that the \(\lambda_i\) are algebraic integers it is sufficient to show that \(\overline{\chi_M(g_i)}\) are algebraic integers.
            Since \(G\) is finite we know that \(\rho_M(g_i)^{\abs{G}} = \id_M\), and hence \(\chi_M(g_i)\) must be sums of roots of unity, which are algebraic integers, so \(\chi_M(g_i)\) are algebraic integers, and hence \(\overline{\chi_M(g_i)}\) are algebraic integers, as they are roots of the conjugate polynomial.
            
            From \cref{prp:sums over conjugacy classes act as an algebraic number} we also have
            \begin{align}
                \sum_i \lambda_i \overline{\chi_M(g_i)} &= \sum_i \abs{[g_i]} \frac{\chi_M(g_i) \overline{\chi_M(g_i)}}{\chi_M(1)}\\
                &= \sum_{g \in G} \frac{\chi_M(g) \overline{\chi_M(g)}}{\dim M}\\
                &= \frac{\abs{G}}{\dim M} \innerprod{\chi_M}{\chi_M}\\
                &= \frac{\abs{G}}{\dim M}.
            \end{align}
            This shows that this quantity is rational, as clearly \(\abs{G}\) and \(\dim M\) are integers.
            Since the left-hand-side is in \(\algIntegers\) and the right-hand-side is in \(\rationals\) they must actually be in \(\algIntegers \cap \rationals = \integers\), and thus \(\dim M\) divides \(\abs{G}\).
        \end{proof}
    \end{thm}
    
    \begin{lma}{}{lma:sum of roots of unity over n alg int implies all roots same or zero}
        If \(\xi_1, \dotsc, \xi_n\) are roots of unity such that \(a \coloneq (\xi_1 + \dotsb + \xi_n)/n\) is an algebraic integer then either \(\xi_1 = \dotsb = \xi_n\) or \(\xi_1 + \dotsb + \xi_n = 0\).
        \begin{proof}
            If the \(\xi_i\) are not all equal then it follows from the geometry of roots of unity that \(\abs{a} < 1\).
            Suppose that \(p(x)\) is the minimal polynomial with \(a\) as a root, then any other root, \(a'\), of this polynomial must also be a root of unity, and as such \(\abs{a'} \le 1\) also.
            The product of all roots of \(p\) is an integer, and since they all have absolute value at most 1, and \(\abs{a} < 1\) it follows that this integer has absolute value less than \(1\), and so must be \(0\).
            Thus, \(a = 0\), and since \(1/n \ne 0\) we achieve the desired result.
        \end{proof}
    \end{lma}
    
    \begin{thm}{}{thm:gcd conjugacy class order and dimension 1 then acts as scalar}
        Let \(G\) be a finite group and \(M\) a simple \(G\)-module.
        Let \(C \in \conjugacyClasses(G)\) be a conjugacy class such that \(\gcd(\abs{C}, \dim M) = 1\).
        Then either \(\chi_M(g) = 0\) or \(\rho_M(g) = \varepsilon\id_M\) for some \(\varepsilon \in \complex\) for all \(g \in C\).
        \begin{proof}
            Since \(\gcd(\abs{C}, \dim M) = 1\) there exist integers \(a\) and \(b\) such that
            \begin{equation}
                a \abs{C} + b \dim M = 1.
            \end{equation}
            Multiplying by \(\chi_M(g)/\dim M\) we get
            \begin{equation}
                \frac{\abs{C}\chi_M(g)}{\dim M} + b \chi_M(g) = \frac{\chi_M(g)}{\dim M} = \frac{\varepsilon_1 + \dotsb + \varepsilon_n}{n}
            \end{equation}
            where \(\varepsilon_i\) are the eigenvalues of \(\rho_M(g)\) and \(n\) is the dimension of \(M\).
            Then the left-hand-side is an algebraic integer, since \(a\) is an integer, \(\abs{C}\chi_M(g)/\dim M\) is an algebraic integer by \cref{prp:sums over conjugacy classes act as an algebraic number}, \(b\) is an integer, and \(\chi_M(g)\) is an algebraic integer as it is a sum of the eigenvalues of \(\rho_M(g)\) which are roots of unity as \(g\) has finite order as \(G\) is finite.
            Thus, \((\varepsilon_1 + \dotsb + \varepsilon_n)/n\) is an algebraic integer by \cref{lma:sum of roots of unity over n alg int implies all roots same or zero}, so it is either \(0\) or \(\varepsilon_1 = \dotsb = \varepsilon_n = \varepsilon\), in which case \(\rho_M(g) = \varepsilon \id_M\).
        \end{proof}
    \end{thm}
    
    \begin{thm}{}{thm:nontrivial normal subgroup for conjugacy classes of prime power}
        Let \(G\) be a finite group and \(C \in \conjugacyClasses(G)\) a conjugacy class such that \(\abs{C} = p^k\) for \(p\) some prime and \(k \in \integers_{> 0}\).
        Then \(G\) has a proper nontrivial normal subgroup.
        \begin{proof}
            We may always split the set of simple \(G\)-modules as
            \begin{equation}
                \Irr G = \{\complex\} \sqcup D \sqcup N
            \end{equation}
            where \(\complex\) is the trivial representation, and \(D\) and \(N\) are the sets of \enquote{divisible} and \enquote{not divisible} dimension irreducible representations.
            That is,
            \begin{equation}
                D = \{M \in \Irr G \, \mid \, p \mid \dim M\}, \qqand N = \{M \in \Irr G \, \mid \, p \nmid \dim M\}.
            \end{equation}
            
            We claim that there exists some \(M \in N\) such that \(\chi_M(g) \ne 0\).
            To see this first note that if \(M \in D\) then \(p\) divides \(\dim M\), so \((\dim M)/p\) is an integer, and hence an algebraic integer.
            Thus,
            \begin{equation}
                \label{eqn:expression for a in proof of theorem leading up to burnside}
                a = \sum_{M \in D} \frac{1}{p} (\dim M) \chi_M(g)
            \end{equation}
            is an algebraic integer.
            Taking some \(g \in C\) we know that \(g \ne 1\) since \(\abs{C} = p^k \ne 1\) and the identity is always in a conjugacy class on its own.
            Thus, by the second orthogonality relation we know that \cref{thm:second orthogonality relation}
            \begin{equation}
                \sum_{M \in \Irr (G)} \overline{\chi_M(e)} \chi_M(g) = 0
            \end{equation}
            and of course the character of the identity is just the dimension, so this is nothing but
            \begin{equation}
                \sum_{M \in \Irr(G)} (\dim M) \chi_M(g) = 0.
            \end{equation}
            We can rewrite this sum in terms of the decomposition of \(\Irr(G)\) as
            \begin{align}
                0 &= \chi_{\complex}(g) + \sum_{M \in D} (\dim M) \chi_M(g) + \sum_{M \in N} (\dim M) \chi_M(g)\\
                &= 1 + p a + \sum_{M \in N} (\dim M) \chi_M(g).
            \end{align}
            Here we've used the fact that the character of the trivial representation is identically \(1\), as well as \cref{eqn:expression for a in proof of theorem leading up to burnside} to identify \(a\).
            Since \(pa \ne -1\), as \(p\) is a prime and \(a\) an integer, we know that
            \begin{equation}
                \sum_{M \in N} (\dim M)\chi_M(g) \ne 0
            \end{equation}
            and thus there must be some \(M \in N\) such that \(\chi_M(g) \ne 0\).
            
            Now fix \(M \in N\) to be such that \(\chi_M(g) \ne 0\) for \(g \in C\).
            Since \(p \nmid \dim M\) we know that \(\abs{C} = p^k\) doesn't divide \(M\), and since \(p\) is prime \(\dim M\) doesn't divide \(\abs{C}\) either.
            Thus, \(\gcd(\abs{C}, \dim M) = 1\), and since \(\chi_M(g) \ne 0\) we know that \(\rho_M(g) = \varepsilon \id_M\) for some \(\varepsilon\) for all \(g \in C\) by \cref{thm:gcd conjugacy class order and dimension 1 then acts as scalar}.
            
            Now define the subgroup
            \begin{equation}
                H = \langle gh^{-1} \mid g, h \in C \rangle.
            \end{equation}
            This is not equal to \(\{1\}\) as \(\abs{C} > 1\) so there exist distinct \(g\) and \(h\) in \(C\) and \(gh^{-1} \ne 1\) as inverses are unique.
            By construction, \(H\) is normal since conjugation simply permutes the, since for all \(k \in G\) we have
            \begin{equation}
                kgh^{-1}k^{-1} = kgk^{-1}kh^{-1}k^{-1} = \hat{g}\hat{h}^{-1}
            \end{equation}
            for some \(\hat{g}, \hat{h} \in C\) by definition of a conjugacy class.
            
            Further, \(H\) acts trivially on \(M\).
            To see this take \(g, h \in C\), and then we know that \(\rho_M(g) = \varepsilon_g \id_M\) and \(\rho_M(h) = \varepsilon_h \id_M\) for some scalars \(\varepsilon_g, \varepsilon_h \in \complex\).
            Thus, \(\chi_M(g) = \varepsilon_g \dim M\) and \(\chi_M(h) = \varepsilon_h \dim M\), but characters are constant on conjugacy classes, so it must be that \(\varepsilon_g = \varepsilon_h\).
            Thus, \(H\) simply acts by some scalar multiple, \(\varepsilon = \varepsilon_1 = \varepsilon_h\), and we're free to choose \(\varepsilon = 1\), as we know that \(H\) does not act as zero.
            
            Finally, it must be that \(H \subsetneq G\), since if \(G = H\) then \(G\) acts trivially on \(M\), but by definition \(M\) is not the trivial representation.
        \end{proof}
    \end{thm}
    
    \subsection{Proof of Burnside's Theorem}
    Finally, we're ready to put all of these technical results together to prove Burnside's theorem.
    We'll do this in two cases.
    The first is to prove that if the order of \(G\) has a unique prime factor then \(G\) is solvable, then the main result can be prove assuming two distinct prime factors.
    
    \begin{prp}{}{prp:p-groups are solvable}
        Let \(G\) be a group of order \(p^a\) for some prime, \(p\), and \(a \in \integers_{\ge 0}\).
        Then \(G\) is solvable.
        \begin{proof}
            First note that if \(a = 0\) then \(G\) is trivial and is trivially solvable.
            We then induct on \(a\).
            Suppose that the statement is true for all \(a < n\) for some integer \(n\).
            Now take \(\abs{G} = p^n\).
            
            The class equation is a result from group theory which tells us that
            \begin{equation}
                \abs{G} = \abs{Z(G)} + \sum_i [G : Z_{g_i}]
            \end{equation}
            where the sum is over conjugacy classes.
            The order of any conjugacy class of \(G\) must divide \(\abs{G}\), and so it follows that all conjugacy classes have size \(p^{k_i}\) for some \(k_i \in \integers_{\ge 0}\).
            Then we have that \(\abs{G} = p^n = \abs{Z(G)} + \sum_i p^{k_i}\).
            Thus, \(p\) must divide \(\abs{Z(G)}\), and so \(Z(G)\) is nontrivial.
            
            If \(G\) is abelian then \(G\) is solvable.
            If \(G\) is not abelian then \(Z(G)\) is an abelian subgroup, which is solvable, meaning there exist normal subgroups
            \begin{equation}
                \{1\} \normalsub Z_1 \normalsub Z_2 \normalsub \dotsb \normalsub Z_n = Z(G).
            \end{equation}
            Quotients of successive terms are abelian as every group in this chain is abelian.
            Then \(Z(G)\) is normal in \(G\) since everything in \(G\) commutes with everything in \(Z(G)\).
            Further, \(G/Z(G)\) is abelian.
            Thus, we have a chain of normal subgroups,
            \begin{equation}
                \{1\} \normalsub Z_1 \normalsub Z_2 \normalsub \dotsb \normalsub Z_n = Z(G) \normalsub G
            \end{equation}
            such that quotients of successive subgroups are abelian.
            This proves \(G\) is solvable.
        \end{proof}
    \end{prp}
    
    \begin{thm}{Burnside's Theorem}{}
        Any group, \(G\), of order \(p^aq^b\) with \(p\) and \(q\) primes and \(a, b \in \integers_{\ge 0}\) is solvable.
        \begin{proof}
            First, since the trivial group is solvable and \cref{prp:p-groups are solvable} shows that all \(p\)-groups (that is, groups of order \(p^a\)) are solvable we may assume that \(p\) and \(q\) are distinct with \(a, b \ne 0\).
            Finally, if \(G\) is abelian it is solvable, so we may assume that \(G\) is nonabelian, and in particular that \(Z(G) \subsetneq G\).
            
            The proof is by contradiction, so assume \(G\) has order \(p^aq^b\) and isn't solvable.
            Further, suppose that \(G\) is the smallest such \(G\).
            Then \(G\) must be simple, else one of its normal subgroups would have this property.
            
            We then know from \cref{thm:nontrivial normal subgroup for conjugacy classes of prime power} that \(G\) cannot have a conjugacy class, \(C \in \conjugacyClasses(G)\), of order \(p^k\) or \(q^k\) for \(k \ge 1\).
            Thus, all conjugacy classes are either singletons or have order divisible by \(pq\).
            However, we also know that
            \begin{equation}
                p^a q^b = \abs{G} = \sum_{C \in \conjugacyClasses(G)} \abs{C} = 1 + \sum_{C \in \conjugacyClasses(g) \setminus \{1\}} \abs{C}
            \end{equation}
            and the only way this can hold is if there is some \(C \in \conjugacyClasses(G)\) with \(\abs{C} = 1\), as if all conjugacy classes other than \(\{1\}\) have order divisible by \(pq\) then \(1\) plus this sum cannot be divisible by \(pq\).
            Thus, whatever element is in this \(C\) with \(\abs{C} = 1\) must be central.
            Hence, \(G\) has nontrivial centre, and thus has a normal subgroup, the centre of \(G\).
            This is a contradiction of the simplicity, and hence a contradiction of our assumption of non-solvability.
        \end{proof}
    \end{thm}
    
    \chapter{Induced Representations and Frobenius Reciprocity}
    \section{Induced Representations}
    Let \(G\) be a finite group, and \(H\) a subgroup of \(G\).
    Any \(G\)-module, \(M\), may be viewed as an \(H\)-module in the obvious way.
    We just \enquote{forget} the fact that elements in \(G \setminus H\) can act on \(M\) and consider only the action of elements in \(H\).
    We call the resulting module the \defineindex{restriction} of \(M\) to \(H\), since if \(\rho \colon G \to \generalLinear(M)\) is the representation map for \(M\) as a \(G\)-module then the corresponding representation map for \(M\) as an \(H\)-module is \(\rho|_H \colon H \to \generalLinear(V)\).
    
    For example, \(S_3\) acts on \(\complex^3\) by permuting basis vectors, and \(\integers_2 = \{\cycle{}, \cycle{1,2}\} \subset S_3\) acts on \(\complex^3\) by just swapping the first two basis vectors back and forth and leaving the third alone.
    
    More formally, given a \(G\)-module, \(M\), we have a canonical method of producing an \(H\)-module, and we can encode this as a functor
    \begin{equation}
        \Res^G_H \colon \Mod{G} \to \Mod{H}
    \end{equation}
    which sends a \(G\)-module, \(M\), to the \(H\)-module, \(\Res^G_H M\), given by forgetting how elements of \(G \setminus H\) act.
    This functor is the identity on module homomorphisms since the underlying sets of \(M\) and \(\Res^G_HM\) are the same.
    We call this the \defineindex{restriction functor}.
    
    A natural question now is can we go the other direction?
    That is, if we have an \(H\)-module, \(M\), is there a sensible way to construct a \(G\)-module?
    With the more formal statement above we might guess that the reverse process should be adjoint to \(\Res^G_H\).
    The following definition gives us exactly this reverse process.
    
    \begin{dfn}{Induced Module}{}
        Let \(G\) be a finite group and \(H\) a subgroup.
        An \(H\)-module, \(M\), gives a \(G\)-module defined by
        \begin{equation}
            \Ind^G_H M \coloneq \field G \otimes_{\field H} M.
        \end{equation}
    \end{dfn}
    
    The action of \(G\) on \(\Ind^G_H M\) is implicit in the definition of the tensor product, explicitly, it's given on simple tensors by
    \begin{equation}
        g \action (g' \otimes m) = gg' \otimes m.
    \end{equation}
    This all works out because \(\field G\) is a \((\field G, \field H)\)-bimodule (with the right \(\field H\)-module simply being restriction of the right regular representation).
    Thus, the tensor product of \(\field G\) and \(\field H\) is naturally a \(\field G\)-module.
    
    As with restriction we have a functor
    \begin{equation}
        \Ind^G_H \colon \Mod{H} \to \Mod{G}
    \end{equation}
    which sends \(M\) to \(\field G \otimes_{\field H} M\) and an \(H\)-module homomorphism, \(\varphi \colon M \to N\) is sent to a \(G\)-module homomorhpism
    \begin{align}
        \Ind^G_H \varphi \colon \field G \otimes_{\field H} M &\to \field G \otimes_{\field H} N\\
        g \otimes m &\mapsto g \otimes \varphi(n).
    \end{align}
    
    Note that there are several equivalent definitions of \(\Ind^G_H M\) yielding isomorphic, but formally distinct, \(G\)-modules.
    One of these is
    \begin{equation}
        \Ind^G_H M \isomorphic \{f \colon G \to M \mid f(hx) = \rho(h)f(x) \forall x \in G, h \in H\}.
    \end{equation}
    That is, we consider all maps \(G \to M\) which intertwine the regular representation of \(G\) and the action of \(G\) on \(M\).
    Another definition is
    \begin{equation}
        \Ind^G_H M \isomorphic \Hom_{\field H}(\field G, M)
    \end{equation}
    which is really just restating the above.
    With these definitions the action of \(g \in G\) on \(f\) is given by
    \begin{equation}
        (g \action f)(x) = f(xg)
    \end{equation}
    for all \(x \in G\).
    Everything we might want then pretty much follows because \(g \in G\) acts on the right of the function argument and \(h \in H\) acts on the left.
    For example, this is a valid representation since we have
    \begin{equation}
        (g \action f)(hx) = f(hxg) = \rho(h)f(xg) = \rho(h)(g \action f)(x)
    \end{equation}
    so \(g \action f\) is again in \(\Hom_{\field H}(\field G, M)\), and
    \begin{equation}
        (g \action (g' \action f))(x) = (g' \action f)(xg) = f(xgg') = (gg' \action f)(x)
    \end{equation}
    and
    \begin{equation}
        (1 \action f)(x) = f(x1) = f(x)
    \end{equation}
    for all \(g, g', x \in G\).
    
    \begin{exm}{}{}
        Let \(\field\) be the trivial representation in which \(H\) acts as the identity, so the representation map is \(\one \colon H \to \generalLinear(\field) \isomorphic \field\) with \(\one(h) = 1\).
        Then, we have
        \begin{equation}
            \Ind^G_H \field = \field G \otimes_{\field H} \field.
        \end{equation}
        Note that the tensor product is \(\otimes_{\field H}\), not \(\otimes_{\field}\), so \(\field G \otimes_{\field H} \field\) is not isomorphic to \(\field G\).
        
        The module structure is completely determined by elements of the form \(g \otimes 1\).
        In fact, since \(gh \otimes 1 = g \otimes (h \action 1) = g \otimes 1\) the action is invariant under multiplication by elements of \(H\).
        Both \(gh\) and \(gh'\) have the same action for \(g \in G\) and \(h, h' \in H\).
        Thus, the action of \(g \in G\) is determined only by the coset, \(gH\), into which it falls.
        
        The induced module, \(\Ind^G_H \field\) is isomorphic to the coset representation, \(\field G/H\), which is a \(G\)-module constructed as the free vector space on the set of cosets, \(G/H\), with the \(G\)-action given by \(g \action g'H = (gg')H\).
    \end{exm}
    
    \begin{exm}{}{}
        Let \(G\) be a finite group with subgroup \(H\).
        Let \(\chi \colon H \to \field^{\times}\) be a homomorphism, and \(\field_\chi\) the corresponding 1-dimensional representation of \(H\).
        That is, \(h \action \lambda = \chi(h)\lambda\) for all \(h \in H\) and \(\lambda \in \field\).
        
        Consider the induced module \(\Ind^G_H\field_{\chi} = \field G \otimes_{\field H} \field_{\chi}\).
        For \(h \in H\) we have \(h \otimes 1 = 1_G \otimes (h \action 1) = 1_G \otimes \chi(h)\).
        The action of \(g \in G\) on \(\Ind^G_H \field_{\chi}\) is given by \(g \action (g' \otimes 1) = gg' \otimes 1\).
        
        Let
        \begin{equation}
            e_\chi = \frac{1}{\abs{K}} \sum_{h \in H} \chi(h)^{-1} h \in \field H.
        \end{equation}
        We claim that \(\Ind^G_H\field_\chi \isomorphic \field G e_\chi\), where elements of \(\field Ge_\chi\) are \(\field\)-linear combinations of elements of the form \(g e_\chi\) and the action on \(\field Ge_\chi\) is by \(g \action g'e_\chi = (gg')e_\chi\), that is, it's just left multiplication.
        
        The isomorphism, \(\varphi \colon \Ind^G_H \field_{\chi} \to \field G e_\chi\), is given by \(\varphi(g \otimes 1) = ge_\chi\).
        This is a \(G\)-module homomorphism since
        \begin{align}
            \varphi(g \action (g' \otimes 1)) &= \varphi(gg' \otimes 1)\\
            &= gg'e_\chi\\
            &= \varphi(gg' \otimes 1).
        \end{align}
        Note that if \(g = kh\) with \(h \in H\) then we have \(g \otimes 1 = k \otimes \chi(h)\), and so we need to check that \(\varphi\) is well defined with respect to this ambiguity.
        In particular, if \(g = kh = k'h'\) for \(h, h' \in H\) then we need to check that \(\varphi(k \otimes \chi(h)) = \varphi(k' \otimes \chi(h'))\).
        This is true since \(\chi(h)\) and \(\chi(h')\) are scalars, so we can pull the out and we have
        \begin{equation}
            \varphi(k \otimes \chi(h)) = \chi(h)\varphi(k \otimes 1) = \chi(h) ke_\chi
        \end{equation}
        and similarly, \(\varphi(k' \otimes \chi(h')) = \chi(h')k' e_\chi\).
        To show that these are equal we start with the definition of \(e_\chi\):
        \begin{align}
            \chi(h)ke_\chi &= \chi(h)k \frac{1}{\abs{H}} \sum_{g \in H} \chi(g)^{-1}g\\
            &= \frac{1}{\abs{H}} \sum_{g \in H} \chi(h)\chi(g)^{-1} kg.
        \end{align}
        We can then reindex the sum by defining \(g' \in G\) such that \(kg = k'g'\), so \(g = k^{-1}k'g'\).
        Since \(kh = k'h'\) this gives \(h = k^{-1}k'h'\).
        Thus,
        \begin{align}
            \chi(h)ke_\chi &= \frac{1}{\abs{H}} \sum_{g' \in H} \chi(k^{-1}k'h')\chi(k^{-1}k'g')^{-1} k'g'\\
            &= \frac{1}{\abs{H}} \sum_{g' \in H} \chi(k^{-1}k'h')\chi(g'^{-1}k'^{-1}) k'g'\\
            &= \frac{1}{\abs{H}} \sum_{g' \in H} \chi(k^{-1}k'h' g'^{-1}k'^{-1}k) k'g'\\
            \shortintertext{using \(k^{-1}k' = hh'^{-1}\) and \(k'^{-1}k = h'h^{-1}\) this becomes}
            \chi(h)ke_\chi &= \frac{1}{\abs{H}} \sum_{g' \in H} \chi(hh'^{-1} h' g'^{-1}h'h^{-1}) k'g'\\
            &= \frac{1}{\abs{H}} \sum_{g' \in H} \chi(h g'^{-1}h'h^{-1}) k'g'.
        \end{align}
        Now, since \(\chi\) maps into \(\complex^{\times}\) and is a group homomorphism we have that \(\chi(ab) \chi(a)\chi(b) = \chi(b)\chi(a) = \chi(ba)\), and it follows that \(\chi(hg'^{-1}h'h^{-1}) = \chi(h^{-1}hg'^{-1}h') = \chi(g'^{-1}h') = \chi(h')\chi(g')^{-1}\), and thus
        \begin{multline}
            \chi(h)ke_\chi = \frac{1}{\abs{H}} \sum_{g' \in H} \chi(h')\chi(g')^{-1}k'g'\\
            = \chi(h')k' \frac{1}{\abs{H}} \sum_{g' \in H} \chi(g')^{-1}g' = \chi(h')k'e_\chi.
        \end{multline}
        This shows that \(\varphi\) is well defined.
        Clearly \(\varphi\) is invertible, and so we have the claimed isomorphism, \(\Ind^G_H\field_{\chi} \isomorphic \field Ge_{\chi}\).
    \end{exm}
    
    The first example above actually gives yet another way of characterising the induced representation.
    If \(G\) is a finite group and \(H\) a (not necessarily normal) subgroup then we can form the coset space, \(G/H\).
    Taking \(\{g_1, \dotsc, g_n\}\) to be a complete set of representatives, that is each coset can be written as \(g_i H\) in exactly one way, we can take
    \begin{equation}
        \Ind^G_H M = \bigoplus_{i=1}^n g_iM
    \end{equation}
    where each \(g_i M\) is an isomorphic copy of \(M\), and we write elements of \(g_i M\) as \(g_i m\).
    For each \(g \in G\) there is some \(h_i \in H\) and \(j(i) \in \{1, \dotsc, n\}\) such that \(gg_i = g_{j(i)}h_i\), which simply restates that \(\{g_1, \dotsc, g_n\}\) is a complete set of representatives. 
    Then \(g \in G\) acts on this space by
    \begin{equation}
        g \action g_i m = g_{j(i)} \rho(h_i)m_i.
    \end{equation}
    So \(g\) acts by permuting the copies of \(M\), sending \(g_iM\) to \(g_{j(i)}M\), with an extra \enquote{twist} provided by the action of \(h_i\) on \(M\).
    Another way of constructing this is to take
    \begin{equation}
        g_i M = \{f \in \Hom_{\field H}(\field G, M) \mid f(g) = 0 \text{ unless } g \in g_i H\}.
    \end{equation}
    Then the action is by
    \begin{equation}
        (g \action f)(x) = f(xg)
    \end{equation}
    again, and we just take evaluating to zero to be equivalent to not being in \(g_i M\) as defined before.
    
    \section{Frobenius Formula for Induced Characters}
    Calculating the character of a restricted module is simple.
    If we have a \(G\)-module, \(M\), with character \(\chi \colon G \to \field\) then the character of \(\Res^G_HM\) is just the restriction of the character to \(H\), \(\chi\downarrow^G_H \coloneq \chi|_H \colon H \to \field\).
    In this section we give a method for calculating characters of induced modules, a more involved process.
    
    \begin{thm}{Frobenius Formula}{thm:frobenius formula}
        Let \(G\) be a finite group with subgroup \(H\).
        Let \(\{g_1, \dotsc, g_n\}\) be a complete set of representatives for \(G/H\).
        Let \(M\) be an \(H\)-module with character \(\chi_M\).
        Write \(\chi_M\uparrow^G_H\) for the character of \(\Ind^G_H M\).
        Then
        \begin{equation}
            \chi_M\uparrow^G_H(g) = \sum_{i=1}^n \chi_M(g_i^{-1}gg_i)
        \end{equation}
        where \(\chi_M\) has been extended from \(H\) to all of \(G\) such that \(\chi_M(x) = 0\) if \(x \notin H\).
        \begin{proof}
            We shall work with
            \begin{equation}
                \Ind^G_H M = \bigoplus_{i=1}^n g_iM.
            \end{equation}
            Thus, we have
            \begin{equation}
                \chi_M\uparrow^G_H(g) = \sum_i \chi_i(g)
            \end{equation}
            where \(\chi_i(g) = \tr_{g_iM}\rho_i(g)\) with \(\rho_i\) defined to be the corresponding blocks in the matrix
            \begin{equation}
                \rho(g) = 
                \begin{pmatrix}
                    \rho_1(g) & & \\
                    & \rho_2(g) & &\\
                    & & \ddots &\\
                    & & & \rho_n(g)
                \end{pmatrix}
            \end{equation}
            extended so that \(\rho_i(g) = 0\) unless \(gg_i \in g_iH\).
            
            For the nonzero terms we know that \(gg_i \in g_iH\) means there is some \(h^{-1} \in H\) such that \(gg_ih^{-1} = g_i\), and thus \(g_i^{-1}gg_i = h \in H\).
            Now define a map \(\alpha \colon g_iM \to M\) by \(\alpha(f) = f(g_i)\).
            This is an isomorphism, and we have
            \begin{equation}
                \alpha(g \action f) = (g \action f)(g_i) = f(g_ig) = f(hg_i) = \rho(h)f(g_i) = h \action \alpha(f)
            \end{equation}
            and so \(g \action f = \alpha^{-1}(h \action \alpha(f))\).
            This means that \(\tr_{g_iM} \rho_i(g) = \chi_M(h)\).
            Thus, we have
            \begin{equation}
                \chi_M\uparrow^G_H(g) = \tr_{\Ind^G_H M} \rho(g) = \sum_{i=1}^n \tr_{g_i M} \rho_i(g_i) = \sum_{i=1}^n \chi_M(g_igg_i^{-1})
            \end{equation}
            as claimed.
        \end{proof}
    \end{thm}
    
    \begin{crl}{}{}
        With notation as in \cref{thm:frobenius formula} if \(\Char \field\) and \(\abs{H}\) are coprime then we have
        \begin{equation}
            \chi_M\uparrow^G_H(g) = \frac{1}{\abs{H}} \sum_{\substack{x \in G\\ x^{-1}gx \in H}} \chi_M(x^-1 g x).
        \end{equation}
        \begin{proof}
            We have that
            \begin{equation}
                \chi_M\uparrow^G_H(g) = \sum_{i=1}^n \chi_M(g_i^{-1}gg_i).
            \end{equation}
            Since \(\chi_M\) is a class function it is invariant under conjugation of its argument, so we can write this as
            \begin{equation}
                \chi_M\uparrow^G_H(g) = \sum_{i=1}^n \chi_M(h^{-1}g_i^{-1}gg_ih).
            \end{equation}
            for any \(h \in H\).
            In fact, we can actually sum over all \(h \in H\), and all this does is give us \(\abs{H}\) identical terms\footnote{This is where we need \(\Char \field \nmid \abs{H}\), if this wasn't the case we may accidentally have everything vanish in this sum.}, so
            \begin{equation}
                \chi_M\uparrow^G_H(g) = \frac{1}{\abs{H}} \sum_{h \in H} \sum_{i=1}^n \chi_M(h^{-1}g_i^{-1}gg_ih).
            \end{equation}
            We can then recognise that the argument of \(\chi_M\) is \(g\) conjugated by \(x = g_i h\), which is chosen such that \(x^{-1}gx \in H\) since \(g_i^{-1}gg_i \in H\) and conjugation by \(h \in H\) doesn't take us out of \(H\).
            Thus, we have
            \begin{equation}
                \chi_M\uparrow^G_H(g) = \frac{1}{\abs{H}} \sum_{\substack{x \in G\\ x^{-1}gx \in H}} \chi_M(x^{-1}gx)
            \end{equation}
            where all we've done is combine the two sums, over \(h \in H\) and \(i \in \{1, \dotsc, n\}\) into a single sum.
        \end{proof}
    \end{crl}
    
    \section{Frobenius Reciprocity}
    Frobenius reciprocity is the relationship between induced and restricted modules.
    The strongest form of this result is that \(\Res^G_H\) and \(\Ind^G_H\) are adjoint functors.
    Before we get to that we'll give a result that holds for characters.
    
    \subsection{Froebnius Reciprocity of Characters}
    \begin{thm}{Frobenius Reciprocity of Characters}{}
        Let \(G\) be a finite group and \(H\) a subgroup.
        Let \(M\) be a \(G\)-module and \(N\) an \(H\)-module, both over \(\complex\).
        Write \(\innerprod{-}{-}_G\) and \(\innerprod{-}{-}_H\) for the inner product on the space of class functions of \(G\) and \(H\) respectively.
        Write \(\chi_M\) and \(\chi_N\) for the characters of \(M\) and \(N\) respectively.
        Write \(\chi_N\uparrow^G_H\) for the character of \(\Ind^G_HN\) and \(\chi_M\downarrow^G_H\) for the character of \(\Res^G_HM\).
        Then
        \begin{equation}
            \innerprod{\chi_N\uparrow^G_H}{\chi_M}_G = \innerprod{\chi_N}{\chi_M\downarrow^G_H}_H.
        \end{equation}
        \begin{proof}
            Write \(\chi = \chi_M\).
            Then, by definition of the inner product of class functions we have
            \begin{equation}
                \innerprod{\chi_N\uparrow^G_H}{\chi_M}_G = \frac{1}{\abs{G}} \sum_{g \in G} \chi_W\uparrow^G_H(g) \overline{\chi(g)}.
            \end{equation}
            Define a function
            \begin{equation}
                \psi(g) = 
                \begin{cases}
                    \chi_N(g) & g \in H,\\
                    0 & \text{else}.
                \end{cases}
            \end{equation}
            Then, using the Frobenius formula to calculate \(\chi_N\uparrow^G_H(g)\) we have
            \begin{align}
                \innerprod{\chi_N\uparrow^G_H}{\chi_M}_G &= \frac{1}{\abs{G}} \sum_{g \in G} \frac{1}{\abs{H}} \sum_{\substack{x \in G\\ xgx^{-1} \in H}} \psi(x^{-1}gx) \overline{\chi(g)}\\
                &= \frac{1}{\abs{G}} \frac{1}{\abs{H}} \sum_{g \in G} \sum_{\substack{x \in G\\ x^{-1}gx \in H}} \psi(x^{-1}gx) \chi(g^{-1})
            \end{align}
            where in the last step we've just rearranged some terms and used \(\overline{\chi(g)} = \chi(g^{-1})\).
            Now we can reindex the sum by taking \(y = x^{-1}gx\), which means \(g^{-1} = xy^{-1}x^{-1}\), and the condition that \(x^{-1}gx \in H\) becomes that \(y \in H\), so we have
            \begin{equation}
                \innerprod{\chi_N\uparrow^G_H}{\chi_M}_G = \frac{1}{\abs{G}} \frac{1}{\abs{H}} \sum_{x \in G} \sum_{y \in H} \psi(y) \chi(xy^{-1}x^{-1}).
            \end{equation}
            We also have that \(\psi(y) = \chi_N(y)\), since \(y \in H\), and thus this becomes
            \begin{equation}
                \innerprod{\chi_N\uparrow^G_H}{\chi_M}_G = \frac{1}{\abs{G}} \frac{1}{\abs{H}} \sum_{x \in G} \sum_{y \in H} \chi_N(y) \chi(xy^{-1}x^{-1}).
            \end{equation}
            Since \(\chi\) is a class function \(\chi(xy^{-1}x^{-1}) = \chi(y^{-1})\), and so we get \(\abs{G}\) terms which are all equal to \(\chi(y^{-1}) = \overline{\chi(y)}\).
            This perfectly cancels with the sum over \(x \in G\), leaving us with
            \begin{equation}
                \innerprod{\chi_N\uparrow^G_H}{\chi_M}_G = \frac{1}{\abs{H}} \sum_{y \in G} \chi_N(y) \overline{\chi(y)} = \innerprod{\chi_N}{\chi}_H.
            \end{equation}
            This proves the result once we realise that since the sum is over \(y \in H\) we can replace \(\chi = \chi_M \colon G \to \complex\) with \(\chi_M\downarrow^G_H = \chi_M|_H \colon H \to \complex\).
        \end{proof}
    \end{thm}
    
    One thing that this result tells us is that the multiplicities of induced modules and restricted modules are related.
    In particular, we have
    \begin{equation}
        \dim(\Hom_G(M, \Ind^G_HN)) = \dim(\Hom_H(\Res^G_HM, N)).
    \end{equation}
    Thus, there exists, at the level of vector spaces, an isomorphism between these hom-spaces.
    
    \subsection{Frobenius Reciprocity}
    \begin{thm}{}{}
        Let \(G\) be a finite group with subgroup \(H\).
        Then the functors
        \begin{equation}
            \Res^G_H \colon \Mod{G} \to \Mod{H}, \qand \Ind^G_H \colon \Mod{H} \to \Mod{G}
        \end{equation}
        are left and right adjoints.
        That is, there is a (natural) isomorphism
        \begin{equation}
            \Hom_G(M, \Ind^G_H N) \isomorphic \Hom_H(\Res^G_H M, N)
        \end{equation}
        for any \(G\)-module, \(M\), and \(H\)-module, \(N\).
        \begin{proof}
            Let
            \begin{equation}
                E = \Hom_G(M, \Ind^G_H N), \qand E' = \Hom_H(\Res^G_H M, N).
            \end{equation}
            We need to define two functions
            \begin{equation}
                \Phi \colon E \to E', \qand \Phi' \colon E' \to E
            \end{equation}
            which should then be inverses.
            
            If \(\alpha \in E\) then \(\alpha \colon M \to \Ind^G_H N\) is a \(G\)-module homomorphism, and \(\Phi(\alpha)\) should be an \(H\)-module homomorphism, \(\Phi(\alpha) \colon \Res^G_H M \to N\).
            That is, \(\Phi(\alpha)\) needs to take in an element of \(\Res^G_H M\), which is just an element of \(M\), and produce an element of \(N\).
            The obvious way to do this is to simply evaluate \(\alpha\), which gives us an element of \(\Ind^G_H N \isomorphic \Hom_{\field H}(\field G, N)\), which we can then evaluate to produce an element of \(N\).
            The only problem is what element of \(\field G\) do we evaluate this map at?
            Fortunately since \(G\) is a group there's an obvious distinguished element, \(1_G\), at which to perform this evaluation.
            Thus, we define \(\Phi(\alpha)\) by
            \begin{equation}
                \Phi(\alpha)(m) = \alpha(m)(1_G)
            \end{equation}
            for \(m \in \Res^G_M\) (which as a set is just \(M\)).
            
            If \(\beta \in E'\) then \(\beta \colon \Res^G_H \to N\) is an \(H\)-module homomorphism, and \(\Phi'(\beta)\) should be a \(G\)-module homomorphism, \(\Phi'(\beta) \colon M \to \Ind^G_H N\).
            That is, \(\Phi'(\beta)\) needs to take in an element of \(M\) and produce an element of \(\Ind^G_HN \isomorphic \Hom_{\field H}(\field G, N)\).
            The correct definition turns out to be
            \begin{equation}
                \Phi'(\beta)(m)(x) = \beta(xm)
            \end{equation}
            where \(m \in M\) and \(x \in \field G\) so \(xm \in M\) using the \(G\)-module structure of \(M\), which is equal to \(\Res^G_HM\) as a set, and so evaluating \(\beta\) at \(xm\) is a valid operation.
            
            With these definitions we need to show that the resulting functions are well-defined.
            This comes down to the following three steps:
            \begin{enumerate}
                \item We need to show that \(\Phi(\alpha)\) is an \(H\)-module homomorphism.
                That is, we need to show that \(\Phi(\alpha)(h \action m) = h \action \Phi(\alpha)(m)\) for all \(h \in H\) and \(m \in M\).
                This is the case, as a direct calculation shows.
                First, using the definition of \(\Phi\) we have
                \begin{equation}
                    \Phi(\alpha)(h \action m) = \alpha(h \action m)(1_G).
                \end{equation}
                Since \(\alpha\) is a \(G\)-module homomorphism we have \(\alpha(h \action m) = h \action \alpha(m)\), and so
                \begin{equation}
                    \Phi(\alpha)(h \action m) = (h \action \alpha(m))(1_G).
                \end{equation}
                Since \(\alpha(m)\) is an \(H\)-module homomorphism the action of \(h\) on \(\alpha(m)\) is to act on the right in the argument, which is just multiplication in this case:
                \begin{equation}
                    \Phi(\alpha)(h \action m) = \alpha(m)(1_Gh).
                \end{equation}
                Since \(1_Gh = h1_G\) we can write this as
                \begin{equation}
                    \Phi(\alpha)(h \action m) = \alpha(m)(h1_G).
                \end{equation}
                We can then identify that acting on the left of the argument is the definition of the action of \(G\) on the \(G\)-module homomorphism \(\alpha\)
                \begin{equation}
                    \Phi(\alpha)(h \action m) = h \action (\alpha (m))(1_G) = h \action (\Phi(\alpha)(m)).
                \end{equation}
                \item Next, we need to show that \(\Phi'(\beta)(m) \in \Ind^G_HN\).
                That is, we need to show that \(\Phi'(\beta)(m)(hx) = h \action \Phi'(\beta)(m)(x)\).
                This also follows from a direct calculation, we have
                \begin{equation}
                    \Phi'(\beta)(m)(hx) = \beta(hxm) = h \action \beta(xm) = h \action \Phi'(\beta)(m)(x)
                \end{equation}
                having used the fact that \(\beta\) is an \(H\)-module homomorphism.
                \item Finally, we need to show that \(\Phi'(\beta)\) is a \(G\)-module homomorphism.
                That is, we need to show that \(\Phi'(\beta)(g \action m) = g \action \Phi'(\beta)(m)\).
                This follows since
                \begin{equation}
                    \Phi'(\beta)(g \action m)(x) = \beta(xg \action m) = \Phi'(\beta)(m)(xg) = (g \action \Phi'(\beta)(m))(x)
                \end{equation}
                having used the fact that \(\Phi'(\beta) \in \Ind^G_H M\) in the last step.
            \end{enumerate}
            
            We now just have to show that \(\Phi\) and \(\Phi'\) are inverses, this follows from two calculations:
            \begin{equation}
                \Phi(\Phi'(\beta))(m) = \Phi'(\beta)(m)(1_G) = \beta(1_Gm) = \beta(m),
            \end{equation}
            so \(\Phi \circ \Phi' = \id_{E'}\), and
            \begin{equation}
                \Phi'(\Phi(\alpha))(m)(x) = \Phi(\alpha)(xm) = \alpha(xm)(1_G) = (x \action \alpha)(m)(1_G) = \alpha(m)(1_Gx) = \alpha(m)(1_G)
            \end{equation}
            so \(\Phi' \circ \Phi = \id_E\).
        \end{proof}
    \end{thm}
    
    
    % Appdendix
	\appendixpage
	\begin{appendices}
	    \include{appendix/complexification}
	\end{appendices}

    \backmatter
    \renewcommand{\glossaryname}{Acronyms}
    \printglossary[acronym]
    \printindex
\end{document}