% !TeX program = lualatex
\documentclass[fleqn]{NotesClass}

\strictpagecheck

\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{ytableau}
\usepackage{siunitx}
\usepackage{subcaption}

\let\oldwidehat=\widehat
\AtBeginDocument{\let\widehat=\oldwidehat}

\usepackage{tikz}
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]

\usepackage{tikz-cd}
\AtBeginEnvironment{tikzcd}{\tikzexternaldisable}
\AtEndEnvironment{tikzcd}{\tikzexternalenable}

\usetikzlibrary{arrows.meta}
\usetikzlibrary{braids}
\usetikzlibrary{hobby}

\usepackage[pdfauthor={Willoughby Seago},pdftitle={Notes from Representation Theory Course},pdfkeywords={representation theory},pdfsubject={Representation Theory}]{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{NotesBoxes}
\usepackage{NotesMaths2}

\setmathfont[range={\int, \oint, \otimes, \oplus, \bigotimes, \bigoplus}]{Latin Modern Math}


% Highlight colour
%\definecolor{highlight}{HTML}{710D78}
%\definecolor{my blue}{HTML}{2A0D77}
%\definecolor{my red}{HTML}{770D38}
%\definecolor{my green}{HTML}{14770D}
%\definecolor{my yellow}{HTML}{E7BB41}

% Title page info
\title{Representation Theory}
\author{Willoughby Seago}
\date{January 13th, 2024}
\subtitle{Notes from}
\subsubtitle{University of Glasgow}
\renewcommand{\abstracttext}{These are my notes from the SMSTC course \emph{Lie Theory} taught by Prof Christian Korff. These notes were last updated at \printtime{} on \today{}.}

% Commands
% Maths
\renewcommand{\field}{\symbb{k}}
\newcommand{\id}{\symrm{id}}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Hom}{Hom}
\newcommand{\action}{\mathbin{.}}
\newcommand{\op}{\symrm{op}}
\makeatletter
\newcommand{\c@egory}[1]{\symsfup{#1}}
\newcommand{\Vect}[1][\field]{#1\text{-}\c@egory{Vect}}
\newcommand{\AMod}[1][A]{#1\text{-}\c@egory{Mod}}
\newcommand{\ModA}[1][A]{\c@egory{Mod}\text{-}#1}
\newcommand{\Mod}[1]{#1\text{-}\c@egory{Mod}}
\newcommand{\Ab}{\c@egory{Ab}}
\newcommand{\Rep}{\c@egory{Rep}}
\newcommand{\Set}{\c@egory{Set}}
\newcommand{\Alg}[1][\field]{{#1}\text{-}\c@egory{Alg}}
\newcommand{\Lie}[1][\field]{{#1}\text{-}\c@egory{Lie}}
\makeatother
\DeclareMathOperator{\im}{im}
\newcommand{\isomorphic}{\cong}
\DeclareMathOperator{\Span}{span}
\ExplSyntaxOn
% Create LaTeX interface command
\NewDocumentCommand{\cycle}{ O{\,} m }{  % optional arg is separator, mandatory
    %arg is comma separated list
    (
    \willoughby_cycle:nn { #1 } { #2 }
    )
}

\clist_new:N \l_willougbhy_cycle_clist  % Create new clist variable
\cs_new_protected:Npn \willoughby_cycle:nn #1 #2 {  % create LaTeX3 function
    \clist_set:Nn \l_willougbhy_cycle_clist { #2 }  % set clist variable with
    %clist #2 passed by user
    \clist_use:Nn \l_willougbhy_cycle_clist { #1 }  % print list separated by #1
}
\ExplSyntaxOff
\newcommand{\universalEnveloping}{\symcal{U}}
\DeclarePairedDelimiterX{\bracket}[2]{[}{]}{#1, #2}
\DeclareMathOperator{\Mat}{Mat}
\RenewDocumentCommand{\matrices}{s o m m}{
    \IfBooleanTF{#1}{
        \IfNoValueTF{#2}{
            \Mat_{#3}\left( #4 \right)
        }{
            \Mat_{#2 \times #3}\left( #4 \right)
        }
    }{
        \IfNoValueTF{#2}{
            \Mat_{#3}(#4)
        }{
            \Mat_{#2 \times #3}(#4)
        }
    }
}
\newcommand{\trans}{\top}
\DeclareMathOperator{\Rad}{Rad}
\DeclareMathOperator{\Irr}{Irr}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Char}{char}
\newcommand{\classFunctions}{\symcal{X}}
\newcommand{\conjugacyClasses}{\symcal{C}}
\DeclareMathOperator{\Func}{Func}
\newcommand{\partition}{\vdash}
\DeclarePairedDelimiterX{\innerprod}[2]{\langle}{\rangle}{#1, #2}
\DeclareMathOperator{\frobeniusSchur}{FS}
\DeclareMathOperator{\standardYoungTableaux}{SYT}
\DeclareMathOperator{\semistandardYoungTableaux}{SSYT}
\newcommand{\normalsub}{\mathrel{\lhd}}
\newcommand{\algNumbers}{\overline{\rationals}}
\newcommand{\algIntegers}{\overline{\integers}}
\newcommand{\Res}{\symrm{Res}}
\newcommand{\Ind}{\symrm{Ind}}
\newcommand{\one}{\symbb{1}}
\newcommand{\rowGroup}{R}
\newcommand{\columnGroup}{C}
\newcommand{\intterobang}{\mathchoice{!\mkern-6.1mu?}{!\mkern-6.2mu?}{!\mkern-6.8mu?}{!\mkern-6.8mu?}}
\DeclareMathOperator{\proj}{proj}
\newcommand{\ch}{\symrm{ch}}
\newcommand{\Gr}{\symrm{Gr}}
\renewcommand{\dd}{\,\symrm{d}}
\newcommand{\ad}{\symrm{ad}}
\DeclareMathOperator{\gr}{gr}
\DeclarePairedDelimiterX{\rootProd}[2]{(}{)}{#1 , #2}
\newcommand{\dynkin}[2]{\symrm{#1}_{#2}}
\newcommand{\purebraid}{\symcal{PB}}
\newcommand{\braid}{\symcal{B}}
\DeclareMathOperator{\Homeo}{Homeo}

\includeonly{}%{parts/algebra-reps, parts/group-reps, parts/Sn-reps}

\begin{document}
    \frontmatter
    \titlepage
    \innertitlepage{tikz-external/dynkin-E8}
    \tableofcontents
    % \listoffigures
    \mainmatter
    \include{parts/algebra-reps}
    \include{parts/group-reps}
    \include{parts/Sn-reps}
    
    \part{Other Topics in Representation Theory}
    \chapter{Lie Algebras}
    In this section we give a rapid, relatively proof free, tour of the representation theory of Lie algebras.
    We refer the reader to other sources for details, such as my lecture notes \url{https://github.com/WilloughbySeago/phd-courses-notes/tree/main/lie-theory}.
    
    \section{Lie Algebras}
    \begin{dfn}{Lie Algebra}{}
        A \defineindex{Lie algebra}, \(\lie{g}\), is a \(\field\)-vector space equipped with a linear map \(\lie{g} \otimes \lie{g} \to \lie{g}\) called the \defineindex{Lie bracket} subject to the following:
        \begin{itemize}
            \item \define{alternativity}\index{alternating}: \(\bracket{x}{x} = 0\) for all \(x \in \lie{g}\);
            \item \defineindex{Jacobi identity}: \(\bracket{x}{\bracket{y}{z}} + \bracket{y}{\bracket{z}{x}} + \bracket{z}{\bracket{x}{y}} = 0\) for all \(x, y, z \in \lie{g}\).
        \end{itemize}
    \end{dfn}
    
    Note that more commonly the definition is given as a bilinear map \(\lie{g} \times \lie{g} \to \lie{g}\).
    The universal property of the tensor product means that these are equivalent.
    For fields of characteristic other than 2 the first relation is usually replaced with antisymmetry, \(\bracket{x}{y} = -\bracket{y}{x}\) for all \(x, y \in \lie{g}\).
    With our definition using the tensor product we can pass to the quotient \(\Lambda^2\lie{g}\) and we see that \(\bracket{-}{-}\) induces a map \(\bracket{-}{-} \colon \Lambda^2\lie{g} \to \lie{g}\) which trivially is such that \(\bracket{x}{x} = 0\) since \(x \otimes x\) maps to zero in \(\Lambda^2 \lie{g}\).
    
    \begin{dfn}{}{}
        Let \(\lie{g}\) and \(\lie{g}'\) be Lie algebras over the same field, \(\field\).
        A morphism of Lie algebras, \(\varphi \colon \lie{g} \to \lie{g}'\) is a linear map which preserves the Lie bracket, that is
        \begin{equation}
            \varphi(\bracket{x}{y}) = \bracket{\varphi(x)}{\varphi(y)}
        \end{equation}
        where the bracket on the left is that of \(\lie{g}\) and on the right it's that of \(\lie{g}'\).
    \end{dfn}
    
    \begin{exm}{}{}
        \begin{itemize}
            \item Let \(A\) be an associative algebra, then we can make this into a Lie algebra by defining the bracket \(\bracket{a}{b} = ab - ba\).
            A special case of this is \(A = \End V\) for some vector space, \(V\).
            Then we call the corresponding Lie algebra \(\generalLinearLie(V)\), or if \(\dim V = n\) we call it \(\generalLinearLie_n\) (note that as vector spaces \(\generalLinearLie(V)\) is exactly \(A = \End V\), the name change just reflects a shifting view point from associative algebras to Lie algebras).
            \item Any vector space, \(V\), can be made into a Lie algebra by defining \(\bracket{x}{y} = 0\) for all \(x, y \in V\).
            Such a Lie algebra is called \define{abelian}\index{abelian Lie algebra}.
            The idea is that the commutator vanishing means that multiplication is commutative, an idea that only makes sense if \(\bracket{-}{-}\) really is the commutator, like in the previous example.
        \end{itemize}
    \end{exm}
     
    \begin{dfn}{Lie Subalgebra}{}
        Let \(\lie{g}\) be a Lie algebra over \(\field\).
        A Lie subalgebra, \(\lie{h}\), is a Lie algebra over \(\field\) equipped with an injective Lie algebra morphism \(\lie{h} \hookrightarrow \lie{g}\).
    \end{dfn}
    
    An almost identical definition is that a Lie subalgebra is a subspace, \(\lie{h} \subseteq \lie{g}\) such that \(\lie{h}\) is a Lie algebra in its own right (with the same bracket as \(\lie{g}\)).
    One can then show that this is true so long as the \(\lie{h}\) is closed under the Lie bracket.
    That is, \(\bracket{\lie{h}}{\lie{h}}\) is a subset of \(\lie{h}\).
    Note that in general if \(U\) and \(V\) are subspaces of \(\lie{g}\) then \(\bracket{U}{V}\) is defined to be the span of all \(\bracket{u}{v}\) with \(u \in U\) and \(v \in V\).
    Similarly, if \(x \in \lie{g}\) then \(\bracket{x}{U}\) is the span of all \(\bracket{x}{y}\) with \(y \in \lie{g}\).
    
    The only subtle difference between these two definitions is that the existence of a monomorphism \(\lie{h} \hookrightarrow \lie{g}\) only implies that \(\lie{h}\) is isomorphic to a subalgebra of \(\lie{g}\) with the second definition, but we'll only consider things up to isomorphism most the time so this is really the definition we want.
    
    \begin{exm}{}{}
        \begin{itemize}
            \item Let \(\lie{g}\) be any Lie algebra.
            Any one-dimensional subspace, \(\lie{l}\), is an abelian subalgebra, since if \(l, l' \in \lie{l}\) then \(l = \lambda l'\) for some \(\lambda \in \field\), and so \(\bracket{l}{l'} = \bracket{kl'}{l'} = k\bracket{l'}{l'} = 0\) and \(0 \in \lie{l}\).
            \item The \define{centre}\index{centre!of a Lie algebra} of a Lie algebra, \(\lie{g}\), is the abelian subalgebra
            \begin{equation}
                \lie{z}(\lie{g}) \coloneq \{x \in \lie{g} \mid [x, \lie{g}] = 0\} \subseteq \lie{g}.
            \end{equation}
            \item For \(V\) a finite-dimensional vector space of dimension \(n\) we know that \(\generalLinearLie_n = \End V\) is a Lie algebra.
            Fixing a basis the elements of \(\generalLinearLie_n\) are just all \(n \times n\) matrices with entries in \(\field\).
            There is a subalgebra, \(\specialLinearLie_n \subset \generalLinearLie_n\), consisting of only the matrices with zero trace.
            This follows because we have
            \begin{equation}
                \tr(\bracket{x}{y}) = \tr(xy) - \tr(yx) = 0.
            \end{equation}
            This holds for all \(x, y \in \generalLinearLie_n\), not just for the traceless case, and so this turns out to be a special case of another construction, called the derived subalgebra, \(\lie{g}' \coloneq \bracket{\lie{g}}{\lie{g}}\).
        \end{itemize}
    \end{exm}
    
    \begin{dfn}{Ideal}{}
        Let \(\lie{g}\) be a Lie algebra.
        A Lie subalgebra, \(\lie{i} \subseteq \lie{g}\), is an \define{ideal}\index{ideal!of a Lie algebra} if \(\bracket{\lie{i}}{\lie{g}} \subseteq \lie{i}\).
    \end{dfn}
    
    Compare this to the definition of a subalgebra, which only requires that \(\bracket{\lie{i}}{\mathcolor{highlight}{\lie{i}}} \subseteq \lie{i}\).
    Compare this also to the notion of an ideal, \(I\), of a ring, \(R\), which is a subgroup of the additive group such that \(IR \subseteq I\).
    
    The idea is that ideals are to Lie algebras as ideals are to rings, or as normal subgroups are to groups.
    In particular, we have a correspondence between ideals, \(\lie{i} \subseteq g\) and Lie algebra morphisms, \(\varphi \colon \lie{g} \to \lie{h}\) given by \(\lie{i} \leftrightarrow \ker \varphi\) (where the kernel is defined as it is for any linear map).
    We also have that \(\lie{g}/\lie{i}\) is a well defined quotient and a Lie algebra.
    Note that the quotient of any vector space by a subspace is again a vector space, but it's only a Lie algebra again if we quotient by an ideal.
    The bracket of this quotient is defined by \(\bracket{x + \lie{i}}{y + \lie{i}} = \bracket{x}{y} + \lie{i}\).
    
    \begin{dfn}{Derived Subalgebra}{}
        Let \(\lie{g}\) be a Lie algebra, then \(\lie{g}' = \bracket{\lie{g}}{\lie{g}}\) is the \defineindex{derived subalgebra}.
    \end{dfn}
    
    \begin{dfn}{Solvable Lie Algebra}{}
        A Lie algebra, \(\lie{g}\), is solvable if the series
        \begin{equation}
            \lie{g} \supseteq \lie{g}' \supseteq \lie{g}'' \supseteq \dotsb
        \end{equation}
        terminates.
    \end{dfn}
    
    \begin{dfn}{Nilpotent Lie ALgebra}{}
        A Lie algebra, \(\lie{g}\), is solvable if the series
        \begin{equation}
            \lie{g} \supseteq \bracket{\lie{g}}{\lie{g}} \supseteq \bracket{\lie{g}}{\bracket{\lie{g}}{\lie{g}}} \supseteq \dotsb
        \end{equation}
        terminates.
    \end{dfn}
    
    The difference between these two is subtle, one nests brackets on both sides, and the other only on the other side.
    More concretely, the upper triangular matrices form a solvable subalgebra of \(\generalLinearLie_n\) (in fact, this is a maximal solvable subalgebra, also known as a \defineindex{Borel subalgebra}), and the \emph{strictly} upper triangular matrices form a (maximal) nilpotent subalgebra of \(\generalLinearLie_n\).
    
    \begin{dfn}{}{}
        The maximal solvable \emph{ideal} of \(\lie{g}\) is called its \define{radical}\index{radical!of a Lie algebra}, \(\Rad \lie{g}\).
    \end{dfn}
    
    \begin{dfn}{}{}
        A Lie algebra, \(\lie{g}\), is \define{semisimple}\index{semisimple!Lie algebra} if \(\Rad \lie{g} = 0\), that is, if \(\lie{g}\) has no proper solvable ideals.
        Similarly, \(\lie{g}\) is \define{simple}\index{simple!Lie algebra} if it has no proper ideals (solvable or not).
    \end{dfn}
    
    \begin{dfn}{Linear Lie Algebra}{}
        A \defineindex{linear Lie algebra} is any Lie algebra which is isomorphic to a Lie subalgebra of some \(\generalLinearLie(V)\) for \(V\) a finite-dimensional vector space.
    \end{dfn}
    
    Ado's theorem tells us that (over a field of characteristic zero) every finite-dimensional Lie algebra is linear.
    
    \begin{thm}{Ado's Theorem}{}
        Let \(\lie{g}\) be a finite-dimensional Lie algebra over a field of characteristic zero.
        Then \(\lie{g}\) admits a faithful representation \(\lie{g} \hookrightarrow \generalLinearLie(V)\) for some finite-dimensional vector space, \(V\).
        Further, one can choose this representation such that the maximal nilpotent ideal, \(\lie{n} \subseteq \lie{g}\) acts nilpotently on \(V\).
    \end{thm}
    
    There are some special linear Lie algebras.
    Over \(\complex\) these are
    \begin{itemize}
        \item \(\generalLinearLie_n = \{x \in \Mat_n(\complex\}\) (real dimension \(2n^2\))
        \item \(\specialLinearLie_n = \{x \in \Mat_n(\complex) \mid \tr x = 0\}\) (real dimension \(2(n^2 - 1)\));
        \item \(\specialOrthogonalLie_n = \{x \in \Mat_n(\complex) \mid x^{\trans} + x^{\trans} = 0\}\) (real dimension \(n(n - 1)\));
        \item \(\symplecticLie_{2n} = \{x \in \Mat_{2n}(\complex) \mid Jx + x^{\trans}J = 0\}\) where \(J = \begin{pmatrix} 0 & -I_n\\ I_n & 0 \end{pmatrix}\) with \(I_n \in \Mat_n(\complex)\) the identity matrix (real dimension \(2 \binom{2n + 1}{2}\)).
    \end{itemize}
    Over \(\reals\) these are
    \begin{itemize}
        \item \(\generalLinearLie_n = \{x \in \Mat_n(\reals)\}\) (real dimension \(n^2\));
        \item \(\specialOrthogonalLie_n = \{x \in \Mat_n(\reals) \mid \tr x = 0\}\) (real dimension \(n^2 - 1\));
        \item \(\unitaryLie_n = \{x \in \Mat_n(\complex) \mid x + x^* = 0\}\) (real dimension \(n^2\));
        \item \(\specialUnitaryLie_n = \{x \in \Mat_n(\complex) \mid x + x^* = 0 \text{ and } \tr x = 0\}\) (real dimension \(n^2 - 1\));
        \item \(\symplecticLie_{2n} = \{x \in \Mat_n(\quaternions) \mid x + x^* = 0\}\) (real dimension \(2n^2 + n\)).
    \end{itemize}
    
    \section{Representation Theory of Lie Algebras}
    \begin{dfn}{Representation}{}
        A \define{representation}\index{representation!of a Lie algebra}, \(\lie{g}\) (over \(\field\)), is a \(\field\)-vector space, \(V\), equipped with a Lie algebra morphism
        \begin{equation}
            \rho \colon \lie{g} \to \generalLinearLie(V).
        \end{equation}
        
        Equivalently, a \define{\(\lie{g}\)-module}, \(V\), is a vector space equipped with a (left) Lie algebra action of \(\lie{g}\), that is, a map \(\lie{g} \times V \to V\), \((x, v) \mapsto x \action v\) subject to the following:
        \begin{itemize}
            \item Linearity in the first argument: \((\alpha x + \beta y) \action v = \alpha (x \action v) + \beta (y \action v)\) for all \(\alpha, \beta \in \field\), \(x, y \in \lie{g}\) and \(v \in V\);
            \item Linearity in the second argument: \(x \action (\alpha v + \beta w) = \alpha(x \action v) + \beta(x \action w)\) for all \(\alpha, \beta \in \field\), \(x \in \lie{g}\) and \(v, w \in V\);
            \item Respects the bracket: \(\bracket{x}{y} \action v = x \action (y \action v) - y \action (x \action v)\) for all \(x, y \in \lie{g}\) and \(v \in V\).
        \end{itemize}
    \end{dfn}
    
    As with groups and associative algebras the \(\lie{g}\)-module and representation of \(\lie{g}\) carry exactly the same information, and as such which we use is a matter of preference.
    
    \begin{dfn}{Adjoint Representation}{}
        Every Lie algebra, \(\lie{g}\), is a \(\lie{g}\)-module in a canonical way, known as the \defineindex{adjoint representation}
        \begin{equation}
            \begin{aligned}
                \ad \colon \lie{g} &\to \generalLinearLie(\lie{g})\\
                x &\mapsto \ad_x
            \end{aligned}
        \end{equation}
        where \(\ad_x \colon \lie{g} \to \lie{g}\) is defined by \(\ad_x(y) = \bracket{x}{y}\) for all \(x, y \in \lie{g}\).
    \end{dfn}
    
    For the adjoint representation to be a representation we need \(\ad\) to be a Lie algebra morphism.
    That is, we need to have \(\ad_{\bracket{x}{y}} = \bracket{\ad_x}{\ad_y}\) for \(x, y \in \lie{g}\).
    It turns out that this is true precisely because the this statement, upon applying both sides of the above to \(z \in \lie{g}\), expands to the Jacobi identity:
    \begin{align}
        \ad_{\bracket{x}{y}}(z) &= \bracket{\bracket{x}{y}}{z}\\
        \bracket{\ad_x}{\ad_y}(z) = (\ad_x \circ \ad_y - \ad_y \circ \ad_x)(z) = \bracket{x}{\bracket{y}{z}} - \bracket{y}{\bracket{x}{z}}.
    \end{align}
    Equality between the two lines above is, after applying the antisymmetry property, exactly the Jacobi identity.
    
    \begin{dfn}{}{}
        Given \(\lie{g}\)-modules \(V\) and \(W\) we can define
        \begin{itemize}
            \item the \define{direct sum}\index{direct sum!of Lie algebra representations}, \(V \oplus W\), which has the action \(x \action (v + w) = x \action v + x \action w\);
            \item the \define{tensor product}\index{tensor product!of Lie algebra representations}, \(V \otimes W\), which has the action \(x \action (v \otimes w) = (x \action v) \otimes w + v \otimes (x \action w)\);
            \item the \define{dual representation}\index{dual representation!of a Lie algebra representation}, \(V^*\), which has the action \(\rho_{V^*}(x) = -\rho_V(x)^*\)
        \end{itemize}
        all for \(x \in \lie{g}\), \(v \in V\), and \(w \in W\).
    \end{dfn}
    
    \section{Universal Enveloping Algebra}
    \begin{dfn}{Universal Enveloping Algebra}{}
        Let \(\lie{g}\) be a Lie algebra.
        An enveloping algebra, \((E, i)\), is an associative unital algebra, \(E\), and an inclusion of vector spaces \(i \colon \lie{g} \hookrightarrow E\) such that
        \begin{equation}
            i(\bracket{x}{y}) = i(x)i(y) - i(y)i(x).
        \end{equation}
        The \defineindex{universal enveloping algebra} is the\footnote{turns out that the universal enveloping algebra both exists, and is unique up to unique isomorphism} enveloping algebra \((U(\lie{g}), \iota)\) such that for any other enveloping algebra, \((E, i)\), there is a unique morphism of associative unital algebras, \(\varphi \colon U(\lie{g}) \to E\) such that \(i = \varphi \circ \iota\).
    \end{dfn}
    
    The definition is a bit terse, the idea is that \(U(\lie{g})\) (dropping \(\iota\) from the notation) is the smallest associative unital algebra containing \(\lie{g}\) in such a way that the bracket of \(\lie{g}\) in \(U(\lie{g})\) really is just the commutator.
    For example, the universal enveloping algebra of \(\generalLinearLie(V)\) is simply \(\End(V)\), which is just \(\generalLinearLie(V)\) but viewed as an associative algebra.
    
    \begin{thm}{}{}
        The universal enveloping algebra exists.
        An explicit construction is as follows.
        Let \(U(\lie{g}) = T(\lie{g})/I\), where \(I\) is the ideal of the tensor algebra, \(T(\lie{g})\), generated by elements of the form
        \begin{equation}
            \bracket{x}{y} - x \otimes y + y \otimes x
        \end{equation}
        for \(x, y \in \lie{g}\).
    \end{thm}
    
    The universal property of the universal enveloping algebra can be characterised as the statement that there is an isomorphism
    \begin{equation}
        \Hom_{\Lie}(\lie{g}, L(A)) \isomorphic \Hom_{\Alg}(U(\lie{g}), A)
    \end{equation}
    where
    \begin{itemize}
        \item \(\Lie\) is the category of Lie algebras and Lie algebra homomorphisms;
        \item \(\lie{g}\) is a Lie algebra
        \item \(A\) is an unital associative algebra;
        \item \(L(A)\) is the Lie algebra given by equipping \(A\) with the commutator;
        \item \(\Alg\) is the category of unital associative algebras and their homomorphisms.
    \end{itemize}
    Simply send the Lie algebra homomorphism \(\varphi \colon \lie{g} \to L(A)\) to the associative algebra homomorphism \(\tilde{\varphi} \colon U(\lie{g}) \to A\) defined by \(\tilde{\varphi}(x) = \varphi(x)\) for \(x \in \lie{g}\) and extended by linearity and the requirement that \(\tilde{\varphi}\) preserves multiplication.
    This works precisely because of the universal property.
    For the inverse, send \(\psi \colon U(\lie{g}) \to A\) to the restriction \(\psi|_{\lie{g}}\).
    
    It turns out that \(L \colon \Alg \to \Lie\) is a functor, if \(f \colon A \to B\) is a morphism of associative algebras then we can define \(L(f) \colon L(A) \to L(B)\) by defining \(L(f)(\bracket{x}{y}) = \bracket{f(x)}{f(y)} = f(x)f(y) - f(y)f(x)\) for \(x, y \in A\).
    That is, we just require that \(L(f)\) is a Lie algebra homomorphism.
    Similarly, \(U \colon \Lie \to \Alg\) is a functor, if \(f \colon \lie{g} \to \lie{h}\) is a morphism of Lie algebras then we can define \(U(f) \colon U(\lie{g}) \to U(\lie{h})\) by defining \(U(f)(xy) = U(f)(x) U(f)(y)\) for \(x, y \in \lie{g}\) and similarly for products of more than two elements, and extended by linearity to all of \(U(\lie{g})\).
    That is, we just require that \(U(f)\) respects the multiplication of the associative algebra.
    Then the above isomorphism happens to be natural, and we thus have that \(L\) is right adjoint to \(U\).
    
    The important thing here is that if we take \(A = \End V\) then we have
    \begin{equation}
        \Hom_{\Lie}(\lie{g}, \generalLinearLie(V)) \isomorphic \Hom_{\Alg}(U(\lie{g}), \End V).
    \end{equation}
    This means that a map \(\lie{g} \to \generalLinearLie(V)\) carries the same data as a map \(U(\lie{g}) \to \End V\).
    We can identify a map of the first type as a Lie algebra representation of \(\lie{g}\), and a map of the second type as a unital associative algebra representation of \(U(\lie{g})\).
    That is, representations of \(\lie{g}\) are \enquote{the same} as representations of \(U(\lie{g})\).
    
    Another way of thinking about this is that \(U(\lie{g})\) is to \(\lie{g}\) as \(\field G\) is to \(G\) for a finite group, \(G\).
    We can study the representation theory of \(\lie{g}\) or \(G\) just by studying the representation theory of the universal enveloping algebra or group algebra.
    
    \begin{prp}{}{}
        The universal enveloping algebra, \(U(\lie{g})\), is a Hopf algebra with the comultiplication
        \begin{equation}
            \Delta(x) = x \otimes 1 + 1 \otimes x,
        \end{equation}
        counit
        \begin{equation}
            \varepsilon(x) = 0,
        \end{equation}
        and antipode
        \begin{equation}
            \chi(x) = -x.
        \end{equation}
    \end{prp}
    
    Compare and contrast this to the group algebra, \(\field G\), which is a Hopf algebra with
    \begin{equation}
        \Delta(g) = g \otimes g, \quad \varepsilon(g) = 1, \qand \chi(g) = g^{-1}.
    \end{equation}
    These are, in some ways, two opposite ends of the scale for how a Hopf algebra can behave.
    
    \begin{dfn}{Filtred Algebra}{}
        Let \(A\) be an associative algebra.
        We say that \(A\) is \define{\(\integers_{\ge 0}\)-filtred}\index{filtred algebra} if we have a chain of subspaces
        \begin{equation}
            0 = F_{-1}A \subseteq F_0A \subseteq F_1A \subseteq \dotsb \subseteq F_nA \subseteq \dotsb
        \end{equation}
        such that \(1 \in F_0 A\),
        \begin{equation}
            \bigcup_{n=0}^{\infty} F_nA = A,
        \end{equation}
        and \(F_iA \cdot F_jA \subseteq F_{i+j} A\).
    \end{dfn}
    
    \begin{dfn}{Degree Filtration}{}
        If \(A\) is an associative algebra generated by \(\{x_\alpha\}\) then we can define a filtration on \(A\) by declaring all \(x_\alpha\) to be of degree \(1\), and defining \(F_nA \coloneq (F_1A)^n\) to be formed of all terms of degree at most \(n\) (note that the degree of \(x_\alpha x_{\alpha'}\) is 2, as is the degree of \(x_\alpha^2\), and so on).
    \end{dfn}
    
    \begin{dfn}{Associated Graded Algebra}{}
        Given a filtred algebra, \(A\), we define the \defineindex{associated graded algebra} to be
        \begin{equation}
            \gr(A) \coloneq \bigoplus_{n=0}^{\infty} F_n(A)/F_{n-1}(A).
        \end{equation}
    \end{dfn}
    
    For the degree filtration the associated graded algebra is
    \begin{equation}
        \gr(A) = \bigoplus_{n=0}^{\infty} A_n
    \end{equation}
    where \(A_n\) is the span of all words of degree exactly \(n\).
    
    If \(\lie{g}\) is a Lie algebra then we can define a degree filtration on \(U(\lie{g})\) by setting the degree of any \(x \in \lie{g}\) to be \(1\).
    Then \(F_nU(\lie{g})\) is the image of \(\bigoplus_{k=0}^n \lie{g}^{\otimes k} \subset T(\lie{g})\) under the quotient map \(T(\lie{g}) \twoheadrightarrow T(\lie{g})/I\).
    Since in \(U(\lie{g})\) we have \(xy - yx = \bracket{x}{y}\) for \(x \in \lie{g}\) and \(y \in U(\lie{g})\) it follows that \(\bracket{F_iU(\lie{g})}{F_jU(\lie{g})} \subseteq F_{i + j - 1}U(\lie{g})\).
    It then follows that when we take \(F_nU(\lie{g}) / F_{n-1}U(\lie{g})\) in \(\gr(U(\lie{g}))\) we are quotenting by (among other things) all commutators of elements of degree less than \(n\).
    This makes \(\gr(U(\lie{g}))\) commutative.
    This in turn means that there is an epimorphism of associative algebras
    \begin{equation}
        S(\lie{g}) \twoheadrightarrow \gr(U(\lie{g})).
    \end{equation}
    This is a statement that \(S(A)\) is universal amongst commutative subalgebras of \(T(A)\), i.e., that any such subalgebra can be recognised by taking \(S(A)\) and applying some quotient to identify certain terms.
    
    \begin{dfn}{PBW Theorem}{}
        The homomorphism \(S(\lie{g}) \to \gr(U(\lie{g}))\) is an isomorphism.
    \end{dfn}
    
    \begin{crl}{}{}
        If \(\{x_i\}\) is a basis of \(\lie{g}\) we can fix an order on the basis.
        Then \(U(\lie{g})\) is spanned by ordered monomials \(\prod_i x_i^{n_i}\) with \(n_i \in \integers_{\ge 0}\).
    \end{crl}
    
    \begin{thm}{PBW Theorem}{}
        The ordered monomials described above are actually linearly independent, and thus form a basis for \(U(\lie{g})\).
    \end{thm}
    
    \begin{exm}{}{}
        Consider \(\specialLinearLie_2(\complex)\).
        This is a three-dimensional Lie algebra with generators \(\{e, h, f\}\).
        If we order them so that \(e < h < f\) then a basis for \(U(\specialLinearLie_2(\complex))\) is \(e^a h^b f^c\) with \(a, b, c \in \integers_{\ge 0}\).
    \end{exm}
    
    \section{Representation Theory of \texorpdfstring{\(\specialLinearLie_2(\complex)\)}{sl2}}
    The representation theory of all finite dimensional semisimple Lie algebras over \(\complex\) is almost entirely controlled by the representation theory of \(\specialLinearLie_2\).
    For this reason we'll now devote some time to the study of \(\specialLinearLie_2\).
    
    Recall that \(\specialLinearLie_2\) (working over \(\complex\)) is defined to consist of all traceless \(2 \times 2\) complex matrices.
    There is a basis for these given by
    \begin{equation}
        e = 
        \begin{pmatrix}
            0 & 1\\
            0 & 0
        \end{pmatrix}
        , \quad 
        h = 
        \begin{pmatrix}
            1 & 0\\
            0 & -1
        \end{pmatrix}
        ,\qand f =
        \begin{pmatrix}
            0 & 0\\
            1 & 0
        \end{pmatrix}
        .
    \end{equation}
    One can check that these satisfy the commutation relations
    \begin{equation}
        \bracket{h}{e} = 2e, \quad \bracket{h}{f} = -2f, \qand \bracket{e}{f} = h.
    \end{equation}
    We can then abstract the definition of \(\specialLinearLie_2\) to be \(\Span_{\complex}\{e, h, f\}\) subject to the above commutation relations, without needing an explicit matrix form.
    
    \begin{lma}{}{lma:weight space decomposition of sl2}
        Let \(V\) be a finite-dimensional representation of \(\specialLinearLie_2\).
        Then we have the decomposition
        \begin{equation}
            V \isomorphic \bigoplus_{\alpha \in \complex} V_\alpha
        \end{equation}
        where \(V_\alpha\) is the \defineindex{weight space}, defined to be the eigenspace
        \begin{equation}
            V_\alpha = \{v \in V \mid h \action v = \alpha v\}.
        \end{equation}
        \begin{proof}
            It is a fact that finite-dimensional \(\specialLinearLie_2\)-representations are completely reducible.
            Thus, we may assume without loss of generality that \(V\) is irreducible, since if it isn't we can decompose it into a sum of irreducibles and then treat each of these separately.
            
            Let \(W\) be the subspace of eigenvectors of \(h\).
            It is then sufficient to show that \(W = V\).
            To do this we show that \(W\) is a subrepresentation, that is, it's closed under \(h\), \(e\), and \(f\).
            Then irreducibility will imply that \(W = V\).
            
            By definition \(h\) acts as a scalar on \(W\), so \(W\) is closed under \(h\).
            For \(e\) let \(v \in W\) be an eigenvector of \(h\), that is \(hv = \alpha v\).
            Then a direct computation gives
            \begin{align}
                he \action v &= (\bracket{h}{e} + eh) \action v\\
                &= (2e + eh) \action v\\
                &= 2e \action v + eh \action v\\
                &= 2e \action v + \alpha e\action v\\
                &= (\alpha + 2) e \action v.
            \end{align}
            Thus, \(e \action v\) is again an eigenvector of \(h\), with eigenvalue \(\alpha + 2\).
            Similarly, one can show that \(f \action v\) is an eigenvector of \(h\) with eigenvalue \(\alpha - 2\).
            
            Thus, \(W\) is closed under the action of \(e\), \(h\), and \(f\), and thus is a subrepresentation, and so by irreducibility \(W = V\).
            Thus, if \(V\) is not irreducible is a direct sum of irreducibles, each of which is an eigenspace of \(h\) with some given eigenvalue \(\alpha\).
            We may as well sum over all possible eigenvalues, \(\alpha \in \complex\), and simply have \(V_\alpha = 0\) for many terms.
        \end{proof}
    \end{lma}
    
    \begin{exm}{}{}
        The definition of \(\specialLinearLie_2\) in terms of \(2 \times 2\) matrices gives us a natural action of \(\specialLinearLie_2\) on \(\complex^2\).
        Let \(\{e_1, e_2\}\) be the standard basis of \(\complex^2\).
        We have \(he_1 = e_1\) and \(he_2 = -e_2\), so we have two eigenvectors, and the corresponding eigenspaces \(V_1 = \complex e_1\) and \(V_{-1} = \complex e_2\).
        Then we have the following picture:
        \begin{equation}
            \begin{tikzcd}
                V_1 \arrow[loop right, "h"] \arrow[d, bend left, "f"]\\
                V_{-1} \arrow[loop right, "h"] \arrow[u, bend left, "e"]
            \end{tikzcd}
        \end{equation}
        The interpretation of this picture is that \(e\) and \(f\) act to shift the eigenvalue up and down by \(2\).
        Note that applying \(e\) to \(e_1\) gives \(ee_1 = 0\), and likewise, \(fe_2 = 0\).
        Thus, we can add \(0\) to the top and bottom of this picture:
        \begin{equation}
            \begin{tikzcd}
                0 \arrow[d, bend left, "f"]\\
                V_1 \arrow[loop right, "h"] \arrow[d, bend left, "f"] \arrow[u, bend left, "e"]\\
                V_{-1} \arrow[loop right, "h"] \arrow[u, bend left, "e"] \arrow[d, bend left, "f"]\\
                0 \arrow[u, bend left, "e"]
            \end{tikzcd}
        \end{equation}
    \end{exm}
    
    The picture above actually generalises to any finite dimensional representation, we can always draw a picture like the following:
    \begin{equation}
        \begin{tikzcd}
            0 \arrow[d, bend left, "f"]\\
            V_{\alpha + 2k} \arrow[u, "e"] \arrow[loop right, "h"] \arrow[d, bend left, "f"]\\
            \vdots \arrow[d, bend left, "f"] \arrow[u, bend left, "e"]\\
            V_{\alpha + 2} \arrow[loop right, "h"] \arrow[d, bend left, "f"] \arrow[u, "e", bend left]\\
            V_{\alpha} \arrow[loop right, "h"] \arrow[u, bend left, "e"] \arrow[d, bend left, "f"]\\
            V_{\alpha - 2} \arrow[loop right, "h"] \arrow[u, bend left, "e"] \arrow[d, bend left, "f"]\\
            \vdots \arrow[d, bend left, "f"] \arrow[u, bend left, "e"]\\
            V_{\alpha - 2\ell} \arrow[loop right, "h"] \arrow[d, bend left, "f"] \arrow[u, "e", bend left]\\
            0 \arrow[u, bend left, "e"]
        \end{tikzcd}
    \end{equation}
    The fact that we must always eventually get to \(0\) going either up or down is simply due to the fact that \(V\) is finite-dimensional.
    
    \begin{exm}{}{exm:homogeneous polynomials as sl2 rep}
        Consider the vector space \(S^k(\complex^2)\).
        We may identify this with the space of degree \(k\) homogenous polynomials (with coefficients in \(\complex\)).
        For example, for \(S^3(\complex^2)\) we identify \(e_1 \otimes e_1 \otimes e_1\) with \(x^3\), \(e_1 \otimes e_1 \otimes e_2 = e_1 \otimes e_2 \otimes e_1 = e_2 \otimes e_1 \otimes e_1\) with \(x^2y\), and so on.
        Basically, send \(e_1\) to \(x\), \(e_2\) to \(y\), and remember that all tensor products are symmetrised.
        Note then that we can identify \(S(\complex^2)\) and \(\complex[x, y]\) (more generally, \(S(\complex^m)\) and \(\complex[x_1, \dotsc, x_m]\)), an important identification in algebraic geometry.
        
        There is a representation of \(\specialLinearLie_2\) on \(\complex[x, y]\) given by
        \begin{equation}
            e = -y\partial_x, \quad h = -x \partial_x + y \partial_y, \qand f = -x\partial_y.
        \end{equation}
        Note that each operator preserves the total degree of any polynomial (so long as it doesn't send it to zero).
        Thus, we can identify submodules of degree \(k\)-polynomials.
        More generally, the above identification defines an action of \(\specialLinearLie_2\) on smooth functions \(\complex^2 \to \complex\), of which the \(S^k(\complex^2)\) are submodules.
        
        Consider \(S^k(\complex^2)\), which we now identify with the space of degree \(k\) polynomials in \(x\) and \(y\).
        A basis for this space consists of vectors
        \begin{equation}
            v_r = \binom{k}{r} x^r y^{k - r}.
        \end{equation}
        Acting on this with \(h\) we have
        \begin{multline}
            hv_r = (-x \partial_x + y\partial_y) \binom{k}{r}x^r y^{k-r}\\
            = -r\binom{k}{r}x^ry^{k-r} -(k - r)\binom{k}{r}x^ry^{k-r}) = (k - 2r)v_r, 
        \end{multline}
        so \(v_r\) has \(h\)-eigenvalue \(\alpha = k - 2r\).
        We also have
        \begin{equation}
            ev_r = -y\partial_x \binom{k}{r} x^r y^{k-r} = -r \binom{k}{r}x^{r-1} y^{k-r+1} = (r - k - 1) v_{r-1}
        \end{equation}
        and the \(h\)-eigenvalue of \(v_{r-1}\) is \(k - 2(r - 1) = k - 2r + 2 = \alpha + 2\).
        Similarly, we have
        \begin{equation}
            fv_r = -x \partial_y \binom{k}{r} x^r y^{k-r} = -(k-r) \binom{k}{r} x^{r + 1} y^{k - r - 1} = -(1 + r)v_{r + 1}
        \end{equation}
        and the \(h\)-eigenvalue of \(v_{r + 1}\) is \(k -2(r + 1) = k - 2r - 2 = \alpha - 2\).
        Then letting \(V_{k - 2r} = \complex v_{r}\) we have
        \begin{equation}
            \begin{tikzcd}
                V_{k - 2r + 2} \arrow[loop right, "h \sim k-2r"] \arrow[d, bend left, "f \sim -(1 + r)"]\\
                V_{k - 2r} \arrow[loop right, "h \sim k-2r"] \arrow[u, bend left, "e \sim r - k - 1"]
            \end{tikzcd}
        \end{equation}
        Here \(a \sim \lambda\) we mean that \(a\) acts by sending the basis vector of one space to the basis vector of the next multiplied by \(\lambda\).
        
        Let \(V(k) = S^k(\complex^2)\) be this \(\specialLinearLie_2\)-module.
        This is an irreducible module.
        Given any basis vector it lives in one of the \(V_\alpha\), and if we continuously act with \(e\) we eventually get \(v_0\).
        Then \(v_0\) generates this entire module by acting with \(f\) and scalar multiplication.
        Note that \(\dim V(k) = k + 1\), since we have the basis \(\{v_0, \dotsc, v_k\}\).
    \end{exm}
    
    The previous example actually captures all irreducible modules of \(\specialLinearLie_2\), as the following proves.
    The argument basically mirrors the argument above without reference to an explicit structure of polynomials.
    
    \begin{prp}{Classification of Finite Dimensional Irreducible \(\specialLinearLie_2\)-Modules}{}
        Let \(V\) be a \((k + 1)\)-dimensional \(\specialLinearLie_2\)-module.
        Then \(V \isomorphic V(k)\) with \(V(k)\) as defined in \cref{exm:homogeneous polynomials as sl2 rep}.
        \begin{proof}
            By the same argument as in the proof of \cref{lma:weight space decomposition of sl2} we know that the eigenvectors of \(h\) span \(V\) (which we're assuming is irreducible).
            Since \(V\) is finite-dimensional \(h\) has a finite number of eigenvalues, so there must be some \(h\)-eigenvector, \(v_0\), for which we have \(h v_0 = 0\).
            Consider \(f^k v_0\), as we have a finite-dimensional space, and thus finitely many eigenvectors of \(h\), we must have for some \(N\) that \(f^N v_0 = 0\), and suppose \(N\) is the smallest such value.
            If we take \(B = \{v_0, fv_0, \dotsc, f^{N-1}v_0\}\) then this is a submodule of \(V\), and thus is all of \(V\).
            Thus, knowing that \(V\) has dimension \(k + 1\) we know that \(N = k + 1\).
            In particular, \(f^{N-1}v_0 = f^kv_0\) is the last element of this basis.
            
            For what follows it's useful to absorb some scale factor into the basis, define \(v_r = f^r v_0 / r!\) for \(r = 0, \dotsc, k\).
            Then \(\{v_r\}\) is a basis of \(V\).
            
            All that remains is to show that the action of \(e\) and \(f\) on this basis is fully determined.
            Starting with \(e\) we use the fact that \(hv_r = (\alpha_0 - 2r)v_r\) where \(\alpha_0\) is the \(h\)-eigenvalue of \(v_0\).
            We then have
            \begin{align}
                ev_0 &= 0\\
                ev_1 &= efv_0 = \bracket{e}{f}v_0 + fev_0 = hv_0 + 0 = \alpha_0 v_0\\
                ev_2 &= efv_1/2 = \bracket{e}{f}v_1/2 + fev_1/2 = hv_1/2 + \alpha_0fv_0/2\\
                &= (\alpha_0 - 2)v_1/2 + \alpha_0v_1/2 = (\alpha_0 - 1)v_1.
            \end{align}
            We thus make the induction hypothesis that
            \begin{equation}
                ev_n = (\alpha_0 - n + 1)v_{n-1}.
            \end{equation}
            Assuming the equivalent statement for \(v_{n - 1}\) holds we then have
            \begin{align}
                ev_n &= efv_{n-1}/n = \bracket{e}{f}v_{n-1}/n + fev_{n-1}/n\\
                &= hv_{n-1}/n + fev_{n-1}/n\\
                &= (\alpha_0 - 2n + 2)v_{n-2} + (\alpha_0 - n + 2) fv_{n-2}/n\\
                &= (\alpha_0 - 2n + 2)v_{n-1}/n + (n - 1)(\alpha_0 - n + 2)v_{n-1}/n\\
                &= (\alpha_0 - n + 1)v_{n-1}.
            \end{align}
            
            This shows that the structure of \(V\) is entirely determined by \(\alpha_0\), we now show that \(\alpha_0\) is fixed.
            We know that \(fv_k = 0\), and we have
            \begin{align}
                efv_k &= \bracket{e}{f}v_k + fev_k = hv_k + (\alpha_0 - k + 1)fv_{k-1}\\
                &= (\alpha_0 - 2k)v_k + (\alpha_0 - k + 1)k v_k\\
                &= (k + 1)(\alpha_0 - k)v_{k-1}.
            \end{align}
            For this to vanish, given that \(k + 1\), the dimension, is positive (for \(k + 1 = 0\) clearly all zero dimensional \(\specialLinearLie_2\)-modules are isomorphic), and thus \(\alpha_0 = k\) is fixed, and so as soon as we know the dimension of a finite-dimensional irreducible \(\specialLinearLie_2\)-module we know everything about it.
        \end{proof}
    \end{prp}
    
    \begin{dfn}{Weight Vectors}{}
        Let \(V\) be an \(\specialLinearLie_2\)-module.
        We call eigenvectors of \(h\) \define{weight vectors}\index{weight vector}, and the eigenvalue is called its weight.
        If \(v\) is a weight vector and \(ev = 0\) we call \(v\) a \defineindex{highest weight vector}, similarly, if \(fv = 0\) we call \(v\) a \defineindex{lowest weight vector}.
    \end{dfn}
    
    The above proposition then says that any finite-dimensional irreducible \(\specialLinearLie_2\)-module is generated by a highest weight vector, \(v_0\).
    
    \section{Classification of Semisimple Lie Algebras Over \texorpdfstring{\(\complex\)}{C}}
    The steps followed for classifying irreducible finite-dimensional irreducible \(\specialLinearLie_2\)-modules actually generalise remarkably well to classifying not just representations of other Lie algebras, but classifying a whole type of algebra, just by studying the adjoint representations in which these algebras act on themselves.
    
    There were three steps we followed with \(\specialLinearLie_2\).
    First, decompose \(V\) into eigenspaces of \(h\).
    Second, use the commutation relations to determine how \(e\) and \(f\) act on these eigenspaces.
    Finally, use the irreducibility of the module to show that it is generated by a single highest weight vector.
    
    In order to apply this method to other Lie algebras we'll need to generalise some things.
    The main one is that instead of just a single operator, \(h\), we end up with a whole subalgebra of operators, \(\lie{h}\).
    Before we get to this we need a few definitions.
    
    \begin{dfn}{Semisimple and Nilpotent Elements}{}
        Let \(\lie{g}\) be a Lie algebra.
        We say that \(x \in \lie{g}\) is \defineindex{semisimple} if \(\ad_x\) is diagonalisable, and \defineindex{nilpotent} if \(\ad_x\) is nilpotent.
    \end{dfn}
    
    For example, in \(\specialLinearLie_2\) \(h\) is semisimple, since in the adjoint representation, with the ordered basis \(\{e, h, f\}\), we have
    \begin{equation}
        \ad_h = 
        \begin{pmatrix}
            2\\
            & 0\\
            && -2
        \end{pmatrix}
        .
    \end{equation}
    On the other hand, \(e\) and \(f\) are nilpotent, since in the adjoint representation
    \begin{equation}
        \ad_e =
        \begin{pmatrix}
            0 & -2 & 0\\
            0 & 0 & 1\\
            0 & 0 & 0
        \end{pmatrix}
        , \qand \ad_f =
        \begin{pmatrix}
            0 & 0 & 0\\
            -1 & 0 & 0\\
            0 & 2 & 0
        \end{pmatrix}
        ,
    \end{equation}
    both of which have vanishing third power.
    
    An abelian subalgebra, \(\lie{h} \subseteq \lie{g}\) is called \defineindex{toral}\footnote{This name comes from the fact that if \(G\) is a Lie group with Lie algebra \(\lie{g}\) then any toral subgroup, \(H\), will have a Lie algebra isomorphic to \(\lie{h}\). In turn, a toral subgroup is a Lie subgroup of \(G\) which is isomorphic to a torus.} if it consists of only semisimple elements.
    For any toral subalgebra we have the following decomposition:
    \begin{equation}
        \lie{g} = \bigoplus_{\alpha \in \lie{h}^*} \lie{g}_\alpha
    \end{equation}
    where
    \begin{equation}
        \lie{g}_\alpha = \{x \in \lie{g}_\alpha \mid \ad_h(x) = \bracket{h}{x} = \alpha(h)x \text{ for } h \in \lie{h}\}.
    \end{equation}
    This is simply the weight space decomposition of \(\lie{g}\) viewed as an \(\lie{h}\)-module through (restricted) adjoint action.
    
    One can show that
    \begin{equation}
        \bracket{\lie{g}_\alpha}{\lie{g}_\beta} \subseteq \lie{g}_{\alpha + \beta}.
    \end{equation}
    In particular, \(\lie{g}_0\) is a Lie subalgebra, since \(\bracket{\lie{g}_0}{\lie{g}_0} \subseteq \lie{g}_0\), and \(\lie{h} \subseteq \lie{g}_0\).
    
    \begin{dfn}{Cartan Subalgebra}{}
        If \(\lie{g}\) is a Lie algebra with toral subalgebra, \(\lie{h}\), such that, with the notation above, we have \(\lie{g}_0 = \lie{h}\) then we call \(\lie{h}\) a \defineindex{Cartan subalgebra} of \(\lie{g}\).
    \end{dfn}
    
    Note that while Cartan subalgebras aren't unique they are all conjugate, so we typically speak of \emph{the} Cartan subalgebra, when it exists.
    
    When we have a Cartan subalgebra we can change the decomposition to
    \begin{equation}
        \lie{g} = \lie{h} \oplus \bigoplus_{\alpha \in \Delta} \lie{g}_\alpha
    \end{equation}
    where \(\Delta = \{\alpha \in \lie{h}^*\setminus 0 \mid \lie{g}_\alpha \ne 0\}\) is the subset of \(\lie{h}^*\) for which \(\alpha \ne 0\) and \(\lie{g}_\alpha\) is nontrivial.
    We call \(\Delta\) a set of \define{simple roots}\index{simple roots}.
    
    For example, for \(\specialLinearLie_2\) we have the Cartan subalgebra \(\lie{h} = \complex h\).
    In this case we have \(\lie{g}_{2} = \complex e\) and \(\lie{g}_{-2} = \complex f\), and we get the decomposition
    \begin{equation}
        \specialLinearLie_2 = \complex h \oplus \complex e \oplus \complex f.
    \end{equation}
    
    \subsection{Root Systems}
    \begin{dfn}{Reflection}{}
        Let \(E\) be a Euclidean space with inner product \(\rootProd{-}{-} \colon E \otimes E \to \reals\).
        A \defineindex{reflection} is a linear map \(s \colon E \to E\) such that there exists some \(v \in E\) such that \(s(v) = -v\) and the hyperplane \((\reals v)^{\perp}\) is fixed pointwise by \(s\).
        Then we call \(s\) a reflection along \(v\).
    \end{dfn}
    
    Note that given \(v\) the following formula gives a reflection along \(v\):
    \begin{equation}
        s_v(w) = w - 2\frac{\rootProd{v}{w}}{\rootProd{v}{v}} v.
    \end{equation}
    
    \begin{dfn}{Root System}{}
        Let \(E\) be a real Euclidean space with inner product \(\rootProd{-}{-}\).
        A \defineindex{root system}, \(\Phi\), in \(E\) is a finite set of nonzero vectors or \define{roots}\index{root} such that
        \begin{enumerate}
            \item \(\Span_{\reals} \Phi = E\);
            \item if \(\alpha \in \Phi\) then \(c \alpha \in \Phi\) only for \(c = \pm 1\);
            \item \(s_\alpha(\Phi) = \Phi\) for \(\alpha \in \Phi\);
            \item \(2\rootProd{\alpha}{\beta}/\rootProd{\alpha}{\alpha} \in \integers\).
        \end{enumerate}
        Sometimes the second condition isn't required, root systems for which the second condition holds are known as \define{reduced root systems}\index{reduced root system}\index{root system!reduced}.
        
        The \defineindex{rank} of the root system is \(\dim_{\reals} E\).
    \end{dfn}
    
    \begin{dfn}{Positive and Simple Roots}{}
        Given a root system we can make arbitrary choice of a hyperplane containing none of the roots.
        We then choose one side of this hyperplane, again, arbitrarily, and declare roots in this half to be \define{positive}\index{positive root}.
        The \define{simple roots}\index{simple root} are the positive roots which cannot be written as a sum, \(\alpha + \beta\), of two elements of the positive roots, \(\alpha\) and \(\beta\), alternatively, the simple roots are precisely the subset of the positive roots which generate the positive roots through linear combinations with positive integral coefficients.
    \end{dfn}
    
    \begin{ntn}{}{}
        Notation varies here, but we'll call \(\Phi\) the set of roots, \(\Pi\) the set of positive roots and \(\Delta\) the set of simple roots.
    \end{ntn}
    
    It turns out that root systems actually turn up in many different areas of mathematics, but we'll focus on how they're relevant to Lie algebras.
    
    It turns out that, up to scaling, there is only one rank 1 root system.
    For reasons we'll get into later this root system is known as \(\dynkin{A}{1}\).
    This root system is depicted in \cref{fig:root system A1}.
    There are also only four rank 2 root systems, known as \(\dynkin{A}{1} \oplus \dynkin{A}{1}\) (being two orthogonal copies of \(\dynkin{A}{1}\)), \(\dynkin{A}{2}\), \(\dynkin{B}{2}\) (or \(\dynkin{C}{2}\)) and \(\dynkin{G}{2}\).
    These are depicted in \cref{fig:root system rank 2}.
    \Cref{tab:root systems of rank 2} lists the roots, \(\Phi\), positive roots, \(\Pi\), and simple roots, \(\Delta\).
    In all cases we've chosen to label our roots by expressing them in terms of two chosen simple roots, \(\alpha\) and \(\beta\).
    
    \begin{figure}
        \centering
        \tikzsetnextfilename{root-system-A1}
        \begin{tikzpicture}
            \draw [->] (0, 0) -- ++ (2, 0) node [right] {\(\alpha\)};
            \draw [->] (0, 0) -- ++ (-2, 0) node [left] {\(-\alpha\)};
            \fill (0, 0) circle [radius = 0.03cm];
        \end{tikzpicture}
        \caption{The \(\dynkin{A}{1}\) root system, \(\Phi = \{\alpha, -\alpha\}\), with chosen positive roots, \(\Pi = \{\alpha\}\), and simple roots, \(\Delta = \{\alpha\}\).}
        \label{fig:root system A1}
    \end{figure}
    
    \begin{figure}
        \centering
        \begin{subfigure}{0.45\textwidth}
            \centering
            \tikzsetnextfilename{root-system-A1+A1}
            \begin{tikzpicture}
                \draw [->] (0, 0) -- ++ (2, 0) node [right] {\(\alpha\)};
                \draw [->] (0, 0) -- ++ (-2, 0) node [left] {\(-\alpha\)};
                \draw [->] (0, 0) -- ++ (0, 2) node [above] {\(\beta\)};
                \draw [->] (0, 0) -- ++ (0, -2) node [below] {\(\mathllap{-}\beta\)};
                \fill (0, 0) circle [radius = 0.03cm];
            \end{tikzpicture}
            \caption{The \(\dynkin{A}{1} \oplus \dynkin{A}{1}\) root system.}
            \label{fig:root system A1+A1}
        \end{subfigure}
        \begin{subfigure}{0.45\textwidth}
            \centering
            \tikzsetnextfilename{root-system-A2}
            \begin{tikzpicture}
                \draw [->] (0, 0) -- ++ (2, 0) node [right] {\(\alpha\)};
                \draw [->] (0, 0) -- ++ (pi/3 r:2) node [right] {\(\alpha + \beta\)};
                \draw [->] (0, 0) -- ++ (2*pi/3 r:2) node [left] {\(\beta\)};
                \draw [->] (0, 0) -- ++ (-2, 0) node [left] {\(-\alpha\)};
                \draw [->] (0, 0) -- ++ (4*pi/3 r:2) node [left] {\(-\alpha - \beta\)};
                \draw [->] (0, 0) -- ++ (5*pi/3 r:2) node [right] {\(-\beta\)};
                \fill (0, 0) circle [radius = 0.03cm];
            \end{tikzpicture}
            \caption{The \(\dynkin{A}{2}\) root system.}
            \label{fig:root system A2}
        \end{subfigure}
        
        \begin{subfigure}{0.7\textwidth}
            \centering
            \tikzsetnextfilename{root-system-B2}
            \begin{tikzpicture}
                \draw [->] (0, 0) -- ++ (2, 0) node [right] {\(\alpha\)};
                \draw [->] (0, 0) -- ++ (45:{2*sqrt(2)}) node [right] {\(2\alpha + \beta\)};
                \draw [->] (0, 0) -- ++ (0, 2) node [above] {\(\alpha + \beta\)};
                \draw [->] (0, 0) -- ++ (135:{2*sqrt(2)}) node [left] {\(\beta\)};
                \draw [->] (0, 0) -- ++ (-2, 0) node [left] {\(-\alpha\)};
                \draw [->] (0, 0) -- ++ (225:{2*sqrt(2)}) node [left] {\(-2\alpha - \beta\)};
                \draw [->] (0, 0) -- ++ (0, -2) node [below] {\(\mathllap{-}\alpha - \beta\)};
                \draw [->] (0, 0) -- ++ (-45:{2*sqrt(2)}) node [right] {\(-\beta\)};
                \fill (0, 0) circle [radius = 0.03cm];
            \end{tikzpicture}
            \caption{The \(\dynkin{B}{2}\) root system.}
            \label{fig:root system B2}
        \end{subfigure}
        \begin{subfigure}{0.7\textwidth}
            \centering
            \tikzsetnextfilename{root-system-G2}
            \begin{tikzpicture}
                \draw [->] (0, 0) -- ++ (2, 0) node [right] {\(\alpha\)};
                \draw [->] (0, 0) -- ++ (pi/3 r:2) node [above] {\(2\alpha + \beta\)};
                \draw [->] (0, 0) -- ++ (2*pi/3 r:2) node [above] {\(\alpha + \beta\)};
                \draw [->] (0, 0) -- ++ (-2, 0) node [left] {\(-\alpha\)};
                \draw [->] (0, 0) -- ++ (4*pi/3 r:2) node [below] {\(\mathllap{-}2\alpha - \beta\)};
                \draw [->] (0, 0) -- ++ (5*pi/3 r:2) node [below] {\(\mathllap{-}\alpha - \beta\)};
                \begin{scope}[rotate=pi/6 r, scale={sqrt(3)}]
                    \draw [->] (0, 0) -- ++ (2, 0) node [right] {\(3\alpha + \beta\)};
                    \draw [->] (0, 0) -- ++ (pi/3 r:2) node [above] {\(3\alpha + 2\beta\)};
                    \draw [->] (0, 0) -- ++ (2*pi/3 r:2) node [left] {\(\beta\)};
                    \draw [->] (0, 0) -- ++ (-2, 0) node [left] {\(\mathllap{-}3\alpha - \beta\)};
                    \draw [->] (0, 0) -- ++ (4*pi/3 r:2) node [below] {\(\mathllap{-}3\alpha - 2\beta\)};
                    \draw [->] (0, 0) -- ++ (5*pi/3 r:2) node [right] {\(-\beta\)};
                \end{scope}
                \fill (0, 0) circle [radius = 0.03cm];
            \end{tikzpicture}
            \caption{The \(\dynkin{G}{2}\) root system.}
            \label{fig:root system G2}
        \end{subfigure}
        
        \caption{The rank \(2\) root systems.}
        \label{fig:root system rank 2}
    \end{figure}
    
    \begin{table}
        \centering
        \caption[Root systems of rank at most 2]{Information on the root systems of rank at most \(2\). Notice that \(\Phi = \Pi \sqcup (-\Pi)\) and in all cases we have chosen our naming of roots such that \(\Delta = \{\alpha, \beta\}\). Notice that the positive roots, \(\Pi\), are always found in the cone between the simple roots.}
        \label{tab:root systems of rank 2}
        \small
        \begin{tabular}{clll}
            \toprule
            & \(\Phi\) & \(\Pi\) & \(\Delta\) \\ \midrule
            \(\dynkin{A}{1}\) & \(\pm\alpha\) & \(\alpha\) & \(\alpha\)\\
            \(\dynkin{A}{1} \oplus \dynkin{A}{1}\) & \(\pm\alpha\), \(\pm\beta\) & \(\alpha\), \(\beta\) & \(\alpha\), \(\beta\)\\
            \(\dynkin{A}{2}\) & \(\pm \alpha\), \(\pm \beta\), \(\pm(\alpha + \beta)\) & \(\alpha\), \(\beta\), \(\alpha + \beta\) & \(\alpha\), \(\beta\)\\
            \(\dynkin{B}{2}\) & \(\pm\alpha\), \(\pm\beta\), \(\pm(\alpha + \beta)\), \(\pm(2\alpha + \beta)\) & \(\alpha\), \(\beta\), \(\alpha + \beta\), \(2\alpha + \beta\) & \(\alpha\), \(\beta\)\\
            \(\dynkin{G}{2}\) & \(\pm \alpha\), \(\pm\beta\), \(\alpha + \beta\), \(\pm(2\alpha + \beta)\), \(\pm(3\alpha + \beta)\) & \(\alpha\), \(\beta\), \(\alpha + \beta\), \(2\alpha + \beta\), \(3\alpha + \beta\) & \(\alpha\), \(\beta\) \\ \bottomrule
        \end{tabular}
    \end{table}
    
    \subsection{Connection to Semisimple Lie Algebras}
    The reason that these root systems, as abstract subsets of some Euclidean space, are relevant is that given a semisimple Lie algebra the set of simple roots, \(\Delta\), (that is \(\alpha \in \lie{h}^*\) such that \(\lie{g}_\alpha \ne 0\)) is actually the set of simple roots of a corresponding root system.
    
    \begin{thm}{}{}
        Let \(\lie{g}\) be a semisimple Lie algebra over \(\complex\), with Cartan subalgebra \(\lie{h}\).
        Let \(E\) be a Euclidean space such that the complexification of \(E\) is \(\lie{h}^*\).
        Then
        \begin{itemize}
            \item \(\Delta\) forms a reduced root system in \(E\);
            \item Eigenspaces are one-dimensional, \(\lie{g}_\alpha \isomorphic \complex\) for \(\alpha \in \Delta\);
            \item \(\bracket{\lie{g}_\alpha}{\lie{g}_\beta} = \lie{g}_{\alpha + \beta}\).
        \end{itemize}
    \end{thm}
    
    It turns out that these properties are exactly as is required in order for the following result to hold.
    
    \begin{thm}{}{}
        There is a bijection between semisimple Lie algebras over \(\complex\) and reduced root systems.
    \end{thm}
    
    We've constructed the root system from a semisimple Lie algebra.
    Since these objects are in bijection we can construct a semisimple Lie algebra in a unique way from a given root system.
    The process is unfortunately not that insightful, and basically reduces to imposing a bunch of relations on a free Lie algebra according to information encoded in the root system.
    The nice thing about this result is that it turns out to be much simpler to classify all of the finite-rank root systems.
    
    \begin{dfn}{Cartan Matrix}{}
        A (finite-type) \defineindex{Cartan matrix} is an \(n \times n\) matrix, \(A = (a_{ij})_{1 \le i, j \le n}\) such that
        \begin{itemize}
            \item \(a_{ii} = 2\) and \(a_{ij} \in \integers_{\le 0}\) for \(i \ne j\);
            \item \(A\) is symmetrisable (there exists some diagonal matrix, \(D\), such that \(DA\) is a symmetric matrix);
            \item \(A\) is positive (all principle minors of \(A\) are positive).
        \end{itemize}
    We consider two Cartan matrices to be the same if they are equal up to a simultaneous permutation of the rows and columns.
    That is, \(A\) and \(B\) are the same if \(a_{i,j} = b_{\sigma(i),\sigma(j)}\) for some \(\sigma \in S_n\).
    \end{dfn}
    
    \begin{lma}{}{}
        Let \(\Phi\) be a root system with chosen simple roots, \(\Delta = \{\alpha_1, \dotsc, \alpha_n\}\).
        Define a matrix \(A = (a_{ij})_{1 \le i, j \le n}\) by
        \begin{equation}
            a_{ij} \coloneq \frac{2\rootProd{\alpha_i}{\alpha_j}}{\rootProd{\alpha_i}{\alpha_i}}.
        \end{equation}
        This is a Cartan matrix, and is uniquely determined by the root system (up to permutation of the labels of our simple roots).
        Conversely, given a Cartan matrix one can construct a root system with that Cartan matrix.
    \end{lma}
    
    The above result means that classifying Cartan matrices classifies root systems, which in turn classifies semisimple Lie algebras.
    
    We're now ready to state the reverse process, for going from a root system or Cartan matrix to the corresponding semisimple Lie algebra.
    
    \begin{prp}{}{}
        Let \(A = (a_{ij})\) be an \(n \times n\) Cartan matrix.
        Let \(\lie{g}\) be the Lie algebra generated by \(\{e_i, h_i, f_i \mid 1 \le i \le n\}\) subject to the relations
        \begin{itemize}
            \item \(\bracket{h_i}{e_j} = a_{ij}e_j\);
            \item \(\bracket{h_i}{f_j} = -a_{ij}f_j\);
            \item \(\bracket{e_i}{f_j} = \delta_{ij}h_i\);
            \item \(\bracket{h_i}{h_j} = 0\);
            \item \((\ad_{e_i})^{1 - a_{ij}}e_j = 0\);
            \item \((\ad_{f_i})^{1 - a_{ij}}f_i = 0\).
        \end{itemize}
        Then this is a semisimple Lie algebra over \(\complex\) and is uniquely determined by \(A\).
    \end{prp}
    
    The last two relations above are called the \defineindex{Serre relations}.
    
    Note that in the above \(1 - a_{ij}\) is always positive, and \((\ad_{e_i})^{k}\) means the \(k\)-nested bracket with \(e_i\), for example, \((\ad_{e_i})^{3}(x) = \bracket{e_i}{\bracket{e_i}{\bracket{e_i}{x}}}\).
    
    \begin{exm}{\(\specialLinearLie_2\)}{}
        Consider \(\specialLinearLie_2\).
        We will demonstrate here that \(\specialLinearLie_2\) is precisely the semisimple Lie algebra corresponding to \(\dynkin{A}{1}\).
        
        To do so we start with finding the Cartan matrix of \(\dynkin{A}{1}\).
        Since \(\Phi = \{\pm\alpha\}\) and \(\Delta = \{\alpha\}\) this Cartan matrix is just \(1 \times 1\), with the single entry being
        \begin{equation}
            a_{1 1} = \frac{2\rootProd{\alpha}{\alpha}}{\rootProd{\alpha}{\alpha}} = 2.
        \end{equation}
        So, \(A = (2)\), of course the diagonal of the Cartan matrix is, by definition, always 2s, so we didn't actually need this calculation.
        
        Then we can take \(\lie{g}\) to be the Lie algebra generated by \(\{e_1, h_1, f_1\}\) subject to the relations
        \begin{itemize}
            \item \(\bracket{h_1}{e_1} = a_{11}e_1 = 2e_2\);
            \item \(\bracket{h_1}{f_1} = -a_{11}e_1 = -2f_1\);
            \item \(\bracket{e_1}{f_1} = \delta_{11}h_1 = h_1\);
            \item \(\bracket{h_1}{h_1} = 0\).
        \end{itemize}
        The last of these is always true, the first three are exactly the relations on \(\{e, h, f\}\) which we impose on \(\specialLinearLie_2\), so \(\lie{g} \isomorphic \specialLinearLie_2\).
        
        More generally, if we construct a Lie algebra from an arbitrary root system and take the subalgebra generated by \(e_i\), \(h_i\) and \(f_i\) for fixed \(i\) then, since \(a_{ii} = 2\) we always get a copy of \(\specialLinearLie_2\).
    \end{exm}
    
    \begin{exm}{\(\specialLinearLie_3\)}{}
        Let's go one dimension up and consider \(\dynkin{A}{2}\).
        This root system has \(\Phi = \{\pm\alpha, \pm\beta, \pm(\alpha + \beta)\}\) and \(\Delta = \{\alpha, \beta\}\).
        Let \(\alpha_1 = \alpha\) and \(\alpha_2 = \beta\) in what follows.
        Then the Cartan matrix has diagonals 2.
        Looking at the root diagram in \cref{fig:root system A2} the angle between \(\alpha\) and \(\beta\) is \(2\pi/3\), and both roots are the same length.
        Thus, \(\rootProd{\alpha}{\beta} = \rootProd{\alpha_1}{\alpha_2} = \cos(2\pi/3) = -1/2\), and thus
        \begin{equation}
            a_{12} = \frac{2\rootProd{\alpha_1}{\alpha_1}}{\rootProd{\alpha_1}{\alpha_1}} = -1, \qand a_{21} = \frac{2\rootProd{\alpha_2}{\alpha_1}}{\rootProd{\alpha_2}{\alpha_2}} = 1 
        \end{equation}
        having chosen a normalisation such that \(\rootProd{\alpha_1}{\alpha_1} = \rootProd{\alpha_2}{\alpha_2} = 1\).
        The Cartan matrix of \(\dynkin{A}{2}\) is thus
        \begin{equation}
            A = 
            \begin{pmatrix}
                2 & -1\\
                -1 & 2
            \end{pmatrix}
            .
        \end{equation}
        
        The corresponding semisimple Lie algebra is generated by \(\{e_1, e_2, h_1, h_2, f_1, f_2\}\) subject to
        \begin{itemize}
            \item \(\bracket{h_1}{e_1} = 2e_1\), \(\bracket{h_1}{e_2} = -e_2\), \(\bracket{h_2}{e_1} = -e_1\), \(\bracket{h_2}{e_2} = 2e_2\);
            \item \(\bracket{h_1}{f_1} = -2e_1\), \(\bracket{h_1}{f_2} = f_2\), \(\bracket{h_2}{f_1} = f_1\), \(\bracket{h_2}{f_2} = -2f_2\);
            \item \(\bracket{e_1}{f_1} = h_1\), \(\bracket{e_2}{f_2} = h_2\), \(\bracket{e_1}{f_2} = \bracket{e_2}{f_1} = 0\);
            \item \(\bracket{h_1}{h_2} = 0\);
            \item \((\ad_{e_1})^{1 - a_{12}}e_2 = (\ad_{e_1})^2e_2 = \bracket{e_1}{\bracket{e_1}{e_2}} = 0\), \(\bracket{e_2}{\bracket{e_2}{e_1}} = 0\);
            \item \(\bracket{f_1}{\bracket{f_1}{f_2}} = \bracket{f_2}{\bracket{f_2}{f_1}} = 0\).
        \end{itemize}
        This algebra is isomorphic to \(\specialLinearLie_3\).
    \end{exm}
    
    \begin{exm}{\(\specialOrthogonalLie_5\)}{}
        Consider the root system \(\dynkin{B}{3}\), which has \(\Delta = \{\alpha_1, \alpha_2\}\).
        Looking at the root diagram, \cref{fig:root system B2}, we see that if we choose \(\alpha = \alpha_1\) to have length \(1\) then \(\alpha_2 = \beta\) has length \(\sqrt{2}\), and the angle between \(\alpha\) and \(\beta\) is \(3\pi/4\), and \(\cos(3\pi/4) = -\sqrt{2}/2\).
        Thus,
        \begin{align*}
            a_{12} &= \frac{2\rootProd{\alpha_1}{\alpha_2}}{\rootProd{\alpha_1}{\alpha_1}} = \frac{2\norm{\alpha_1}\norm{\alpha_2}\cos(3\pi/4)}{\norm{\alpha_1}^2} = \frac{2 \cdot 1 \cdot \sqrt{2} \cdot (-\sqrt{2}/2)}{1} = -2,\\
            a_{21} &= \frac{2\rootProd{\alpha_2}{\alpha_1}}{\rootProd{\alpha_2}{\alpha_2}} = \frac{2\norm{\alpha_2}\norm{\alpha_1} \cos(3\pi/4)}{\norm{\alpha_2}^2} = \frac{2 \cdot \sqrt{2} \cdot 1 \cdot (-\sqrt{2}/2)}{(\sqrt{2})^2} = -1.
        \end{align*}
        So, the Cartan matrix of \(\dynkin{B}{3}\) is
        \begin{equation}
            A =
            \begin{pmatrix}
                2 & -2\\
                -1 & 2
            \end{pmatrix}
            .
        \end{equation}
        Note that this is symmetrisable:
        \begin{equation}
            D = 
            \begin{pmatrix}
                1 & 0\\
                0 & 2
            \end{pmatrix}
            \implies DA = 
            \begin{pmatrix}
                2 & -2\\
                -2 & 4
            \end{pmatrix}
            .
        \end{equation}
        
        The corresponding Lie algebra is generated by \(\{e_1, e_2, h_1, h_2, f_1, f_2\}\), subject to the relations that
        \begin{itemize}
            \item \(\bracket{h_1}{e_1} = 2e_1\), \(\bracket{h_2}{e_2} = 2e_2\), \(\bracket{h_1}{e_2} = -2e_2\), \(\bracket{h_2}{e_1} = -e_1\);
            \item \(\bracket{h_1}{f_1} = -2f_1\), \(\bracket{h_2}{f_2} = -2f_2\), \(\bracket{h_1}{f_2} = 2f_2\), \(\bracket{h_2}{f_1} = f_1\);
            \item \(\bracket{e_1}{f_1} = h_1\), \(\bracket{e_2}{f_2} = h_2\), \(\bracket{e_1}{f_2} = \bracket{e_2}{f_1} = 0\);
            \item \(\bracket{h_i}{h_j} = 0\) for \(i, j \in \{1, 2\}\);
            \item \((\ad_{e_1})^{1 - a_{12}}e_2 = (\ad_{e_1})^3e_2 = \bracket{e_1}{\bracket{e_1}{\bracket{e_1}{e_2}}} = 0\), \(\bracket{e_2}{\bracket{e_2}{e_1}} = 0\);
            \item \(\bracket{f_1}{\bracket{f_1}{\bracket{f_1}{f_2}}} = \bracket{f_2}{\bracket{f_2}{f_1}} = 0\).
        \end{itemize}
        This Lie algebra is isomorphic to that of \(\specialOrthogonalLie_5\).
    \end{exm}
    
    Notice that in all of these examples, and more generally by inspecting the relations defining \(\lie{g}\), we always have that \(\{e_i, h_i, f_i\}\) (for fixed \(i\)) generates a copy of \(\specialLinearLie_2\).
    These copies of \(\specialLinearLie_2\) are such that the \(e_i\)s and \(f_j\)s of distinct copies don't \enquote{interact} (i.e., they commute).
    The interaction only occurs when \(h_i\)s are involved.
    The \(h_i\)s themselves form a subalgebra, which is exactly the Cartan subalgebra, which we can see from these relations is always abelian.
    
    \subsection{Classification of Cartan Matrices}
    The final part to classifying all finite-dimensional semisimple Lie algebras over \(\complex\) is to classify all finite-type Cartan matrices.
    This has been done.
    The tidiest way to frame this classification is to encode the information of a root system into a labelled graph, and then it turns out that all of the corresponding graphs either fall into one of four families of graphs, or one of five exceptional cases.
    
    First, given an \(n \times n\) Cartan matrix, \(A\), or the corresponding root system, \((\Phi, \Pi, \Delta)\), we can construct a labelled graph as follows:
    \begin{itemize}
        \item The nodes are the simple roots, \(\alpha_i \in \Delta\);
        \item Draw \(a_{ij}a_{ji}\) edges between \(\alpha_i\) and \(\alpha_j\) (\(i \ne j\));
        \item If \(\alpha_i\) is longer than \(\alpha_j\) draw an arrow on the edge pointing towards the shorter root.
    \end{itemize}
    The graph that we get is called the \defineindex{Dynkin diagram} of the root system/Cartan matrix.
    
    \begin{exm}{}{}
        Consider \(\dynkin{A}{2}\), this has two simple roots, \(\alpha_1\) and \(\alpha_2\).
        We have \(a_{12} a_{21} = (-1) (-11) = 1\), and so the corresponding Dynkin diagram is
        \begin{equation}
            \tikzsetnextfilename{dynkin-A2}
            \begin{tikzpicture}
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_1}\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_2}\)};
                \draw (0, 0) -- (1, 0);
            \end{tikzpicture}
        \end{equation}
        
        Now consider \(\dynkin{B}{2}\), this has two simple roots, \(\alpha_1\) and \(\alpha_2\).
        We have \(a_{12} a_{21} = (-2)(-1) = 2\), and \(\alpha_2\) is longer than \(\alpha_1\), so the corresponding Dynkin diagram is
        \begin{equation}
            \tikzsetnextfilename{dynkin-B2}
            \begin{tikzpicture}
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(\alpha_1\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(\alpha_2\)};
                \draw (0, 0.03) -- ++ (1, 0);
                \draw (0, -0.03) -- ++ (1, 0);
                \draw [-{>[length=1.5mm,width=3mm]}, xshift=0.49cm] (0, 0) -- ++ (-0.001, 0);
            \end{tikzpicture}
        \end{equation}
    \end{exm}
    
    This process is invertible, since the Dynkin diagram fully encodes the angles between roots and their relative lengths (well, it encodes which is longer, the actual relative length can then be computed by requiring that the Cartan matrix have integral entries).
    
    \begin{thm}{Classification of Root Systems}{}
        Every (finite-type) \(n \times n\) Cartan matrix and its corresponding root system has a Dynkin diagram which is in one of the following infinite families (all with \(n\) vertices),
        \begin{gather}
            \tikzsetnextfilename{dynkin-An}
            \begin{tikzpicture}[font=\small]
                \node at (-1, 0) {\normalsize\(\dynkin{A}{n}\)};
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_1}\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_2}\)};
                \fill (2, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_3}\)};
                \fill (3, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_4}\)};
                \draw (0, 0) -- (3, 0);
                \draw [dashed] (3, 0) -- ++ (1, 0);
                \fill (4, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_{n-2}}\)};
                \fill (5, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_{n-1}}\)};
                \fill (6, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_n}\)};
                \draw (4, 0) -- (6, 0);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{dynkin-Bn}
            \begin{tikzpicture}[font=\small]
                \node at (-1, 0) {\normalsize\(\dynkin{B}{n}\)};
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_1}\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_2}\)};
                \fill (2, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_3}\)};
                \fill (3, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_4}\)};
                \draw (0, 0) -- (3, 0);
                \draw [dashed] (3, 0) -- ++ (1, 0);
                \fill (4, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_{n-2}}\)};
                \fill (5, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_{n-1}}\)};
                \fill (6, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_n}\)};
                \draw (4, 0) -- (5, 0);
                \draw (5, 0.03) -- ++ (1, 0); 
                \draw (5, -0.03) -- ++ (1, 0);
                \draw [-{>[length=1.5mm,width=3mm]}, xshift=5.52cm] (0, 0) -- ++ (0.001, 0);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{dynkin-Cn}
            \begin{tikzpicture}[font=\small]
                \node at (-1, 0) {\normalsize\(\dynkin{C}{n}\)};
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_1}\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_2}\)};
                \fill (2, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_3}\)};
                \fill (3, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_4}\)};
                \draw (0, 0) -- (3, 0);
                \draw [dashed] (3, 0) -- ++ (1, 0);
                \fill (4, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_{n-2}}\)};
                \fill (5, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_{n-1}}\)};
                \fill (6, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_n}\)};
                \draw (4, 0) -- (5, 0);
                \draw (5, 0.03) -- ++ (1, 0); 
                \draw (5, -0.03) -- ++ (1, 0);
                \draw [-{>[length=1.5mm,width=3mm]}, xshift=5.48cm] (0, 0) -- ++ (-0.001, 0);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{dynkin-Dn}
            \begin{tikzpicture}[font=\small]
                \node at (-1, 0) {\normalsize\(\dynkin{D}{n}\)};
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_1}\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_2}\)};
                \fill (2, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_3}\)};
                \fill (3, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_4}\)};
                \draw (0, 0) -- (3, 0);
                \draw [dashed] (3, 0) -- ++ (1, 0);
                \fill (4, 0) circle [radius=0.075cm] node [below] {\(\alpha\mathrlap{_{n-3}}\)};
                \fill (5, 0) circle [radius=0.075cm] node [right] {\(\alpha\mathrlap{_{n-2}}\)};
                \fill[xshift=5cm, shift={(45:1)}] coordinate (A) circle [radius=0.075cm] node [right] {\(\alpha\mathrlap{_{n-1}}\)};
                \fill[xshift=5cm, shift={(-45:1)}] coordinate (B) circle [radius=0.075cm] node [right] {\(\alpha\mathrlap{_n}\)};
                \draw (4, 0) -- (5, 0);
                \draw (5, 0) -- (A);
                \draw (5, 0) -- (B);
            \end{tikzpicture}
        \end{gather}
        or is one of the following exceptional cases,
        \begin{gather}
            \tikzsetnextfilename{dynkin-G2}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{G}{2}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \draw (0, 0) -- (1, 0);
                \draw (0, -0.03) -- ++ (1, 0);
                \draw (0, 0.03) -- ++ (1, 0);
                \draw [-{>[length=1.5mm,width=3mm]}, xshift=0.48cm] (0, 0) -- ++ (-0.001, 0);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{dynkin-F4}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{F}{4}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \fill (3, 0) circle [radius=0.075cm];
                \draw (0, 0) -- (1, 0);
                \draw (1, -0.03) -- ++ (1, 0);
                \draw (1, 0.03) -- ++ (1, 0);
                \draw [-{>[length=1.5mm,width=3mm]}, xshift=1.48cm] (0, 0) -- ++ (-0.001, 0);
                \draw (2, 0) -- (3, 0);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{dynkin-E6}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{E}{6}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \fill (3, 0) circle [radius=0.075cm];
                \fill (4, 0) circle [radius=0.075cm];
                \fill (2, 1) circle [radius=0.075cm];
                \draw (0, 0) -- (4, 0);
                \draw (2, 0) -- (2, 1);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{dynkin-E7}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{E}{7}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \fill (3, 0) circle [radius=0.075cm];
                \fill (4, 0) circle [radius=0.075cm];
                \fill (5, 0) circle [radius=0.075cm];
                \fill (2, 1) circle [radius=0.075cm];
                \draw (0, 0) -- (5, 0);
                \draw (2, 0) -- (2, 1);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{dynkin-E8}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{E}{8}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \fill (3, 0) circle [radius=0.075cm];
                \fill (4, 0) circle [radius=0.075cm];
                \fill (5, 0) circle [radius=0.075cm];
                \fill (6, 0) circle [radius=0.075cm];
                \fill (2, 1) circle [radius=0.075cm];
                \draw (0, 0) -- (6, 0);
                \draw (2, 0) -- (2, 1);
            \end{tikzpicture}
        \end{gather}
    \end{thm}
    
    There is much more to be said about Dynkin diagrams and the things that they classify, but this is all we have time for here.
    
    \section{Verma Modules}
    We can use this classification to say something about the representation theory of semisimple Lie algebras over \(\complex\).
    To start with, when \(\lie{g}\) is defined from a root system in terms of the generators \(e_i\), \(h_i\), and \(f_i\) we can make the following definition.
    
    \begin{dfn}{Verma Module}{}
        Let \(\lie{g}\) be a semisimple Lie algebra over \(\complex\) with Cartan subalgebra \(\lie{h}\), and let \(\lambda \in \lie{h}^*\) be a weight.
        Let \(I_\lambda \subseteq U(\lie{g})\) be the left ideal generated by the elements \(h - \lambda(h)1\) for \(h \in \lie{h}\) and \(e_i\) for \(i = 1, \dotsc, r\).
        The \defineindex{Verma module}, \(M_\lambda\), is \(U(\lie{g}) / I_\lambda\).
    \end{dfn}
    
    The idea of this definition is that \(M_\lambda\) is the largest (with respect to inclusion) highest weight representation with highest weight \(\lambda\).
    Recall that by \enquote{highest weight representation} we mean that \(M_\lambda\) is generated (as a \(U(\lie{g})\)-module) by some highest weight vector, \(v\), which is such that \(h \action v = \lambda(h) v\) and \(e_i \action v = 0\).
    Thus, \(M_\lambda\) consists of linear combinations of elements of the form \(f_{i_1} \dotsm f_{i_k} \action v\).
    The only relations imposed amongst these elements are those that are enforced by the commutation relations of the \(f_i\)s.
    As a consequence \(f_i\) need not act nilpotently, and thus \(M_\lambda\) is infinite dimensional.
    
    Let \(\lie{n}_+\) (\(\lie{n}_-\)) denote the subalgebra of \(\lie{g}\) generated by the \(e_i\) (\(f_i\)).
    Then one can show that the Verma module, \(M_\lambda\), is isomorphic to \(U(\lie{g}) \otimes_{U(\lie{h} \oplus \lie{n}_+)} \complex_\lambda\) where \(\complex_\lambda\) is the one-dimensional representation of \(\lie{h} \oplus \lie{n}_+\) in which \(h \in \lie{h}\) acts as \(h \action v = \lambda(h)v\) and \(e \in \lie{n}_+\) acts as \(e \action v = 0\) (define \(\lambda_+ \colon \lie{h} \oplus \lie{n}_+ \to \complex\) by \(\lambda_+(h) = h\) and \(\lambda_+(e) = 0\) and then this is the \enquote{obvious} one-dimensional representation).
    We can identify this construction as inducing \(\complex_\lambda\) up to all of \(\lie{g}\), so
    \begin{equation}
        M_\lambda \isomorphic \Ind^{U(\lie{g})}_{U(\lie{h} \oplus \lie{n}_+)} \complex_\lambda.
    \end{equation}
    This makes sense, the Verma module is such that \(\lie{h} \oplus \lie{n}_+\) acts by highest weight, which is what \(\complex_\lambda\) captures, and then \(\lie{n}_-\) acts freely imposing only the required commutation relations, which is captured by inducing up to \(\lie{g} \isomorphic \lie{n}_- \oplus \lie{h} \oplus \lie{n}_+\).
    
    The Verma module is infinite dimensional, but nevertheless it is still important in the theory of finite dimensional representations of \(\lie{g}\).
    
    \begin{prp}{}{}
        Let \(\lie{g}\) be a semisimple Lie algebra with Cartan subalgebra \(\lie{h}\) and fix a weight \(\lambda \in \lie{h}^*\).
        Let \(\complex\langle f_1, \dotsc, f_n \rangle\) be the free algebra generated by the noncommuting symbols \(f_1, \dotsc, f_n\), and let \(\tilde{M}_\lambda = \complex \langle f_1, \dotsc, f_n \rangle v\) be the free module generated by \(v\).
        There exists an action of \(\lie{g}\) on \(\tilde{M}_\lambda\) such that
        \begin{align}
            f_i \action \left( \prod_{k} f_{j_k} v \right) &= \left( f_i\prod_k f_{jk} \right);\\
            h_i \action \left( \prod_k f_{j_k}v \right) &= \left( \lambda(h_i) - \sum_k a_{i,j_k} \right) \left( \prod_k f_{j_k} v \right);\\
            e_i \action \left( \prod_{k=1}^l f_{j_k} v \right) &= \sum_{k | j_k = i} f_{j_1} \dotsm f_{j_{k-1}} h_i f_{j_{k+1}} \dotsm f_{j_l} v.
        \end{align}
        \begin{proof}
            The \(\lie{g}\)-module defined here is simply the Verma module, the only difference is that we're not imposing any condition on the \(f_i\)s in the monomials in \(\tilde{M}_\lambda\), whereas in \(M_\lambda\) we impose the Serre relations.
        \end{proof}
    \end{prp}
    
    The \defineindex{weight lattice} of \(\lie{g}\) is \(P = \integers \Phi \subset E\), the lattice generated by the roots.
    For example, for \(\dynkin{A}{1}\) the weight lattice is just \(\integers\), for \(\dynkin{A}{1} \oplus \dynkin{A}{1}\) it's \(\integers^2\), for \(\dynkin{A}{2}\) it's a hexagonal lattice, and for \(\dynkin{B}{2}\) it's again a square lattice, \(\integers^2\) (but scaled differently to \(\dynkin{A}{1} \oplus \dynkin{A}{1}\)).
    
    \begin{crl}{}{}
        The Verma module, \(M_\lambda\), has a weight decomposition.
        In this weight decomposition the weight lattice is \(P = \lambda - \integers \Phi\), and the \(\lambda\)-weight eigenspace of \(M_\lambda\) is one-dimensional, further, all weight subspaces are finite dimensional.
    \end{crl}
    
    We are now ready to give the result which links \(M_\lambda\) to the finite-dimensional representations.
    
    \begin{prp}{Universal Property of Verma Modules}{}
        Let \(\lie{g}\) be a semisimple Lie algebra and use notation as above.
        If \(V\) is a \(\lie{g}\)-module and \(v \in V\) is a highest weight vector (\(h \action v = \lambda(h)v\) for \(h \in \lie{h}\) and \(e_i \action v = 0\)) then there exists a unique homomorphism \(\varphi \colon M_\lambda \to V\) such that \(\eta(v_\lambda) = v\) where \(v_\lambda \in M_\lambda\) is the highest weight element of the Verma module \(M_\lambda\).
        In particular, if such a nonzero \(v\) generates \(V\), that is \(V\) is a highest weight representation with weight vector \(v\), then \(V\) is a quotient of \(M_\lambda\).
    \end{prp}
    
    The above result says that \(M_\lambda\) is universal amongst highest weight representations of \(\lie{g}\).
    Any map into any highest weight representation, \(V\), can be achieved by first mapping into \(M_\lambda\), then mapping into \(V\) in a unique way (using \(\varphi\)).
    
    \begin{prp}{}{}
        Every highest weight representation has a weight decomposition into finite-dimensional weight subspaces.
    \end{prp}
    
    So, every highest weight module is a quotient of the Verma module.
    It turns out that only one of these quotients is irreducible.
    
    \begin{prp}{}{}
        For every \(\lambda \in \lie{h}^*\) the Verma module, \(M_\lambda\), has a unique simple quotient, \(L_\lambda\).
        Further, \(L_\lambda\) arises as a quotient of any highest weight \(\lie{g}\)-module with highest weight \(\lambda\).
    \end{prp}
    
    The idea of the above is that as long as we never include \(v_\lambda\) in any submodule of \(M_\lambda\) we never get all of \(M_\lambda\), and so we can sum all proper submodules of \(M_\lambda\), and we know that the result will still be a proper submodule.
    We can then quotient by this sum, and the result is \(L_\lambda\), we've quotiented out all submodules which could appear, and thus \(L_\lambda\) is simple.
    
    \begin{crl}{}{}
        Simple highest weight \(\lie{g}\)-modules (for \(\lie{g}\) a semisimple Lie algebra over \(\complex\)) are classified by their highest weight, \(\lambda \in \lie{h}^*\), by the bijection \(\lambda \mapsto L_\lambda\).
    \end{crl}
    
    
    \chapter{Braids, Knots, and Hecke Algebras}
    \section{The Pure Braid Group}
    We start with a technical definition, assuming the reader is familiar with the notion of a braid group, if not maybe skip the definition and look at the pictures.
    
    \begin{dfn}{Pure Braid Group}{def:pure braid group}
        Let \(M_n = \{(z_1, \dotsc, z_n) \in \complex^n \mid z_i \ne z_j \text{ for } i \ne j\}\), which is a topological space as a subspace of \(\complex^n\).
        The \defineindex{pure braid group}, is the fundamental group, \(\purebraid_n = \pi_1(M_n)\).
    \end{dfn}
    
    A pure braid is then a (homotopy class) continuous function \(\beta \colon [0, 1] \to M_n\) with \(\beta(0) = \beta(1)\), given by \(t \mapsto (\beta_1(t), \dotsc, \beta_n(t))\) where the \(\beta_i\) are continuous functions \([0, 1] \to \complex \setminus \{z_1, \dotsc, \widehat{z_i}, \dotsc, z_n\}\) such that at no \(t \in [0, 1]\) do we have \(\beta_i(t) = \beta_j(t)\).
    
    Let \(\complex_n\) be the \(n\)-punctured complex plane\footnote{We're treating \(\complex\) as a topological space here, the position of the points doesn't matter, we won't use any algebraic properties of this copy of \(\complex\)}.
    Then for a pure braid, \(\beta\), fixing some \(t \in [0, 1]\) we can view \(\beta(t)\) as a choice of \(n\) distinct points in \(\complex\).
    Further, as \(t\) varies these points move around continuously.
    We can draw the whole path by considering \(t \in [0, 1]\) as a third dimension, and considering the positions traced by these points as time goes from \(0\) to \(1\).
    By lining up the punctures we can then project this down onto two dimensions, but keeping track of when a path goes over or under another.
    This gives us the standard picture of a pure braid.
    For example, \cref{fig:pure braid example} shows an element of \(\purebraid_4\).
    
    \begin{figure}
        \centering
        \tikzsetnextfilename{pure-braid-example}
        \begin{tikzpicture}[braid/.cd]
            \pic [thick] at (0, 0) {
                braid={s_1-s_3 s_1 s_2 s_2 s_3}
            };
        \end{tikzpicture}
        \caption[An element of the pure braid group]{An element of the pure braid group, \(\purebraid_4\).}
        \label{fig:pure braid example}
    \end{figure}
    
    The group operation of \(\pi_1(M_n)\) is path concatenation (with rescaling of time so that we still have \(t \in [0, 1]\)).
    The corresponding operation for pure braids is given by taking \(\beta \beta'\) to be given by concatenating the diagram for \(\beta\) below the diagram for \(\beta'\) (reading the braid from the top down we want to do \(\beta'\) first\footnote{The alternative convention gives us a perfectly well defined group, but to match conventions with the symmetric group we want this order.}).
    
    \section{The Braid Group}
    So far we've restricted our pure braids so that if a strand ends at the same puncture it begins at.
    The braid group relaxes this condition.
    
    Let \(M_n\) be as in \cref{def:pure braid group}.
    There is an obvious action of \(S_n\) on \(M_n\) given by permuting elements within a tuple, and this defines an equivalence relation on \(M_n\), in which two tuples are equivalent if they are related by permuting elements.
    Let \(M_n/S_n\) be the quotient of \(M_n\) by this equivalence relation.
    
    \begin{dfn}{Braid Group}{def:braid group}
        The \defineindex{braid group} is \(\braid_n = \pi_1(M_n/S_n)\).
    \end{dfn}
    
    In terms of the pictures of braids the only difference is that we no longer require that braids start and end at the same point.
    See \cref{fig:braid example}.
    The group operation is still concatenation.
    Notice that by tracking where each strand starts and ends we get a permutation, \(w \in S_n\).
    It is always possible to write a braid, \(b \colon [0, 1] \to M_n/S_n\), as a composite, \(b = \beta \circ p\), where \(\beta\) is a pure braid and \(w \in S_n\) is a permutation such that \(\beta(1) = w^{-1}(b(0))\).
    
    \begin{figure}
        \centering
        \tikzsetnextfilename{braid-example}
        \begin{tikzpicture}[braid/.cd]
            \pic [thick] at (0, 0) {
                braid={s_1-s_3 s_1 s_2 s_3 s_3}
            };
        \end{tikzpicture}
        \caption[An element of the braid group]{An element of the braid group, \(\braid_n\). Notice that the strands starting at \(1\), \(2\), \(3\) and \(4\) end at \(1\), \(3\), \(4\), and \(2\) respectively, defining a permutation \(\cycle{2,3,4}\).}
        \label{fig:braid example}
    \end{figure}
    
    \begin{thm}{Artin}{thm:braid group presentation}
        The braid group has the standard presentation
        \begin{equation*}
            \braid_n = \langle \sigma_1, \dotsc, \sigma_{n-1} \mid \sigma_i \sigma_{i + 1} \sigma_i = \sigma_{i + 1} \sigma_i \sigma_{i + 1}, \sigma_i \sigma_j = \sigma_j \sigma_i \text{ for } \abs{i - j} > 1 \rangle.
        \end{equation*}
    \end{thm}
    
    The relationship
    \begin{equation}
        \sigma_i \sigma_{i + 1} \sigma_i = \sigma_{i + 1} \sigma_i \sigma_{i + 1}
    \end{equation}
    is called the \defineindex{braid relation}.
    The identification between this presentation and \(\braid_n\) is pretty simple.
    For simplicity we'll just look at the \(n = 3\) case, but for other values of \(n\) the pictures generalise in the obvious way.
    First, the identity, \(e\), is simply leaving all strands fixed:
    \tikzexternaldisable
    \begin{equation}
        e =\ 
        \tikzsetnextfilename{braid-3-strand-identity} 
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={1}
            };
        \end{tikzpicture}
        \,.
    \end{equation}
    Then \(\sigma_1\) is the braid
    \begin{equation}
        \tikzsetnextfilename{braid-3-strand-sigma1}
        \sigma_1 =\  
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_1}
            };
        \end{tikzpicture}
        \,,
    \end{equation}
    and \(\sigma_1^{-1}\) is given by crossing in the other direction:
    \begin{equation}
        \sigma_1^{-1} =\  
        \tikzsetnextfilename{braid-3-strand-sigma1-inverse}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_1^{-1}}
            };
        \end{tikzpicture}
        \,.
    \end{equation}
    This makes sense, since we then have
    \begin{equation}
        \sigma_1 \sigma_1^{-1} = 
        \tikzsetnextfilename{braid-3-strand-sigma1-sigma1-inverse}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_1 s_1^{-1}}
            };
        \end{tikzpicture}
        \ =\ 
        \tikzsetnextfilename{braid-3-strand-identity-2}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={1 1}
            };
        \end{tikzpicture}
        \ = e.
    \end{equation}
    Similarly, we have
    \begin{equation}
        \sigma_2 =\  
        \tikzsetnextfilename{braid-3-strand-sigma2}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_2}
            };
        \end{tikzpicture}
        \qand \sigma_2^{-1} =\  
        \tikzsetnextfilename{braid-3-strand-sigma2-inverse}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_2^{-1}}
            };
        \end{tikzpicture}
        \,.
    \end{equation}
    So, \(\sigma_i\) means passing the \(i\)th strand over strand \(i + 1\), and \(\sigma_i^{-1}\) means passing the \(i\)th strand \emph{under} strand \(i + 1\).
    
    The braid relation in this case tells us that
    \begin{equation}
        \sigma_1 \sigma_2 \sigma_1 = \sigma_2 \sigma_1 \sigma_2,
    \end{equation}
    which is just the following picture:
    \begin{equation}  
        \tikzsetnextfilename{braid-relation-lhs}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_1 s_2 s_1}
            };
        \end{tikzpicture}
        \ =\ 
        \tikzsetnextfilename{braid-relation-rhs}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_2 s_1 s_2}
            };
        \end{tikzpicture}
        \,.
    \end{equation}
    For \(n = 3\) we never have \(\abs{i - j} > 1\), so let's look at \(n = 4\), where this relation simply tells us that \enquote{sufficiently separated} swaps commute:
    \begin{equation}
        \tikzsetnextfilename{braid-commuting-swaps-lhs}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_1 s_3}
            };
        \end{tikzpicture}
        \ =\ 
        \tikzsetnextfilename{braid-commuting-swaps-lhs}
        \begin{tikzpicture}[braid/.cd, baseline=(current bounding box), number of strands=3]
            \pic [thick] at (0, 0) {
                braid={s_3 s_1}
            };
        \end{tikzpicture}
        .
    \end{equation}
    
    So far, we've just been looking at braids and deciding if they're equal if they intuitively give the same picture after rearranging strands without passing them through each other.
    This can be made rigorous as follows.
    
    First, we define the \define{Reidemeister moves}\index{Reidemeister move} of types II and III.
    These are \enquote{local} operations on braids, in that we can apply them to any portion of the diagram without changing the rest of the diagram.
    To represent this we use a dashed circle to \enquote{zoom in} on just a portion of the diagram:
    The Reidemeister move of type II is
    \begin{equation}
        \tikzsetnextfilename{reidemeister-II}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [dashed] (0, 0) circle [radius=2];
            \draw [thick, rounded corners=20] (135:2) -- (1, 0) -- (-135:2);
            \draw [line width=2mm, white, rounded corners=20] (45:2) -- (-1, 0) -- (-45:2);
            \draw [thick, rounded corners=20] (45:2) -- (-1, 0) -- (-45:2);
            \node at (2.5, 0) {\(\xleftrightarrow{\symrm{II}}\)};
            \begin{scope}[xshift=5cm]
                \draw [dashed] (0, 0) circle [radius=2];
                \draw [thick, rounded corners=20] (135:2) -- (-0.5, 0) -- (-135:2);
                \draw [line width=1.5mm, white, rounded corners=20] (45:2) -- (0.5, 0) -- (-45:2);
                \draw [thick, rounded corners=20] (45:2) -- (0.5, 0) -- (-45:2);
            \end{scope}
        \end{tikzpicture}
        \,.
    \end{equation}
    The Reidemeister move of type III is
    \begin{equation}
        \tikzsetnextfilename{reidemeister-III}
        \begin{tikzpicture}[baseline=(current bounding box)]
            \draw [dashed] (0, 0) circle [radius=2];
            \draw [thick] (pi/12 r:2) -- (14*pi/12 r:2);
            \draw [line width=1.5mm, white] (7*pi/12 r:2) -- (17*pi/12 r:2);
            \draw [thick] (7*pi/12 r:2) -- (17*pi/12 r:2);
            \draw [line width=1.5mm, white] (10*pi/12 r:2) -- (23*pi/12 r:2);
            \draw [thick] (10*pi/12 r:2) -- (23*pi/12 r:2);
            \node at (2.5, 0) {\(\xleftrightarrow{\symrm{III}}\)};
            \begin{scope}[xshift=5cm]
                \draw [dashed] (0, 0) circle [radius=2];
                \draw [thick] (13*pi/12 r:2) -- (2*pi/12 r:2);
                \draw [line width=1.5mm, white] (5*pi/12 r:2) -- (19*pi/12 r:2);
                \draw [thick] (5*pi/12 r:2) -- (19*pi/12 r:2);
                \draw [line width=1.5mm, white] (11*pi/12 r:2) -- (22*pi/12 r:2);
                \draw [thick] (11*pi/12 r:2) -- (22*pi/12 r:2);
            \end{scope}
        \end{tikzpicture}
        \,.
    \end{equation}
    These are simply capturing the fact that we want \(\sigma_i^{-1} \sigma_i = e\) and the braid relation.
    
    \begin{remark}{}{}
        The Reidemeister moves first arose in knot theory, in which there is a third Reidemenster move, move number I, which is
        \begin{equation}
            \tikzsetnextfilename{reidemeister-I}
            \begin{tikzpicture}[baseline=(current bounding box)]
                \draw [dashed] (0, 0) circle [radius=1];
                \draw [thick, rounded corners] (0, -1) -- ++ (0, 0.75) -- ++ (0.5, 0.5) -- ++ (0, -0.25) coordinate (A);
                \draw [thick, rounded corners] (A) -- ++ (0, -0.25) -- ++ (-0.5, 0.5) -- ++ (0, 0.75);
                \draw [line width=1mm, azure(web)(azuremist)!45, rounded corners] (0, -1) -- ++ (0, 0.75) -- ++ (0.5, 0.5) -- ++ (0, -0.25) coordinate (A);
                \draw [thick, rounded corners] (0, -1) -- ++ (0, 0.75) -- ++ (0.5, 0.5) -- ++ (0, -0.25) coordinate (A);
                \node at (1.5, 0) {\(\xleftrightarrow{\symrm{I}}\)};
                \begin{scope}[xshift=3cm]
                    \draw [dashed] (0, 0) circle [radius=1];
                    \draw [thick] (0, -1) -- (0, 1);
                \end{scope}
            \end{tikzpicture}
            \,
        \end{equation}
        We don't consider this as strands in braids aren't allowed to loop back up.
    \end{remark}
    
    \begin{prp}{}{}
        Two braids are the same if and only if they are related by an isotopy and a sequence of Reidemeister moves of type II and III.
    \end{prp}
    
    \begin{remark}{}{}
        In physics a braid describes the adiabatic exchange of indistinguishable quasi particles in two dimensions.
        This is important in, for example, the fractional quantum Hall effect.
        This idea has applications to quantum computing.
        Particles whose exchange is governed by the braid group are called \emph{anyons} (cf.\@ bosons and whose exchange is governed by the trivial and antisymmetric representations of the symmetric group).
    \end{remark}
    
    \begin{remark}{}{}
        From a geometric view point \(\braid_n\) is the \enquote{mapping class group} of the punctured disc with \(n\) points.
        We swap the plane to a disc just because it's nicer to work with compact things, and we're not allowing punctures at infinity anyway.
        
        The mapping class group is defined as follows.
        Let \(S\) be a surface, and \(Q \subset S\) a finite set of marked points.
        Denote by \(\Homeo(S, Q)\) the group of homeomorhpisms of \(S\) which fix \(Q\) as a set and fix the boundary pointwise.
        That is, \(\varphi \in \Homeo(S, Q)\) is such that for every marked point, \(p\), there is some marked point \(q\) (not necessarily distinct) such that \(\varphi(p) = q\), and for every boundary point, \(x\) we have \(\varphi(x) = x\).
        The \defineindex{mapping class group} of the marked surface \((S, Q)\) is
        \begin{equation}
            \symrm{Mod}(S, Q) = \Homeo^+(S, Q) / \Homeo_0(S, Q)
        \end{equation}
        where \(\Homeo^+(S, Q)\) denotes the collection of orientation preserving homeomorphisms in \(\Homeo(S, Q)\), and \(\Homeo_0(S, Q)\) denotes the connected component of \(\Homeo(S, Q)\) containing the identity (in the compact-open topology).
        
        This group is also sometimes called the modular group, hence the notation \(\symrm{Mod}(S, Q)\).
        This is because when we take the torus with no marked points the mapping class group ends up being isomorphic to the modular group, \(\specialLinear_2(\integers)\).
        
        When we say that \(\braid_n\) is the mapping class group of the disc with \(n\) punctured points we mean that if \(D\) is this punctured disc and \(Q\) is our set of punctures then there is an isomorphism \(\braid_n \to \symrm{Mod}(D, Q)\) given by \(\sigma_i \mapsto H_i\) where \(\sigma_i\) is a generator of the braid group in the standard presentation and \(H_i\) is the homeomorphism of the \(n\)-punctured disc given by a half twist exchanging the points numbered \(i\) and \(i + 1\).
        See \cref{fig:half twist of the disc}.
    \end{remark}
    
    \begin{figure}
        \centering
        \tikzsetnextfilename{disc-half-twist}
        \begin{tikzpicture}
            \draw (0, 0) circle [radius = 2cm];
            \draw [dashed] (-2, 0) -- ++ (4, 0);
            \fill [highlight] (-0.5, 0) circle [radius=0.1];
            \fill [highlight!40] (0.5, 0) circle [radius=0.1];
            \draw [|->] (2.5, 0) -- ++ (1, 0) node [midway, above] {\(H_i\)};
            \draw (6, 0) circle [radius = 2cm];
            \draw [dashed, use Hobby shortcut] (4, 0) .. (6, 1) .. (7, 0.1) .. (6.5, 0) .. (5.5, 0) .. (5, -0.1) .. (6, -1) .. (8, 0);
            \fill [highlight!40] (5.5, 0) circle [radius=0.1];
            \fill [highlight] (6.5, 0) circle [radius=0.1];
        \end{tikzpicture}
        \caption[Half twist of the disc]{Half twist of the disc exchanging \(i\) and \(i + 1\). The dashed line shows some curve and its image under \(H_i\).}
        \label{fig:half twist of the disc}
    \end{figure}
    
    \section{Coxeter Groups}
    \begin{dfn}{Coxeter System}{}
        Let \(S\) be a finite set.
        A \define{Coxeter matrix}\index{Coxeter!matrix} for \(S\) is a symmetric matrix \(M = (m_{st})_{s,t \in S}\) such that
        \begin{enumerate}
            \item \(m_{ss} = 1\) for all \(s \in S\);
            \item \(m_{st} \in \{2, 3, \dotsc\} \cup \{\infty\}\) for all distinct \(s, t \in S\).
        \end{enumerate}
        We call \((S, M)\) a \define{Coxeter system}\index{Coxeter!system}.
    \end{dfn}
    
    For example, take \(S = \{1, \dotsc, 4\}\) and
    \begin{equation}
        \label{eqn:S5 coxeter matrix}
        M = 
        \begin{pmatrix}
            1 & 3 & 2 & 2\\
            3 & 1 & 3 & 2\\
            2 & 3 & 1 & 3\\
            2 & 2 & 3 & 1
        \end{pmatrix}
        .
    \end{equation}
    
    \begin{dfn}{Coxeter Group}{}
        Given a Coxeter system, \((S, M)\), the corresponding \define{Coxeter group}\index{Coxeter!group}, is the pair \((W, S)\), where \(W\) is the group given by the presentation
        \begin{equation}
            W \coloneq \langle s \in S \mid (st)^{m_{st}} = 1 \forall s, t \in S \rangle.
        \end{equation}
        The \defineindex{rank} of \(W\) is defined to be \(\abs{S}\).
    \end{dfn}
    
    Notice that if we take \(s = t\) then we have \(m_{ss} = 1\), so \((ss)^{m_{ss}} = s^2\), and so in a Coxeter group we always have \(s^2 = 1\), and hence \(s^{-1} = s\) for all generators.
    If \(m_{st} = 2\) then \((st)^2 = 1\), which means that \(st = ts\), since \(s^{-1} = s\) and \(t^{-1} = t\).
    Thus, a \(2\) in the matrix means that the corresponding generators commute.
    More generally, we can always take the equation \((st)^{m_{st}} = stst \dotsm st = 1\), which has \(2m_{st}\) factors, and rearrange to get
    \begin{equation}
        \underbrace{sts \dotsm}_{m_{st} \text{ terms}} = \underbrace{tst \dotsm}_{m_{st} \text{ terms}}.
    \end{equation}
    For example, when \(m_{st} = 3\) this relation is
    \begin{equation}
        sts = tst.
    \end{equation}
    We call this the \defineindex{braid relation}.
    
    Consider the Coxeter system \((\{s_1, s_2, s_3, s_4\}, M)\) with \(M\) given by \cref{eqn:S5 coxeter matrix}.
    The corresponding Coxeter group is
    \begin{equation}
        \label{eqn:symmetric group presentation}
        W = \langle s_1, s_2, s_3, s_4 \mid s_i^2 = 1, s_is_{i+1}s_i = s_{i+1}s_is_{i+1}, s_is_j = s_js_i \text{ for } \abs{i - j} > 1 \rangle.
    \end{equation}
    We can recognise this as a presentation of \(S_5\), where \(s_i = \cycle{i,i+1}\).
    This presentation generalises fully to \(S = \{s_1, \dotsc, s_{n-1}\}\) to give the Coxeter presentation of \(S_n\).
    
    \begin{dfn}{}{}
        Let \((W, S)\) be a Coxeter group.
        Given \(w \in W\) we can write \(w\) as \(s_{i_1} \dotsm s_{i_k}\), with \(s_i \in S\).
        We say that this is a \defineindex{reduced expression} for \(w\) if \(k\) is minimal, and we define \(\ell(w) = k\) to be the \defineindex{length} of \(w\).
    \end{dfn}
    
    \begin{thm}{Matsumoto}{}
        Let \(W\) be a group generated by \(S = \{s_1, \dotsc, s_n\}\) subject to some relations, such that \(s_i^2 = 1\).
        Then the following are equivalent:
        \begin{itemize}
            \item \((W, S)\) is a Coxeter group.
            \item Any two reduced expressions for \(w \in W\) can be transformed into each other by a series of braid relations.
        \end{itemize}
    \end{thm}
    
    Note that two Coxeter groups may be isomorphic as groups, but not as Coxeter groups, since if they have different generating sets and/or relations then we consider them to be different as Coxeter groups, but not as groups.
    The problem of producing a general algorithm to decide if two Coxeter systems produce isomorphic groups is unsolved.
    A related open problem is deciding, given \(W\), what subset, \(S\), and relations can we take to make \((W, S)\) a Coxeter group.
    
    \subsection{Classification of Coxeter Groups}
    \begin{dfn}{Coxeter Diagram}{}
        Let \((S, M)\) be a Coxeter system.
        The corresponding \define{Coxeter diagram}\index{Coxeter!diagram} is constructed as follows:
        \begin{itemize}
            \item The vertices are elements of \(S\);
            \item If \(m_{st} < 3\) there are no edges between \(s\) and \(t\);
            \item If \(m_{st} = 3\) there is an unlabelled edge between \(s\) and \(t\);
            \item If \(m_{st} \ge 4\) there is an edge between \(s\) and \(t\) labelled with \(m_{st}\).
        \end{itemize}
    \end{dfn}
    
    For example, if we take \(M\) as in \cref{eqn:S5 coxeter matrix} then the corresponding Coxeter graph has as vertices \(\{s_1, s_2, s_3, s_4\}\), and there are edges connecting \(s_1\) to \(s_2\), \(s_2\) to \(s_3\), and \(s_3\) to \(s_4\), so the graph is
    \begin{equation}
        \tikzsetnextfilename{coxeter-A4}
        \begin{tikzpicture}[font=\small]
            \fill (0, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_1}\)};
            \fill (1, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_2}\)};
            \fill (2, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_3}\)};
            \fill (3, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_4}\)};
            \draw (0, 0) -- (3, 0);
        \end{tikzpicture}
    \end{equation}
    
    \begin{remark}{}{}
        Notice that this is exactly the Dynkin diagram \(\dynkin{A}{4}\).
        Dynkin diagrams and Coxeter diagrams are very closely related.
        There are some distinctions though: Dynkin diagrams can be directed, Coxeter graphs can be labelled, Dynkin diagrams can have multiple edges between a pair of vertices.
        
        Both Dynkin and Coxeter diagrams encode a matrix, Cartan and Coxeter respectively, which encode some properties of an algebraic structure, a Lie algebra and group respectively.
    \end{remark}
    
    \begin{thm}{Classification of Coxeter Groups}{}
        Every Coxeter system has a Coxeter diagram falling into one of the following infinite families,
        \begin{gather}
            \tikzsetnextfilename{coxeter-An}
            \begin{tikzpicture}[font=\small]
                \node at (-1, 0) {\normalsize\(\dynkin{A}{n}\)};
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_1}\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_2}\)};
                \fill (2, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_3}\)};
                \fill (3, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_4}\)};
                \draw (0, 0) -- (3, 0);
                \draw [dashed] (3, 0) -- ++ (1, 0);
                \fill (4, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_{n-2}}\)};
                \fill (5, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_{n-1}}\)};
                \fill (6, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_n}\)};
                \draw (4, 0) -- (6, 0);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-BnCn}
            \begin{tikzpicture}[font=\small]
                \node at (-1, 0) {\normalsize\(\dynkin{B}{n} = \dynkin{C}{n}\)};
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_1}\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_2}\)};
                \fill (2, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_3}\)};
                \fill (3, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_4}\)};
                \draw (0, 0) -- (3, 0);
                \draw [dashed] (3, 0) -- ++ (1, 0);
                \fill (4, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_{n-2}}\)};
                \fill (5, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_{n-1}}\)};
                \fill (6, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_n}\)};
                \draw (4, 0) -- (6, 0) node [pos=0.75, above] {\(4\)};
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-Dn}
            \begin{tikzpicture}[font=\small]
                \node at (-1, 0) {\normalsize\(\dynkin{D}{n}\)};
                \fill (0, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_1}\)};
                \fill (1, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_2}\)};
                \fill (2, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_3}\)};
                \fill (3, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_4}\)};
                \draw (0, 0) -- (3, 0);
                \draw [dashed] (3, 0) -- ++ (1, 0);
                \fill (4, 0) circle [radius=0.075cm] node [below] {\(s\mathrlap{_{n-3}}\)};
                \fill (5, 0) circle [radius=0.075cm] node [right] {\(s\mathrlap{_{n-2}}\)};
                \fill[xshift=5cm, shift={(45:1)}] coordinate (A) circle [radius=0.075cm] node [right] {\(s\mathrlap{_{n-1}}\)};
                \fill[xshift=5cm, shift={(-45:1)}] coordinate (B) circle [radius=0.075cm] node [right] {\(\alpha\mathrlap{_n}\)};
                \draw (4, 0) -- (5, 0);
                \draw (5, 0) -- (A);
                \draw (5, 0) -- (B);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-I2}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{I}{2}(m)\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \draw (0, 0) -- (1, 0) node [midway, above] {\(m\)};
            \end{tikzpicture}
        \end{gather}
        or is one of the following exceptional cases,
        \begin{gather}
            \tikzsetnextfilename{coxeter-G2}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{G}{2}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \draw (0, 0) -- (1, 0) node [midway, above] {\(6\)};
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-H2}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{H}{2}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \draw (0, 0) -- (1, 0) node [midway, above] {\(5\)};
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-H3}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{H}{3}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \draw (0, 0) -- (2, 0) node [pos=0.25, above] {\(5\)};
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-F4}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{F}{4}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \fill (3, 0) circle [radius=0.075cm];
                \draw (0, 0) -- ++ (3, 0) node [midway, above] {\(4\)};
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-E6}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{E}{6}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \fill (3, 0) circle [radius=0.075cm];
                \fill (4, 0) circle [radius=0.075cm];
                \fill (2, 1) circle [radius=0.075cm];
                \draw (0, 0) -- (4, 0);
                \draw (2, 0) -- (2, 1);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-E7}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{E}{7}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \fill (3, 0) circle [radius=0.075cm];
                \fill (4, 0) circle [radius=0.075cm];
                \fill (5, 0) circle [radius=0.075cm];
                \fill (2, 1) circle [radius=0.075cm];
                \draw (0, 0) -- (5, 0);
                \draw (2, 0) -- (2, 1);
            \end{tikzpicture}
            \\
            \tikzsetnextfilename{coxeter-E8}
            \begin{tikzpicture}
                \node at (-1, 0) {\(\dynkin{E}{8}\)};
                \fill (0, 0) circle [radius=0.075cm];
                \fill (1, 0) circle [radius=0.075cm];
                \fill (2, 0) circle [radius=0.075cm];
                \fill (3, 0) circle [radius=0.075cm];
                \fill (4, 0) circle [radius=0.075cm];
                \fill (5, 0) circle [radius=0.075cm];
                \fill (6, 0) circle [radius=0.075cm];
                \fill (2, 1) circle [radius=0.075cm];
                \draw (0, 0) -- (6, 0);
                \draw (2, 0) -- (2, 1);
            \end{tikzpicture}
        \end{gather}
    \end{thm}
    
    Note that \(\dynkin{G}{2} = \dynkin{I}{2}(6)\) and \(\dynkin{H}{2} = \dynkin{I}{2}(5)\).
    It is also common to see \(\dynkin{I}{2}(m)\) written as \(\dynkin{I}{m}\), but this is bad notation as \(\dynkin{I}{2}(m)\) is rank \(2\), not \(m\).
    We have both \(\dynkin{B}{n}\) and \(\dynkin{C}{n}\) because in generalisations (such as affine Coxeter groups) these become different.
    Sometimes we write \(\dynkin{BC}{n}\) for these finite-type diagrams which are the same.
    
    For many of these the corresponding Coxeter group has quite a nice interpretation:
    \begin{itemize}
        \item \(\dynkin{A}{n}\) is the symmetric group \(S_{n + 1}\), it can also be thought of as the symmetries of the \(n\)-simplex.
        \item \(\dynkin{B}{n}\) is the symmetries of the \(n\)-cube, and can be thought of as signed permutations of \(\{-n, \dotsc, n\}\), this group is \(S_n \wr \integers/2\integers = S_n \ltimes (\integers/2\integers)^n\).
        \item \(\dynkin{D}{n}\) is a subgroup of the symmetries of the \(n\)-cube consisting only of signed permutations which fix the number of minus signs.
        \item \(\dynkin{I}{2}(m)\) is the symmetries of the regular \(m\)-gon (that is, the dihedral group of order \(2m\)).
        \item \(\dynkin{A}{3}\), \(\dynkin{B}{3}\), and \(\dynkin{H}{3}\) are symmetries of the platonic solids (including reflections), being the tetrahedron, cube/octahedron, and dodecahedron/icosahedron symmetry groups respectively.
    \end{itemize}
    
    \subsection{Reflection Groups}
    A \defineindex{reflection} of a Euclidean space, \(V\), with inner product \(\innerprod{-}{-}\), is a linear transformation, \(s \colon V \to V\) such that there exists some \(\alpha \in V\) with \(s(\alpha) = -\alpha\) and \(s\) fixes the hyperplane perpendicular to \(\alpha\), \(H_\alpha = (\reals\alpha)^{\perp}\), pointwise.
    A \defineindex{reflection group} is a group which is (isomorphic to) a subgroup of \(\orthogonal(V)\) generated only by reflections.
    It turns out that every Coxeter group can be viewed as a reflection group.
    
    Let \(V\) be a finite-dimensional Euclidean space, and \(\symcal{H}\) a finite collection of hyperplanes.
    Removing these hyperplanes from \(V\) we get \(V \setminus \bigcup_{H \in \symcal{H}}H\).
    We call the connected components of this space \define{alcoves}\index{alcove}.
    For example, if we take \(V = \reals^2\), the plane, then a hyperplane is just a line.
    If we take two non-parallel lines in the plane then they split the plane into four segments, which are our alcoves.
    
    Fix some alcove, \(A\), in \(V\), then define
    \begin{equation}
        S_A = \{s_H \mid s_H \text{ is a reflection in } H \text{ with } H \in \symcal{H} \text{ bounding } A\}.
    \end{equation}
    Then if we take \(W\) to be the group generated by such reflections then \((W, S_A)\) is a Coxeter group.
    
    Conversely, if we're given a Coxeter system, \((W, S)\), then we can define a Euclidean space,
    \begin{equation}
        V = \reals S \isomorphic \bigoplus_{s \in S} \reals \vv{s}
    \end{equation}
    where we're defining a basis vector, \(\vv{s}\), for each \(s \in S\).
    More abstractly, \(V\) is the free vector space on \(S\).
    The inner product on this space is given on this basis by
    \begin{equation}
        \innerprod{\vv{s}}{\vv{t}} = \cos\left( \frac{\pi}{m_{st}} \right).
    \end{equation}
    This is always positive because \(\pi / m_{st} \in [0, \pi]\) (defining \(\pi/\infty = 0\)), and it's symmetric because \(M\) is a symmetric matrix.
    We can then define a reflection in the hyperplane orthogonal to \(\vv{s}\) by
    \begin{equation}
        \sigma_{\vv{s}}(\vv{v}) = \vv{v} - 2\innerprod{\vv{v}}{\vv{s}} \vv{s}
    \end{equation}
    for all \(\vv{v} \in V\).
    
    \begin{prp}{Tits Representation}{}
        With the notation above the map \(W \to \generalLinear(V)\) defined by \(s \mapsto \sigma_{\vv{s}}\) is a faithful representation, and \(\innerprod{-}{-}\) is \(W\)-invariant.
    \end{prp}
    
    \subsection{Generalisations}
    If we remove the requirement that \(s_i^2 = 1\) (equivalently allowing the diagonals to not be \(1\)), then the group that we get is called an \defineindex{Artin group}.
    Matsumoto's theorem then states that two words in a Coxeter group are related only by the relations of the corresponding Artin group.
    Examples of Artin groups include all Coxeter groups, as well as the braid groups.
    Some other examples are \(\langle S \mid st = ts\, \forall s, t \in S\rangle\), which is the free abelian group on \(S\), and \(\langle S \rangle\), the free group on \(S\).
    This suggests the following generalisation of the braid group.
    
    \begin{dfn}{Artin Braid Group}{}
        The \defineindex{Artin braid group} of a Coxeter group, \((W, S)\), is
        \begin{equation}
            \braid_W = \langle \{b_s \mid s \in S\} \mid \underbrace{b_s b_t b_s \dotsm}_{m_{st}} = \underbrace{b_t b_s b_t \dotsm}_{m_{st}} \rangle.
        \end{equation}
    \end{dfn}
    
    Note that the braid group \(\braid_n\) as defined in \cref{def:braid group} is simply \(\braid_{S_n}\), as can be seen by the standard presentation (\cref{thm:braid group presentation}) which mirrors the relations of the \(S_n\) presentation (\cref{eqn:symmetric group presentation}) after removing the condition that \(s_i^2 = 1\).
    
    \begin{prp}{Burau}{}
        Let \(\Lambda = \integers[t, t^{-1}]\).
        There is a group homomorphism
        \begin{equation}
            \rho_n \colon \braid_n \to \generalLinear_n(\Lambda)
        \end{equation}
        given by
        \begin{equation}
            \rho_n(b_i) = 
            \begin{pmatrix}
                1\\
                & \ddots \\
                & & 1 \\
                & & & 1 - t & t\\
                & & & 1 & 0 \\
                & & & & & 1\\
                & & & & & & \ddots\\
                & & & & & & & 1
            \end{pmatrix}
        \end{equation}
        where \(1 - t\) appears in position \((i, i)\).
        This representation of \(\braid_n\) is known as the \defineindex{Burau representation}.
    \end{prp}
    
    For example, if \(n = 3\) then we have
    \begin{equation}
        \rho_3(b_1) = B_1 = 
        \begin{pmatrix}
            1 - t & t & 0\\
            1 & 0 & 0\\
            0 & 0 & 1
        \end{pmatrix}
        , \qand \rho_3(b_2) = B_2 = 
        \begin{pmatrix}
            1 & 0 & 0\\
            0 & 1 - t & t\\
            0 & 1 & 0
        \end{pmatrix}
        .
    \end{equation}
    One can then check that these two matrices satisfy \(B_1B_2B_1 = B_2B_1B_2\).
    
    \begin{remark}{}{}
        For \(n \le 3\) the Burau representation is faithful (as the above shows in the \(n = 3\) case, the \(n = 2\) case is trivially faithful).
        For \(n \ge 5\) the Burau representation is not faithful.
        For \(n = 4\) faithfulness is unknown, but it is known that \(\rho_4\) is a faithful representation if the Jones polynomial detects the unknot.
    \end{remark}
    
    % Appdendix
%	\appendixpage
%	\begin{appendices}
%	    \include{appendix/complexification}
%	\end{appendices}

    \backmatter
    \renewcommand{\glossaryname}{Acronyms}
    \printglossary[acronym]
    \printindex
\end{document}