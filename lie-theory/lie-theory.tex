% !TeX program = lualatex
\documentclass[fleqn]{NotesClass}

\usepackage{csquotes}

\usepackage{tikz}
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]

\usepackage[pdfauthor={Willoughby Seago},pdftitle={Notes from Solitons: Differential Equations, Symmetries, Infinite Dimensional Algebras},pdfkeywords={},pdfsubject={}]{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{NotesBoxes}
\usepackage{NotesMaths2}

\setmathfont[range={\int, \oint, \otimes, \oplus, \bigotimes, \bigoplus}]{Latin Modern Math}


% Highlight colour
\definecolor{highlight}{HTML}{710D78}
\definecolor{my blue}{HTML}{2A0D77}
\definecolor{my red}{HTML}{770D38}
\definecolor{my green}{HTML}{14770D}
\definecolor{my yellow}{HTML}{E7BB41}

% Title page info
\title{Lie Theory}
\author{Willoughby Seago}
\date{September 26th, 2024}
\subtitle{Notes from}
\subsubtitle{University of Glasgow}
\renewcommand{\abstracttext}{These are my notes from the master's course \emph{Lie Theory} taught at the University of Glasgow by Dr Dinakar Muthiah. I also found \href{https://www.youtube.com/playlist?list=PLVMgvCDIRy1zJ6iJHLC_lWgIwY4AhviZ-}{this video series}\footnote{\url{https://www.youtube.com/playlist?list=PLVMgvCDIRy1zJ6iJHLC_lWgIwY4AhviZ-}} from Michael Penn useful. These notes were last updated at \printtime{} on \today{}.}

% Commands
% Maths
\renewcommand{\field}{\symbb{k}}
\newcommand{\cat}[1]{\symsfup{#1}}
\makeatletter
\newcommand{\c@egory}[1]{\symsfup{#1}}
\newcommand{\Vect}[1][\field]{#1\text{-}\c@egory{Vect}}
\newcommand{\LieAlg}[1][\field]{#1\text{-}\c@egory{LieAlg}}
\makeatother
\DeclareMathOperator{\Hom}{Hom}
\DeclarePairedDelimiterX{\commutator}[2]{[}{]}{#1, #2}
\DeclarePairedDelimiterX{\bracket}[2]{[}{]}{#1, #2}
\newcommand{\isomorphic}{\cong}
\DeclareMathOperator{\Span}{span}
\newcommand{\id}{\symrm{id}}
\DeclareMathOperator{\Char}{char}
\RenewDocumentCommand{\matrices}{o m m}{%
    \IfNoValueTF{#1}{%
        \symrm{Mat}(#2, #3)
    }{%
        \symrm{Mat}(#1 \times #2, #3)
    }
}
\newcommand{\trans}{{\top}}
\newcommand{\hermit}{*}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\im}{im}
\newcommand{\ad}{\symrm{ad}}

\begin{document}
	\frontmatter
	\titlepage
	\innertitlepage{}
	\tableofcontents
	% \listoffigures
	\mainmatter
	\chapter{Introduction}
	In this course we will study Lie algebras.
	The motivation for the definition of a Lie algebra is as a tool for studying Lie groups.
	Despite this we will rarely mention Lie groups, since the study of Lie groups requires a knowledge of differential geometry.
	We will stick to the study of Lie algebras, which can be defined purely algebraically.
	
	Roughly speaking, the storey is thus: Lie groups are both smooth manifolds and groups in a compatible way.
	By this, we mean that if \(G\) is a Lie group then the maps
	\begin{itemize}
		\item \(m \colon G \times G \to G\) given by \(m(x, y) = xy\)
	    \item \(i \colon G \to G\) given by \(i(g) = g^{-1}\)
	\end{itemize}
	are smooth and these maps satisfy the obvious requirements to make \(G\) a group.
    
    Lie groups were first used by Sophus Lie, who lends his name to these objects.
    Lie is pronounced Lee by the way, not lye.
    He was looking at the continuous symmetries possessed by the solutions to differential equations, and found that these naturally formed what we now know as a Lie group.
    The problem with studying Lie groups, apart from all of the topology, is that they are generally very non-linear.
    We can think of Lie algebras as being a linear approximation of Lie groups, given by expanding about the identity and discarding non-linear terms.
	
	To get a Lie algebra out of a Lie group we take the tangent space at the identity, \(T_eG\), which is a vector space.
	It turns out that the group commutator, \(\commutator{g}{h} = ghg^{-1}h^{-1}\), induces a natural operation on the tangent space, called the Lie bracket.
	A Lie algebra is then this tangent space equipped with this Lie bracket.
	
	It is possible to abstract the properties of this Lie bracket to define a Lie algebra without reference to a Lie group.
	It is also possible to go in reverse, given a Lie algebra there is always a corresponding Lie group.
	It should be noted that this assignment is not unique, in general many different Lie groups can have the same Lie algebra.
	The assignment does become invertible if we restrict ourselves to simply connected Lie groups, but then we're really getting into topology in a way that this course hopes to avoid.
	This assignment of a Lie algebra to a Lie group is functorial.
	
	\section{Notation}
	Throughout \(\field\) will denote a field.
	Often we will place further restrictions on \(\field\), such as being algebraically closed.
	Most of the time we'll be interested in the case \(\field = \complex\), with occasional use of \(\field = \reals\).
	Unless stated otherwise vector spaces are assumed to be vector spaces over \(\field\).
    
    We will denote by \(0\) the zero vector space (for a given field), \(\{0\}\), which consists of only the zero vector.
	
	If we have a category, \(\cat{C}\), we will denote by \(\cat{C}(A, B) = \Hom_{\cat{C}}(A, B)\) the set of all morphisms \(A \to B\), or simply \(\Hom(A, B)\) if \(\cat{C}\) is clear from context.
	This is just notation, we won't make much use of categories apart from borrowing some of the language.
	The main category we'll make use of is \(\Vect\), the category of vector spaces over \(\field\) and linear maps.
    
    We will mostly denote Lie algebras with lowercase fraktur letters, or so called \verb*|\mathfrak| letters.
    For reference, here's the alphabet in \verb*|\mathfrak|:
    \begin{equation*}
        \symfrak{a\,b\,c\,d\,e\,f\,g\,h\,i\,j\,k\,l\,m\,n\,o\,p\,q\,r\,s\,t\,u\,v\,w\,x\,y\,z}
    \end{equation*}
	
	\chapter{Direct Sums and Diagonalisation}
	Our goal in this section is to state a definition of a diagonalisable linear operator in such a basis free way.
	We will then generalise this to get an \enquote{almost diagonal form} for arbitrary linear operators when \(\field\) is algebraically closed.
	To this end we will recap some linear algebra in this first section.
	
	\section{Direct Sums}
	\subsection{Binary Direct Sums}
	\begin{dfn}{Binary Direct Sum}{}
		Let \(V\) be a vector space over \(\field\), and let \(U_1, U_2 \subseteq V\) be subspaces.
		Then we say that \(V = U_1 \oplus U_2\), that is \(V\) is the (internal) \defineindex{direct sum} of \(U_1\) and \(U_2\), if
		\begin{itemize}
			\item \(V = U_1 + U_2 \coloneq \{u_1 + u_2 \mid u_1 \in U_1 \text{ and } u_2 \in U_2\}\); and
			\item \(U_1 \cap U_2 = 0\).
		\end{itemize}
		
		If \(U_1\) and \(U_2\) are vector spaces over \(\field\) then we can construct a vector space \(V = U_1 \oplus U_2\), also over \(\field\), by defining
		\begin{equation}
			V = \{(u_1, u_2) \mid u_1 \in U_1 \text{ and } u_2 \in U_2\}
		\end{equation}
		and defining addition and scalar multiplication by
		\begin{equation}
			(u_1, u_2) + (u_1', u_2') = (u_1 + u_1', u_2 + u_2') \qqand \lambda (u_1, u_2) = (\lambda u_1, \lambda u_2)
		\end{equation}
        for all \(u_1, u_1' \in U_1\), \(u_2, u_2' \in U_2\), and \(\lambda \in \field\).
	\end{dfn}
	
    After constructing the external direct sum we may identify \(U_1\) with the subspace consisting of elements of the form \((u_1, 0)\) with \(u_1 \in U_1\), and \(U_2\) with the subspace of elements of the form \((0, u_2)\) with \(u_2 \in U_2\).
    Then the external direct sum coincides with the internal direct sum.
    For this reason we won't distinguish internal and external direct sums, making such identifications as necessary.
    
    The following lemma gives an alternative characterisation of the direct sum.
    
    \begin{lma}{}{lma:binary direct sum gives decompositon of vectors}
        If \(V = U_1 \oplus U_2\) then every vector \(v \in V\) can be written \emph{uniquely} as a sum \(v = u_1 + u_2\) with \(u_1 \in U_1\) and \(u_2 \in U_2\).
        Conversely, if every \(v\) has a unique decomposition as \(v = u_1 + u_2\) with \(u_1 \in U_1\) and \(u_2 \in U_2\) then \(V = U_1 \oplus U_2\).
        \begin{proof}
            Suppose that \(V = U_1 \oplus U_2\).
            From the definition of a direct sum we know that \(v \in V = U_1 + U_2\) and as such \(v = u_1 + u_2\) for some \(u_1 \in U_1\) and \(u_2 \in U_2\) because this is how elements of \(U_1 + U_2\) are defined.
            We need only prove uniqueness.
            Suppose that \(v = u_1 + u_2\) and \(v = u_1' + u_2'\) with \(u_1, u_1' \in U_1\) and \(u_2, u_2' \in U_2\) are two decompositions of \(v\).
            Then we have \(u_1 + u_2 = u_1' + u_2'\), which we can rearrange to get \((u_1 - u_1') + (u_2 - u_2') = 0\).
            This means that \(u_1 - u_1' = u_2' - u_2 \eqqcolon w\).
            Now, \(u_1 - u_1' \in U_1\), since it's a linear combination of elements of \(U_1\), and similarly \(u_2' - u_2 \in U_2\).
            Thus, \(w \in U_1 \cap U_2 = 0\) and so \(w = 0\).
            
            Suppose instead that every \(v \in V\) has a unique decomposition as \(v = u_1 + u_2\).
            Then clearly \(v\) corresponds to \((u_1, u_2) \in U_1 \oplus U_2\) and if \(v = u_1 + u_2\) and \(v' = u_1' + u_2'\) and \(\lambda \in \field\) then \(v + \lambda v' = (u_1 + u_2) + \lambda(u_1' + u_2')\) corresponds to \((u_1, u_2) + \lambda(u_1', u_2')\), but also \(v + \lambda v' = (u_1 + \lambda u_1') + (u_2 + \lambda u_2')\) corresponds to \((u_1 + \lambda u_1', u_2 + \lambda u_2')\).
            This shows that this correspondence defines a linear map.
            Clearly this correspondence is invertible, and thus we have an isomorphism \(V \isomorphic U_1 \oplus U_2\).
        \end{proof}
    \end{lma}
    
    Thus, the direct sum may be characterised as giving a unique decomposition of each vector into a pair of vectors from two subspaces with only the zero vector in common.
    
    \subsection{Finite Direct Sums}
    Direct sums of two vector spaces generalise to direct sums of a finite number of vector spaces in an obvious way.
    
    \begin{dfn}{Finite Direct Sum}{}
        Let \(V\) be a vector space over \(\field\), and let \(U_1, \dotsc, U_r \subseteq V\) be subspaces.
        Then we say that
        \begin{equation}
            V = U_1 \oplus \dotsb \oplus U_r = \bigoplus_{i=1}^r U_i,
        \end{equation}
        that is \(V\) is the (internal) \define{direct sum} of the \(U_i\), if
        \begin{itemize}
            \item \(V = U_1 + \dotsb + U_r \coloneq \{u_1 + \dotsb + u_r \mid u_i \in U_i \text{for all } i = 1, \dotsc, r\}\); and
            \item for all \(i = 1, \dotsc, r\) we have \(U_i \cap (U_1 + \dotsb + U_{i-1} + U_{i+1} + \dotsb + U_r) = 0\) where the sum is over all subspaces apart from \(U_i\).
        \end{itemize}
    \end{dfn}
    
    Note that we can define the external direct sum, but care has to be taken as if we define \(U_1 \oplus U_2 \oplus U_3\) to consist of elements of the form \((u_1, u_2, u_3)\) then this is not the same as defining \(U_1 \oplus (U_2 \oplus U_3)\) to consist of elements of the form \((u_1, (u_2, u_3))\) and \((U_1 \oplus U_2) \oplus U_3\) to consist of elements of the form \(((u_1, u_2), u_3)\).
    However these spaces are naturally (in the technical sense) isomorphic, and as such we will identify them with each other.
    A complicated way to put this is that the direct sum is associative up to natural isomorphism.
    An even more complicated way to put this, along with the fact that \(V \oplus 0 \isomorphic V \isomorphic 0 \oplus V\), is that \((\Vect, \oplus, 0)\) is a monoidal category.
    
    The same characterisation of the direct sum giving a unique decomposition of \(v \in V\) carries over to finite direct sums.
    
    \begin{lma}{}{}
        If \(V = U_1 \oplus \dotsb \oplus U_r\) then for all \(v \in V\) there exist unique \(u_i \in U_i\) such that \(v = u_1 + \dotsb + u_r\).
        \begin{proof}
            We proceed by induction on \(r\).
            The case \(r = 2\) is \cref{lma:binary direct sum gives decompositon of vectors}.
            Suppose that the result holds some \(k \ge 2\) and that \(V = U_1 \oplus \dotsb \oplus U_k \oplus U_{k+1}\).
            Take \(v \in V\).
            Writing \(V = (U_1 \oplus \dotsb \oplus U_k) \oplus U_{k+1}\) we see that there are unique vectors \(u \in U_1 \oplus \dotsb \oplus U_k\) and \(u_{k+1} \in U_{k+1}\) such that \(v = u + u_{k+1}\).
            By the induction hypothesis since \(U_1 \oplus \dotsb \oplus U_k\) is a \(k\)-fold direct sum we can uniquely decompose \(u\) as \(u = u_1 + \dotsb + u_k\).
            Then \(v = u_1 + \dotsb + u_k + u_{k+1}\) gives a unique decomposition of \(v\).
            Thus, by induction the result holds for any finite direct sum.
        \end{proof}
    \end{lma}
    
    \subsection{Arbitrary Direct Sums}
    We can further generalise direct sums to arbitrary collections of spaces.
    One thing we have to be cautious about is that infinite sums of vectors are not defined, and we get around this by considering sums over infinite sets, but requiring that only finitely many of the vectors in the sum are not zero.
    More formally, we can view a sequence \((u_i)_{i \in I}\) of vectors as a function \(u \colon I \to V\) with \(u(i) = u_i\) and then we require that \(u\) has finite support.
    
    \begin{dfn}{Arbitrary Direct Sums}{}
        Let \(V\) be a vector space over \(\field\), and let \(\{U_i\}_{i \in I}\) be an indexed family of subspaces, that is \(U_i \subseteq V\) is a subspace for all \(i \in I\).
        Then we say that
        \begin{equation}
            V = \bigoplus_{i \in I} U_i,
        \end{equation}
        that is \(V\) is the (internal) \define{direct sum} of the \(U_i\), if
        \begin{itemize}
            \item we have
            \begin{equation*}
                V = \sum_{i\in I} U_i \coloneq \bigg\{ \sum_{i \in I} u_i \,\bigg\vert\, u_i \in U_i \text{ and } u_i \ne 0 \text{ for only finitely many terms}\bigg\};
            \end{equation*}
            \item for all \(i \in I\) we have \(U_i \cap \sum_{j \in I \setminus \{i\}} U_j = 0\).
        \end{itemize}
    \end{dfn}
    
    We can define the external direct sum by replacing sequences such as \((u_1, u_2, \dotsc)\) with functions \(I \to \bigcup_{j \in I} U_j\) with finite support, the idea being that \(u(i)\) corresponds to \(u_i\) in the case where \(I = \{1, \dotsc, r\}\).
    
    \begin{lma}{}{}
        If \(V = \bigoplus_{i \in I} U_i\) then for all \(v \in V\) there exist unique \(u_i \in U_i\) such that \(v = \sum_{i \in I} u_i\) with only finitely many of the \(u_i\) being nonzero.
        \begin{proof}
            The proof is essentially the same as before, but now we work with formally infinite sums in which most terms are zero, and all we have to do is check that at each stage this is still the case, which it is, since adding two such sums cannot result in infinitely many nonzero terms.
        \end{proof}
    \end{lma}
    
    \begin{exm}{}{}
        Let \(V = \field[x]\) be the vector space of polynomials in \(x\) with coefficients in \(\field\).
        That is
        \begin{equation}
            V = \Bigg\{ \sum_{i = 0}^N a_i x^i \,\Bigg\vert\, N \in \naturals \text{ and } a_i \in \field \Bigg\}.
        \end{equation}
        Addition is just addition of polynomials, and scaling is simply scaling each coefficient, which we can phrase as multiplying by the corresponding constant polynomial.
        We have the subspaces
        \begin{equation}
            U_i = \Span \{x^i\} \coloneq \{a x^i \mid a \in \field\}
        \end{equation}
        and it's easy to see that
        \begin{equation}
            V = \bigoplus_{i \in \naturals} U_i.
        \end{equation}
        Specifically, if we have a polynomial, \(f(x) \in \field[x]\), then
        \begin{equation}
            f(x) = a_0 + a_1x + a_2 x^2 + \dotsb + a_N x^N
        \end{equation}
        and \(a_0 \in U_0\), \(a_1 x \in U_1\), \(a_2 x^2 \in U_2\), and so on up to \(a_N x^N \in U_N\), so  we get a finite decomposition of \(f(x)\), and clearly this is unique, showing that \(\field[x]\) is indeed this direct sum as claimed.
        
        \begin{rmk}
            There's actually more structure here.
            First, we can multiply polynomials, so we have an algebra (see \cref{def:algebra}), not just a vector space.
            Define \(\deg f(t)\) to be the highest power of \(t\) appearing in \(f\) with a nonzero coefficient.
            Then \(U_i\) consists of all homogenous polynomials of degree \(i\) (homogenous meaning each term has the same degree).
            Further, since \(x^n x^m = x^{n + m}\) we have \(\deg (x^n x^m) = n + m\), and so \(U_iU_j \coloneq \{u_i u_j \mid u_i \in U_i \text{ and } u_j \in U_j\} = U_{i + j}\).
            When we have a decomposition like this and \(U_i U_j \subseteq U_{i + j}\) we say that \(V\) is an \(\naturals\)-graded algebra.
        \end{rmk}
    \end{exm}
    
    \begin{remark}{}{}
        The most general definition of the direct sum is as the coproduct in the category \(\Vect\).
    \end{remark}
    
    \section{Diagonalisable Operators and the Direct Sum Decomposition}
    In this section we'll give the standard definition of a diagonalisable operator, which should be familiar.
    We'll then give an equivalent definition which makes no mention of a basis.
    
    Let \(V\) be a finite dimensional vector space over \(\field\) with basis \(\symcal{B} = \{v_1, \dotsc, v_n\}\).
    Recall that if \(T \colon V \to V\) is a linear operator then its matrix in basis \(\symcal{B}\) is the matrix
    \begin{equation}
        [T]_{\symcal{B}} \coloneq 
        \begin{bmatrix}
            \uparrow & & \uparrow \\
            Tv_1 & \dots & Tv_n \\
            \downarrow & & \downarrow
        \end{bmatrix}
        =
        \begin{bmatrix}
            c_{11} & c_{12} & \dots & c_{1n}\\
            c_{21} & c_{22} & \dots & c_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            c_{n1} & c_{n2} & \dots & c_{nn}
        \end{bmatrix}
    \end{equation}
    where \(c_{ij}\) is the coefficient of \(v_i\) in \(Tv_j\) when expressed in the basis \(\symcal{B}\).
    That is, the \(i\)th column of \([T]_{\symcal{B}}\) is \(Tv_i\) as a column vector in basis \(\symcal{B}\).
    
    \begin{dfn}{Diagonalisable Operator}{}
        Let \(V\) be a finite dimensional vector space over \(\field\).
        Then a linear operator, \(T \colon V \to V\), is \defineindex{diagonalisable} if there exists some basis, \(\symcal{B}\), in which the matrix \([T]_{\symcal{B}}\) is diagonal.
    \end{dfn}
    
    Notice that if \(T \colon V \to V\) is a linear operator diagonalised by the basis \(\symcal{B} = \{v_1, \dotsc, v_n\}\) then
    \begin{equation}
        [T]_{\symcal{B}} = 
        \begin{bmatrix}
            \lambda_1 & & \\
            & \ddots & \\
            & & \lambda_n
        \end{bmatrix}
    \end{equation} 
    for some \(\lambda_i \in \field\).
    We can also express the basis vectors \(v_i\) in this basis, they are simply the standard basis, in which \([v_i]_{\symcal{B}} = e_i\) is the column vector with 1 in the \(i\)th position and 0 everywhere else.
    Then, for example, we have
    \begin{equation}
        [Tv_1]_{\symcal{B}} = [T]_{\symcal{B}}[v_1]_{\symcal{B}} = 
        \begin{bmatrix}
            \lambda_1 & & \\
            & \ddots & \\
            & & \lambda_n
        \end{bmatrix}
        \begin{bmatrix}
            1\\ \vdots\\ 0
        \end{bmatrix}
        =
        \begin{bmatrix}
            \lambda_1\\ \vdots\\ 0
        \end{bmatrix}
        = \lambda_1
        \begin{bmatrix}
            1\\ \vdots\\ 0
        \end{bmatrix}
        = \lambda_1 [v_1]_{\symcal{B}}.
    \end{equation}
    Thus, we have \(Tv_1 = \lambda v_1\), that is, \(v_1\) is an eigenvector of \(T\) with eigenvalue \(\lambda_1\).
    In general, \(v_i\) will be an eigenvector of \(T\) with eigenvalue \(\lambda_i\).
    
    Now make the following observations.
    We may define the subspaces \(U_i = \Span\{v_i\}\).
    The fact that \(V = \bigoplus_{i=1}^n U_i\) follows immediately from (and is equivalent to) the fact that \(\symcal{B}\) is a basis.
    Each subspace \(U_i\) is \(T\)-\defineindex{invariant}, meaning that \(T(U_i) \subseteq U_i\), which simply means that if \(u \in U_i\) then \(Tu \in U_i\) also, so it's not possible to leave \(U_i\) just by the action of \(T\).
    This is clear in this case because \(T\) acts on each subspace, \(U_i\), by scalar multiplication, specifically, by multiplication by \(\lambda_i\).
    Because of this it makes sense to consider the restricted linear map \(T|_{U_i} \colon U_i \to U_i\), defined by \(T|_{U_i}(u) = T(u)\) for \(u \in U_i\).
    By definition \(T(u) = \lambda_i u\) for \(u \in U_i\), and thus we have \(T = \lambda_i \id_{U_i}\) where \(\id_W \colon W \to W\) is the identity linear map.
    
    \subsection{Basis Independent Definition}
    We can now move towards making a basis independent definition of diagonalisability.
    To do so we need the notion of an eigenspace.
    
    \begin{dfn}{Eigenspace}{}
        Let \(\field\) be a field and let \(V\) be a vector space over \(V\).
        Let \(T \colon V \to V\) be a linear operator.
        Then for \(\alpha \in \field\) define the \defineindex{eigenspace} corresponding to \(\alpha\) to be
        \begin{equation}
            W_\alpha \coloneq \{w \in V \mid T(w) = \alpha w\}.
        \end{equation}
    \end{dfn}
    
    Note that \(W_\alpha\) can alternatively be characterised as
    \begin{equation}
        W_\alpha = \{w \in V \mid (T - \alpha)(w) = 0\} = \ker(T - \alpha)
    \end{equation}
    where we perform the common abuse of notation writing \(T - \alpha\) when we mean \(T - \alpha \id_V\).
    
    For example, in the case of a diagonalisable operator where the eigenvalues are all distinct, as discussed at the end of the previous section, the subspaces \(U_i\) are exactly the eigenspaces \(W_{\lambda_i}\).
    Also, \(W_{\alpha} = 0\) if \(\alpha \ne \lambda_i\) for any \(i\).
    This is a general fact, if \(\alpha\) is not an eigenvalue then there will be no \(w \in V\) satisfying \(T(w) = \alpha w\).
    
    The problem is that eigenvalues needn't be distinct, in fact, eigenvalues are distinct if and only if the nonzero eigenspaces have dimension 1.
    Regardless of this problem, we can still perform the direct sum decomposition from the last part, replacing the \(U_i\) with the eigenspaces \(W_\alpha\).
    In fact, this gives us an equivalent definition of diagonalisability without reference to a basis.
    
    \begin{dfn}{Diagonalisable Operator}{}
        Let \(V\) be a finite dimensional vector space over \(\field\).
        Then a linear operator, \(T \colon V \to V\), is \defineindex{diagonalisable} if 
        \begin{equation}
            V = \bigoplus_{\alpha \in \field} W_\alpha.
        \end{equation}
    \end{dfn}
    
    \subsection{Decomposition Theorem}
    We now ask if there is a generalisation of this decomposition to not-necessarily-diagonalisable operators.
    We'll start with an example.
    
    \begin{exm}{Non-diagonalisable Operator}{exm:E not diagonalisable}
        Consider the case \(\field = \complex\), \(V = \complex^2\), and
        \begin{equation}
            T = 
            \begin{bmatrix}
                0 & 1\\
                0 & 0
            \end{bmatrix}
        \end{equation}
        in the standard basis
        \begin{equation}
            e_1 = 
            \begin{bmatrix}
                1\\ 0
            \end{bmatrix},
            \qqand e_2 = 
            \begin{bmatrix}
                0\\ 1
            \end{bmatrix}
            .
        \end{equation}
        To find the eigenvalues we solve the characteristic polynomial, which is
        \begin{equation}
            \chi_T(t) = \det(\lambda - T) = \det
            \begin{bmatrix}
                0 & 1\\
                0 & 0
            \end{bmatrix}
            = \lambda^2.
        \end{equation}
        The only solution to \(\lambda^2 = 0\) is \(\lambda = 0\), and thus we have one repeated eigenvalue.
        This means that \(T\) is not diagonalisable.
        For \(\alpha \ne 0\) we have \(W_\alpha = 0\), and we have
        \begin{equation}
            W_0 = \{w \in V \mid Tw = 0\} = \ker(T - 0) = \ker T = \Span\{e_1\}.
        \end{equation}
        This follows simply by considering the action of \(T\) on the basis vectors, \(Te_1 = 0\) and \(Te_2 = e_1 \ne 0\).
        This means that
        \begin{equation}
            \bigoplus_{\alpha \in \complex} W_{\alpha} = W_0 = \Span\{e_1\} \ne \complex^2.
        \end{equation}
        
        From this calculation we see that \(T^2e_2 = 0\), and so if we instead consider the operator \(T^2\) then the eigenspaces of this cover all of \(\complex^2\), so this suggests that we should look not just at \(T\), but also powers of \(T\) in our decomposition.
        
        \begin{remark}{}{}
            The matrix
            \begin{equation}
                T = 
                \begin{bmatrix}
                    0 & 1\\
                    0 & 0
                \end{bmatrix}
                = E
            \end{equation}
            is important for the Lie algebra \(\specialLinearLie(2, \complex)\), which is one of the most important Lie algebras.
            The fact that this matrix is not diagonalisable is important in that it effects whether \(E\) commutes with other matrices.
            Recall that if two matrices can be simultaneously diagonalised then they commute, an important fact in quantum mechanics.
        \end{remark}
    \end{exm}
    
    Following on from the idea that we should look at powers of \(T\) we replace the eigenspace with the following definition.
    
    \begin{dfn}{Generalised Eigenspace}{}
        Let \(\field\) be a field and let \(T\) be a vector space over \(\field\).
        Let \(T \colon V \to V\) be a linear operator.
        Then for \(\alpha \in \field\) define the \defineindex{generalised eigenspace} corresponding to \(\alpha\) to be
        \begin{equation}
            V_\alpha \coloneq \{w \in V \mid (T - \alpha)^N(w) = 0 \text{ for some } N \in \naturals\}.
        \end{equation}
    \end{dfn}
    
    We can then often make a decomposition into a direct sum of generalised eigenspaces.
    There's just one hitch, this only works if \(\field\) is algebraically closed.
    
    \begin{dfn}{Algebraically Closed Field}{}
        A field, \(\field\), is algebraically closed if every nonconstant polynomial \(f(t) \in \field[t]\) has a root, \(\alpha \in \field\), such that \(f(\alpha) = 0\).
    \end{dfn}
    
    The complex numbers, \(\complex\), are algebraically closed, this is the fundamental theorem of algebra.
    The real numbers, \(\reals\), are not algebraically closed, for example \(t^2 + 1\) has no (real) roots.
    
    The following proof relies on three standard results, which we state without proof.
    \begin{thm}{Cayley--Hamilton}{}
        If \(\chi_T(t) = t^n + a_{n-1}t^{n-1} + \dotsb + a_1t + a_0\) is the characteristic polynomial of the linear operator \(T \colon V \to V\) then the linear operator
        \begin{equation}
            T^n + a_{n-1}T^{n-1} + \dotsb + a_1T + a_0\id_V \colon V \to V
        \end{equation}
        is the zero map.
        That is, \(T\) satisfies it's own characteristic polynomial, \(\chi_T(T) = 0\).
    \end{thm}
    
    \begin{lma}{B\'ezout's Lemma}{}
        If \(R\) is a principal ideal domain and \(x, y \in R\) have greatest common divisor \(d\) then there exist \(a, b \in R\) such that \(ax + by = d\).
    \end{lma}
    
    \begin{lma}{}{}
        If \(\field\) is a field then \(\field[x]\) is a PID, and in particular B\'ezout's lemma applies to polynomials.
    \end{lma}
    
    \begin{thm}{Decomposition Theorem}{}
        Let \(\field\) be an algebraically closed field, \(V\) a finite dimensional vector space, and \(T \colon V \to V\) a linear operator.
        Then
        \begin{equation}
            V = \bigoplus_{\alpha \in \field} V_\alpha 
        \end{equation}
        where \(V_\alpha\) is the generalised eigenspace associated with \(\alpha\) and only finitely many of the \(V_\alpha\) are nonzero.
        \begin{proof}
            We start by showing that \(V_\alpha \cap \sum_{\beta \in \field \setminus \{\alpha\}} V_\beta = 0\).
            Take \(v V_\alpha \cap \sum_{\beta \in \field \setminus \alpha} V_\beta\), then we have \(v \in V_\alpha\) and \(v \in V_{\beta_1} + \dotsb + V_{\beta_r}\) for some \(\beta_1, \dotsc, \beta_r \in \field \setminus \{\alpha\}\).
            This means that there exist some \(N, N_1, \dotsc, N_r \in \naturals\) such that
            \begin{equation}
                (T - \alpha)^Nv = (T - \beta_1)^{N_1} \dotsm (T - \beta_r)^{N_r} v = 0.
            \end{equation}
            Since \(\alpha \ne \beta_i\) we know that the greatest common factor of \((t - \alpha)^N\) and \((t - \beta_1)^{N_1} \dotsm (t - \beta_r)^{N_r}\) is 1.
            Thus, by B\'ezout's lemma there exist polynomials \(f(t), g(t) \in \field[t]\) such that
            \begin{equation}
                f(t)(t - \alpha)^N + g(t)(t - \beta_1)^{N_1} \dotsm (t - \beta_r)^{N_r} = 1.
            \end{equation}
            Evaluating at \(T\) and applying this to \(v\) we then have
            \begin{equation}
                f(T)(T - \alpha)^Nv + g(T)(T - \beta_1)^{N_1}v \dotsm (T - \beta_r)^{N_r}v = v,
            \end{equation}
            and we know that these operators acting on \(v\) both give zero, so the left hand side vanishes, and thus \(v = 0\), and so \(V_\alpha \cap \sum_{\beta \in \field \setminus \{\alpha\}} V_\beta = 0\).
            
            We now show that \(\sum_{\alpha \in \field} V_\alpha = V\).
            Factorise the characteristic polynomial, \(\chi_T(t)\), as follows
            \begin{equation}
                \chi_T(t) = (t - \alpha_1)^{N_1} \dotsm (t - \alpha_s)^{N_s},
            \end{equation}
            with \(\alpha_i \ne \alpha_j\) for \(i \ne j\).
            Note that the existence of such a factorisation relies on \(\field\) being algebraically closed.
            We claim that \(v \in V_{\alpha_1} + \dotsb + V_{\alpha_s}\).
            This can be proven by induction on \(s\).
            The basis case, \(s = 1\), has the characteristic polynomial factorise as
            \begin{equation}
                \chi_T(t) = (t - \alpha_1)^{N_1}.
            \end{equation}
            Then by the Cayley--Hamilton theorem we have
            \begin{equation}
                (T - \alpha_1)^{N_1}v = 0
            \end{equation}
            for all \(v \in V\), and thus \(V_{\alpha_1} = V\).
            For the inductive step suppose that \(s > 1\) and the statement is true for \(s - 1\).
            The highest common factor of \((t - \alpha_s)^{N_s}\) and \((t - \alpha_1)^{N_1} \dotsm (t - \alpha_{s-1})^{N_{s-1}}\) is 1.
            Thus, by B\'ezout's lemma there exist \(f(t), g(t) \in \field[t]\) such that
            \begin{equation}
                f(t)(t - \alpha_s)^{N_s} + g(t)(t - \alpha_1)^{N_1} \dotsm (t - \alpha_{s-1})^{N_{s-1}} = 1.
            \end{equation}
            Evaluating at \(T\) and applying the map to \(v \in V\) we have
            \begin{equation}
                f(T)(T - \alpha_s)^{N_s}v + g(T)(T - \alpha_1)^{N_1} \dotsm (T - \alpha_{s-1})^{N_{s-1}}v = v.
            \end{equation}
            Define
            \begin{equation}
                v' = f(T)(T - \alpha_s)^{N_s}v, \qqand v_s = g(T)(T - \alpha_1)^{N_1} \dotsm (T - \alpha_{s-1})^{N_{s-1}}v,
            \end{equation}
            so we have \(v = v' + v_s\).
            Note that we have \((T - \alpha_s)^{N_s}v_s = 0\) and \((T - \alpha_1)^{N_1} \dotsm (T - \alpha_{s-1})^{N_{s-1}}v' = 0\).
            This is simply the Cayley--Hamilton theorem applied to the linear map \(T\) restricted to the subspaces corresponding to \(v_s\) and \(v'\) respectively.
            Define
            \begin{equation}
                V' = \{u \in V \mid (T - \alpha_1)^{N_1} \dotsm (T - \alpha_{s-1})^{N_{s-1}}u = 0\}.
            \end{equation}
            Then \(v_s \in V_{\alpha_s}\) and \(v' \in V'\).
            The characteristic polynomial of the restricted map, \(T|_{V'}\), has all of its roots in \(\{\alpha_1, \dotsc, \alpha_{s-1}\}\).
            By induction, we therefore have \(v' = v_1 + \dotsb + v_{s-1}\) with \(v_i \in V_{\alpha_i}' = V' \cap V_{\alpha_i}\).
            Thus, \(v = v_1 + \dotsb + v_{s-1} + v_s in V_{\alpha_1} + \dotsb + V_{\alpha_s}\).
            Since \(v \in V\) was arbitrary this shows that \(V = V_{\alpha_1} + \dotsb + V_{\alpha_s}\), proving that \(V = V_{\alpha_1} \oplus \dotsb \oplus V_{\alpha_s}\).
            This gives the final result by realising that if \(\beta \ne \alpha_i\) then \(V_\beta = 0\) since \(V_\beta \cap (V_{\alpha_1} + \dotsb + V_{\alpha_s}) = 0\) so \(V_\beta \cap V = 0\), so \(V_\beta = 0\).
            Thus, \(V = \bigoplus_{\alpha \in \field} V_{\alpha}\) and \(V_{\alpha}\) is only nonzero for the finite index set \(\{\alpha_1, \dotsc, \alpha_s\}\).
        \end{proof}
    \end{thm}
    
    Note that both conditions, the finite dimension of \(V\) and the algebraic closure of \(\field\), are required here.
    Dropping either condition will result in cases where this decomposition doesn't exist.
    
    \begin{exm}{}{}
        Consider again the example of \(\field = \complex\), \(V = \complex^2\), and
        \begin{equation}
            T = 
            \begin{bmatrix}
                0 & 1\\
                0 & 0
            \end{bmatrix}
            .
        \end{equation}
        As we saw before (\cref{exm:E not diagonalisable}) \(T\) is not diagonalisable, and \(\bigoplus_{\alpha \in \complex} W_\alpha = W_0 = \Span\{e_1\} \ne \complex^2\).
        We also saw there that \(T^2e_2 = 0\), meaning that when considering generalised eigenspaces we also have \(e_2 \in V_0\), and thus
        \begin{equation}
            \bigoplus_{\alpha \in \complex} V_\alpha = V_0 = \complex^2.
        \end{equation}
    \end{exm}
    
    \begin{dfn}{Jordan Block and Jordan Normal Form}{}
        A \defineindex{Jordan block} is a matrix of the form
        \begin{equation}
            \begin{bmatrix}
                \alpha & 1 \\
                & \alpha & 1 \\
                & & \ddots & \ddots\\
                & & & \alpha & 1\\
                & & & & \alpha
            \end{bmatrix}
            .
        \end{equation}
        That is, we have some \(\alpha \in \field\) on the diagonal and 1s above the diagonal, and 0 everywhere else.
        
        A matrix, \(X\), is a sum of Jordan blocks, or is in \defineindex{Jordan normal form} if it is given by the block diagonal matrix
        \begin{equation}
            X = 
            \begin{bmatrix}
                J_1\\
                & \ddots \\
                & & J_r
            \end{bmatrix}
        \end{equation}
        with the \(J_i\) Jordan blocks.
    \end{dfn}
    
    \begin{crl}{Existence of Jordan Normal Form}{}
        Let \(\field\) be an algebraically closed field, let \(V\) be a finite dimensional vector space, and let \(T \colon V \to V\) be a linear map.
        There exists a basis of \(V\) such that \([T]_{\symcal{B}}\) is in Jordan normal form.
        \begin{proof}
            Consider the decomposition of \(V\) into a sum of generalised eigenspaces of \(T\).
            Each generalised eigenspace is invariant under \(T\), and thus if \(v_\alpha \in V_\alpha\) then \(Tv_\alpha = c_1v_1 + \dotsb c_rv_r\) where \(\{v_1, \dotsc, v_r\}\) is a basis for \(V_\alpha\).
            We can further choose this basis by scaling and taking linear combinations untile \(Tv_{i} = \alpha v_i + v_{i+1}\), which can be shown by induction on the size of the dimension of \(V_\alpha\).
            Then \(T\) acts on \(V_\alpha\) in this basis as a Jordan block, and so \(T\) acts on \(V\), in the basis formed by the union of these bases, as a matrix in Jordan normal form.
        \end{proof}
    \end{crl}
    
    \chapter{Lie Algebras}
    \section{Definition and Remarks}
    \begin{dfn}{Lie Algebra}{}
        Let \(\field\) be a field.
        A \defineindex{Lie algebra} over \(\field\) is a vector space, \(\lie{g}\), over \(\field\) equipped with a bilinear map,
        \begin{align}
            \bracket{-}{-} \colon \lie{g} \times \lie{g} &\to \lie{g}\\
            (X, Y) &\mapsto \bracket{X}{Y},
        \end{align}
        called the Lie bracket, or simply the bracket.
        This must satisfy two properties:
        \begin{enumerate}
            \item \defineindex{alternating}: for all \(X \in \lie{g}\) we have \(\bracket{X}{X} = 0\);
            \item \defineindex{Jacobi identity}: for all \(X, Y, Z \in \lie{g}\) we have
            \begin{equation}
                \bracket{X}{\bracket{Y}{Z}} + \bracket{Y}{\bracket{Z}{X}} + \bracket{Z}{\bracket{X}{Y}} = 0.
            \end{equation}
        \end{enumerate}
    \end{dfn}
    
    Bilinearity means that for all \(\lambda \in \field\) and \(X, Y, Z \in \lie{g}\) we have
    \begin{align}
        \bracket{X + \lambda Y}{Z} &= \bracket{X}{Z} + \lambda \bracket{Y}{Z},\\
        \bracket{X}{Y + \lambda Z} &= \bracket{X}{Y} + \lambda \bracket{X}{Z}.
    \end{align}
    
    We say that \(\bracket{-}{-}\) is antisymmetric if \(\bracket{X}{Y} = -\bracket{Y}{X}\).
    It turns out that this property is almost the same as, but not quite, the alternating property, and as such the definition of a Lie algebra is often given with antisymmetry in place of alternativity, the catch being that over fields of characteristic 2 these aren't equivalent.
    Alternativity is the \enquote{correct} condition, but if we're only looking at \(\field = \complex, \reals\), as is often the case, then there's little harm in taking antisymmetry as the defining condition.
    
    \begin{lma}{}{}
        The Lie bracket is antisymmetric.
        \begin{proof}
            Let \(\lie{g}\) be a Lie algebra and take \(X, Y \in \lie{g}\).
            Consider the bracket of \(X + Y\) with itself.
            On the one hand, alternativity tells us that \(\bracket{X + Y}{X + Y} = 0\), and on the other hand using bilinearity and alternativity we have
            \begin{align}
                \bracket{X + Y}{X + Y} &= \bracket{X}{X} + \bracket{X}{Y} + \bracket{Y}{X} + \bracket{Y}{Y}\\
                &= \bracket{X}{Y} + \bracket{Y}{X}
            \end{align}
            Now, if this is to vanish we must have
            \begin{equation}
                \bracket{X}{Y} = -\bracket{Y}{X}. \qedhere
            \end{equation}
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{}
        Over a field of characteristic other than 2 antisymmetry of a bilinear bracket implies alternativity.
        \begin{proof}
            Let \(\lie{g}\) be a vector space over \(\field\) with \(\Char \field \ne 2\), equipped with an \emph{antisymmetric} bracket, \(\bracket{-}{-} \colon \lie{g} \times \lie{g} \to \lie{g}\).
            Antisymmetry means that \(\bracket{X}{Y} = -\bracket{Y}{X}\) for all \(X, Y \in \lie{g}\).
            In particular, we are free to take \(X = Y\), then we have \(\bracket{X}{X} = -\bracket{X}{X}\).
            Rearranging this gives
            \begin{equation}
                \bracket{X}{X} + \bracket{X}{X} = 2 \bracket{X}{X} = 0.
            \end{equation}
            Now, in a field with \(2 \ne 0\) (that is, in a field of characteristic other than 2) this immediately implies \(\bracket{X}{X} = 0\).
        \end{proof}
    \end{lma}
    
    This distinction is subtle, and ultimately not that important since we'll mostly concern ourselves with \(\field = \complex\), which has characteristic 0 (and \(\Char \reals = 0\) also).
    
    The Jacobi identity is a bit weird the first time you see it.
    There are a couple of ways to think about it that help to remember the definition.
    The first is to notice that the second and third term are cyclic permutations of the first, so it's often shortened to
    \begin{equation}
        \bracket{X}{\bracket{Y}{Z}} + \text{cycilc permutations} = 0.
    \end{equation}
    The second is to write it like
    \begin{equation}
        \bracket{X}{\bracket{Y}{Z}} = \bracket{\bracket{X}{Y}}{Z} + \bracket{Y}{\bracket{X}{Z}},
    \end{equation}
    which is an equivalent form.
    This doesn't look much simpler, but fix some \(X \in \lie{g}\) and define a linear map \(D \colon \lie{g} \to \lie{g}\) by \(D(Y) = \bracket{X}{Y}\).
    Then this becomes
    \begin{equation}
        D(\bracket{Y}{Z}) = \bracket{D(Y)}{Z} + \bracket{Y}{D(Z)}.
    \end{equation}
    To make this even simpler, write \(A \cdot B\) in place of \(\bracket{A}{B}\), and we have
    \begin{equation}
        D(Y \cdot Z) = D(Y) \cdot Z + Y \cdot D(Z)
    \end{equation}
    and we see that this is a version of the product rule, or Leibniz rule.
    So \(D\) acts a bit like a derivative.
    The fancy way to say this is that the adjoint representation of \(\lie{g}\) acts on \(\lie{g}\) by derivations.
    A derivation is just any linear map satisfying the Leibniz rule, and the \enquote{adjoint representation} is simply \(\lie{g}\) acting on itself where \(X\) acts on \(Y\) by \(X \cdot Y \mapsto \bracket{X}{Y}\).
    
    Now\footnote{I've brought this forwards in the notes, it feels wrong to define objects and not move on to morphisms immediately.} that we've defined Lie algebras, an algebraic object, we should define maps between them.
    The appropriate maps will be \enquote{structure preserving}, in the same way that a group homomorphism is structure preserving (it preserves the multiplicative structure) and a linear map is structure preserving (it preserves addition and scalar multiplication).
    The structure that we have to preserve here is the Lie bracket, as well as the underlying vector space structure of the Lie algebra.
    
    \begin{dfn}{Homomorphism}{}
        Let \(\lie{g}\) be a Lie algebra with bracket \(\bracket{-}{-}_{\lie{g}}\), and let \(\lie{h}\) be a Lie algebra with bracket \(\bracket{-}{-}_{\lie{h}}\).
        Then a \defineindex{Lie algebra homomorphism}, \(\varphi \colon \lie{g} \to \lie{h}\), is a linear map which preserves the bracket, meaning that for all \(X, Y \in \lie{g}\) we have
        \begin{equation}
            \varphi(\bracket{X}{Y}_{\lie{g}}) = \bracket{\varphi(X)}{\varphi(Y)}_{\lie{h}}.
        \end{equation}
        An invertible Lie algebra homomorphism is a \defineindex{Lie algebra isomorphism}.
    \end{dfn}
    
    We'll usually just say \enquote{homomorphism}, or simply \enquote{morphism}, rather than \enquote{Lie algebra homomorphism}, and likewise we'll just speak of an \enquote{isomorphism} rather than a \enquote{Lie algebra isomorphism}.
    We will writhe \(\lie{g} \isomorphic \lie{h}\) if there is a Lie algebra isomorphism \(\lie{g} \to \lie{h}\).
    For notational simplicity we'll usually write \(\bracket{-}{-}\) for the bracket of both \(\lie{g}\) and \(\lie{h}\), allowing context (i.e., what elements we put into it) to make clear which we mean.
    
    We now prove the standard results about morphisms.
    
    \begin{lma}{}{}
        If \(\varphi \colon \lie{g} \to \lie{h}\) is a Lie algebra isomorphism then \(\varphi^{-1} \colon \lie{h} \to \lie{g}\) is a Lie algebra isomorphism.
        \begin{proof}
            We need to show that for all \(X', Y' \in \lie{h}\) we have
            \begin{equation}
                \varphi^{-1}(\bracket{X'}{Y'}_{\lie{h}}) = \bracket{\varphi^{-1}(X')}{\varphi^{-1}(Y')}_{\lie{g}}.
            \end{equation}
            To do so note that \(\varphi^{-1}\) is an invertible linear map, and as such is a bijection.
            This means that there exist unique \(X, Y \in \lie{g}\) such that \(\varphi(X) = X'\) and \(\varphi(Y) = Y'\).
            Thus, we may write the left hand side as
            \begin{equation}
                \varphi^{-1}(\bracket{X'}{Y'}_{\lie{h}}) = \varphi^{-1}(\bracket{\varphi(X)}{\varphi(Y)}_{\lie{h}}).
            \end{equation}
            Using the fact that \(\varphi\) is an isomorphism, and hence is a homomorphism, we can pull the \(\varphi\) out of the bracket on the right, to give
            \begin{equation}
                \varphi^{-1}(\bracket{X'}{Y'}_{\lie{h}}) = \varphi^{-1}(\varphi(\bracket{X}{Y}_{\lie{g}})).
            \end{equation}
            Then, since by definition \(\varphi^{-1} \circ \varphi = \id_{\lie{g}}\) we have
            \begin{equation}
                \varphi^{-1}(\bracket{X'}{Y'}_{\lie{h}}) = \bracket{X}{Y}_{\lie{g}}.
            \end{equation}
            We can then invert \(X' = \varphi(X)\) and \(Y' = \varphi(Y)\) to write \(X = \varphi^{-1}(X')\) and \(Y = \varphi^{-1}(Y')\).
            This lets us write this as
            \begin{equation}
                \varphi^{-1}(\bracket{X'}{Y'}_{\lie{h}}) = \bracket{\varphi^{-1}(X')}{\varphi^{-1}(Y')}_{\lie{g}},
            \end{equation}
            which is the result we wanted.
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{}
        The composite of Lie algebra homomorphisms is a Lie algebra homomorphism.
        \begin{proof}
            Let \(\lie{g}\), \(\lie{h}\), and \(\lie{l}\) be Lie algebras with brackets \(\bracket{-}{-}_{\lie{g}}\), \(\bracket{-}{-}_{\lie{h}}\), and \(\bracket{-}{-}_{\lie{l}}\) respectively.
            Let \(\varphi \colon \lie{g} \to \lie{h}\) and \(\psi \colon \lie{h} \to \lie{l}\) be Lie algebra homomorphisms.
            Then we may consider \(\psi \circ \varphi \colon \lie{g} \to \lie{l}\).
            We wish to show that this is a Lie algebra homomorphism.
            
            First, note that \(\psi\) and \(\varphi\) are linear, so their composite is too.
            We then need only show that for all \(X, Y \in \lie{g}\) we have
            \begin{equation}
                (\psi \circ \varphi)(\bracket{X}{Y}_{\lie{g}}) = \bracket{(\psi \circ \varphi)(X)}{(\psi \circ \varphi)(Y)}_{\lie{l}}.
            \end{equation}
            Starting with the left-hand side we can use the definition of composition to write
            \begin{equation}
                (\psi \circ \varphi)(\bracket{X}{Y}_{\lie{g}}) = \psi(\varphi(\bracket{X}{Y}_{\lie{g}})).
            \end{equation}
            Using the fact that \(\varphi\) is a Lie algebra homomorphism into \(\lie{h}\) we have \(\varphi(\bracket{X}{Y}_{\lie{g}}) = \bracket{\varphi(X)}{\varphi(Y)}_{\lie{h}}\), so we have
            \begin{equation}
                (\psi \circ \varphi)(\bracket{X}{Y}_{\lie{g}}) = \psi(\bracket{\varphi(X)}{\varphi(Y)}_{\lie{h}}).
            \end{equation}
            Now, using the fact that \(\psi\) is a Lie algebra homomorphism into \(\lie{l}\) we have \(\psi(\bracket{X'}{Y'}_{\lie{h}}) = \bracket{\psi(X')}{\psi(Y')}_{\lie{l}}\) for any \(X', Y' \in \lie{h}\), and in particular this is true when \(X' = \varphi(X)\) and \(Y' = \varphi(Y)\), giving
            \begin{equation}
                (\psi \circ \varphi)(\bracket{X}{Y}_{\lie{g}}) = \bracket{\psi(\varphi(X))}{\psi(\varphi(Y))}_{\lie{l}}.
            \end{equation}
            Finally, using the definition of composition again, we get our result,
            \begin{equation}
                (\psi \circ \varphi)(\bracket{X}{Y}_{\lie{g}}) = \bracket{(\psi \circ \varphi)(X)}{(\psi \circ \varphi)(Y)}_{\lie{l}}. \qedhere
            \end{equation}
        \end{proof}
    \end{lma}
    
    Noting that function composition is associative we have the following result.
    
    \begin{crl}{}{}
        For a field \(\field\) there is a category \(\LieAlg\) whose objects are Lie algebras over \(\field\) and whose morphisms are Lie algebra homomorphisms.
    \end{crl}
    
    \section{Subalgebras, Ideals, and Quotients}
    To state these definitions it's useful to abuse the notation slightly as follows.
    Let \(U, V \subseteq \lie{g}\) be subspaces of a Lie algebra.
    Define the subspace
    \begin{equation}
        \bracket{U}{V} = \Span\{\bracket{u}{v} \mid u \in U \text{ and } v \in V\}.
    \end{equation}
    
    \begin{dfn}{Subalgebra}{}
        Let \(\lie{g}\) be a Lie algebra.
        A \defineindex{Lie subalgebra} (or just \defineindex{subalgebra}), \(\lie{h}\), is a subspace which is closed under the Lie bracket.
        That is, \(\bracket{\lie{h}}{\lie{h}} \subseteq \lie{h}\), or equivalently for all \(h, h' \in \lie{h}\) we have \(\bracket{h}{h'} \in \lie{h}\).
    \end{dfn}
    
    The notion of a Lie subalgebra is the Lie algebra analogue of a subset of a set, subspace of a vector space, subgroup of a group, or subring of a ring.
    We can't form a quotient by an arbitrary subset, subgroup or subring, we need a set generated by an equivalence relation, a normal subgroup, or an ideal.
    This is exactly the case with Lie algebras as well.
    
    Note that a Lie subalgebra, \(\lie{h}\), is necessarily a Lie algebra in its own right after restricting the bracket to \(\lie{h}\).
    This follows immediately because the requirements for a Lie algebra are universally quantified so remain true after restricting to a subspace.
    
    \begin{dfn}{Ideal}{}
        Let \(\lie{g}\) be a Lie algebra.
        An \defineindex{ideal} of \(\lie{g}\) is a Lie subalgebra, \(\lie{i}\), such that \(\bracket{\lie{i}}{\lie{g}} \subseteq \lie{i}\), or equivalently for all \(X \in \lie{g}\) and \(I \in \lie{i}\) we have \(\bracket{X}{I} \in \lie{i}\).
    \end{dfn}
    
    Note that there's no notion of left- or right-ideals, any ideal is two sided since \(\bracket{\lie{h}}{\lie{h}'} = \bracket{\lie{h}}{\lie{h}'}\), as elements of the two differ only by a sign and the fact that these are subspaces means that all the elements that differ only by a sign are also included.
    
    With ideals we can define quotients.
    
    \begin{dfn}{Quotient}{}
        Let \(\lie{g}\) be a Lie algebra, and let \(\lie{i} \subseteq \lie{g}\) be an ideal.
        Then the \defineindex{quotient} vector space \(\lie{g}/\lie{i}\) is a Lie algebra when we define the bracket by
        \begin{equation}
            \bracket{X + \lie{i}}{Y + \lie{i}} = \bracket{X}{Y} + \lie{i}.
        \end{equation}
    \end{dfn}
    
    Note that in the definition of the bracket of \(\lie{g}/\lie{i}\) the bracket \(\bracket{X + \lie{i}}{Y + \lie{i}}\) is computed in \(\lie{g}/\lie{i}\), and the bracket \(\bracket{X}{Y}\) is computed in \(\lie{g}\).
    
    \begin{lma}{}{}
        The quotient algebra as defined above is a Lie algebra.
        \begin{proof}
            Let \(\lie{g}\) be a Lie algebra and let \(\lie{i}\) be an ideal.
            We have three things to show:
            \begin{enumerate}
                \item The bracket on \(\lie{g}/\lie{i}\) is well-defined;
                \item The bracket on \(\lie{g}/\lie{i}\) is alternating;
                \item The bracket on \(\lie{g}/\lie{i}\) satisfies the Jacobi identity.
            \end{enumerate}
            The first relies on \(\lie{i}\) being an ideal, and the others are simply an exercise in algebra.
            
            \Step{Well-Defined}
            Let \(X, X' \in \lie{g}\) be such that \(X + \lie{i} = X' + \lie{i}\).
            Recall that this means \(X - X' \in \lie{i}\).
            Then for all \(Y \in \lie{g}\) we have
            \begin{equation}
                \bracket{X + \lie{i}}{Y + \lie{i}} = \bracket{X}{Y} + \lie{i}
            \end{equation}
            and
            \begin{equation}
                \bracket{X' + \lie{i}}{Y + \lie{i}} = \bracket{X'}{Y} + \lie{i}.
            \end{equation}
            We need to show that these are equal.
            This means we need to show that
            \begin{equation}
                \bracket{X}{Y} - \bracket{X'}{Y} \in \lie{i}.
            \end{equation}
            This is necessarily true however, since bilinearity gives us
            \begin{equation}
                \bracket{X}{Y} - \bracket{X'}{Y} = \bracket{X - X'}{Y}
            \end{equation}
            which is in \(\lie{i}\) since \(X - X' \in \lie{i}\) and \(\lie{i}\) is an ideal.
            
            \Step{Alternating}
            Let \(X \in \lie{g}\), then
            \begin{equation}
                \bracket{X + \lie{i}}{X + \lie{i}} = \bracket{X}{X} + \lie{i} = 0 + \lie{i} = \lie{i} = 0
            \end{equation}
            since \(\lie{i} = 0 + \lie{i}\) is the zero vector in the quotient space \(\lie{g}/\lie{i}\).
            
            \Step{Jacobi Identity}
            Let \(X, Y, Z \in \lie{g}\),
            \begin{equation}
                \bracket{X + \lie{i}}{\bracket{Y + \lie{i}}{Z + \lie{i}}} = \bracket{X + \lie{i}}{\bracket{Y}{Z} + \lie{i}} = \bracket{X}{\bracket{Y}{Z}} + \lie{i}.
            \end{equation}
            Doing this with the other terms in the Jacobi identity we see that we end up with the left hand side of the Jacobi identity in \(\lie{g}/\lie{i}\) being
            \begin{equation}
                (\bracket{X}{\bracket{Y}{Z}} + \lie{i}) + (\bracket{Y}{\bracket{Z}{X}} + \lie{i}) + (\bracket{Z}{\bracket{X}{Y}} + \lie{i}).
            \end{equation}
            Recall that addition in the quotient space is defined by \((X + \lie{i}) + (Y + \lie{i}) = (X + Y) + \lie{i}\), and so this becomes
            \begin{equation}
                (\bracket{X}{\bracket{Y}{Z}} + \bracket{Y}{\bracket{Z}{X}} + \bracket{Z}{\bracket{X}{Y}}) + \lie{i} = 0 + \lie{i} = 0
            \end{equation}
            where we've used the Jacobi identity in \(\lie{g}\), and then identified \(0 + \lie{i}\) as the zero vector in the quotient \(\lie{g}/\lie{i}\).
        \end{proof}
    \end{lma}
    
    \section{Examples of Lie Algebras}
    We've done quite a lot now without ever actually looking at any examples of a Lie algebra, so let's change that.
    
    \subsection{Abelian Lie Algebra}
    We start with the simplest example, where the bracket always vanishes.
    
    \begin{dfn}{Abelian Lie Algebra}{}
        A Lie algebra, \(\lie{g}\), is \define{abelian}\index{abelian Lie algebra} if \(\bracket{\lie{g}}{\lie{g}} = 0\), that is, if \(\bracket{X}{Y} = 0\) for all \(X, Y \in \lie{g}\).
    \end{dfn}
    
    The terminology here, abelian, should not be confused with this terminology referring to groups.
    An abelian Lie algebra is not one in which the bracket is \enquote{commutative} (although it is, since every bracket is the 0 we do technically have \(\bracket{X}{Y} = \bracket{Y}{X}\)).
    Instead, this terminology is inherited from the Lie group, an abelian Lie group will give us an abelian Lie algebra.
    
    Note that, up to isomorphism, there is one abelian Lie algebra of each dimension for each field.
    Any vector space can be made into an abelian Lie algebra by equipping it with the trivial bracket, and any two such Lie algebras are isomorphic so long as the underlying vector spaces are.
    For this reason abelian Lie algebras on their own are \enquote{boring}, we only look at them when they arise naturally as subalgebras of nonabelian Lie algebras.
    
    \subsection{Low Dimension}
    In an attempt to classify Lie algebras (something we'll return to later) one might start with dimension.
    Let's do that.
    For dimension 0 there is one Lie algebra, the zero vector space with the trivial bracket.
    
    For dimension 1 (and a fixed field) there is one Lie algebra (up to isomorphism), the one-dimensional space equipped with the trivial bracket.
    Note that there cannot be a nonabelian one-dimensional Lie algebra.
    Suppose that \(\lie{g}\) was such a Lie algebra, then \(\lie{g} = \field X = \Span\{X\}\) for some \(X\).
    Then if \(Y, Z \in \lie{g}\) we know that \(Y = \lambda X\) and \(Z = \mu X\), and then
    \begin{equation}
        \bracket{Y}{Z} = \bracket{\lambda X}{\mu X} = \lambda \mu \bracket{X}{X} = \lambda \mu \cdot 0 = 0.
    \end{equation}
    Thus, all one-dimensional Lie algebras are necessarily abelian.
    
    For dimension 2 (and a fixed field) we of course have the abelian Lie algebra, but there is another.
    Suppose that \(\lie{g}\) is two-dimensional with basis \(\{X, Y\}\).
    Assuming that \(\lie{g}\) is nonabelian the bracket \(\bracket{X}{Y} = -\bracket{Y}{X}\) must be nonzero.
    This means that \(\bracket{\lie{g}}{\lie{g}}\) is a one-dimensional subspace of \(\lie{g}\), since every element of \(\bracket{\lie{g}}{\lie{g}}\) is a linear combination of brackets of elements of \(\lie{g}\), but all nonzero such brackets are multiples of \(\bracket{X}{Y}\).
    Further, we can rescale \(X\) so that \(\bracket{X}{Y} = Y\), and this is the only 2-dimensional non-abelian Lie algebra up to isomorphism (for a fixed field).
    So, there are two Lie algebras of dimension 2 up to isomorphism.
    
    In three dimensions one can make similar arguments, and it turns out that there are infinitely many isomorphism classes of three-dimensional Lie algebra, but we can still classify them.
    Here are the isomorphism classes over \(\complex\) (or any algebraically closed field of characteristic 0):
    \begin{itemize}
        \item The abelian Lie algebra of dimension 3.
        \item \(\lie{g} = \Span\{X, Y, Z\}\) with \(\bracket{X}{Y} = Z\) and \(\braket{X}{Z} = \bracket{Y}{Z} = 0\).
        \item \(\lie{g} = \Span\{X, Y, Z\}\) with \(\bracket{X}{Y} = Y\) and \(\bracket{X}{Z} = \bracket{X}{Z} = 0\).
        \item \(\lie{g}_b = \Span\{X, Y, Z\}\) with \(\bracket{X}{Y} = Y\), \(\bracket{Y}{Z} = bZ\), and \(\bracket{Y}{Z} = 0\) for \(b \in \complex^{\times}\).
        Note that \(\lie{g}_b \isomorphic \lie{g}_{b'}\) if and only if \(b = b'\) or \(b = 1/b'\).
        \item \(\lie{g} = \Span\{X, Y, Z\}\) with \(\bracket{X}{Y} = Y\), \(\bracket{X}{Z} = Y + Z\), and \(\bracket{Y}{Z} = 0\).
        \item \(\lie{g} = \Span\{X, Y, Z\}\) with \(\bracket{X}{Y} = 2X\), \(\bracket{X}{Z} = -2Z\), and \(\bracket{Y}{Z} = X\).
    \end{itemize}
    Of these, by far the most important is the last one, which we'll later see is the Lie algebra known as \(\specialLinearLie(2, \complex)\).
    
    If we classify over \(\reals\) instead then we get another familiar example of a Lie algebra, \(\reals^3\) equipped with the bracket given by the cross-product.
    
    Hopefully, these examples are enough to convince you that a full classification for Lie algebras of dimension 4 is, while maybe possible, probably quite challenging.
    
    All hope is not lost, we do have, as we'll see later, a classification of a particularly nice type of Lie algebra, called simple Lie algebras, over \(\complex\).
    
    \subsection{Associative Algebras}
    \begin{dfn}{Algebra}{def:algebra}
        An \defineindex{algebra} is a vector space, \(A\), equipped with a bilinear product, \(\cdot \colon A \times A \to A\).
        An \defineindex{associative algebra} is an algebra for which the product is associative.
    \end{dfn}
    
    Note that Lie algebras are algebras in the above sense, but they are generally not associative.
    Examples of associative algebras include:
    \begin{itemize}
        \item \(n \times n\) matrices with matrix multiplication.
        \item \(\reals\), \(\complex\), or \(\quaternions\) (the quaternions) with their usual multiplication are associative algebras over \(\reals\).
        In fact, these are \emph{division} algebras: an algebra, \(D\), is a division algebra if for any \(a, b \in D\) with \(b \ne 0\) there is exactly one \(x \in D\) with \(a = bx\) and exactly one \(y \in D\) with \(a = yb\).
        It turns out that these are the only finite-dimensional associative division algebras over \(\reals\).
        If we drop the associativity condition then we also get \(\symbb{O}\), the octonions.
    \end{itemize}
    
    Our interest in associative algebras is mostly the following.
    
    \begin{lma}{}{}
        If \(A\) is an associative algebra then defining the bracket by \(\commutator{X}{Y} = XY - YX\) defines a Lie algebra.
        \begin{proof}
            First note that we clearly have
            \begin{equation}
                \commutator{X}{X} = XX - XX = 0.
            \end{equation}
            The commutator is bilinear, here we show linearity in the second argument, taking \(X, Y, Z \in A\) and \(\lambda \in \field\)
            \begin{align}
                \commutator{X}{Y + \lambda Z} &= X(Y + \lambda Z) - (Y + \lambda Z)X\\
                &= XY + \lambda XZ - YX - \lambda ZX\\
                &= XY - YX + \lambda (XZ - ZX)\\
                &= \commutator{X}{Y} + \lambda \commutator{X}{Z}.
            \end{align}
            The Jacobi identity follows by some algebra.
            First note that
            \begin{align}
                \commutator{X}{\commutator{Y}{Z}} &= \commutator{X}{YZ - ZY}\\
                &= X(YZ - ZY) - (YZ - ZY)X\\
                &= XYZ - XZY - YZX + ZYX.
            \end{align}
            Then, using the fact that the other terms are simply linear combinations of these, we have that the Jacobi relation is reduced to
            \begin{multline}
                XYZ - XZY - YZX + ZYX + YZX - YXZ - ZXY + XZY\\
                + ZXY - ZYX - XYZ + YXZ
            \end{multline}
            and these terms all cancel to give zero.
        \end{proof}
    \end{lma}
    
    When the Lie bracket can be written as \(\commutator{X}{Y} = XY - YX\) we call it the \defineindex{commutator}, because it is a measure of the failure of \(A\) to be a commutative algebra.
    This terminology is also often used for the Lie bracket in general, but we'll try to avoid it in the general case.
    
    \subsection{Classical Lie Algebras}
    The complex classical Lie algebras are some classes of Lie algebras defined with similar definitions.
    These were first of interest because they correspond to particularly common Lie groups.
    First, some notation.
    
    \begin{ntn}{}{}
        Let \(\matrices{n}{\field}\) denote the set of \(n \times n\) matrices with entries in \(\field\).
        
        For \(A \in \matrices{n}{\field}\) denote the transpose by \(A^{\trans}\).
        
        For \(A \in \matrices{n}{\field}\) with \(\field = \reals, \complex\) denote the complex conjugate by \(\overbar{A}\).
        
        For \(A \in \matrices{n}{\field}\) with \(\field = \reals, \complex\) denote the Hermitian conjugate by \(A^{\hermit}\), recall that \(A^{\hermit} = \overbar{A}^{\trans} = \overline{A^{\trans}}\)
        
        \begin{wrn}
            Physicists will denote the complex conjugate by \(A^*\), and the Hermitian conjugate by \(A^{\dagger}\).
        \end{wrn}
    \end{ntn}
    
    \begin{dfn}{General Linear Lie Algebra}{}
        Let \(V\) be a vector space, and define
        \begin{equation}
            \generalLinearLie(V) = \End V \coloneq \{T \colon V \to V \mid T \text{ is linear}\} = \Vect(V, V).
        \end{equation}
        This is an associative algebra under composition of linear maps, and as such \(\generalLinearLie(V)\) is a Lie algebra under the commutator.
    \end{dfn}
    
    Typically we write \(\generalLinearLie(V)\) when thinking of the Lie algebra, and \(\End V\) when thinking of the associative algebra.
    
    When \(V\) is finite dimensional, say \(\dim V = n\), we can fix a basis and then each linear map corresponds to an \(n \times n\) matrix with entries in \(\field\).
    This leads us to make the following definition.
    \begin{equation}
        \generalLinearLie(n, \field) \coloneq \matrices{n}{\field} \isomorphic \generalLinearLie(V).
    \end{equation}
    Note that if \(\dim V = n\) then \(V \isomorphic \field^n\) and \(\generalLinearLie(n, \field) = \generalLinearLie(\field^n)\).
    
    The following are some of the subalgebras of \(\generalLinearLie(n, \complex)\) viewed as a \emph{real} vector space.
    This is important, not all of these are closed under multiplication by arbitrary complex numbers (in particular, if \(A\) is skew-Hermitian then \(iA\) is Hermitian).
    \begin{itemize}
        \item \(\generalLinearLie(n, \reals) = \matrices{n}{\reals}\), the general linear Lie algebra.
        \item \(\specialLinearLie(n, \reals) \coloneq \{A \in \generalLinearLie(n, \reals) \mid \tr A = 0\}\), the \defineindex{special linear Lie algebra};
        \item \(\orthogonalLie(n, \reals) = \specialOrthogonalLie(n, \reals) \coloneq \{A \in \generalLinearLie(n, \reals) \mid A^{\trans} + A = 0\}\), the \defineindex{orthogonal Lie algebra} and \defineindex{special orthogonal Lie algebra}.
        Note that these two are equal but given different names because they both come from important Lie groups which are not equal.
        Note that \(A^{\trans} + A = 0\) means \(A\) is antisymmetric.
        \item \(\unitaryLie(n) \coloneq \{A \in \generalLinearLie(n, \complex) \mid A^{\hermit} + A = 0\}\), the \defineindex{unitary Lie algebra}.
        Note that \(A^{\hermit} + A = 0\) means \(A\) is skew-Hermitian.
        \item \(\specialUnitaryLie(n) \coloneq \{A \in \generalLinearLie(n, \complex) \mid \tr A = 0 \text{ and } A^* + A = 0\}\), the \defineindex{special unitary Lie algebra};
        \item \(\symplecticLie(n, \reals) \coloneq \{A \in \generalLinearLie(2n, \reals) \mid A^\trans J_n + J_n A = 0\}\), the \defineindex{symplectic Lie algebra}, where \(J_n \in \matrices{2n}{\reals}\) is the block matrix
        \begin{equation}
            \begin{bmatrix}
                0 & I_n\\
                -I_n & 0
            \end{bmatrix}
        \end{equation}
        where \(I_n \in \matrices{n}{\reals}\) is the \(n \times n\) identity matrix.
        The matrix \(J_n\) (or rather, the symmetric bilinear form it represents) is called a \defineindex{symplectic form}.
    \end{itemize}
    
    \begin{wrn}
        Physicists like operators to be Hermitian, for quantum mechanics, so they will define things with a few factors of \(i\) different so that \(\unitaryLie(n)\) and \(\specialUnitaryLie(n)\) consist of Hermitian matrices, as opposed to \emph{skew-}Hermitian matrices.
        This distinction unfortunately causes these factors of \(i\) to propagate through many formulae, so be careful when looking things up that the source uses the correct convention.
    \end{wrn}
    
    \begin{wrn}
        Notation differs for the symplectic Lie algebra (and group), some authors denote it \(\symplecticLie(2n)\), it's the \(D_n\) vs \(D_{2n}\) debate all over again.
    \end{wrn}
    
    The names here are all derived from the names of the associated Lie groups, \(\generalLinear(n, \reals)\) the general linear group, \(\specialLinear(n, \reals)\) the special linear group, \(\orthogonal(n, \reals)\) the orthogonal group, \(\specialOrthogonal(n, \reals)\) the special orthogonal group, \(\unitary(n)\) the unitary group, \(\specialUnitary(n)\) the special unitary group, and various different but related symplectic Lie groups.
    
    These are defined as follows:
    \begin{itemize}
        \item \(\generalLinear(n, \reals) \coloneq \{A \in \matrices{n}{\reals} \mid \det A \ne 0\}\);
        \item \(\specialLinearLie(n, \reals) \coloneq \{A \in \generalLinear(n, \reals) \mid \det A = 1\}\);
        \item \(\orthogonal(n, \reals) \coloneq \{A \in \generalLinear(n, \reals) \mid A^{\trans}A = I_n\}\);
        \item \(\specialOrthogonal(n, \reals) \coloneq \{A \in \generalLinear(n, \reals) \mid A^{\trans}A = I_n \text{ and } \det A = 1\}\).
        \item \(\unitary(n) \coloneq \{A \in \generalLinear(n, \complex) \mid U^{\hermit} U = I_n\}\);
        \item \(\specialUnitary(n) \coloneq \{A \in \generalLinear(n, \complex) \mid U^{\hermit}U = I_n \text{ and } \det U = 1\}\);
        \item \(\symplectic(n, \reals) \coloneq \{A \in \generalLinear(2n, \reals) \mid A^{\trans}J_nA = J_n\}\).
    \end{itemize}
    
    These definitions are all about preserving some structure on \(\reals^n\) (or \(\reals^{2n}\) for the symplectic group).
    \begin{itemize}
        \item The general linear group preserves the vector space structure, including dimension.
        \item The special linear group preserves volues.
        \item The orthogonal group preserves angles.
        \item The special orthogonal group preserves the inner product on \(\reals^n\).
        \item The (special) unitary group is a complex analogue of the (special) orthogonal group.
        \item The symplectic group preserves \(J_n\).
    \end{itemize}
    
    Note that \enquote{special} means that we impose the condition \(\det A = 1\).
    In the case of the Lie algebras this becomes the condition that \(\tr A = 0\).
    This is the reason that \(\orthogonalLie(n, \reals) = \specialOrthogonalLie(n, \reals)\), the requirement that \(A\) is antisymmetric means that the diagonal of \(A\) is zero, so it automatically has trace zero.
    In terms of Lie groups, the reason is that both \(\orthogonal(n, \reals)\) and \(\specialOrthogonal(n, \reals)\) are very similar, in particular, \(\specialOrthogonal(n, \reals) \isomorphic \orthogonal(n, \reals)/(\integers/2\integers)\) where \(\integers/2\integers\) here is \(\{\pm I_n\}\), so \(\specialOrthogonal(n, \reals)\) preserves everything that \(\orthogonal(n, \reals)\) preserves, and also preserves orientation.
    This similarity means that \(\orthogonal(n, \reals)\) has two connected components, one preserving orientation, corresponding to the subgroup \(\specialOrthogonal(n, \reals)\), and one reversing orientation.
    Then the Lie algebra corresponds only to the component of the Lie group connected to the identity, which is \(\specialOrthogonal(n, \reals)\) in both cases, so the Lie algebras are the same.
    
    In some of these cases, but not all, it is possible to extend the field to \(\complex\), giving
    \begin{itemize}
        \item \(\generalLinearLie(n, \complex) \coloneq \matrices{n}{\complex}\);
        \item \(\orthogonalLie(n, \complex) \coloneq \specialOrthogonalLie(n, \complex) = \{A \in \generalLinearLie(n, \complex) \mid A^{\trans} + A\} = 0\).
        \item \(\symplecticLie(n, \complex) \coloneq \{A \in \generalLinearLie(2n, \complex) \mid A^{\trans}J_n + J_n A = 0\}\).
    \end{itemize}
    
    Showing that the classical Lie algebras are indeed Lie algebras, specifically subalgebras of \(\generalLinearLie(N, \field)\), requires a little bit of work.
    Mostly we have to show that they are closed under the bracket, which is the commutator in all cases.
    There are a few tricks though.
    The first is that \(\specialLinearLie(n, \field) = \ker \tr\), when we view \(\tr \colon \generalLinearLie(n, \field) \to \generalLinearLie(n, \field)\) as a linear operator, and the kernel of a linear operator is always a subspace.
    We also have to show that the commutator of two traceless matrices is again traceless for \(\specialLinearLie(n, \field)\) to be closed under the bracket:
    \begin{equation}
        \tr(\commutator{X}{Y}) = \tr(XY - YX) = \tr(XY) - \tr(YX) = \tr(XY) - \tr(XY) = 0
    \end{equation}
    where we've used the fact that the trace of a product is invariant under cyclic permutations of that product (henceforth, \enquote{the trace is cyclic}).
    Notice that this doesn't actually use that \(X\) and \(Y\) are traceless, the trace of a commutator is always zero.
    
    \subsubsection{Dimensions}
    An important exercise is to compute the dimensions of the classical Lie algebras.
    We'll do it for the real ones.
    \begin{itemize}
        \item \(\dim \generalLinearLie(n, \reals) = n^2\), since an arbitrary element of \(\generalLinearLie(n, \reals)\) is an \(n \times n\) matrix, which is parametrised by \(n^2\) entries.
        A basis for \(\generalLinearLie(n, \reals)\) is
        \begin{equation}
            \{E_{ij} \mid i = 1, \dotsc, n \text{ and } j = 1, \dotsc, n\}
        \end{equation}
        where \(E_{ij}\) has 1 in the \(i\)th row and \(j\)th column and \(0\) everywhere else.
        Note that
        \begin{equation}
            \commutator{E_{ij}}{E_{kl}} = E_{ij}E_{kl} - E_{kl}E_{ij} = \delta_{ij}E_{il} - \delta_{il}E_{kj}.
        \end{equation}
        \item \(\dim \specialLinearLie(n, \reals) = n^2 - 1\), since setting \(\tr A = 0\) fixes one element on the diagonal, say the last element, by the requirement that if \(A = (a_{ij})\) then \(a_{11} + \dotsb + a_{n-1,n-1} = -a_{nn}\) to get \(\tr A = a_{11} + \dotsb + a_{n-1,n-1} + a_{nn} = 0\).
        Alternatively, note that \(\specialLinearLie(n, \reals) = \ker \tr\), and \(\im \tr = \reals\) and so by the rank-nullity theorem we have
        \begin{equation}
            \dim \generalLinearLie(n, \reals) = \dim(\ker \tr) + \dim(\im \tr) = \dim \specialLinearLie(n, \reals) + 1.
        \end{equation}
        The result then follows from this and our calculation of \(\dim \generalLinearLie(n, \reals) = n^2\).
        \item \(\dim \specialOrthogonalLie(n, \reals) = n(n - 1)/2\), requiring that \(A\) be antisymmetric means that \(A\) is zero on the diagonal, and below the diagonal is fixed by above the diagonal.
        The above diagonal elements form a triangle with a base of \(n - 1\) elements, and thus the number of entries above the diagonal is the \((n-1)\)st triangle number, \(T_{n-1} = n(n-1)/2\) (\(T_n = n(n + 1)/2\)).
        We can check this by identifying that \(\specialOrthogonal(3)\) corresponds to rotations in three dimensions, and this is a three dimensional group, since any rotation is specified by either a) three Euler angles, or b) an angle and an axis of rotation (requiring two numbers to fix the direction, the third component fixed by requiring it to be a unit vector).
        Note that \(\dim \specialOrthogonalLie(3) = 3\) is a coincidence, we have \(\dim \specialOrthogonalLie(2) = 1\) (rotations in two dimensions are specified by just an angle) and \(\dim \specialOrthogonalLie(4) = 6\).
        Of course, here we're using the fact that a Lie group and its associated Lie algebra have the same dimension, this is because the tangent space has the same dimension as the manifold (assuming the manifold is connected, which is always the case for at least one Lie group corresponding to a given Lie algebra).
        \item \(\dim \unitaryLie(n) = n^2\), note that we are talking about the dimension as a real vector space, despite the entries being complex numbers.
        This is the dimension because an arbitrary \(n \times n\) matrix with entries in \(\complex\) has \(2n^2\) real parameters (one for the real part and one for the imaginary part of each entry).
        Requiring that the matrix is skew-Hermitian means that each entry on the diagonal must be equal to its conjugate, so it must have zero imaginary part (fixing \(n\) parameters), then the entries below the diagonal are fixed by the entries above the diagonal (fixing \(2 n(n - 1)/2 = n^2 - n\) parameters).
        Thus, the dimension is \(2n^2 - n - (n^2 - n) = n^2\).
        \item \(\dim \specialUnitaryLie(n) = n^2 - 1\), since any matrix in \(\specialUnitaryLie(n)\) is in \(\unitaryLie(n)\), and setting its trace to zero fixes the final entry on the diagonal (which is purely real).
        \item \(\dim \symplecticLie(n) = 2n^2 + n\), but showing this is a bit tricky.
        
    \end{itemize}
    
    \subsubsection{\texorpdfstring{\(\specialLinearLie(2, \complex)\)}{sl(2, C)}}
    As we've said a couple of times already the most important Lie algebra is \(\specialLinearLie(2, \complex)\), which is a Lie algebra over \(\complex\).
    A general element of this algebra is a \(2 \times 2\) matrix with complex entries such that the trace vanishes, we can write such an element as
    \begin{equation}
        \begin{bmatrix}
            a & b\\
            c & -a
        \end{bmatrix}
    \end{equation}
    for \(a, b, c \in \complex\).
    This shows that \(\dim \specialLinearLie(2, \complex) = 3\).
    
    An explicit basis for \(\specialLinearLie(2, \complex)\) is
    \begin{equation}
        \quad H = 
        \begin{bmatrix}
            1 & 0\\
            0 & -1
        \end{bmatrix}
        , \qquad
        E = 
        \begin{bmatrix}
            0 & 1\\
            0 & 0
        \end{bmatrix}
        , \qqand F = 
        \begin{bmatrix}
            0 & 0\\
            1 & 0
        \end{bmatrix}
        .
    \end{equation}
    Then
    \begin{equation}
        \begin{bmatrix}
            a & b\\
            c & -a
        \end{bmatrix}
        = aH + bE + cF.
    \end{equation}
    
    A simple calculation of commutators shows that we have the relations
    \begin{equation}
        \commutator{H}{E} = 2E, \qquad \commutator{H}{F} = -2F, \qqand \commutator{E}{F} = H.
    \end{equation}
    An important observation here is that if we define the map\footnote{This is the so called adjoint representation.} \(\ad_H \colon \specialLinearLie(2, \complex) \to \specialLinearLie(2, \complex)\) by \(\ad_H(X) = \commutator{H}{X}\) then we see that this map has \(E\) and \(F\) as eigenvectors, with eigenvalues \(2\) and \(-2\) respectively.
    The other eigenvector of \(\ad_H\) is \(H\) itself, since \(\ad_H(H) = \commutator{H}{H} = 0 = 0H\).
    Then we have the eigensspaces \(V_0 = \complex H = \lie{h}\), \(V_2 = \complex E\), and \(V_{-2} = \complex F\), giving the decomposition
    \begin{equation}
        \specialLinearLie(2, \complex) = \complex H \oplus \complex E \oplus \complex F = \lie{h} \oplus V_2 \oplus V_{-2} = \lie{h} \oplus \bigoplus_{\alpha \in \complex \setminus \{0\}} V_{\alpha}.
    \end{equation}
    
    The important thing here is that we get \(\lie{h}\), which is a special subalgebra called the Cartan subalgebra which we'll see a lot later, and we get the generalised eigenspaces \(V_{\alpha}\).
    We'll see later\footnote{so ignore any terminology here that is unfamiliar} that replacing the generalised eigenspaces with something called root spaces makes this construction generalise to finite dimensional semisimple Lie algebras over \(\complex\), and further that the subalgebras appearing in this decomposition are all simple, so a classification of simple Lie algebras extends to a classification of semisimple Lie algebras.
    This requires replacing the indexing set \(\complex \setminus \{0\}\) with the root space \(\lie{h}^* \setminus \{0\}\), or rather we've already done that, it's just that \(\dim \lie{h} = 1\) so \(\lie{h}^* \isomorphic \lie{h}\) and \(\dim \lie{h}^* = 1\) so \(\lie{h}^* \isomorphic \complex\).
    
    While this last paragraph is beyond what we're ready for yet it's important to start seeing the recurring pattern of this type of decomposition now.
    The fact that \(\specialLinearLie(2, \complex)\) is the simplest example of such a decomposition is why it's so important.
    In particular, every finite dimension semisimple Lie algebra over \(\complex\) of dimension at least 3 contains (many) copies of \(\specialLinearLie(2, \complex)\).
    
	
	% Appdendix
%	\appendixpage
%	\begin{appendices}
%	
%	\end{appendices}
	
%	\backmatter
%	\renewcommand{\glossaryname}{Acronyms}
%	\printglossary[acronym]
%	\printindex
\end{document}
