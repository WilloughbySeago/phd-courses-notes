% !TeX program = lualatex
\documentclass[fleqn]{NotesClass}

\strictpagecheck

\usepackage{csquotes}

\usepackage{tikz}
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]

\usepackage{tikz-cd}

\usepackage[pdfauthor={Willoughby Seago},pdftitle={Notes from Lie Theory Course},pdfkeywords={Lie theory, Lie algebra},pdfsubject={Lie algebras}]{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{NotesBoxes}
\usepackage{NotesMaths2}

\setmathfont[range={\int, \oint, \otimes, \oplus, \bigotimes, \bigoplus}]{Latin Modern Math}


% Highlight colour
\definecolor{highlight}{HTML}{710D78}
\definecolor{my blue}{HTML}{2A0D77}
\definecolor{my red}{HTML}{770D38}
\definecolor{my green}{HTML}{14770D}
\definecolor{my yellow}{HTML}{E7BB41}

% Title page info
\title{Lie Theory}
\author{Willoughby Seago}
\date{September 26th, 2024}
\subtitle{Notes from}
\subsubtitle{University of Glasgow}
\renewcommand{\abstracttext}{These are my notes from the master's course \emph{Lie Theory} taught at the University of Glasgow by Dr Dinakar Muthiah. I also found \href{https://www.youtube.com/playlist?list=PLVMgvCDIRy1zJ6iJHLC_lWgIwY4AhviZ-}{this video series}\footnote{\url{https://www.youtube.com/playlist?list=PLVMgvCDIRy1zJ6iJHLC_lWgIwY4AhviZ-}} from Michael Penn useful. These notes were last updated at \printtime{} on \today{}.}

\hyphenation{ei-gen-space ei-gen-spaces}

% Commands
% Maths
\renewcommand{\field}{\symbb{k}}
\newcommand{\cat}[1]{\symsfup{#1}}
\makeatletter
\newcommand{\c@egory}[1]{\symsfup{#1}}
\newcommand{\Vect}[1][\field]{#1\text{-}\c@egory{Vect}}
\newcommand{\LieAlg}[1][\field]{#1\text{-}\c@egory{LieAlg}}
\newcommand{\Rep}[1][\lie{g}]{#1\text{-}\c@egory{Rep}}
\makeatother
\DeclareMathOperator{\Hom}{Hom}
\DeclarePairedDelimiterX{\commutator}[2]{[}{]}{#1, #2}
\DeclarePairedDelimiterX{\bracket}[2]{[}{]}{#1, #2}
\newcommand{\isomorphic}{\cong}
\DeclareMathOperator{\Span}{span}
\newcommand{\id}{\symrm{id}}
\DeclareMathOperator{\Char}{char}
\RenewDocumentCommand{\matrices}{o m m}{%
    \IfNoValueTF{#1}{%
        \symrm{Mat}(#2, #3)
    }{%
        \symrm{Mat}(#1 \times #2, #3)
    }
}
\newcommand{\trans}{{\top}}
\newcommand{\hermit}{*}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\im}{im}
\newcommand{\ad}{\symrm{ad}}
\newcommand{\borelLie}{\symfrak{b}}
\newcommand{\nilpotentLie}{\symfrak{n}}
\DeclareMathOperator{\Der}{Der}
\DeclareMathOperator{\Perm}{Perm}
\newcommand{\centre}{\symfrak{z}}
\DeclareMathOperator{\rad}{rad}

\begin{document}
	\frontmatter
	\titlepage
	\innertitlepage{}
	\tableofcontents
	% \listoffigures
	\mainmatter
	\chapter{Introduction}
	In this course we will study Lie algebras.
	The motivation for the definition of a Lie algebra is as a tool for studying Lie groups.
	Despite this we will rarely mention Lie groups, since the study of Lie groups requires a knowledge of differential geometry.
	We will stick to the study of Lie algebras, which can be defined purely algebraically.
	
	Roughly speaking, the storey is thus: Lie groups are both smooth manifolds and groups in a compatible way.
	By this, we mean that if \(G\) is a Lie group then the maps
	\begin{itemize}
		\item \(m \colon G \times G \to G\) given by \(m(x, y) = xy\)
	    \item \(i \colon G \to G\) given by \(i(g) = g^{-1}\)
	\end{itemize}
	are smooth and these maps satisfy the obvious requirements to make \(G\) a group.
    In fancy terms, a Lie group is a group object in the category of smooth manifolds.
    
    Lie groups were first used by Sophus Lie, who lends his name to these objects.
    Lie is pronounced Lee by the way, not lye.
    He was looking at the continuous symmetries possessed by the solutions to differential equations, and found that these naturally formed what we now know as a Lie group.
    The problem with studying Lie groups, apart from all of the topology, is that they are generally very non-linear.
    We can think of Lie algebras as being a linear approximation of Lie groups, given by expanding about the identity and discarding non-linear terms.
	
	To get a Lie algebra out of a Lie group we take the tangent space at the identity, \(T_eG\), which is a vector space.
	It turns out that the group commutator, \(\commutator{g}{h} = ghg^{-1}h^{-1}\), induces a natural operation on the tangent space, called the Lie bracket.
	A Lie algebra is then this tangent space equipped with this Lie bracket.
	
	It is possible to abstract the properties of this Lie bracket to define a Lie algebra without reference to a Lie group.
	It is also possible to go in reverse, given a Lie algebra there is always a corresponding Lie group.
	It should be noted that this assignment is not unique, in general many different Lie groups can have the same Lie algebra.
	The assignment does become invertible if we restrict ourselves to simply connected Lie groups, but then we're really getting into topology in a way that this course hopes to avoid.
	This assignment of a Lie algebra to a Lie group is functorial.
	
	\section{Notation}
	Throughout \(\field\) will denote a field.
	Often we will place further restrictions on \(\field\), such as being algebraically closed.
	Most of the time we'll be interested in the case \(\field = \complex\), with occasional use of \(\field = \reals\).
	Unless stated otherwise vector spaces are assumed to be vector spaces over \(\field\).
    
    We will denote by \(0\) the zero vector space (for a given field), \(\{0\}\), which consists of only the zero vector.
	
	If we have a category, \(\cat{C}\), we will denote by \(\cat{C}(A, B) = \Hom_{\cat{C}}(A, B)\) the set of all morphisms \(A \to B\), or simply \(\Hom(A, B)\) if \(\cat{C}\) is clear from context.
	This is just notation, we won't make much use of categories apart from borrowing some of the language.
	The main category we'll make use of is \(\Vect\), the category of vector spaces over \(\field\) and linear maps.
    
    We will mostly denote Lie algebras with lowercase fraktur letters, or so called \verb*|\mathfrak| letters.
    For reference, here's the alphabet in \verb*|\mathfrak|:
    \begin{equation*}
        \symfrak{a\,b\,c\,d\,e\,f\,g\,h\,i\,j\,k\,l\,m\,n\,o\,p\,q\,r\,s\,t\,u\,v\,w\,x\,y\,z}
    \end{equation*}
	
	\chapter{Direct Sums and Diagonalisation}
	Our goal in this section is to state a definition of a diagonalisable linear operator in such a basis free way.
	We will then generalise this to get an \enquote{almost diagonal form} for arbitrary linear operators when \(\field\) is algebraically closed.
	To this end we will recap some linear algebra in this first section.
	
	\section{Direct Sums}
	\subsection{Binary Direct Sums}
	\begin{dfn}{Binary Direct Sum}{}
		Let \(V\) be a vector space over \(\field\), and let \(U_1, U_2 \subseteq V\) be subspaces.
		Then we say that \(V = U_1 \oplus U_2\), that is \(V\) is the (internal) \defineindex{direct sum} of \(U_1\) and \(U_2\), if
		\begin{itemize}
			\item \(V = U_1 + U_2 \coloneq \{u_1 + u_2 \mid u_1 \in U_1 \text{ and } u_2 \in U_2\}\); and
			\item \(U_1 \cap U_2 = 0\).
		\end{itemize}
		
		If \(U_1\) and \(U_2\) are vector spaces over \(\field\) then we can construct a vector space \(V = U_1 \oplus U_2\), also over \(\field\), by defining
		\begin{equation}
			V = \{(u_1, u_2) \mid u_1 \in U_1 \text{ and } u_2 \in U_2\}
		\end{equation}
		and defining addition and scalar multiplication by
		\begin{equation}
			(u_1, u_2) + (u_1', u_2') = (u_1 + u_1', u_2 + u_2') \qqand \lambda (u_1, u_2) = (\lambda u_1, \lambda u_2)
		\end{equation}
        for all \(u_1, u_1' \in U_1\), \(u_2, u_2' \in U_2\), and \(\lambda \in \field\).
	\end{dfn}
	
    After constructing the external direct sum we may identify \(U_1\) with the subspace consisting of elements of the form \((u_1, 0)\) with \(u_1 \in U_1\), and \(U_2\) with the subspace of elements of the form \((0, u_2)\) with \(u_2 \in U_2\).
    Then the external direct sum coincides with the internal direct sum.
    For this reason we won't distinguish internal and external direct sums, making such identifications as necessary.
    
    The following lemma gives an alternative characterisation of the direct sum.
    
    \begin{lma}{}{lma:binary direct sum gives decompositon of vectors}
        If \(V = U_1 \oplus U_2\) then every vector \(v \in V\) can be written \emph{uniquely} as a sum \(v = u_1 + u_2\) with \(u_1 \in U_1\) and \(u_2 \in U_2\).
        Conversely, if every \(v\) has a unique decomposition as \(v = u_1 + u_2\) with \(u_1 \in U_1\) and \(u_2 \in U_2\) then \(V = U_1 \oplus U_2\).
        \begin{proof}
            Suppose that \(V = U_1 \oplus U_2\).
            From the definition of a direct sum we know that \(v \in V = U_1 + U_2\) and as such \(v = u_1 + u_2\) for some \(u_1 \in U_1\) and \(u_2 \in U_2\) because this is how elements of \(U_1 + U_2\) are defined.
            We need only prove uniqueness.
            Suppose that \(v = u_1 + u_2\) and \(v = u_1' + u_2'\) with \(u_1, u_1' \in U_1\) and \(u_2, u_2' \in U_2\) are two decompositions of \(v\).
            Then we have \(u_1 + u_2 = u_1' + u_2'\), which we can rearrange to get \((u_1 - u_1') + (u_2 - u_2') = 0\).
            This means that \(u_1 - u_1' = u_2' - u_2 \eqqcolon w\).
            Now, \(u_1 - u_1' \in U_1\), since it's a linear combination of elements of \(U_1\), and similarly \(u_2' - u_2 \in U_2\).
            Thus, \(w \in U_1 \cap U_2 = 0\) and so \(w = 0\).
            
            Suppose instead that every \(v \in V\) has a unique decomposition as \(v = u_1 + u_2\).
            Then clearly \(v\) corresponds to \((u_1, u_2) \in U_1 \oplus U_2\) and if \(v = u_1 + u_2\) and \(v' = u_1' + u_2'\) and \(\lambda \in \field\) then \(v + \lambda v' = (u_1 + u_2) + \lambda(u_1' + u_2')\) corresponds to \((u_1, u_2) + \lambda(u_1', u_2')\), but also \(v + \lambda v' = (u_1 + \lambda u_1') + (u_2 + \lambda u_2')\) corresponds to \((u_1 + \lambda u_1', u_2 + \lambda u_2')\).
            This shows that this correspondence defines a linear map.
            Clearly this correspondence is invertible, and thus we have an isomorphism \(V \isomorphic U_1 \oplus U_2\).
        \end{proof}
    \end{lma}
    
    Thus, the direct sum may be characterised as giving a unique decomposition of each vector into a pair of vectors from two subspaces with only the zero vector in common.
    
    \subsection{Finite Direct Sums}
    Direct sums of two vector spaces generalise to direct sums of a finite number of vector spaces in an obvious way.
    
    \begin{dfn}{Finite Direct Sum}{}
        Let \(V\) be a vector space over \(\field\), and let \(U_1, \dotsc, U_r \subseteq V\) be subspaces.
        Then we say that
        \begin{equation}
            V = U_1 \oplus \dotsb \oplus U_r = \bigoplus_{i=1}^r U_i,
        \end{equation}
        that is \(V\) is the (internal) \define{direct sum} of the \(U_i\), if
        \begin{itemize}
            \item \(V = U_1 + \dotsb + U_r \coloneq \{u_1 + \dotsb + u_r \mid u_i \in U_i \text{for all } i = 1, \dotsc, r\}\); and
            \item for all \(i = 1, \dotsc, r\) we have \(U_i \cap (U_1 + \dotsb + U_{i-1} + U_{i+1} + \dotsb + U_r) = 0\) where the sum is over all subspaces apart from \(U_i\).
        \end{itemize}
    \end{dfn}
    
    Note that we can define the external direct sum, but care has to be taken as if we define \(U_1 \oplus U_2 \oplus U_3\) to consist of elements of the form \((u_1, u_2, u_3)\) then this is not the same as defining \(U_1 \oplus (U_2 \oplus U_3)\) to consist of elements of the form \((u_1, (u_2, u_3))\) and \((U_1 \oplus U_2) \oplus U_3\) to consist of elements of the form \(((u_1, u_2), u_3)\).
    However these spaces are naturally (in the technical sense) isomorphic, and as such we will identify them with each other.
    A complicated way to put this is that the direct sum is associative up to natural isomorphism.
    An even more complicated way to put this, along with the fact that \(V \oplus 0 \isomorphic V \isomorphic 0 \oplus V\), is that \((\Vect, \oplus, 0)\) is a monoidal category.
    
    The same characterisation of the direct sum giving a unique decomposition of \(v \in V\) carries over to finite direct sums.
    
    \begin{lma}{}{}
        If \(V = U_1 \oplus \dotsb \oplus U_r\) then for all \(v \in V\) there exist unique \(u_i \in U_i\) such that \(v = u_1 + \dotsb + u_r\).
        \begin{proof}
            We proceed by induction on \(r\).
            The case \(r = 2\) is \cref{lma:binary direct sum gives decompositon of vectors}.
            Suppose that the result holds some \(k \ge 2\) and that \(V = U_1 \oplus \dotsb \oplus U_k \oplus U_{k+1}\).
            Take \(v \in V\).
            Writing \(V = (U_1 \oplus \dotsb \oplus U_k) \oplus U_{k+1}\) we see that there are unique vectors \(u \in U_1 \oplus \dotsb \oplus U_k\) and \(u_{k+1} \in U_{k+1}\) such that \(v = u + u_{k+1}\).
            By the induction hypothesis since \(U_1 \oplus \dotsb \oplus U_k\) is a \(k\)-fold direct sum we can uniquely decompose \(u\) as \(u = u_1 + \dotsb + u_k\).
            Then \(v = u_1 + \dotsb + u_k + u_{k+1}\) gives a unique decomposition of \(v\).
            Thus, by induction the result holds for any finite direct sum.
        \end{proof}
    \end{lma}
    
    \subsection{Arbitrary Direct Sums}
    We can further generalise direct sums to arbitrary collections of spaces.
    One thing we have to be cautious about is that infinite sums of vectors are not defined, and we get around this by considering sums over infinite sets, but requiring that only finitely many of the vectors in the sum are not zero.
    More formally, we can view a sequence \((u_i)_{i \in I}\) of vectors as a function \(u \colon I \to V\) with \(u(i) = u_i\) and then we require that \(u\) has finite support.
    
    \begin{dfn}{Arbitrary Direct Sums}{}
        Let \(V\) be a vector space over \(\field\), and let \(\{U_i\}_{i \in I}\) be an indexed family of subspaces, that is \(U_i \subseteq V\) is a subspace for all \(i \in I\).
        Then we say that
        \begin{equation}
            V = \bigoplus_{i \in I} U_i,
        \end{equation}
        that is \(V\) is the (internal) \define{direct sum} of the \(U_i\), if
        \begin{itemize}
            \item we have
            \begin{equation*}
                V = \sum_{i\in I} U_i \coloneq \bigg\{ \sum_{i \in I} u_i \,\bigg\vert\, u_i \in U_i \text{ and } u_i \ne 0 \text{ for only finitely many terms}\bigg\};
            \end{equation*}
            \item for all \(i \in I\) we have \(U_i \cap \sum_{j \in I \setminus \{i\}} U_j = 0\).
        \end{itemize}
    \end{dfn}
    
    We can define the external direct sum by taking a family of vector spaces, \(\{U_i\}_{i \in I}\), all over \(\field\), and defining
    \begin{equation}
        V = \bigoplus_{i \in I} U_i \coloneqq \{(u_i)_{i \in I} \mid u_i \text{ nonzero for only finitely many } i\} \subseteq \prod_{i \in I} U_i
    \end{equation}
    where
    \begin{equation}
        \prod_{i \in I} U_i = \{(u_i)_{i \in I}\}
    \end{equation}
    is the direct product of vector spaces (that is, the product in the category \(\Vect\)).
    Note that when \(I\) is finite the product and direct sum coincide.
    
    \begin{lma}{}{}
        If \(V = \bigoplus_{i \in I} U_i\) then for all \(v \in V\) there exist unique \(u_i \in U_i\) such that \(v = \sum_{i \in I} u_i\) with only finitely many of the \(u_i\) being nonzero.
        \begin{proof}
            The proof is essentially the same as before, but now we work with formally infinite sums in which most terms are zero, and all we have to do is check that at each stage this is still the case, which it is, since adding two such sums cannot result in infinitely many nonzero terms.
        \end{proof}
    \end{lma}
    
    \begin{exm}{}{}
        Let \(V = \field[x]\) be the vector space of polynomials in \(x\) with coefficients in \(\field\).
        That is
        \begin{equation}
            V = \Bigg\{ \sum_{i = 0}^N a_i x^i \,\Bigg\vert\, N \in \naturals \text{ and } a_i \in \field \Bigg\}.
        \end{equation}
        Addition is just addition of polynomials, and scaling is simply scaling each coefficient, which we can phrase as multiplying by the corresponding constant polynomial.
        We have the subspaces
        \begin{equation}
            U_i = \Span \{x^i\} \coloneq \{a x^i \mid a \in \field\}
        \end{equation}
        and it's easy to see that
        \begin{equation}
            V = \bigoplus_{i \in \naturals} U_i.
        \end{equation}
        Specifically, if we have a polynomial, \(f(x) \in \field[x]\), then
        \begin{equation}
            f(x) = a_0 + a_1x + a_2 x^2 + \dotsb + a_N x^N
        \end{equation}
        and \(a_0 \in U_0\), \(a_1 x \in U_1\), \(a_2 x^2 \in U_2\), and so on up to \(a_N x^N \in U_N\), so  we get a finite decomposition of \(f(x)\), and clearly this is unique, showing that \(\field[x]\) is indeed this direct sum as claimed.
        
        \begin{rmk}
            There's actually more structure here.
            First, we can multiply polynomials, so we have an algebra (see \cref{def:algebra}), not just a vector space.
            Define \(\deg f(t)\) to be the highest power of \(t\) appearing in \(f\) with a nonzero coefficient.
            Then \(U_i\) consists of all homogenous polynomials of degree \(i\) (homogenous meaning each term has the same degree).
            Further, since \(x^n x^m = x^{n + m}\) we have \(\deg (x^n x^m) = n + m\), and so \(U_iU_j \coloneq \{u_i u_j \mid u_i \in U_i \text{ and } u_j \in U_j\} = U_{i + j}\).
            When we have a decomposition like this and \(U_i U_j \subseteq U_{i + j}\) we say that \(V\) is an \(\naturals\)-graded algebra.
        \end{rmk}
    \end{exm}
    
    \begin{remark}{}{}
        The most general definition of the direct sum is as the coproduct in the category \(\Vect\).
        Recall (if you know anything about categories) that the coproduct \(V = \bigoplus_{i \in I} U_i\) comes equipped with inclusion maps \(\iota_i \colon U_i \hookrightarrow V\) which are such that if we have a family of maps \(f_i \colon U_i \to W\) for some other vector space \(W\) then there is a unique linear map \(f \colon V \to W\) such that \(f\iota_i = f_i\).
        This is the universal property of the coproduct, and can be summarised as the following diagram commuting for all \(j \in I\):
        \begin{equation}
            \tikzexternaldisable
            \begin{tikzcd}
                U_j \arrow[r, hookrightarrow, "\iota_j"] \arrow[dr, "f_j"'] & \displaystyle \bigoplus_{i \in I} U_i \arrow[d, dashed, "f", "\exists!"', pos=0.4]\\
                & W\mathrlap{.}
            \end{tikzcd}
            \tikzexternalenable
        \end{equation}
        
        The inclusion map \(\iota_i \colon U_i \hookrightarrow V\) is what allows us to identify \(U_i\) in the external direct sum with the subspace \(\iota_i(U_i) \subseteq V\), and since \(\iota_i\) is injective we have \(U_i \isomorphic \iota_i(U_i)\).
    \end{remark}
    
    \section{Diagonalisable Operators and the Direct Sum Decomposition}
    In this section we'll give the standard definition of a diagonalisable operator, which should be familiar.
    We'll then give an equivalent definition which makes no mention of a basis.
    
    Let \(V\) be a finite dimensional vector space over \(\field\) with basis \(\symcal{B} = \{v_1, \dotsc, v_n\}\).
    Recall that if \(T \colon V \to V\) is a linear operator then its matrix in basis \(\symcal{B}\) is the matrix
    \begin{equation}
        [T]_{\symcal{B}} \coloneq 
        \begin{bmatrix}
            \uparrow & & \uparrow \\
            Tv_1 & \dots & Tv_n \\
            \downarrow & & \downarrow
        \end{bmatrix}
        =
        \begin{bmatrix}
            c_{11} & c_{12} & \dots & c_{1n}\\
            c_{21} & c_{22} & \dots & c_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            c_{n1} & c_{n2} & \dots & c_{nn}
        \end{bmatrix}
    \end{equation}
    where \(c_{ij}\) is the coefficient of \(v_i\) in \(Tv_j\) when expressed in the basis \(\symcal{B}\).
    That is, the \(i\)th column of \([T]_{\symcal{B}}\) is \(Tv_i\) as a column vector in basis \(\symcal{B}\).
    
    \begin{dfn}{Diagonalisable Operator}{}
        Let \(V\) be a finite dimensional vector space over \(\field\).
        Then a linear operator, \(T \colon V \to V\), is \defineindex{diagonalisable} if there exists some basis, \(\symcal{B}\), in which the matrix \([T]_{\symcal{B}}\) is diagonal.
    \end{dfn}
    
    Notice that if \(T \colon V \to V\) is a linear operator diagonalised by the basis \(\symcal{B} = \{v_1, \dotsc, v_n\}\) then
    \begin{equation}
        [T]_{\symcal{B}} = 
        \begin{bmatrix}
            \lambda_1 & & \\
            & \ddots & \\
            & & \lambda_n
        \end{bmatrix}
    \end{equation} 
    for some \(\lambda_i \in \field\).
    We can also express the basis vectors \(v_i\) in this basis, they are simply the standard basis, in which \([v_i]_{\symcal{B}} = e_i\) is the column vector with 1 in the \(i\)th position and 0 everywhere else.
    Then, for example, we have
    \begin{equation}
        [Tv_1]_{\symcal{B}} = [T]_{\symcal{B}}[v_1]_{\symcal{B}} = 
        \begin{bmatrix}
            \lambda_1 & & \\
            & \ddots & \\
            & & \lambda_n
        \end{bmatrix}
        \begin{bmatrix}
            1\\ \vdots\\ 0
        \end{bmatrix}
        =
        \begin{bmatrix}
            \lambda_1\\ \vdots\\ 0
        \end{bmatrix}
        = \lambda_1
        \begin{bmatrix}
            1\\ \vdots\\ 0
        \end{bmatrix}
        = \lambda_1 [v_1]_{\symcal{B}}.
    \end{equation}
    Thus, we have \(Tv_1 = \lambda v_1\), that is, \(v_1\) is an eigenvector of \(T\) with eigenvalue \(\lambda_1\).
    In general, \(v_i\) will be an eigenvector of \(T\) with eigenvalue \(\lambda_i\).
    
    Now make the following observations.
    We may define the subspaces \(U_i = \Span\{v_i\}\).
    The fact that \(V = \bigoplus_{i=1}^n U_i\) follows immediately from (and is equivalent to) the fact that \(\symcal{B}\) is a basis.
    Each subspace \(U_i\) is \(T\)-\defineindex{invariant}, meaning that \(T(U_i) \subseteq U_i\), which simply means that if \(u \in U_i\) then \(Tu \in U_i\) also, so it's not possible to leave \(U_i\) just by the action of \(T\).
    This is clear in this case because \(T\) acts on each subspace, \(U_i\), by scalar multiplication, specifically, by multiplication by \(\lambda_i\).
    Because of this it makes sense to consider the restricted linear map \(T|_{U_i} \colon U_i \to U_i\), defined by \(T|_{U_i}(u) = T(u)\) for \(u \in U_i\).
    By definition \(T(u) = \lambda_i u\) for \(u \in U_i\), and thus we have \(T = \lambda_i \id_{U_i}\) where \(\id_W \colon W \to W\) is the identity linear map.
    
    \subsection{Basis Independent Definition}
    We can now move towards making a basis independent definition of diagonalisability.
    To do so we need the notion of an eigenspace.
    
    \begin{dfn}{Eigenspace}{}
        Let \(\field\) be a field and let \(V\) be a vector space over \(V\).
        Let \(T \colon V \to V\) be a linear operator.
        Then for \(\alpha \in \field\) define the \defineindex{eigenspace} corresponding to \(\alpha\) to be
        \begin{equation}
            W_\alpha \coloneq \{w \in V \mid T(w) = \alpha w\}.
        \end{equation}
    \end{dfn}
    
    Note that \(W_\alpha\) can alternatively be characterised as
    \begin{equation}
        W_\alpha = \{w \in V \mid (T - \alpha)(w) = 0\} = \ker(T - \alpha)
    \end{equation}
    where we perform the common abuse of notation writing \(T - \alpha\) when we mean \(T - \alpha \id_V\).
    
    For example, in the case of a diagonalisable operator where the eigenvalues are all distinct, as discussed at the end of the previous section, the subspaces \(U_i\) are exactly the eigenspaces \(W_{\lambda_i}\).
    Also, \(W_{\alpha} = 0\) if \(\alpha \ne \lambda_i\) for any \(i\).
    This is a general fact, if \(\alpha\) is not an eigenvalue then there will be no \(w \in V\) satisfying \(T(w) = \alpha w\).
    
    The problem is that eigenvalues needn't be distinct, in fact, eigenvalues are distinct if and only if the nonzero eigenspaces have dimension 1.
    Regardless of this problem, we can still perform the direct sum decomposition from the last part, replacing the \(U_i\) with the eigenspaces \(W_\alpha\).
    In fact, this gives us an equivalent definition of diagonalisability without reference to a basis.
    
    \begin{dfn}{Diagonalisable Operator}{}
        Let \(V\) be a finite dimensional vector space over \(\field\).
        Then a linear operator, \(T \colon V \to V\), is \defineindex{diagonalisable} if 
        \begin{equation}
            V = \bigoplus_{\alpha \in \field} W_\alpha.
        \end{equation}
    \end{dfn}
    
    \subsection{Decomposition Theorem}
    We now ask if there is a generalisation of this decomposition to not-necessarily-diagonalisable operators.
    We'll start with an example.
    
    \begin{exm}{Non-diagonalisable Operator}{exm:E not diagonalisable}
        Consider the case \(\field = \complex\), \(V = \complex^2\), and
        \begin{equation}
            T = 
            \begin{bmatrix}
                0 & 1\\
                0 & 0
            \end{bmatrix}
        \end{equation}
        in the standard basis
        \begin{equation}
            e_1 = 
            \begin{bmatrix}
                1\\ 0
            \end{bmatrix},
            \qqand e_2 = 
            \begin{bmatrix}
                0\\ 1
            \end{bmatrix}
            .
        \end{equation}
        To find the eigenvalues we solve the characteristic polynomial, which is
        \begin{equation}
            \chi_T(t) = \det(\lambda - T) = \det
            \begin{bmatrix}
                0 & 1\\
                0 & 0
            \end{bmatrix}
            = \lambda^2.
        \end{equation}
        The only solution to \(\lambda^2 = 0\) is \(\lambda = 0\), and thus we have one repeated eigenvalue.
        This means that \(T\) is not diagonalisable.
        For \(\alpha \ne 0\) we have \(W_\alpha = 0\), and we have
        \begin{equation}
            W_0 = \{w \in V \mid Tw = 0\} = \ker(T - 0) = \ker T = \Span\{e_1\}.
        \end{equation}
        This follows simply by considering the action of \(T\) on the basis vectors, \(Te_1 = 0\) and \(Te_2 = e_1 \ne 0\).
        This means that
        \begin{equation}
            \bigoplus_{\alpha \in \complex} W_{\alpha} = W_0 = \Span\{e_1\} \ne \complex^2.
        \end{equation}
        
        From this calculation we see that \(T^2e_2 = 0\), and so if we instead consider the operator \(T^2\) then the eigenspaces of this cover all of \(\complex^2\), so this suggests that we should look not just at \(T\), but also powers of \(T\) in our decomposition.
    \end{exm}
    
    \begin{remark}{}{}
        The matrix
        \begin{equation}
            T = 
            \begin{bmatrix}
                0 & 1\\
                0 & 0
            \end{bmatrix}
            = E
        \end{equation}
        is important for the Lie algebra \(\specialLinearLie(2, \complex)\), which is one of the most important Lie algebras.
        The fact that this matrix is not diagonalisable is important in that it effects whether \(E\) commutes with other matrices.
        Recall that if two matrices can be simultaneously diagonalised then they commute, an important fact in quantum mechanics.
    \end{remark}
    
    Following on from the idea that we should look at powers of \(T\) we replace the eigenspace with the following definition.
    
    \begin{dfn}{Generalised Eigenspace}{}
        Let \(\field\) be a field and let \(T\) be a vector space over \(\field\).
        Let \(T \colon V \to V\) be a linear operator.
        Then for \(\alpha \in \field\) define the \defineindex{generalised eigenspace} corresponding to \(\alpha\) to be
        \begin{equation}
            V_\alpha \coloneq \{w \in V \mid (T - \alpha)^N(w) = 0 \text{ for some } N \in \naturals\}.
        \end{equation}
    \end{dfn}
    
    We can then often make a decomposition into a direct sum of generalised eigenspaces.
    There's just one hitch, this only works if \(\field\) is algebraically closed.
    
    \begin{dfn}{Algebraically Closed Field}{}
        A field, \(\field\), is algebraically closed if every nonconstant polynomial \(f(t) \in \field[t]\) has a root, \(\alpha \in \field\), such that \(f(\alpha) = 0\).
    \end{dfn}
    
    The complex numbers, \(\complex\), are algebraically closed, this is the fundamental theorem of algebra.
    The real numbers, \(\reals\), are not algebraically closed, for example \(t^2 + 1\) has no (real) roots.
    
    The following proof relies on three standard results, which we state without proof.
    \begin{thm}{Cayley--Hamilton}{}
        If \(\chi_T(t) = t^n + a_{n-1}t^{n-1} + \dotsb + a_1t + a_0\) is the characteristic polynomial of the linear operator \(T \colon V \to V\) then the linear operator
        \begin{equation}
            T^n + a_{n-1}T^{n-1} + \dotsb + a_1T + a_0\id_V \colon V \to V
        \end{equation}
        is the zero map.
        That is, \(T\) satisfies it's own characteristic polynomial, \(\chi_T(T) = 0\).
    \end{thm}
    
    \begin{lma}{B\'ezout's Lemma}{}
        If \(R\) is a principal ideal domain and \(x, y \in R\) have greatest common divisor \(d\) then there exist \(a, b \in R\) such that \(ax + by = d\).
    \end{lma}
    
    \begin{lma}{}{}
        If \(\field\) is a field then \(\field[x]\) is a PID, and in particular B\'ezout's lemma applies to polynomials.
    \end{lma}
    
    \begin{thm}{Decomposition Theorem}{}
        Let \(\field\) be an algebraically closed field, \(V\) a finite dimensional vector space, and \(T \colon V \to V\) a linear operator.
        Then
        \begin{equation}
            V = \bigoplus_{\alpha \in \field} V_\alpha 
        \end{equation}
        where \(V_\alpha\) is the generalised eigenspace associated with \(\alpha\) and only finitely many of the \(V_\alpha\) are nonzero.
        \begin{proof}
            We start by showing that \(V_\alpha \cap \sum_{\beta \in \field \setminus \{\alpha\}} V_\beta = 0\).
            Take \(v V_\alpha \cap \sum_{\beta \in \field \setminus \alpha} V_\beta\), then we have \(v \in V_\alpha\) and \(v \in V_{\beta_1} + \dotsb + V_{\beta_r}\) for some \(\beta_1, \dotsc, \beta_r \in \field \setminus \{\alpha\}\).
            This means that there exist some \(N, N_1, \dotsc, N_r \in \naturals\) such that
            \begin{equation}
                (T - \alpha)^Nv = (T - \beta_1)^{N_1} \dotsm (T - \beta_r)^{N_r} v = 0.
            \end{equation}
            Since \(\alpha \ne \beta_i\) we know that the greatest common factor of \((t - \alpha)^N\) and \((t - \beta_1)^{N_1} \dotsm (t - \beta_r)^{N_r}\) is 1.
            Thus, by B\'ezout's lemma there exist polynomials \(f(t), g(t) \in \field[t]\) such that
            \begin{equation}
                f(t)(t - \alpha)^N + g(t)(t - \beta_1)^{N_1} \dotsm (t - \beta_r)^{N_r} = 1.
            \end{equation}
            Evaluating at \(T\) and applying this to \(v\) we then have
            \begin{equation}
                f(T)(T - \alpha)^Nv + g(T)(T - \beta_1)^{N_1}v \dotsm (T - \beta_r)^{N_r}v = v,
            \end{equation}
            and we know that these operators acting on \(v\) both give zero, so the left hand side vanishes, and thus \(v = 0\), and so \(V_\alpha \cap \sum_{\beta \in \field \setminus \{\alpha\}} V_\beta = 0\).
            
            We now show that \(\sum_{\alpha \in \field} V_\alpha = V\).
            Factorise the characteristic polynomial, \(\chi_T(t)\), as follows
            \begin{equation}
                \chi_T(t) = (t - \alpha_1)^{N_1} \dotsm (t - \alpha_s)^{N_s},
            \end{equation}
            with \(\alpha_i \ne \alpha_j\) for \(i \ne j\).
            Note that the existence of such a factorisation relies on \(\field\) being algebraically closed.
            We claim that \(v \in V_{\alpha_1} + \dotsb + V_{\alpha_s}\).
            This can be proven by induction on \(s\).
            The basis case, \(s = 1\), has the characteristic polynomial factorise as
            \begin{equation}
                \chi_T(t) = (t - \alpha_1)^{N_1}.
            \end{equation}
            Then by the Cayley--Hamilton theorem we have
            \begin{equation}
                (T - \alpha_1)^{N_1}v = 0
            \end{equation}
            for all \(v \in V\), and thus \(V_{\alpha_1} = V\).
            For the inductive step suppose that \(s > 1\) and the statement is true for \(s - 1\).
            The highest common factor of \((t - \alpha_s)^{N_s}\) and \((t - \alpha_1)^{N_1} \dotsm (t - \alpha_{s-1})^{N_{s-1}}\) is 1.
            Thus, by B\'ezout's lemma there exist \(f(t), g(t) \in \field[t]\) such that
            \begin{equation}
                f(t)(t - \alpha_s)^{N_s} + g(t)(t - \alpha_1)^{N_1} \dotsm (t - \alpha_{s-1})^{N_{s-1}} = 1.
            \end{equation}
            Evaluating at \(T\) and applying the map to \(v \in V\) we have
            \begin{equation}
                f(T)(T - \alpha_s)^{N_s}v + g(T)(T - \alpha_1)^{N_1} \dotsm (T - \alpha_{s-1})^{N_{s-1}}v = v.
            \end{equation}
            Define
            \begin{equation}
                v' = f(T)(T - \alpha_s)^{N_s}v, \qqand v_s = g(T)(T - \alpha_1)^{N_1} \dotsm (T - \alpha_{s-1})^{N_{s-1}}v,
            \end{equation}
            so we have \(v = v' + v_s\).
            Note that we have \((T - \alpha_s)^{N_s}v_s = 0\) and \((T - \alpha_1)^{N_1} \dotsm (T - \alpha_{s-1})^{N_{s-1}}v' = 0\).
            This is simply the Cayley--Hamilton theorem applied to the linear map \(T\) restricted to the subspaces corresponding to \(v_s\) and \(v'\) respectively.
            Define
            \begin{equation}
                V' = \{u \in V \mid (T - \alpha_1)^{N_1} \dotsm (T - \alpha_{s-1})^{N_{s-1}}u = 0\}.
            \end{equation}
            Then \(v_s \in V_{\alpha_s}\) and \(v' \in V'\).
            The characteristic polynomial of the restricted map, \(T|_{V'}\), has all of its roots in \(\{\alpha_1, \dotsc, \alpha_{s-1}\}\).
            By induction, we therefore have \(v' = v_1 + \dotsb + v_{s-1}\) with \(v_i \in V_{\alpha_i}' = V' \cap V_{\alpha_i}\).
            Thus, \(v = v_1 + \dotsb + v_{s-1} + v_s in V_{\alpha_1} + \dotsb + V_{\alpha_s}\).
            Since \(v \in V\) was arbitrary this shows that \(V = V_{\alpha_1} + \dotsb + V_{\alpha_s}\), proving that \(V = V_{\alpha_1} \oplus \dotsb \oplus V_{\alpha_s}\).
            This gives the final result by realising that if \(\beta \ne \alpha_i\) then \(V_\beta = 0\) since \(V_\beta \cap (V_{\alpha_1} + \dotsb + V_{\alpha_s}) = 0\) so \(V_\beta \cap V = 0\), so \(V_\beta = 0\).
            Thus, \(V = \bigoplus_{\alpha \in \field} V_{\alpha}\) and \(V_{\alpha}\) is only nonzero for the finite index set \(\{\alpha_1, \dotsc, \alpha_s\}\).
        \end{proof}
    \end{thm}
    
    Note that both conditions, the finite dimension of \(V\) and the algebraic closure of \(\field\), are required here.
    Dropping either condition will result in cases where this decomposition doesn't exist.
    
    \begin{exm}{}{}
        Consider again the example of \(\field = \complex\), \(V = \complex^2\), and
        \begin{equation}
            T = 
            \begin{bmatrix}
                0 & 1\\
                0 & 0
            \end{bmatrix}
            .
        \end{equation}
        As we saw before (\cref{exm:E not diagonalisable}) \(T\) is not diagonalisable, and \(\bigoplus_{\alpha \in \complex} W_\alpha = W_0 = \Span\{e_1\} \ne \complex^2\).
        We also saw there that \(T^2e_2 = 0\), meaning that when considering generalised eigenspaces we also have \(e_2 \in V_0\), and thus
        \begin{equation}
            \bigoplus_{\alpha \in \complex} V_\alpha = V_0 = \complex^2.
        \end{equation}
    \end{exm}
    
    \begin{dfn}{Jordan Block and Jordan Normal Form}{}
        A \defineindex{Jordan block} is a matrix of the form
        \begin{equation}
            \begin{bmatrix}
                \alpha & 1 \\
                & \alpha & 1 \\
                & & \ddots & \ddots\\
                & & & \alpha & 1\\
                & & & & \alpha
            \end{bmatrix}
            .
        \end{equation}
        That is, we have some \(\alpha \in \field\) on the diagonal and 1s above the diagonal, and 0 everywhere else.
        
        A matrix, \(X\), is a sum of Jordan blocks, or is in \defineindex{Jordan normal form} if it is given by the block diagonal matrix
        \begin{equation}
            X = 
            \begin{bmatrix}
                J_1\\
                & \ddots \\
                & & J_r
            \end{bmatrix}
        \end{equation}
        with the \(J_i\) Jordan blocks.
    \end{dfn}
    
    \begin{crl}{Existence of Jordan Normal Form}{}
        Let \(\field\) be an algebraically closed field, let \(V\) be a finite dimensional vector space, and let \(T \colon V \to V\) be a linear map.
        There exists a basis of \(V\) such that \([T]_{\symcal{B}}\) is in Jordan normal form.
        \begin{proof}
            Consider the decomposition of \(V\) into a sum of generalised eigenspaces of \(T\).
            Each generalised eigenspace is invariant under \(T\), and thus if \(v_\alpha \in V_\alpha\) then \(Tv_\alpha = c_1v_1 + \dotsb c_rv_r\) where \(\{v_1, \dotsc, v_r\}\) is a basis for \(V_\alpha\).
            We can further choose this basis by scaling and taking linear combinations untile \(Tv_{i} = \alpha v_i + v_{i+1}\), which can be shown by induction on the size of the dimension of \(V_\alpha\).
            Then \(T\) acts on \(V_\alpha\) in this basis as a Jordan block, and so \(T\) acts on \(V\), in the basis formed by the union of these bases, as a matrix in Jordan normal form.
        \end{proof}
    \end{crl}
    
    \chapter{Lie Algebras}
    \section{Definition and Remarks}
    \begin{dfn}{Lie Algebra}{}
        Let \(\field\) be a field.
        A \defineindex{Lie algebra} over \(\field\) is a vector space, \(\lie{g}\), over \(\field\) equipped with a bilinear map,
        \begin{align}
            \bracket{-}{-} \colon \lie{g} \times \lie{g} &\to \lie{g}\\
            (X, Y) &\mapsto \bracket{X}{Y},
        \end{align}
        called the Lie bracket, or simply the bracket.
        This must satisfy two properties:
        \begin{enumerate}
            \item \defineindex{alternating}: for all \(X \in \lie{g}\) we have \(\bracket{X}{X} = 0\);
            \item \defineindex{Jacobi identity}: for all \(X, Y, Z \in \lie{g}\) we have
            \begin{equation}
                \bracket{X}{\bracket{Y}{Z}} + \bracket{Y}{\bracket{Z}{X}} + \bracket{Z}{\bracket{X}{Y}} = 0.
            \end{equation}
        \end{enumerate}
    \end{dfn}
    
    Bilinearity means that for all \(\lambda \in \field\) and \(X, Y, Z \in \lie{g}\) we have
    \begin{align}
        \bracket{X + \lambda Y}{Z} &= \bracket{X}{Z} + \lambda \bracket{Y}{Z},\\
        \bracket{X}{Y + \lambda Z} &= \bracket{X}{Y} + \lambda \bracket{X}{Z}.
    \end{align}
    
    We say that \(\bracket{-}{-}\) is antisymmetric if \(\bracket{X}{Y} = -\bracket{Y}{X}\).
    It turns out that this property is almost the same as, but not quite, the alternating property, and as such the definition of a Lie algebra is often given with antisymmetry in place of alternativity, the catch being that over fields of characteristic 2 these aren't equivalent.
    Alternativity is the \enquote{correct} condition, but if we're only looking at \(\field = \complex, \reals\), as is often the case, then there's little harm in taking antisymmetry as the defining condition.
    
    \begin{lma}{}{}
        The Lie bracket is antisymmetric.
        \begin{proof}
            Let \(\lie{g}\) be a Lie algebra and take \(X, Y \in \lie{g}\).
            Consider the bracket of \(X + Y\) with itself.
            On the one hand, alternativity tells us that \(\bracket{X + Y}{X + Y} = 0\), and on the other hand using bilinearity and alternativity we have
            \begin{align}
                \bracket{X + Y}{X + Y} &= \bracket{X}{X} + \bracket{X}{Y} + \bracket{Y}{X} + \bracket{Y}{Y}\\
                &= \bracket{X}{Y} + \bracket{Y}{X}
            \end{align}
            Now, if this is to vanish we must have
            \begin{equation}
                \bracket{X}{Y} = -\bracket{Y}{X}. \qedhere
            \end{equation}
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{}
        Over a field of characteristic other than 2 antisymmetry of a bilinear bracket implies alternativity.
        \begin{proof}
            Let \(\lie{g}\) be a vector space over \(\field\) with \(\Char \field \ne 2\), equipped with an \emph{antisymmetric} bracket, \(\bracket{-}{-} \colon \lie{g} \times \lie{g} \to \lie{g}\).
            Antisymmetry means that \(\bracket{X}{Y} = -\bracket{Y}{X}\) for all \(X, Y \in \lie{g}\).
            In particular, we are free to take \(X = Y\), then we have \(\bracket{X}{X} = -\bracket{X}{X}\).
            Rearranging this gives
            \begin{equation}
                \bracket{X}{X} + \bracket{X}{X} = 2 \bracket{X}{X} = 0.
            \end{equation}
            Now, in a field with \(2 \ne 0\) (that is, in a field of characteristic other than 2) this immediately implies \(\bracket{X}{X} = 0\).
        \end{proof}
    \end{lma}
    
    This distinction is subtle, and ultimately not that important since we'll mostly concern ourselves with \(\field = \complex\), which has characteristic 0 (and \(\Char \reals = 0\) also).
    
    The Jacobi identity is a bit weird the first time you see it.
    There are a couple of ways to think about it that help to remember the definition.
    The first is to notice that the second and third term are cyclic permutations of the first, so it's often shortened to
    \begin{equation}
        \bracket{X}{\bracket{Y}{Z}} + \text{cycilc permutations} = 0.
    \end{equation}
    The second is to write it like
    \begin{equation}
        \bracket{X}{\bracket{Y}{Z}} = \bracket{\bracket{X}{Y}}{Z} + \bracket{Y}{\bracket{X}{Z}},
    \end{equation}
    which is an equivalent form.
    This doesn't look much simpler, but fix some \(X \in \lie{g}\) and define a linear map \(D \colon \lie{g} \to \lie{g}\) by \(D(Y) = \bracket{X}{Y}\).
    Then this becomes
    \begin{equation}
        D(\bracket{Y}{Z}) = \bracket{D(Y)}{Z} + \bracket{Y}{D(Z)}.
    \end{equation}
    To make this even simpler, write \(A \cdot B\) in place of \(\bracket{A}{B}\), and we have
    \begin{equation}
        D(Y \cdot Z) = D(Y) \cdot Z + Y \cdot D(Z)
    \end{equation}
    and we see that this is a version of the product rule, or Leibniz rule.
    So \(D\) acts a bit like a derivative.
    The fancy way to say this is that the adjoint representation of \(\lie{g}\) acts on \(\lie{g}\) by derivations.
    A \defineindex{derivation} is just any linear map on an algebra satisfying the Leibniz rule with respect to the product of that algebra, which for a Lie group is the Lie bracket.
    The \enquote{adjoint representation} (\cref{def:adjoint rep}) is simply \(\lie{g}\) acting on itself where \(X\) acts on \(Y\) by \(X \cdot Y \mapsto \bracket{X}{Y}\).
    
    Now\footnote{I've brought this forwards in the notes, it feels wrong to define objects and not move on to morphisms immediately.} that we've defined Lie algebras, an algebraic object, we should define maps between them.
    The appropriate maps will be \enquote{structure preserving}, in the same way that a group homomorphism is structure preserving (it preserves the multiplicative structure) and a linear map is structure preserving (it preserves addition and scalar multiplication).
    The structure that we have to preserve here is the Lie bracket, as well as the underlying vector space structure of the Lie algebra.
    
    \begin{dfn}{Homomorphism}{}
        Let \(\lie{g}\) be a Lie algebra with bracket \(\bracket{-}{-}_{\lie{g}}\), and let \(\lie{h}\) be a Lie algebra with bracket \(\bracket{-}{-}_{\lie{h}}\).
        Then a \defineindex{Lie algebra homomorphism}, \(\varphi \colon \lie{g} \to \lie{h}\), is a linear map which preserves the bracket, meaning that for all \(X, Y \in \lie{g}\) we have
        \begin{equation}
            \varphi(\bracket{X}{Y}_{\lie{g}}) = \bracket{\varphi(X)}{\varphi(Y)}_{\lie{h}}.
        \end{equation}
        An invertible Lie algebra homomorphism is a \defineindex{Lie algebra isomorphism}.
    \end{dfn}
    
    We'll usually just say \enquote{homomorphism}, or simply \enquote{morphism}, rather than \enquote{Lie algebra homomorphism}, and likewise we'll just speak of an \enquote{isomorphism} rather than a \enquote{Lie algebra isomorphism}.
    We will writhe \(\lie{g} \isomorphic \lie{h}\) if there is a Lie algebra isomorphism \(\lie{g} \to \lie{h}\).
    For notational simplicity we'll usually write \(\bracket{-}{-}\) for the bracket of both \(\lie{g}\) and \(\lie{h}\), allowing context (i.e., what elements we put into it) to make clear which we mean.
    
    We now prove the standard results about morphisms.
    
    \begin{lma}{}{}
        If \(\varphi \colon \lie{g} \to \lie{h}\) is a Lie algebra isomorphism then \(\varphi^{-1} \colon \lie{h} \to \lie{g}\) is a Lie algebra isomorphism.
        \begin{proof}
            We need to show that for all \(X', Y' \in \lie{h}\) we have
            \begin{equation}
                \varphi^{-1}(\bracket{X'}{Y'}_{\lie{h}}) = \bracket{\varphi^{-1}(X')}{\varphi^{-1}(Y')}_{\lie{g}}.
            \end{equation}
            To do so note that \(\varphi^{-1}\) is an invertible linear map, and as such is a bijection.
            This means that there exist unique \(X, Y \in \lie{g}\) such that \(\varphi(X) = X'\) and \(\varphi(Y) = Y'\).
            Thus, we may write the left hand side as
            \begin{equation}
                \varphi^{-1}(\bracket{X'}{Y'}_{\lie{h}}) = \varphi^{-1}(\bracket{\varphi(X)}{\varphi(Y)}_{\lie{h}}).
            \end{equation}
            Using the fact that \(\varphi\) is an isomorphism, and hence is a homomorphism, we can pull the \(\varphi\) out of the bracket on the right, to give
            \begin{equation}
                \varphi^{-1}(\bracket{X'}{Y'}_{\lie{h}}) = \varphi^{-1}(\varphi(\bracket{X}{Y}_{\lie{g}})).
            \end{equation}
            Then, since by definition \(\varphi^{-1} \circ \varphi = \id_{\lie{g}}\) we have
            \begin{equation}
                \varphi^{-1}(\bracket{X'}{Y'}_{\lie{h}}) = \bracket{X}{Y}_{\lie{g}}.
            \end{equation}
            We can then invert \(X' = \varphi(X)\) and \(Y' = \varphi(Y)\) to write \(X = \varphi^{-1}(X')\) and \(Y = \varphi^{-1}(Y')\).
            This lets us write this as
            \begin{equation}
                \varphi^{-1}(\bracket{X'}{Y'}_{\lie{h}}) = \bracket{\varphi^{-1}(X')}{\varphi^{-1}(Y')}_{\lie{g}},
            \end{equation}
            which is the result we wanted.
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{}
        The composite of Lie algebra homomorphisms is a Lie algebra homomorphism.
        \begin{proof}
            Let \(\lie{g}\), \(\lie{h}\), and \(\lie{l}\) be Lie algebras with brackets \(\bracket{-}{-}_{\lie{g}}\), \(\bracket{-}{-}_{\lie{h}}\), and \(\bracket{-}{-}_{\lie{l}}\) respectively.
            Let \(\varphi \colon \lie{g} \to \lie{h}\) and \(\psi \colon \lie{h} \to \lie{l}\) be Lie algebra homomorphisms.
            Then we may consider \(\psi \circ \varphi \colon \lie{g} \to \lie{l}\).
            We wish to show that this is a Lie algebra homomorphism.
            
            First, note that \(\psi\) and \(\varphi\) are linear, so their composite is too.
            We then need only show that for all \(X, Y \in \lie{g}\) we have
            \begin{equation}
                (\psi \circ \varphi)(\bracket{X}{Y}_{\lie{g}}) = \bracket{(\psi \circ \varphi)(X)}{(\psi \circ \varphi)(Y)}_{\lie{l}}.
            \end{equation}
            Starting with the left-hand side we can use the definition of composition to write
            \begin{equation}
                (\psi \circ \varphi)(\bracket{X}{Y}_{\lie{g}}) = \psi(\varphi(\bracket{X}{Y}_{\lie{g}})).
            \end{equation}
            Using the fact that \(\varphi\) is a Lie algebra homomorphism into \(\lie{h}\) we have \(\varphi(\bracket{X}{Y}_{\lie{g}}) = \bracket{\varphi(X)}{\varphi(Y)}_{\lie{h}}\), so we have
            \begin{equation}
                (\psi \circ \varphi)(\bracket{X}{Y}_{\lie{g}}) = \psi(\bracket{\varphi(X)}{\varphi(Y)}_{\lie{h}}).
            \end{equation}
            Now, using the fact that \(\psi\) is a Lie algebra homomorphism into \(\lie{l}\) we have \(\psi(\bracket{X'}{Y'}_{\lie{h}}) = \bracket{\psi(X')}{\psi(Y')}_{\lie{l}}\) for any \(X', Y' \in \lie{h}\), and in particular this is true when \(X' = \varphi(X)\) and \(Y' = \varphi(Y)\), giving
            \begin{equation}
                (\psi \circ \varphi)(\bracket{X}{Y}_{\lie{g}}) = \bracket{\psi(\varphi(X))}{\psi(\varphi(Y))}_{\lie{l}}.
            \end{equation}
            Finally, using the definition of composition again, we get our result,
            \begin{equation}
                (\psi \circ \varphi)(\bracket{X}{Y}_{\lie{g}}) = \bracket{(\psi \circ \varphi)(X)}{(\psi \circ \varphi)(Y)}_{\lie{l}}. \qedhere
            \end{equation}
        \end{proof}
    \end{lma}
    
    Noting that function composition is associative we have the following result.
    
    \begin{crl}{}{}
        For a field \(\field\) there is a category \(\LieAlg\) whose objects are Lie algebras over \(\field\) and whose morphisms are Lie algebra homomorphisms.
    \end{crl}
    
    The objects of this category, Lie algebras, are vector spaces, and the morphisms are linear maps.
    This means that \(\LieAlg\) is a subcategory of \(\Vect\), and as such inherits a lot of its structure from \(\Vect\).
    For example, the coproduct (direct sum) of Lie algebras is just the coproduct (direct sum) of the underlying vector spaces, equipped with an appropriate bracket.
    That is, given Lie algebras \(\lie{g}\) and \(\lie{h}\) we can equip the vector space \(\lie{g} \oplus \lie{h}\) with a Lie algebra structure by defining
    \begin{equation}
        \bracket{X_1 + Y_1}{X_2 + Y_2}_{\lie{g} \oplus \lie{h}} = \bracket{X_1}{X_2}_{\lie{g}} + \bracket{Y_1}{Y_2}_{\lie{h}}
    \end{equation} 
    where \(X_1 + Y_1\) and \(X_2 + Y_2\) are two arbitrary elements of \(\lie{g} \oplus \lie{h}\) decomposed into \(X_1, X_2 \in \lie{g}\) and \(Y_1, Y_2 \in \lie{h}\).
    The alternativity and Jacobi identity for \(\lie{g}\) and \(\lie{h}\) apply directly when brackets in \(\lie{g} \oplus \lie{h}\) are written as on the right-hand side here, and so these properties also hold for the bracket on \(\lie{g} \oplus \lie{h}\).
    This gives the external direct sum.
    We can similarly give an internal direct sum by saying that if \(\lie{g}\) is a Lie algebra with subalgebras \(\lie{h}_1\) and \(\lie{h}_2\) then \(\lie{g} = \lie{h}_1 \oplus \lie{h}_2\) if \(\lie{g} = \lie{h}_1 \oplus \lie{h}_2\) as vector spaces and for all \(X \in \lie{h}_1\) and \(Y \in \lie{h}_2\) we have \(\bracket{X}{Y} = 0\).
    The idea here is that the two parts of the direct sum should not \enquote{interact}.
    For vector spaces this simply means that they should be disjoint except for 0, and for Lie algebras this means that elements from the two parts should commute (which is what we say when the bracket vanishes, since often the bracket is literally a commutator).
        
    \section{Subalgebras, Ideals, and Quotients}
    To state these definitions it's useful to abuse the notation slightly as follows.
    Let \(U, V \subseteq \lie{g}\) be subspaces of a Lie algebra.
    Define the subspace
    \begin{equation}
        \bracket{U}{V} = \Span\{\bracket{u}{v} \mid u \in U \text{ and } v \in V\}.
    \end{equation}
    
    \begin{dfn}{Subalgebra}{}
        Let \(\lie{g}\) be a Lie algebra.
        A \defineindex{Lie subalgebra} (or just \defineindex{subalgebra}), \(\lie{h}\), is a subspace which is closed under the Lie bracket.
        That is, \(\bracket{\lie{h}}{\lie{h}} \subseteq \lie{h}\), or equivalently for all \(h, h' \in \lie{h}\) we have \(\bracket{h}{h'} \in \lie{h}\).
    \end{dfn}
    
    The notion of a Lie subalgebra is the Lie algebra analogue of a subset of a set, subspace of a vector space, subgroup of a group, or subring of a ring.
    We can't form a quotient by an arbitrary subset, subgroup or subring, we need a set generated by an equivalence relation, a normal subgroup, or an ideal.
    This is exactly the case with Lie algebras as well.
    
    Note that a Lie subalgebra, \(\lie{h}\), is necessarily a Lie algebra in its own right after restricting the bracket to \(\lie{h}\).
    This follows immediately because the requirements for a Lie algebra are universally quantified so remain true after restricting to a subspace.
    
    \begin{dfn}{Ideal}{}
        Let \(\lie{g}\) be a Lie algebra.
        An \defineindex{ideal} of \(\lie{g}\) is a Lie subalgebra, \(\lie{i}\), such that \(\bracket{\lie{i}}{\lie{g}} \subseteq \lie{i}\), or equivalently for all \(X \in \lie{g}\) and \(I \in \lie{i}\) we have \(\bracket{X}{I} \in \lie{i}\).
    \end{dfn}
    
    Note that there's no notion of left- or right-ideals, any ideal is two sided since \(\bracket{\lie{h}}{\lie{h}'} = \bracket{\lie{h}}{\lie{h}'}\), as elements of the two differ only by a sign and the fact that these are subspaces means that all the elements that differ only by a sign are also included.
    
    With ideals we can define quotients.
    
    \begin{dfn}{Quotient}{}
        Let \(\lie{g}\) be a Lie algebra, and let \(\lie{i} \subseteq \lie{g}\) be an ideal.
        Then the \defineindex{quotient} vector space \(\lie{g}/\lie{i}\) is a Lie algebra when we define the bracket by
        \begin{equation}
            \bracket{X + \lie{i}}{Y + \lie{i}} = \bracket{X}{Y} + \lie{i}.
        \end{equation}
    \end{dfn}
    
    Note that in the definition of the bracket of \(\lie{g}/\lie{i}\) the bracket \(\bracket{X + \lie{i}}{Y + \lie{i}}\) is computed in \(\lie{g}/\lie{i}\), and the bracket \(\bracket{X}{Y}\) is computed in \(\lie{g}\).
    
    \begin{lma}{}{}
        The quotient algebra as defined above is a Lie algebra.
        \begin{proof}
            Let \(\lie{g}\) be a Lie algebra and let \(\lie{i}\) be an ideal.
            We have four things to show:
            \begin{enumerate}
                \item The bracket on \(\lie{g}/\lie{i}\) is well-defined;
                \item The bracket on \(\lie{g}/\lie{i}\) is bilinear;
                \item The bracket on \(\lie{g}/\lie{i}\) is alternating;
                \item The bracket on \(\lie{g}/\lie{i}\) satisfies the Jacobi identity.
            \end{enumerate}
            The first relies on \(\lie{i}\) being an ideal, and the others are simply an exercise in algebra.
            
            \Step{Well-Defined}
            Let \(X, X' \in \lie{g}\) be such that \(X + \lie{i} = X' + \lie{i}\).
            Recall that this means \(X - X' \in \lie{i}\).
            Then for all \(Y \in \lie{g}\) we have
            \begin{equation}
                \bracket{X + \lie{i}}{Y + \lie{i}} = \bracket{X}{Y} + \lie{i}
            \end{equation}
            and
            \begin{equation}
                \bracket{X' + \lie{i}}{Y + \lie{i}} = \bracket{X'}{Y} + \lie{i}.
            \end{equation}
            We need to show that these are equal.
            This means we need to show that
            \begin{equation}
                \bracket{X}{Y} - \bracket{X'}{Y} \in \lie{i}.
            \end{equation}
            This is necessarily true however, since bilinearity gives us
            \begin{equation}
                \bracket{X}{Y} - \bracket{X'}{Y} = \bracket{X - X'}{Y}
            \end{equation}
            which is in \(\lie{i}\) since \(X - X' \in \lie{i}\) and \(\lie{i}\) is an ideal.
            
            \Step{Bilinear}
            We will demonstrate linearity in the first argument.
            Linearity in the second argument then follows from a similar argument, or in a field of characteristic other than 2 it follows from linearity in the first argument and alternativity, which implies anticommutativity.
            Let \(X, X', Y \in \lie{g}\), then
            \begin{align}
                \bracket{(X + \lie{i}) + (X' + \lie{i})}{Y + \lie{i}} &= \bracket{(X + X') + \lie{i}}{Y + \lie{i}}\\
                &= \bracket{X + X'}{Y} + \lie{i}\\
                &= (\bracket{X}{Y} + \bracket{X'}{Y}) + \lie{i}\\
                &= (\bracket{X}{Y} + \lie{i}) + (\bracket{X'}{Y} + \lie{i}).
            \end{align}
            Further, take \(\lambda \in \field\) and we have
            \begin{align}
                \bracket{(\lambda X) + \lie{i}}{Y + \lie{i}} &= \bracket{\lambda X}{Y} + \lie{i}\\
                &= (\lambda \bracket{X}{Y}) + \lie{i}\\
                &= \lambda(\bracket{X}{Y} + \lie{i}).
            \end{align}
            Thus, the bracket is linear in the first argument.
            
            \Step{Alternating}
            Let \(X \in \lie{g}\), then
            \begin{equation}
                \bracket{X + \lie{i}}{X + \lie{i}} = \bracket{X}{X} + \lie{i} = 0 + \lie{i} = \lie{i} = 0
            \end{equation}
            since \(\lie{i} = 0 + \lie{i}\) is the zero vector in the quotient space \(\lie{g}/\lie{i}\).
            
            \Step{Jacobi Identity}
            Let \(X, Y, Z \in \lie{g}\),
            \begin{equation}
                \bracket{X + \lie{i}}{\bracket{Y + \lie{i}}{Z + \lie{i}}} = \bracket{X + \lie{i}}{\bracket{Y}{Z} + \lie{i}} = \bracket{X}{\bracket{Y}{Z}} + \lie{i}.
            \end{equation}
            Doing this with the other terms in the Jacobi identity we see that we end up with the left hand side of the Jacobi identity in \(\lie{g}/\lie{i}\) being
            \begin{equation}
                (\bracket{X}{\bracket{Y}{Z}} + \lie{i}) + (\bracket{Y}{\bracket{Z}{X}} + \lie{i}) + (\bracket{Z}{\bracket{X}{Y}} + \lie{i}).
            \end{equation}
            Recall that addition in the quotient space is defined by \((X + \lie{i}) + (Y + \lie{i}) = (X + Y) + \lie{i}\), and so this becomes
            \begin{equation}
                (\bracket{X}{\bracket{Y}{Z}} + \bracket{Y}{\bracket{Z}{X}} + \bracket{Z}{\bracket{X}{Y}}) + \lie{i} = 0 + \lie{i} = 0
            \end{equation}
            where we've used the Jacobi identity in \(\lie{g}\), and then identified \(0 + \lie{i}\) as the zero vector in the quotient \(\lie{g}/\lie{i}\).
        \end{proof}
    \end{lma}
    
    \section{Examples of Lie Algebras}
    We've done quite a lot now without ever actually looking at any examples of a Lie algebra, so let's change that.
    
    \subsection{Abelian Lie Algebra}
    We start with the simplest example, where the bracket always vanishes.
    
    \begin{dfn}{Abelian Lie Algebra}{}
        A Lie algebra, \(\lie{g}\), is \define{abelian}\index{abelian Lie algebra} if \(\bracket{\lie{g}}{\lie{g}} = 0\), that is, if \(\bracket{X}{Y} = 0\) for all \(X, Y \in \lie{g}\).
    \end{dfn}
    
    The terminology here, abelian, should not be confused with this terminology referring to groups.
    An abelian Lie algebra is not one in which the bracket is \enquote{commutative} (although it is, since every bracket is the 0 we do technically have \(\bracket{X}{Y} = \bracket{Y}{X}\)).
    Instead, this terminology is inherited from the Lie group, an abelian Lie group will give us an abelian Lie algebra.
    
    Note that, up to isomorphism, there is one abelian Lie algebra of each dimension for each field.
    Any vector space can be made into an abelian Lie algebra by equipping it with the trivial bracket, and any two such Lie algebras are isomorphic so long as the underlying vector spaces are.
    For this reason abelian Lie algebras on their own are \enquote{boring}, we only look at them when they arise naturally as subalgebras of nonabelian Lie algebras.
    
    \subsection{Low Dimension}
    In an attempt to classify Lie algebras (something we'll return to later) one might start with dimension.
    Let's do that.
    For dimension 0 there is one Lie algebra, the zero vector space with the trivial bracket.
    
    For dimension 1 (and a fixed field) there is one Lie algebra (up to isomorphism), the one-dimensional space equipped with the trivial bracket.
    Note that there cannot be a nonabelian one-dimensional Lie algebra.
    Suppose that \(\lie{g}\) was such a Lie algebra, then \(\lie{g} = \field X = \Span\{X\}\) for some \(X\).
    Then if \(Y, Z \in \lie{g}\) we know that \(Y = \lambda X\) and \(Z = \mu X\), and then
    \begin{equation}
        \bracket{Y}{Z} = \bracket{\lambda X}{\mu X} = \lambda \mu \bracket{X}{X} = \lambda \mu \cdot 0 = 0.
    \end{equation}
    Thus, all one-dimensional Lie algebras are necessarily abelian.
    
    For dimension 2 (and a fixed field) we of course have the abelian Lie algebra, but there is another.
    Suppose that \(\lie{g}\) is two-dimensional with basis \(\{X, Y\}\).
    Assuming that \(\lie{g}\) is nonabelian the bracket \(\bracket{X}{Y} = -\bracket{Y}{X}\) must be nonzero.
    This means that \(\bracket{\lie{g}}{\lie{g}}\) is a one-dimensional subspace of \(\lie{g}\), since every element of \(\bracket{\lie{g}}{\lie{g}}\) is a linear combination of brackets of elements of \(\lie{g}\), but all nonzero such brackets are multiples of \(\bracket{X}{Y}\).
    Further, we can rescale \(X\) so that \(\bracket{X}{Y} = Y\), and this is the only 2-dimensional non-abelian Lie algebra up to isomorphism (for a fixed field).
    So, there are two Lie algebras of dimension 2 up to isomorphism.
    
    In three dimensions one can make similar arguments, and it turns out that there are infinitely many isomorphism classes of three-dimensional Lie algebra, but we can still classify them.
    Here are the isomorphism classes over \(\complex\) (or any algebraically closed field of characteristic 0):
    \begin{itemize}
        \item The abelian Lie algebra of dimension 3.
        \item \(\lie{g} = \Span\{X, Y, Z\}\) with \(\bracket{X}{Y} = Z\) and \(\bracket{X}{Z} = \bracket{Y}{Z} = 0\)\footnote{This is the three-dimensional \defineindex{Heisenberg algebra}.}.
        \item \(\lie{g} = \Span\{X, Y, Z\}\) with \(\bracket{X}{Y} = Y\) and \(\bracket{X}{Z} = \bracket{X}{Z} = 0\).
        \item \(\lie{g}_b = \Span\{X, Y, Z\}\) with \(\bracket{X}{Y} = Y\), \(\bracket{Y}{Z} = bZ\), and \(\bracket{Y}{Z} = 0\) for \(b \in \complex^{\times}\).
        Note that \(\lie{g}_b \isomorphic \lie{g}_{b'}\) if and only if \(b = b'\) or \(b = 1/b'\).
        \item \(\lie{g} = \Span\{X, Y, Z\}\) with \(\bracket{X}{Y} = Y\), \(\bracket{X}{Z} = Y + Z\), and \(\bracket{Y}{Z} = 0\).
        \item \(\lie{g} = \Span\{X, Y, Z\}\) with \(\bracket{X}{Y} = 2X\), \(\bracket{X}{Z} = -2Z\), and \(\bracket{Y}{Z} = X\).
    \end{itemize}
    Of these, by far the most important is the last one, which we'll later see is the Lie algebra known as \(\specialLinearLie(2, \complex)\).
    
    If we classify over \(\reals\) instead then we get another familiar example of a Lie algebra, \(\reals^3\) equipped with the bracket given by the cross-product.
    
    Hopefully, these examples are enough to convince you that a full classification for Lie algebras of dimension 4 is, while maybe possible, probably quite challenging.
    
    All hope is not lost, we do have, as we'll see later, a classification of a particularly nice type of Lie algebra, called simple Lie algebras, over \(\complex\).
    
    \subsection{Associative Algebras}
    \begin{dfn}{Algebra}{def:algebra}
        An \defineindex{algebra} is a vector space, \(A\), equipped with a bilinear product, \(\cdot \colon A \times A \to A\).
        An \defineindex{associative algebra} is an algebra for which the product is associative.
    \end{dfn}
    
    Note that Lie algebras are algebras in the above sense, but they are generally not associative.
    Examples of associative algebras include:
    \begin{itemize}
        \item \(n \times n\) matrices with matrix multiplication.
        \item \(\reals\), \(\complex\), or \(\quaternions\) (the quaternions) with their usual multiplication are associative algebras over \(\reals\).
        In fact, these are \emph{division} algebras: an algebra, \(D\), is a division algebra if for any \(a, b \in D\) with \(b \ne 0\) there is exactly one \(x \in D\) with \(a = bx\) and exactly one \(y \in D\) with \(a = yb\).
        It turns out that these are the only finite-dimensional associative division algebras over \(\reals\).
        If we drop the associativity condition then we also get \(\symbb{O}\), the octonions.
    \end{itemize}
    
    Our interest in associative algebras is mostly the following.
    
    \begin{lma}{}{}
        If \(A\) is an associative algebra then defining the bracket by \(\commutator{X}{Y} = XY - YX\) defines a Lie algebra.
        \begin{proof}
            First note that we clearly have
            \begin{equation}
                \commutator{X}{X} = XX - XX = 0.
            \end{equation}
            The commutator is bilinear, here we show linearity in the second argument, taking \(X, Y, Z \in A\) and \(\lambda \in \field\)
            \begin{align}
                \commutator{X}{Y + \lambda Z} &= X(Y + \lambda Z) - (Y + \lambda Z)X\\
                &= XY + \lambda XZ - YX - \lambda ZX\\
                &= XY - YX + \lambda (XZ - ZX)\\
                &= \commutator{X}{Y} + \lambda \commutator{X}{Z}.
            \end{align}
            The Jacobi identity follows by some algebra.
            First note that
            \begin{align}
                \commutator{X}{\commutator{Y}{Z}} &= \commutator{X}{YZ - ZY}\\
                &= X(YZ - ZY) - (YZ - ZY)X\\
                &= XYZ - XZY - YZX + ZYX.
            \end{align}
            Then, using the fact that the other terms are simply linear combinations of these, we have that the Jacobi relation is reduced to
            \begin{multline}
                XYZ - XZY - YZX + ZYX + YZX - YXZ - ZXY + XZY\\
                + ZXY - ZYX - XYZ + YXZ
            \end{multline}
            and these terms all cancel to give zero.
        \end{proof}
    \end{lma}
    
    When the Lie bracket can be written as \(\commutator{X}{Y} = XY - YX\) we call it the \defineindex{commutator}, because it is a measure of the failure of \(A\) to be a commutative algebra.
    This terminology is also often used for the Lie bracket in general, but we'll try to avoid it in the general case.
    
    \subsection{Classical Lie Algebras}
    The complex classical Lie algebras are some classes of Lie algebras defined with similar definitions.
    These were first of interest because they correspond to particularly common Lie groups.
    First, some notation.
    
    \begin{ntn}{}{}
        Let \(\matrices{n}{\field}\) denote the set of \(n \times n\) matrices with entries in \(\field\).
        
        For \(A \in \matrices{n}{\field}\) denote the transpose by \(A^{\trans}\).
        
        For \(A \in \matrices{n}{\field}\) with \(\field = \reals, \complex\) denote the complex conjugate by \(\overbar{A}\).
        
        For \(A \in \matrices{n}{\field}\) with \(\field = \reals, \complex\) denote the Hermitian conjugate by \(A^{\hermit}\), recall that \(A^{\hermit} = \overbar{A}^{\trans} = \overline{A^{\trans}}\)
        
        \begin{wrn}
            Physicists will denote the complex conjugate by \(A^*\), and the Hermitian conjugate by \(A^{\dagger}\).
        \end{wrn}
    \end{ntn}
    
    \begin{dfn}{General Linear Lie Algebra}{}
        Let \(V\) be a vector space, and define
        \begin{equation}
            \generalLinearLie(V) = \End V \coloneq \{T \colon V \to V \mid T \text{ is linear}\} = \Vect(V, V).
        \end{equation}
        This is an associative algebra under composition of linear maps, and as such \(\generalLinearLie(V)\) is a Lie algebra under the commutator.
    \end{dfn}
    
    Typically we write \(\generalLinearLie(V)\) when thinking of the Lie algebra, and \(\End V\) when thinking of the associative algebra.
    
    When \(V\) is finite dimensional, say \(\dim V = n\), we can fix a basis and then each linear map corresponds to an \(n \times n\) matrix with entries in \(\field\).
    This leads us to make the following definition.
    \begin{equation}
        \generalLinearLie(n, \field) \coloneq \matrices{n}{\field} \isomorphic \generalLinearLie(V).
    \end{equation}
    Note that if \(\dim V = n\) then \(V \isomorphic \field^n\) and \(\generalLinearLie(n, \field) = \generalLinearLie(\field^n)\).
    
    The following are some of the subalgebras of \(\generalLinearLie(n, \complex)\) viewed as a \emph{real} vector space.
    This is important, not all of these are closed under multiplication by arbitrary complex numbers (in particular, if \(A\) is skew-Hermitian then \(iA\) is Hermitian).
    \begin{itemize}
        \item \(\generalLinearLie(n, \reals) = \matrices{n}{\reals}\), the general linear Lie algebra.
        \item \(\specialLinearLie(n, \reals) \coloneq \{A \in \generalLinearLie(n, \reals) \mid \tr A = 0\}\), the \defineindex{special linear Lie algebra};
        \item \(\orthogonalLie(n, \reals) = \specialOrthogonalLie(n, \reals) \coloneq \{A \in \generalLinearLie(n, \reals) \mid A^{\trans} + A = 0\}\), the \defineindex{orthogonal Lie algebra} and \defineindex{special orthogonal Lie algebra}.
        Note that these two are equal but given different names because they both come from important Lie groups which are not equal.
        Note that \(A^{\trans} + A = 0\) means \(A\) is antisymmetric.
        \item \(\unitaryLie(n) \coloneq \{A \in \generalLinearLie(n, \complex) \mid A^{\hermit} + A = 0\}\), the \defineindex{unitary Lie algebra}.
        Note that \(A^{\hermit} + A = 0\) means \(A\) is skew-Hermitian.
        \item \(\specialUnitaryLie(n) \coloneq \{A \in \generalLinearLie(n, \complex) \mid \tr A = 0 \text{ and } A^* + A = 0\}\), the \defineindex{special unitary Lie algebra};
        \item \(\symplecticLie(n, \reals) \coloneq \{A \in \generalLinearLie(2n, \reals) \mid A^\trans J_n + J_n A = 0\}\), the \defineindex{symplectic Lie algebra}, where \(J_n \in \matrices{2n}{\reals}\) is the block matrix
        \begin{equation}
            \begin{bmatrix}
                0 & I_n\\
                -I_n & 0
            \end{bmatrix}
        \end{equation}
        where \(I_n \in \matrices{n}{\reals}\) is the \(n \times n\) identity matrix.
        The matrix \(J_n\) (or rather, the symmetric bilinear form it represents) is called a \defineindex{symplectic form}.
    \end{itemize}
    
    \begin{wrn}
        Physicists like operators to be Hermitian, for quantum mechanics, so they will define things with a few factors of \(i\) different so that \(\unitaryLie(n)\) and \(\specialUnitaryLie(n)\) consist of Hermitian matrices, as opposed to \emph{skew-}Hermitian matrices.
        This distinction unfortunately causes these factors of \(i\) to propagate through many formulae, so be careful when looking things up that the source uses the correct convention.
    \end{wrn}
    
    \begin{wrn}
        Notation differs for the symplectic Lie algebra (and group), some authors denote it \(\symplecticLie(2n)\), it's the \(D_n\) vs \(D_{2n}\) debate all over again.
    \end{wrn}
    
    The names here are all derived from the names of the associated Lie groups, \(\generalLinear(n, \reals)\) the general linear group, \(\specialLinear(n, \reals)\) the special linear group, \(\orthogonal(n, \reals)\) the orthogonal group, \(\specialOrthogonal(n, \reals)\) the special orthogonal group, \(\unitary(n)\) the unitary group, \(\specialUnitary(n)\) the special unitary group, and various different but related symplectic Lie groups.
    
    These are defined as follows:
    \begin{itemize}
        \item \(\generalLinear(n, \reals) \coloneq \{A \in \matrices{n}{\reals} \mid \det A \ne 0\}\);
        \item \(\specialLinearLie(n, \reals) \coloneq \{A \in \generalLinear(n, \reals) \mid \det A = 1\}\);
        \item \(\orthogonal(n, \reals) \coloneq \{A \in \generalLinear(n, \reals) \mid A^{\trans}A = I_n\}\);
        \item \(\specialOrthogonal(n, \reals) \coloneq \{A \in \generalLinear(n, \reals) \mid A^{\trans}A = I_n \text{ and } \det A = 1\}\).
        \item \(\unitary(n) \coloneq \{A \in \generalLinear(n, \complex) \mid U^{\hermit} U = I_n\}\);
        \item \(\specialUnitary(n) \coloneq \{A \in \generalLinear(n, \complex) \mid U^{\hermit}U = I_n \text{ and } \det U = 1\}\);
        \item \(\symplectic(n, \reals) \coloneq \{A \in \generalLinear(2n, \reals) \mid A^{\trans}J_nA = J_n\}\).
    \end{itemize}
    
    These definitions are all about preserving some structure on \(\reals^n\) (or \(\reals^{2n}\) for the symplectic group).
    \begin{itemize}
        \item The general linear group preserves the vector space structure, including dimension.
        \item The special linear group preserves volues.
        \item The orthogonal group preserves angles.
        \item The special orthogonal group preserves the inner product on \(\reals^n\).
        \item The (special) unitary group is a complex analogue of the (special) orthogonal group.
        \item The symplectic group preserves \(J_n\).
    \end{itemize}
    
    Note that \enquote{special} means that we impose the condition \(\det A = 1\).
    In the case of the Lie algebras this becomes the condition that \(\tr A = 0\).
    This is the reason that \(\orthogonalLie(n, \reals) = \specialOrthogonalLie(n, \reals)\), the requirement that \(A\) is antisymmetric means that the diagonal of \(A\) is zero, so it automatically has trace zero.
    In terms of Lie groups, the reason is that both \(\orthogonal(n, \reals)\) and \(\specialOrthogonal(n, \reals)\) are very similar, in particular, \(\specialOrthogonal(n, \reals) \isomorphic \orthogonal(n, \reals)/(\integers/2\integers)\) where \(\integers/2\integers\) here is \(\{\pm I_n\}\), so \(\specialOrthogonal(n, \reals)\) preserves everything that \(\orthogonal(n, \reals)\) preserves, and also preserves orientation.
    This similarity means that \(\orthogonal(n, \reals)\) has two connected components, one preserving orientation, corresponding to the subgroup \(\specialOrthogonal(n, \reals)\), and one reversing orientation.
    Then the Lie algebra corresponds only to the component of the Lie group connected to the identity, which is \(\specialOrthogonal(n, \reals)\) in both cases, so the Lie algebras are the same.
    
    In some of these cases, but not all, it is possible to extend the field to \(\complex\), giving
    \begin{itemize}
        \item \(\generalLinearLie(n, \complex) \coloneq \matrices{n}{\complex}\);
        \item \(\orthogonalLie(n, \complex) \coloneq \specialOrthogonalLie(n, \complex) = \{A \in \generalLinearLie(n, \complex) \mid A^{\trans} + A\} = 0\).
        \item \(\symplecticLie(n, \complex) \coloneq \{A \in \generalLinearLie(2n, \complex) \mid A^{\trans}J_n + J_n A = 0\}\).
    \end{itemize}
    
    Showing that the classical Lie algebras are indeed Lie algebras, specifically subalgebras of \(\generalLinearLie(N, \field)\), requires a little bit of work.
    Mostly we have to show that they are closed under the bracket, which is the commutator in all cases.
    There are a few tricks though.
    The first is that \(\specialLinearLie(n, \field) = \ker \tr\), when we view \(\tr \colon \generalLinearLie(n, \field) \to \generalLinearLie(n, \field)\) as a linear operator, and the kernel of a linear operator is always a subspace.
    We also have to show that the commutator of two traceless matrices is again traceless for \(\specialLinearLie(n, \field)\) to be closed under the bracket:
    \begin{equation}
        \tr(\commutator{X}{Y}) = \tr(XY - YX) = \tr(XY) - \tr(YX) = \tr(XY) - \tr(XY) = 0
    \end{equation}
    where we've used the fact that the trace of a product is invariant under cyclic permutations of that product (henceforth, \enquote{the trace is cyclic}).
    Notice that this doesn't actually use that \(X\) and \(Y\) are traceless, the trace of a commutator is always zero.
    This means that if \(X \in \generalLinearLie(n, \field)\) and \(Y \in \specialLinearLie(n, \field)\) we still have \(\tr(\bracket{X}{Y}) = 0\), and thus \(\bracket{X}{Y} \in \specialLinearLie(n, \field)\), meaning that \(\specialLinearLie(n, \field)\) is an ideal of \(\generalLinearLie(n, \field)\)
    
    \subsubsection{Dimensions}
    An important exercise is to compute the dimensions of the classical Lie algebras.
    We'll do it for the real ones.
    \begin{itemize}
        \item \(\dim \generalLinearLie(n, \reals) = n^2\), since an arbitrary element of \(\generalLinearLie(n, \reals)\) is an \(n \times n\) matrix, which is parametrised by \(n^2\) entries.
        A basis for \(\generalLinearLie(n, \reals)\) is
        \begin{equation}
            \{E_{ij} \mid i = 1, \dotsc, n \text{ and } j = 1, \dotsc, n\}
        \end{equation}
        where \(E_{ij}\) has 1 in the \(i\)th row and \(j\)th column and \(0\) everywhere else.
        Note that
        \begin{equation}
            \commutator{E_{ij}}{E_{kl}} = E_{ij}E_{kl} - E_{kl}E_{ij} = \delta_{ij}E_{il} - \delta_{il}E_{kj}.
        \end{equation}
        \item \(\dim \specialLinearLie(n, \reals) = n^2 - 1\), since setting \(\tr A = 0\) fixes one element on the diagonal, say the last element, by the requirement that if \(A = (a_{ij})\) then \(a_{11} + \dotsb + a_{n-1,n-1} = -a_{nn}\) to get \(\tr A = a_{11} + \dotsb + a_{n-1,n-1} + a_{nn} = 0\).
        Alternatively, note that \(\specialLinearLie(n, \reals) = \ker \tr\), and \(\im \tr = \reals\) and so by the rank-nullity theorem we have
        \begin{equation}
            \dim \generalLinearLie(n, \reals) = \dim(\ker \tr) + \dim(\im \tr) = \dim \specialLinearLie(n, \reals) + 1.
        \end{equation}
        The result then follows from this and our calculation of \(\dim \generalLinearLie(n, \reals) = n^2\).
        \item \(\dim \specialOrthogonalLie(n, \reals) = n(n - 1)/2\), requiring that \(A\) be antisymmetric means that \(A\) is zero on the diagonal, and below the diagonal is fixed by above the diagonal.
        The above diagonal elements form a triangle with a base of \(n - 1\) elements, and thus the number of entries above the diagonal is the \((n-1)\)st triangle number, \(T_{n-1} = n(n-1)/2\) (\(T_n = n(n + 1)/2\)).
        We can check this by identifying that \(\specialOrthogonal(3)\) corresponds to rotations in three dimensions, and this is a three dimensional group, since any rotation is specified by either a) three Euler angles, or b) an angle and an axis of rotation (requiring two numbers to fix the direction, the third component fixed by requiring it to be a unit vector).
        Note that \(\dim \specialOrthogonalLie(3) = 3\) is a coincidence, we have \(\dim \specialOrthogonalLie(2) = 1\) (rotations in two dimensions are specified by just an angle) and \(\dim \specialOrthogonalLie(4) = 6\).
        Of course, here we're using the fact that a Lie group and its associated Lie algebra have the same dimension, this is because the tangent space has the same dimension as the manifold (assuming the manifold is connected, which is always the case for at least one Lie group corresponding to a given Lie algebra).
        \item \(\dim \unitaryLie(n) = n^2\), note that we are talking about the dimension as a real vector space, despite the entries being complex numbers.
        This is the dimension because an arbitrary \(n \times n\) matrix with entries in \(\complex\) has \(2n^2\) real parameters (one for the real part and one for the imaginary part of each entry).
        Requiring that the matrix is skew-Hermitian means that each entry on the diagonal must be equal to its conjugate, so it must have zero imaginary part (fixing \(n\) parameters), then the entries below the diagonal are fixed by the entries above the diagonal (fixing \(2 n(n - 1)/2 = n^2 - n\) parameters).
        Thus, the dimension is \(2n^2 - n - (n^2 - n) = n^2\).
        \item \(\dim \specialUnitaryLie(n) = n^2 - 1\), since any matrix in \(\specialUnitaryLie(n)\) is in \(\unitaryLie(n)\), and setting its trace to zero fixes the final entry on the diagonal (which is purely real).
        \item \(\dim \symplecticLie(n) = 2n^2 + n\), a general \(2n \times 2n\) matrix with entries in \(\reals\) has \((2n)^2 = 4n^2\) parameters.
        Write this as a block matrix,
        \begin{equation}
            A = 
            \begin{bmatrix}
                X & Y\\
                Z & W
            \end{bmatrix}
            , \qquad A^{\trans} = 
            \begin{bmatrix}
                X^{\trans} & Z^{\trans}\\
                Y^{\trans} & W^{\trans}
            \end{bmatrix}
        \end{equation}
        with \(X, Y, Z, W \in \matrices{n}{\reals}\).
        Then computing the defining relationship, \(A^{\trans}J_n + J_nA = 0\), we have
        \begin{equation}
            \begin{bmatrix}
                Z^{\trans} - Z & -W - X^{\trans}\\
                W^{\trans} + X & Y - Y^{\trans}
            \end{bmatrix}
            =
            \begin{bmatrix}
                0 & 0\\
                0 & 0
            \end{bmatrix}
            .
        \end{equation}
        We see that this fixes \(Z = Z^{\trans}\), \(Y = Y^{\trans}\), and \(W = -X^{\trans}\).
        Requiring that \(Z\) be symmetric means that the elements below the diagonal are fixed, removing \(n(n-1)/2\) free parameters.
        Similarly, requiring that \(Y\) is symmetric removes \(n(n-1)/2\) parameters.
        Since \(X\) determines \(W\) we also fix another \(n^2\) parameters.
        Thus, the number of remaining free parameters is
        \begin{equation}
            4n^2 - \left( \frac{1}{2}n(n - 1) + \frac{1}{2}n(n - 1) + n^2 \right) = 2n^2 + n.
        \end{equation}
    \end{itemize}
    
    \subsubsection{\texorpdfstring{\(\specialLinearLie(2, \complex)\)}{sl(2, C)}}
    As we've said a couple of times already the most important Lie algebra is \(\specialLinearLie(2, \complex)\), which is a Lie algebra over \(\complex\).
    A general element of this algebra is a \(2 \times 2\) matrix with complex entries such that the trace vanishes, we can write such an element as
    \begin{equation}
        \begin{bmatrix}
            a & b\\
            c & -a
        \end{bmatrix}
    \end{equation}
    for \(a, b, c \in \complex\).
    This shows that \(\dim \specialLinearLie(2, \complex) = 3\).
    
    An explicit basis for \(\specialLinearLie(2, \complex)\) is
    \begin{equation}
        \quad H = 
        \begin{bmatrix}
            1 & 0\\
            0 & -1
        \end{bmatrix}
        , \qquad
        E = 
        \begin{bmatrix}
            0 & 1\\
            0 & 0
        \end{bmatrix}
        , \qqand F = 
        \begin{bmatrix}
            0 & 0\\
            1 & 0
        \end{bmatrix}
        .
    \end{equation}
    Then
    \begin{equation}
        \begin{bmatrix}
            a & b\\
            c & -a
        \end{bmatrix}
        = aH + bE + cF.
    \end{equation}
    
    A simple calculation of commutators shows that we have the relations
    \begin{equation}
        \commutator{H}{E} = 2E, \qquad \commutator{H}{F} = -2F, \qqand \commutator{E}{F} = H.
    \end{equation}
    
    Later we will work more abstractly, rather than defining \(\specialLinearLie(2, \complex)\) to be \(2 \times 2\) complex traceless matrices we'll define \(\specialLinearLie(2, \complex)\) to be a three dimensional Lie algebra with basis \(\{H, E, F\}\) satisfying the above bracket relations.
    Note that this doesn't require the bracket to be the commutator, in fact, the commutator isn't even defined in this abstract setting since we don't have a notion of multiplication\footnote{It turns out that every Lie algebra, \(\lie{g}\), can be embedded into an associative algebra, \(\symcal{U}(\lie{g})\) in which the Lie bracket on \(\lie{g}\) coincides with the commutator in \(\symcal{U}(\lie{g})\). This larger algebra is called the \defineindex{universal enveloping algebra}. Formally, this algebra can be constructed by taking the tensor algebra, \(T(\lie{g}) = \field \oplus \lie{g} \oplus (\lie{g} \otimes \lie{g}) \oplus (\lie{g} \otimes \lie{g} \otimes \lie{g}) \oplus \dotsb\), and defining the quotient \(\symcal{U}(\lie{g}) = T(\lie{g})/I\) where \(I\) is the two sided ideal generated by elements of the form \(a \otimes b - b \otimes a - \commutator{a}{b} \in \lie{g} \oplus (\lie{g} \otimes \lie{g}) \subseteq T(\lie{g})\). A general element of this ideal is thus of the form \(c \otimes d \otimes \dotsb \otimes (a \otimes b - b \otimes a - \bracket{a}{b}) \otimes e \otimes f \otimes \dotsb\) with \(a, b, c, d, e, f \in \lie{g}\). Note that the product of two elements of \(T(\lie{g})\) is their tensor product, although we usually write \(a \otimes b = ab\).} with which to form, say, \(HE\).
    Once we do this we see that the matrices above form a two-dimensional representation (\cref{def:representation}) of \(\specialLinearLie(2, \complex)\), called the defining representation, which has an obvious action on \(\complex^2\).
    
    An important observation here is that if we define the map\footnote{This is the so called adjoint representation (\cref{def:adjoint rep}).} \(\ad_H \colon \specialLinearLie(2, \complex) \to \specialLinearLie(2, \complex)\) by \(\ad_H(X) = \commutator{H}{X}\) then we see that this map has \(E\) and \(F\) as eigenvectors, with eigenvalues \(2\) and \(-2\) respectively.
    The other eigenvector of \(\ad_H\) is \(H\) itself, since \(\ad_H(H) = \commutator{H}{H} = 0 = 0H\).
    Then we have the eigensspaces \(V_0 = \complex H = \lie{h}\), \(V_2 = \complex E\), and \(V_{-2} = \complex F\), giving the decomposition
    \begin{equation}
        \specialLinearLie(2, \complex) = \complex H \oplus \complex E \oplus \complex F = \lie{h} \oplus V_2 \oplus V_{-2} = \lie{h} \oplus \bigoplus_{\alpha \in \complex \setminus \{0\}} V_{\alpha}.
    \end{equation}
    
    The important thing here is that we get \(\lie{h}\), which is a special subalgebra called the Cartan subalgebra which we'll see a lot later, and we get the generalised eigenspaces \(V_{\alpha}\).
    We'll see later\footnote{so ignore any terminology here that is unfamiliar} that replacing the generalised eigenspaces with something called root spaces makes this construction generalise to finite dimensional semisimple (\cref{def:semisimple lie alg}) Lie algebras over \(\complex\), and further that the subalgebras appearing in this decomposition are all simple \cref{def:simple lie alg}, so a classification of simple Lie algebras extends to a classification of semisimple Lie algebras.
    This requires replacing the indexing set \(\complex \setminus \{0\}\) with the root space \(\lie{h}^* \setminus \{0\}\), or rather we've already done that, it's just that \(\dim \lie{h} = 1\) so \(\lie{h}^* \isomorphic \lie{h}\) and \(\dim \lie{h}^* = 1\) so \(\lie{h}^* \isomorphic \complex\).
    
    While this last paragraph is beyond what we're ready for yet it's important to start seeing the recurring pattern of this type of decomposition now.
    The fact that \(\specialLinearLie(2, \complex)\) is the simplest example of such a decomposition is why it's so important.
    In particular, every finite dimension semisimple Lie algebra over \(\complex\) of dimension at least 3 contains (many) copies of \(\specialLinearLie(2, \complex)\).
    
    \subsection{Other Subalgebras of \texorpdfstring{\(\generalLinearLie(n, \field)\)}{gl(n, k)}}
    There are two further subalgebras of \(\generalLinearLie(n, \field)\) which are worth mentioning.
    \begin{dfn}{Triangular Matrices}{}
        Denote by \(\borelLie(n, \field) \subseteq \generalLinearLie(n, \field)\) the subalgebra of upper triangular matrices, and by \(\nilpotentLie(n, \field) \subseteq \generalLinearLie(n, \field)\) the subalgebra of \emph{strictly} upper triangular matrices (meaning the diagonal is all zeros).
    \end{dfn}
    
    \begin{remark}{}{}
        Here \(\borelLie\) stands for \enquote{Borel} because \(\borelLie(n, \field)\) is a \defineindex{Borel subalgebra}, meaning it's a maximal solvable subalgebra (\cref{def:solvable lie alg}).
        The \(\nilpotentLie\) stands for \enquote{nilpotent} because \(\nilpotentLie(n, \field)\) is a (maximal) nilpotent algebra (\cref{def:nilpotent lie alg})
        
        These properties ultimately come from the fact that taking nested commutators of (strictly) upper triangular matrices will eventually result in zero.
        The difference in how the commutators are nested is the distinction between solvable (e.g., \(\bracket{\bracket{-}{-}}{\bracket{-}{-}}\)) and nilpotent (e.g., \(\bracket{-}{\bracket{-}{-}}\)).
    \end{remark}
    
    It turns out that \(\nilpotentLie(n, \field)\) is an ideal of \(\borelLie(n, \field)\), since the product of an upper triangular matrix and a strictly upper triangular matrix is strictly upper triangular, and so the commutator is too.
    This means we can consider the quotient \(\lie{g} = \borelLie(n, \field)/\nilpotentLie(n, \field)\).
    Two upper triangular matrices in \(\borelLie(n, \field)\) are identified in \(\lie{g}\) if their difference is a strictly upper triangular matrix in \(\nilpotentLie(n, \field)\).
    This means that they must have the same diagonal.
    Suppose then that \(X, Y \in \borelLie(n, \field)\) have the same diagonal, so \(X_{ii} = Y_{ii}\) for all \(i = 1, \dotsc, n\).
    By definition of upper triangular we also know that \(X_{ij} = Y_{ij} = 0\) if \(i > j\).
    Basic matrix multiplication tells us an element of the diagonal of \(XY\) is
    \begin{equation}
        (XY)_{ii} = \sum_{j=1}^{n} X_{ij} Y_{ji}.
    \end{equation}
    If \(i > j\) then \(X_{ij} = 0\), and if \(i < j\) then \(Y_{ji} = 0\), so the only nonzero diagonal term comes from \(X_{ii} Y_{ii}\).
    The same analysis can be applied to \(YX\) and we see that the only nonzero diagonal term is \((YX)_{ii} = Y_{ii} X_{ii}\).
    In the commutator these terms cancel out, and as such \(\commutator{X}{Y} \in \nilpotentLie(n, \field)\) for all \(X, Y \in \borelLie(n, \field)\).
    This means that in the quotient \(\lie{g}\) we identify \(\commutator{X}{Y}\) with \(\commutator{X}{Y} + \nilpotentLie(n, \field) = 0 + \nilpotentLie(n, \field)\), which is zero, and so this quotient is abelian.
    
    Note that as \(\field\)-vector spaces \(\dim \borelLie(n, \field) = n(n + 1)/2 = T_n\) and \(\dim \borelLie(n, \field) = n(n - 1)/2 = T_{n-1}\).
    Thus, the quotient \(\lie{g}\) has dimension \(T_n - T_{n-1} = n\).
    
    If \(A\) is an algebra with then another important Lie subalgebra of \(\generalLinearLie(A)\) is \(\Der A\), the algebra of derivations of \(A\).
    Recall that a derivation is a linear map \(D \colon A \to A\) such that
    \begin{equation}
        D(ab) = aD(b) + D(a)b
    \end{equation}
    for all \(a, b \in A\).
    To show that this is a subalgebra of \(\generalLinearLie(A)\) we need to show that \(\Der A\) is closed under the commutator, that is, that the commutator of two derivations is a derivation.
    This follows from the following calculation with \(D_1, D_2 \in \Der A\) and \(a, b \in A\):
    \begingroup
    \allowdisplaybreaks
    \begin{align}
        \commutator{D_1}{D_2}(ab) &= (D_1D_2 - D_2D_1)(ab)\\
        &= D_1(D_2(ab)) - D_2(D_1(ab)) \notag\\
        &= D_1(aD_2(b) + D_2(a)b) - D_2(aD_1(b) - D_1(a)b) \notag\\
        &= D_1(aD_2(b)) + D_1(D_2(a)b) - D_2(aD_1(b)) - D_2(D_1(a)b) \notag\\
        &= aD_1(D_2(b)) + D_1(a)D_2(b) + D_2(a)D_1(b) + D_1(D_2(a))b \notag\\
        &\quad- aD_2(D_1(b)) - D_2(a)D_1(b) - D_1(a)D_2(b) - D_2(D_1(a))b \notag\\
        &= aD_1(D_2(b)) + D_1(D_2(a))b - aD_2(D_1(b)) - D_2(D_1(a))b \notag\\
        &= a\{D_1(D_2(b)) - D_2(D_1(b))\} + \{D_1(D_2(a)) - D_2(D_1(a))\}b \notag\\
        &= a(D_1D_2 - D_2D_1)(b) + (D_1D_2 - D_2D_1)(a)b \notag\\
        &= a\commutator{D_1}{D_2}(b) + \commutator{D_1}{D_2}(a)b. \notag
    \end{align}
    \endgroup
    This shows that \(\commutator{D_1}{D_2}\) is a derivation, so \(\Der A\) is closed under the commutator.
    
    \chapter{Representation Theory}
    \section{Definition}
    \begin{dfn}{Representation}{def:representation}
        Let \(\lie{g}\) be a Lie algebra over a field, \(\field\).
        A \defineindex{representation} of \(\lie{g}\) is a pair, \((V, \rho)\), where \(V\) is a vector space over \(\field\) and \(\rho \colon \lie{g} \to \generalLinearLie(V)\) is a Lie algebra homomorphism.
    \end{dfn}
    
    The idea behind this definition is that \(\lie{g}\) is acting on \(V\) by specifying an endomorphism \(\varphi_X = \rho(X) \in \End V = \generalLinearLie(V)\) for each \(X \in \lie{g}\).
    Then \(X\) acts on \(v \in V\) by \(x \mapsto \varphi_X(V)\).
    
    This should be compared to the idea of a group action, which can similarly be specified by a map \(\rho \colon G \to \Perm(X)\) where \(\Perm(X)\) is the group of permutations of elements of \(X\).
    If you're familiar with group representations then the idea here is the same as a group representation \((V, \rho)\) with \(\rho \colon G \to \generalLinear(V)\) a group homomorphism.
    
    One of the most important examples of a representation is when the Lie algebra acts on itself in the obvious way, through the bracket.
    
    \begin{dfn}{Adjoint Representation}{def:adjoint rep}
        Let \(\lie{g}\) be a Lie algebra.
        There is a Lie algebra representation called the \defineindex{adjoint representation}, \((\lie{g}, \ad)\) where \(\ad \colon \lie{g} \to \generalLinearLie(\lie{g})\) is defined by \(\ad(X) = \ad_X\) where \(\ad_X \in \generalLinearLie(\lie{g})\) is the linear map \(\ad_X \colon \lie{g} \to \lie{g}\) defined by \(\ad_X(Y) = \bracket{X}{Y}\).
    \end{dfn}
    
    This is related to the adjoint action of a the corresponding Lie group on itself, which is by conjugation.
    
    \begin{dfn}{Modules}{}
        Let \(\lie{g}\) be a Lie algebra over a field, \(\field\).
        A (left) \define{\(\lie{g}\)-module}\index{g-module@\(\lie{g}\)-module}\index{module}, \(V\), is a vector space over \(\field\) equipped with a bilinear map \({-} \cdot {-} \colon \lie{g} \times V\) such that for all \(X, Y \in \lie{g}\) and \(v \in V\) we have
        \begin{equation}
            \bracket{X}{Y} \cdot v = X \cdot (Y \cdot v) - Y \cdot (X \cdot v).
        \end{equation}
    \end{dfn}
    
    If you're familiar with the idea of an \(R\)-module for \(R\) a ring then the idea is the same here.
    An \(R\)-module is an abelian group equipped with an \(R\)-action, and a \(\lie{g}\)-module is a vector space equipped with a \(\lie{g}\)-action.
    
    \begin{prp}{}{}
        A Lie algebra representation of \(\lie{g}\) carries the same information as a \(\lie{g}\)-module.
        \begin{proof}
            Let \((V, \rho)\) be a representation of \(\lie{g}\).
            Then we may define a \(\lie{g}\)-action on \(V\) by \(X \cdot v \coloneqq \rho(X)(v)\).
            This is linear in the first argument, \(X\), because \(\rho\) is linear, and linear in the second argument, \(v\), because \(\rho(X) \in \generalLinearLie(V)\) is linear.
            Thus, this is a bilinear map.
            We further have
            \begin{align}
                \bracket{X}{Y} \cdot v &= \rho(\bracket{X}{Y})(v)\\
                &= \bracket{\rho(X)}{\rho(Y)}(v)\\
                &= \rho(X)(\rho(Y)(v)) - \rho(Y)(\rho(X)(v))\\
                &= X \cdot (Y \cdot v) - Y \cdot (X \cdot v)
            \end{align}
            where we've used the fact that \(\rho\) is a homomorphism and in \(\generalLinearLie(V)\) the Lie bracket is just the commutator.
            
            Conversely, if \(V\) is a \(\lie{g}\)-module then we may define a homomorphism \(\rho \colon \lie{g} \to \generalLinearLie(V)\) by \(X \mapsto X \cdot {-}\) which acts by \(v \mapsto X \cdot v\).
            This is a linear map \(V \to V\) because the \(\lie{g}\)-action is bilinear in the second argument, and it's a Lie algebra homomorphism because by definition the Lie bracket acts as a commutator, which is the Lie bracket in \(\generalLinearLie(V)\).
            
            Thus, every representation of \(\lie{g}\) uniquely specifies a \(\lie{g}\)-module and vice versa.
        \end{proof} 
    \end{prp}
    
    The \enquote{modern} treatment of representations is mostly via the notion of modules, and we shall prefer this method.
    
    \subsection{Morphism of Representations}
    \begin{dfn}{}{}
        Let \(V_1\) and \(V_2\) be \(\lie{g}\)-modules for some Lie algebra \(\lie{g}\).
        A \define{morphism}\index{morphism!of g-modules@morphism!of \(\lie{g}\)-modules} of \(\lie{g}\)-modules is then a linear map \(\varphi \colon V_1 \to V_2\) such that for all \(X \in \lie{g}\) and \(v \in V_1\) we have
        \begin{equation}
            \varphi(X \cdot v) = X \cdot v.
        \end{equation}
        We also call \(\varphi\) an \defineindex{equivariant map}.
        
        An invertible \(\lie{g}\)-module homomorphism is a \(\lie{g}\)-module isomorhpism.
    \end{dfn}
    
    Writing \(X \cdot {-}\) for the map \(v \mapsto X \cdot v\) the definition above is equivalent to the commutativity of the following square for all \(X \in \lie{g}\):
    \begin{equation}
        \tikzexternaldisable
        \begin{tikzcd}
            V_1 \arrow[r, "\varphi"] \arrow[d, "X \cdot {-}"'] & V_2 \arrow[d, "X \cdot {-}"]\\
            V_1 \arrow[r, "\varphi"'] & V_2\mathrlap{.}
        \end{tikzcd}
        \tikzexternalenable
    \end{equation}
    Note that \(X \cdot {-}\) on the left is the action of \(X\) on \(V_1\), whereas on the right it's the action of \(X\) on \(V_2\).
    We get away with the same notation by looking at the domain of the action to specify which we mean.
    
    Now that we have a notion of morphisms between representations we can form a category.
    Specifically, \(\Rep\) is the category whose objects are \(\lie{g}\)-modules (or equivalently representations of \(\lie{g}\)) and whose morhpisms are morphisms of representations.
    Note that formally the category with representations and the category with \(\lie{g}\)-modules are different, but they are isomorphic, which is why we don't need to distinguish between them.
    The isomorphism is given by the functor sending \((V, \rho)\) to the \(\lie{g}\)-module \(V\) with the action given by \(X \cdot v = \rho(X)(v)\).
    
    \begin{lma}{}{}
        If \(\varphi \colon V_1 \to V_2\) is an isomorphism of \(\lie{g}\)-modules then \(\varphi^{-1} \colon V_1 \to V_2\) is also.
        \begin{proof}
            We need to show that for all \(X \in \lie{g}\) and \(v \in V_2\) we have
            \begin{equation}
                \varphi^{-1}(X \cdot v) = X \cdot \varphi^{-1}(v).
            \end{equation}
            To do so note that \(\varphi^{-1}\) is an invertible linear map, and as such is a bijection.
            This means that there exists unique \(v' \in V_1\) such that \(\varphi(v') = v\) and \(v' = \varphi^{-1}(v)\).
            Thus, we may write the left hand side as
            \begin{equation}
                \varphi^{-1}(X \cdot v) = \varphi^{-1}(X \cdot \varphi(v')).
            \end{equation}
            Using the fact that \(\varphi\) is a morphism of \(\lie{g}\)-modules we have
            \begin{equation}
                \varphi^{-1}(X \cdot v) = \varphi^{-1}(\varphi(X \cdot v')) = X \cdot v' = X \cdot \varphi^{-1}(v). \qedhere
            \end{equation}
        \end{proof}
    \end{lma}
    
    \begin{ntn}{}{}
        If \(M\) and \(N\) are \(\lie{g}\)-modules over \(\field\) we write
        \begin{align}
            \Hom_{\field}(M, N) &= \Hom(M, N) = \{T \colon M \to N \mid T \text{ is linear}\}\\
            &= \Vect(M, N)
        \end{align}
        and
        \begin{align}
            \Hom_{\lie{g}}(M, N) &= \{T \colon M \to N \mid T \text{ is a } \lie{g} \text{-module homomorphism}\}\\
            &= \Rep(M, N).
        \end{align}
        Similarly, we write \(\End_{\field}(M) = \End(M) = \Hom(M, M)\) and \(\End_{\lie{g}}(M) = \Hom_{\lie{g}}(M, M)\).
    \end{ntn}
    
    \section{Simple Modules}
    \begin{dfn}{Submodule}{}
        Let \(\lie{g}\) be a Lie algebra over \(\field\) and let \(V\) be a \(\lie{g}\)-module.
        A subspace, \(W \subseteq V\), is a \defineindex{submodule} if it is a \(\lie{g}\)-module under the restricted action of \(\lie{g}\).
        Equivalently, \(W\) is a submodule if \(X \cdot w \in W\) for all \(X \in \lie{g}\) and for all \(w \in W\).
        
        A \defineindex{proper submodule} of \(V\) is a submodule that is neither \(0\) nor \(V\).
    \end{dfn}
    
    \begin{lma}{}{}
        Let \(\varphi \colon M \to N\) be a \(\lie{g}\)-module homomorphism.
        Then \(\ker \varphi\) is a submodule of \(M\) and \(\im \varphi\) is a submodule of \(N\).
        \begin{proof}
            We already know that \(\ker \varphi\) and \(\im \varphi\) are subspaces of \(M\) and \(N\) respectively, so it remains only to show that they are submodules, that they are closed under the action of \(\lie{g}\).
            Let \(v \in \ker \varphi\), then \(\varphi(v) = 0\).
            From this we know that
            \begin{equation}
                \varphi(X \cdot v) = X \cdot \varphi(v) = X \cdot 0 = 0
            \end{equation}
            and so \(X \cdot v \in \ker \varphi\).
            Now, let \(w \in \im \varphi\), then there exists some \(w' \in M\) with \(\varphi(w') = w\).
            We then have
            \begin{equation}
                X \cdot w = X \cdot \varphi(w') = \varphi(X \cdot w')
            \end{equation}
            which means \(X \cdot w \in \im \varphi\).
        \end{proof}
    \end{lma}
    
    \begin{dfn}{Simple Module}{}
        A \define{simple \(\lie{g}\)-module}\index{simple module} is a \(\lie{g}\)-module with no proper submodules.
        The corresponding representation is called an \defineindex{irreducible representation}, often shortened to \defineindex{irrep}.
    \end{dfn}
    
    The simple \(\lie{g}\)-modules turn out to be the most important.
    Often, but not always (but always over \(\complex\)) it is possible to write any module, \(V\), as a direct sum of simple \(\lie{g}\)-modules.
    Because of this one of the first goals when studying representations of a given Lie algebra is to classify all simple \(\lie{g}\)-modules.
    This is often possible, but not always easy.
    
    \begin{lma}{Schur's Lemma}{lma:schur}
        Let \(V\) be a finite-dimensional \(\lie{g}\)-module over an algebraically closed field, \(\field\).
        Then eery \(\lie{g}\)-module endomorphism of \(V\) is a multiple of the identity and \(\End_{\lie{g}}(V) \isomorphic \field\).
        \begin{proof}
            Let \(\varphi \colon V \to V\) be a \(\lie{g}\)-module endomorphism.
            Then \(\varphi\) is a linear map.
            Since \(\field\) is assumed to be algebraically closed we know that this linear map has an eigenvalue, call it \(\lambda\) and let \(v \in V\) be the associated eigenvector.
            We know that \(v \in \ker \varphi'\) by construction.
            The kernel is a submodule, and since it's not zero (it contains \(v\)) and \(V\) is simple the kernel must be all of \(V\).
            Thus, \(\varphi'(w) = 0\) for all \(w \in V\), and so \(\varphi - \lambda \id\) is the zero map, and so \(\varphi = \lambda \id\) is just a scalar multiple of the identity.
        \end{proof}
    \end{lma}
    
    Note that from this we can immediately conclude that the only \(\lie{g}\)-module endomorphisms are invertible (if \(\lambda \ne 0\)) or the zero map (\(\lambda = 0\)).
    
    \begin{lma}{}{}
        If \(V\) and \(W\) are non-isomorphic simple \(\lie{g}\)-modules then \(\Hom_{\lie{g}}(V, W) = 0\).
        \begin{proof}
            Suppose that \(\varphi \in \Hom_{\lie{g}}(V, W)\).
            That is, \(\varphi \colon V \to W\) is a \(\lie{g}\)-module homomorphism.
            Then we may consider the submodule \(\ker \varphi \subseteq V\).
            Supposing that \(\varphi \ne 0\) there must be some \(v \in V\) such that \(\varphi(v) \ne 0\), thus \(\ker \varphi\) is not the whole of \(V\), and as a submodule of the simple module \(V\) it must be that \(\ker \varphi = 0\), so \(\varphi\) is injective.
            Now consider the submodule \(\im \varphi \subseteq W\).
            We know that \(\im \varphi \ne 0\) since \(0 \ne \varphi(v) \in \im \varphi\).
            Thus, since \(W\) is simple, \(\im \varphi\) must be all of \(W\), so \(\varphi\) is surjective.
            This leaves us with two cases, either \(\varphi\) is a injective and surjective, and hence an isomorphism, or \(\varphi\) is the zero map.
            By assumption there are no isomorphisms \(V \to W\), and further \(\varphi \ne 0\).
            Thus, there are no non-trivial morphisms \(V \to W\) as claimed.
        \end{proof}
    \end{lma}
    
    \subsection{New Modules From Old}
    \begin{lma}{}{}
        Let \(\lie{m}\) and \(\lie{l}\) be Lie algebras, and let \(V\) and \(W\) be simple \(\lie{m}\)- and \(\lie{l}\)-modules, respectively.
        Then \(V \otimes W\) is made into an \((\lie{m} \oplus \lie{l})\)-module by extending the action
        \begin{equation}
            (X, Y) \cdot (v \otimes w) = (X \cdot v) \otimes w + v \otimes (Y \cdot w)
        \end{equation}
        by linearity to non-simple tensors.
        Note that \(X \in \lie{m}\), \(Y \in \lie{l}\), \(v \in V\) and \(w \in W\).
        \begin{proof}
            We first show that this action is bilinear.
            Linearity in the first argument follows from the calculation
            \begin{align}
                (\lambda(X, Y)) \cdot (v \otimes w) &= (\lambda X, \lambda Y) \cdot (v \otimes w)\\
                &= (\lambda X \cdot w) \otimes v + w \otimes (\lambda Y \cdot w)\\
                &= \lambda ((X \cdot w) \otimes v) + \lambda (w \otimes (Y \cdot w))\\
                &= \lambda ((X \cdot w) \otimes v + w \otimes (Y \cdot w))\\
                &= \lambda ((X, Y) \cdot (v \otimes w))
            \end{align}
            and
            \begin{align}
                &((X, Y) + (X', Y')) \cdot (v \otimes w) = (X + X', Y + X') \cdot (v \otimes w)\\
                &= ((X + X') \cdot v) \otimes w + v \otimes ((Y + Y') \cdot w)\\
                &= (X \cdot v + X' \cdot v) \otimes w + v \otimes (Y \cdot w + Y' \cdot w)\\
                &= (X \cdot v) \otimes w + (X' \cdot v) \otimes w + v \otimes (Y \cdot w) + v \otimes (Y' \cdot w)\\
                &= (X \cdot v) \otimes w + v \otimes (Y \cdot w) + (X' \cdot v) \otimes w + v \otimes (Y' \cdot w)\\
                &= (X, Y) \cdot (v \otimes w) + (X', Y') \cdot (v \otimes w).
            \end{align}
            Linearity in the second argument may be demonstrated by a similar argument.
            We can also demonstrate that brackets act as they must:
            \begin{align}
                &\bracket{(X, Y)}{(X', Y')} \cdot (v \otimes w) = (\bracket{X}{X'}, \bracket{Y}{Y'}) \cdot (v \otimes w)\\
                &= (\bracket{X}{X'} \cdot v) \otimes w + v \otimes (\bracket{Y}{Y'} \cdot w)\\
                &= (X \cdot (X' \cdot v) - X' \cdot (X \cdot v)) \otimes w + v \otimes (Y \cdot (Y' \cdot w) + Y' \cdot (Y \cdot w)) \notag\\
                &= (X \cdot (X' \cdot v)) \otimes w - (X' \cdot (X \cdot v)) \otimes w\\
                &\qquad\qquad+ v \otimes (Y \cdot (Y' \cdot w)) - v \otimes (Y' \cdot (Y \cdot w)) \notag\\
                &= (X \cdot (X' \cdot v)) \otimes w + v \otimes (Y \cdot (Y' \cdot w))\\
                &\qquad\qquad- (X' \cdot (X \cdot v)) \otimes w - v \otimes (Y' \cdot (Y \cdot w)) \notag\\
                &= (X, Y) \cdot ((X', Y') \cdot (v \otimes w)) - (X', Y') \cdot ((X, Y) \cdot (v \otimes w)).\notag
            \end{align}
            Thus, this is indeed an \((\lie{m} \oplus \lie{l})\)-module.
        \end{proof}
    \end{lma}
    
    The goal of the next couple of lemmas is to prove \cref{prp:tensor product of simple modules is simple}, that the tensor product of simple modules is again simple.
    
    \begin{lma}{}{lma:smallest submodule of V+V}
        Let \(V\) be a simple \(\lie{m}\)-module and \(v_1, v_2 \in V\) such that \(v_1\) is not proportional to \(v_2\).
        Then the smallest submodule of \(V \oplus V\) containing \((v_1, v_2)\) is \(V \oplus V\).
        \begin{proof}
            Let \(U \subseteq V \oplus V\) be the minimal submodule (with respect to inclusion) containing \((v_1, v_2)\).
            Consider the inclusion map \(i \colon U \hookrightarrow V \oplus V\) and the projection maps \(p_1, p_2 \colon V \oplus V \to V\).
            These are all module homomorphisms, and so the composites \(p_1 \circ i, p_2 \circ i \colon U \to V\) are also homomorphisms.
            Since \(V\) is simple and \(U\) is nonzero and \(p_1 \circ i\) is a nonzero map this map must be surjective.
            The kernel of \(p_1 \circ i\) is contained in the kernel of \(p_1\), which is \(0 \oplus V\).
            Clearly \(\{0\} \oplus \isomorphic V\) is simple, and as such the kernel of \(p_1 \circ i\) is either \(0\), in which case \(U \isomorphic V\), or it's \(0 \oplus V\), in which case \(U = V \oplus V\).
            Suppose that the kernel of \(p_1 \circ i\) is zero, then \(p_1 \circ i \colon U \to V\) is an isomorphism, and we can denote its inverse by \(\varphi \colon V \to U\).
            The map \(p_1 \circ i \colon U \to V\) will also be an isomorphism in this case, and thus we can consider the composite \(p_2 \circ i \circ \varphi\), which is an isomorphism \(V \to V\).
            Since \(V\) is simple we know by Schur's lemma (\cref{lma:schur}) that \(p_2 \circ i \circ \varphi\) is a multiple of the identity, so \(p_2 \circ i \circ \varphi(v_1) = \lambda v_1\) for some \(\lambda \in \field\).
            However, we have \(p_2 \circ i \circ \varphi(v_1) = p_2 \circ i(v_1, v_2) = p_2(v_1, v_2) = v_2\), contradicting the assumption that \(v_2\) is not proportional to \(v_1\).
            Thus, we are left with the case where \(U = V \oplus V\).
        \end{proof}
    \end{lma}
    
    The following lemma is then a special case of the result of \cref{prp:tensor product of simple modules is simple}.
    
    \begin{lma}{}{lma:tensor product of simple modules is simple if it contains a pure tensor}
        Let \(V\) be a simple \(\lie{m}\)-module and \(W\) a simple \(\lie{l}\)-module.
        The smallest \((\lie{m} \oplus \lie{l})\)-submodule of \(V \otimes W\) containing a pure tensor \(v \otimes w \ne 0\) is \(V \otimes W\).
        \begin{proof}
            The module \(V \otimes W\) is spanned by vectors of the form \(v' \otimes w'\) for \(v' \in V\) and \(w' \in W\).
            Therefore, it suffices to show that \(v' \otimes w'\) is contained in the smallest submodule, \(U\), of \(V \otimes W\) containing \(v \otimes w\), and then necessarily \(U\) will contain all elements of \(V \otimes W\).
            Linearity of the \(\lie{g}\)-action tells us that \(0 \cdot v = 0\) for all \(v \in V\), and thus we have
            \begin{equation}
                (0, Y) \cdot (v \otimes w) = (0 \cdot v) \otimes w + v \otimes (Y \cdot w) = 0 \otimes w + v \otimes (Y \cdot w) = v \otimes (Y \cdot w).
            \end{equation}
            This is true for all \(Y \in \lie{l} \subseteq \lie{m} \oplus \lie{l}\).
            Now, we can generate a submodule in this way starting with \(v \otimes w\) and acting with \(Y \in \lie{l}\) as above.
            Doing so we generate a submodule of \(W\) which is not zero, and thus must be isomorphic to all of \(W\).
            This submodule is thus \(\field v \otimes W\) where \(\field v = \Span\{v\}\).
            Thus, \(v \otimes w' \in U\) for any \(w' \in W\).
            Similarly, one can show \(v' \otimes w \in U\) for any \(v' \in V\).
            Thus, \(v' \otimes w' \in U\).
        \end{proof}
    \end{lma}
    
    We are now ready to prove our result.
    
    \begin{prp}{}{prp:tensor product of simple modules is simple}
        Let \(V\) be a simple \(\lie{m}\)-module and \(W\) a simple \(\lie{l}\)-module.
        Then \(V \otimes W\) is a simple \((\lie{m} \oplus \lie{l})\)-module.
        \begin{proof}
            Let \(u \in V \otimes W\) be such that \(u \ne 0\).
            Let \(U\) be the smallest \((\lie{m} \oplus \lie{l})\)-submodule of \(V \otimes W\) containing \(u\).
            We want to show that \(U = V \otimes W\).
            If we know that \(0 \ne v \otimes w \in U\) then the result follows from \cref{lma:tensor product of simple modules is simple if it contains a pure tensor}.
            As an element of \(V \otimes W\) we can express \(u\) as a sum of simple tensors,
            \begin{equation}
                u = \sum_{i=1}^k v_i \otimes w_i.
            \end{equation}
            Note that we don't need scalar factors since we can just absorb them into the definition of the \(v_i\).
            We may further assume that the \(v_i\) are linearly independent, and that no pair of \(w_i\)s is proportional, since if \(w_i = \alpha w_j\) then \(v_i \otimes w_i + v_j \otimes w_j = (v_i + \alpha v_j) \otimes w_i\).
            It is also possible to find some element of \(U\) in place of \(u\) such that \(k\) is minimal in making the above sum true.
            If \(k = 1\) then \(u\) is a simple tensor and we're done.
            Suppose then that \(k > 1\).
            Since the \(v_i\) are linearly independent we can define an injective \(\lie{l}\)-module homomorphism \(\psi \colon W \oplus \dotsb \oplus W \to V \otimes W\) by
            \begin{equation}
                (w_1', \dotsc, w_k') \mapsto \sum_{i=1}^k v_i \otimes w_i'.
            \end{equation}
            Then \(u\) is the image of some \(\vv{w} = (w_1, \dotsc, w_k)\).
            If \(\vv{w}'\) is some element of the smallest \(\lie{l}\)-submodule \(W'\) of \(W \oplus \dotsb \oplus W\) containing \(\vv{w}\) then \(\psi(\vv{w}') \in U\).
            Consider the projection of \(W'\) onto the first two terms, \(W \oplus W \oplus 0 \oplus \dotsb\).
            This is an \(\lie{l}\)-submodule of \(W \oplus W\) and contains \((w_1, w_2)\).
            We have assumed that \(w_1\) and \(w_2\) are not proportional, and thus by \cref{lma:smallest submodule of V+V} this projection must be the whole of \(W \oplus W\).
            In particular, \((w_1, 0)\) is in the image of the projection.
            Let \(\vv{w}' \in W'\) be some element projecting to \((w_1, 0)\).
            Then \(\vv{w}' = (w_1, 0, w_2', w_3', \dotsc, w_k')\).
            This then gives us an element in \(U\) that can be formed by summing \(k - 1\) vectors of the form \(v_i \otimes w_i'\), which contradicts the minimality assumption of \(k\), and thus we must have \(k = 1\) and we are done.
        \end{proof}
    \end{prp}
    
    \begin{lma}{}{}
        Let \(V\) and \(W\) be finite-dimensional \(\lie{g}\)-modules and let \(\Hom_{\field}(V, W)\) be the space of linear maps \(V \to W\).
        Equipping this space with the \(\lie{g}\)-action
        \begin{equation}
            (X \cdot f)(v) = X \cdot f(v) - f(X \cdot v) \qquad X \in \lie{g}, v \in V, \text{ and } f \in \Hom_{\field}(V, W)
        \end{equation}
        defines a \(\lie{g}\)-module.
        \begin{proof}
            Linearity is immediate because \(f\) is linear so we may pull out any scalar factors and pull apart any sums.
            We need only show that the bracket acts as required:
            \begin{align}
                &(\bracket{X}{Y} \cdot f)(v) = \bracket{X}{Y} \cdot f(v) - f(\bracket{X}{Y} \cdot f)\\
                &= X \cdot (Y \cdot f(v)) - Y \cdot (X \cdot f(v)) - f(X \cdot (Y \cdot v) - Y \cdot (X \cdot v)) \notag\\
                &= X \cdot (Y \cdot f(v)) - Y \cdot (X \cdot f(v)) - f(X \cdot (Y \cdot v)) + f(Y \cdot (X \cdot v)) \notag\\
                &= X \cdot (Y \cdot f(v)) - f(X \cdot (Y \cdot v)) - (Y \cdot (X \cdot f(v)) - f(Y \cdot (X \cdot v))) \notag\\
                &= (X \cdot (Y \cdot f))(v) - (Y \cdot (X \cdot f))(v).
            \end{align}
            This holds for all \(v \in V\), and thus
            \begin{equation}
                \bracket{X}{Y} \cdot f = X \cdot (Y \cdot f) - Y \cdot (X \cdot f)
            \end{equation}
            as required.
        \end{proof}
    \end{lma}
    
    Since the sum of two \(\lie{g}\)-module homomorphisms is again a \(\lie{g}\)-module homomorphism this shows that \(\Hom_{\lie{g}}(V, W)\) is a \(\lie{g}\)-submodule of \(\Hom_{\field}(V, W)\), and hence \(\Hom_{\lie{g}}(V, W)\) is a \(\lie{g}\)-module in its own right.
    A fancy way of putting this is that \(\Rep\) is enriched over itself.
    
    \section{Representation Theory of \texorpdfstring{\(\specialLinearLie(2, \complex)\)}{sl(2, C)}}
    Recall that the complex Lie algebra \(\specialLinearLie(2, \complex)\) can be defined as the span of \(\{E, F, H\}\) with the bracket defined by
    \begin{equation}
        \bracket{H}{E} = 2E, \quad \bracket{H}{F} = -2F, \qand \bracket{E}{F} = H.
    \end{equation}
    These relations make \(H\) a \defineindex{semisimple element}, meaning that the linear map \(X \mapsto \bracket{H}{X}\) is diagonalisable.
    We can see this is the case because the bracket's definition above shows that \(E\) and \(F\) are eigenvectors of the map \(\bracket{H}{-}\), with eigenvalues \(+2\) and \(-2\) respectively, and alternativity tells us that \(\bracket{H}{H} = 0\), so \(H\) is an eigenvector of \(\bracket{H}{-}\) with eigenvalue \(0\).
    
    Let \(V\) be a finite-dimensional \(\specialLinearLie(2, \complex)\)-module.
    Then we can decompose \(V\) into generalised eigenspaces of \(H\):
    \begin{equation}
        V = \bigoplus_{\alpha \in \complex} V_\alpha
    \end{equation}
    where
    \begin{equation}
        V_\alpha = \{v \in V \mid (H - \alpha)^N \cdot v = 0 \text{ for some } N \in \naturals\}.
    \end{equation}
    We call the \(V_\alpha\) the \define{\(\symbf{\alpha}\)-weight spaces}\index{weight space} for \(H\).
    The \define{weight vectors}\index{weight vector} of \(V\) are those \(v \in V\) such that \(v \in V_\alpha\) for some \(\alpha \in \complex\).
    We call \(\alpha\) the \defineindex{weight} of such a \(v\).
    Note that not all vectors are weight vectors, any \(v \in V\) can be written as \(\sum_{\alpha \in \complex} v_\alpha\) with \(v_\alpha \in V_\alpha\), and so \(v\) is a weight vector only if \(v_\alpha = 0\) for all but one value of \(\alpha\).
    
    \begin{lma}{}{lma:e vecs of H are still e vecs after E and F act}
        If \(v\) is an eigenvector of \(H\) with eigenvalue \(\alpha\) then \(E \cdot v\) (\(F \cdot v\)) is an eigenvector of \(H\) with eigenvalue \(\alpha + 2\) (\(\alpha - 2\)).
        \begin{proof}
            We know that \(H \cdot v = \alpha v\).
            Using the action of a Lie bracket we have
            \begin{align}
                \bracket{H}{E} \cdot v &= H \cdot (E \cdot v) - E \cdot (H \cdot v)\\
                &= H \cdot (E \cdot v) - E \cdot (\alpha v)\\
                &= H \cdot (E \cdot v) - \alpha E\cdot v.
            \end{align}
            Using the defining relations of \(\specialLinearLie(2, \complex)\) we have
            \begin{equation}
                \bracket{H}{E} = 2E \implies 2E \cdot v = H \cdot (E \cdot v) - \alpha E \cdot v.
            \end{equation}
            Rearranging this we have
            \begin{equation}
                H \cdot (E \cdot v) = 2E \cdot v + \alpha E \cdot v = (\alpha + 2)(E \cdot v),
            \end{equation}
            and thus \(E \cdot v\) is an eigenvector of \(H\) with eigenvalue \(\alpha + 2\).
            The proof for \(F \cdot v\) is analogous.
        \end{proof}
    \end{lma}
    
    This extends to the following result about generalised eigenspaces.
    
    \begin{lma}{}{}
        If \(v \in V_\alpha\) is a weight vector of weight \(\alpha\) then \(E \cdot v\) (\(F \cdot v\)) is a weight vector of weight \(\alpha + 2\) (\(\alpha - 2\)).
%        \begin{proof}
            % TODO
%        \end{proof}
    \end{lma}
    
    If \(V_\alpha \ne 0\) but \(V_{\alpha + 2} = 0\) (\(V_{\alpha - 2} = 0\)) then this implies that \(E \cdot v = 0\) (\(F \cdot v\)) for all \(v \in V_\alpha\).
    We call \(v \in V\) a \defineindex{highest weight} (\defineindex{lowest weight}) if \(E \cdot v = 0\) (\(F \cdot v = 0\)) and \(H \cdot v = \alpha v\).
    
    \begin{lma}{}{}
        Let \(V\) be a finite-dimensional \(\specialLinearLie(2, \complex)\)-module.
        Then \(V\) contains a highest weight vector, \(v_0\).
        Setting \(v_{-1} = 0\) and \(v_i = \frac{1}{i!} F^i \cdot v_0\) for \(i \ge 0\) if \(v_0\) has weight \(\alpha\) then
        \begin{equation}
            H \cdot v_i = (\alpha - 2i)v_i, \quad E \cdot v_i  (\alpha - i + 1) v_{i-1}, \qand F \cdot v_i = (i + 1) v_{i+1}.
        \end{equation}
        \begin{proof}
            \Step{\(H \cdot v_i\)}
            First note that \(H \cdot v_0 = \alpha v_0\) so the \(i = 0\) case holds.
            We proceed by induction.
            For the base case, \(i = 1\), we have \(v_1 = F \cdot v_0\).
            We then have \(H \cdot (F \cdot v_0) = (\alpha - 2)(F \cdot v_0)\) by \cref{lma:e vecs of H are still e vecs after E and F act}, and so \(H \cdot v_1 = (\alpha - 2 \cdot 1)v_1\).
            Now suppose that the result holds for some \(k \in \naturals\), that is, we have \(H \cdot v_k = (\alpha - 2k) v_k\).
            Consider \(H \cdot v_{k+1}\):
            \begin{align}
                H \cdot v_{k+1} &= H \cdot \left( \frac{1}{k + 1}F \cdot v_k \right)\\
                &= \frac{1}{k + 1}H \cdot (F \cdot v_k)\\
                &= \frac{1}{k + 1} (\bracket{H}{F} \cdot v_k + F \cdot (H \cdot v_k))\\
                &= \frac{1}{k + 1} (-2F \cdot v_k + F \cdot (\alpha - 2k)v_k)\\
                &= \frac{1}{k + 1} (-2 + (\alpha - 2k))(F \cdot v_k)\\
                &= (\alpha - 2(k + 1)) v_{k + 1}.
            \end{align}
            Thus, by induction, the result holds for all \(i \in \naturals\).
            
            \Step{\(E \cdot v_i\)}
            First note that \(E \cdot v_0 = 0 = (\alpha - 0 + 1)v_{0-1}\) as \(v_{-1} = 0\), so the \(i = 0\) case holds.
            We again proceed by induction.
            for the base case, \(i = 1\), we have \(v_1 = F \cdot v_0\).
            We then have
            \begin{align}
                E \cdot v_1 &= E \cdot (F \cdot v_0)\\
                &= \underbrace{\bracket{E}{F}}_{=H} \cdot v_0 + F \cdot \underbrace{(E \cdot v_0)}_{=0}\\
                &= H \cdot v_0\\
                &= \alpha v_0\\
                &= (\alpha - 1 + 1)v_{1 - 1}.
            \end{align}
            Now suppose that the result holds for some \(k \in \naturals\), that is, we have \(E \cdot v_k = (\alpha - k + 1)v_{k - 1}\).
            Consider \(E \cdot v_{k+1}\):
            \begin{align}
                E \cdot v_{k+1} &= \frac{1}{k + 1}E \cdot (F \cdot v_k)\\
                &= \frac{1}{k + 1}(\bracket{E}{F} \cdot v_k + F \cdot (E \cdot v_k))\\
                &= \frac{1}{k + 1}(H \cdot v_k + (\alpha - k + 1)F \cdot v_{k-1})\\
                &= \frac{1}{k + 1}((\alpha - 2k)v_k + (\alpha - k + 1)(k + 1)v_k)\\
                &= \text{\textcolor{red}{Something isn't working here}}\\ % TODO: finish this proof
                &= (\alpha - k)v_k\\
                &= (\alpha - (k + 1) + 1)v_k
            \end{align}
            
            \Step{\(F \cdot v_i = (i + 1)v_{i+1}\)}
            This is evident from the definition of \(v_i\).
        \end{proof}
    \end{lma}
    
    Since we have \(H \cdot v_i = (\alpha - 2i)v_i\) the vectors \(v_i\) are all in different eigenspaces of \(H\), and thus are linearly independent.
    Since, by assumption, \(V\) is finite-dimensional there must therefore be some \(N \in \naturals\) such that \(v_N \ne 0\) but \(v_{N + 1} = 0\).
    Considering the action of \(E\) according to the above result we have \(0 = E \cdot 0 = E \cdot v_{N+1} = (\alpha - N)v_N\) and since \(v_N \ne 0\) we must therefore have \(\alpha - N = 0\), so \(\alpha = N\), and thus the only possible eigenvalues of \(H\) are non-negative integers.
    
    Further assume that \(V\) is a simple module, and that \(V \ne 0\).
    Then by simplicity the submodule \(\Span\{v_0, \dotsc, v_N\}\) must be all of \(V\), that is, \(\{v_0, \dotsc, v_N\}\) provides a basis for \(V\), and \(\dim V = N + 1\).
    Further, the weight spaces are \(V_{N-2i}\) for \(i = 0, \dotsc, N\), and each is one-dimensional, being spanned by \(v_i\), that is \(V_{N - 2i} = \complex v_i\).
    
    Combining these results we have proven the following.
    
    \begin{thm}{Classification of Finite Dimensional Simple \(\specialLinearLie(2, \complex)\)-Modules}{}
        Let \(V\) be a finite-dimensional simple \(\specialLinearLie(2, \complex)\)-module with a highest weight vector of weight \(\alpha\).
        Then
        \begin{enumerate}
            \item \(\alpha\) is a non-negative integer, \(N\);
            \item \(\dim V = N + 1\) and the nonzero weight spaces of \(V\) are \(V_N, V_{N-2}, \dotsc, V_{-N}\), each of which is one-dimensional.
        \end{enumerate}
        Conversely, for any positive integer, \(N\), there exists a unique simple \(\specialLinearLie(2, \complex)\)-module \(V(N)\) with a highest weight vector of weight \(N\).
    \end{thm}
    
    This leads to the following picture of \(\specialLinearLie(2, \complex)\) representations:
    \begin{equation}
        \hfil
        \tikzexternaldisable
        \begin{tikzcd}
            0 \\
            V_{N} \arrow[u, "E"] \arrow[d, phantom, "\vdots"] \arrow[loop right, looseness=15, "H"]\\
            V_{\alpha + 2} \arrow[d, shift left, "F"] \arrow[loop right, looseness=10, "H"]\\
            V_{\alpha} \arrow[d, shift left, "F"] \arrow[u, shift left, "E"] \arrow[loop right, looseness=15, "H"]\\
            V_{\alpha - 2} \arrow[u, shift left, "E"] \arrow[loop right, looseness=10, "H"]\\
            V_{-N} \arrow[d, "F"] \arrow[u, phantom, "\vdots"] \arrow[loop right, looseness=10, "H"]\\
            0
        \end{tikzcd}
        \tikzexternalenable
    \end{equation}
    
    Consider the defining \(\specialLinearLie(2, \complex)\) module \(\complex^2\) where \(H\), \(E\), and \(F\) act as the matrices
    \begin{equation}
        H =
        \begin{bmatrix}
            1 & 0\\
            0 & -1
        \end{bmatrix}
        , \quad E = 
        \begin{bmatrix}
            0 & 1\\
            0 & 0
        \end{bmatrix}
        , \qand F = 
        \begin{bmatrix}
            0 & 0\\
            1 & 0
        \end{bmatrix}
        .
    \end{equation}
    The eigenvectors of \(H\) are
    \begin{equation}
        e_1 = 
        \begin{bmatrix}
            1\\ 0
        \end{bmatrix}
        \qqand
        e_2 =  
        \begin{bmatrix}
            0\\ 1
        \end{bmatrix}
        .
    \end{equation}
    These have eigenvalues \(+1\) and \(-1\) respectively.
    Of these only the first vanishes when we act with \(E\), giving
    \begin{equation}
        \begin{bmatrix}
            0\\ 1
        \end{bmatrix}
        .
    \end{equation}
    This means that the first eigenvector is a highest weight vector of \(\complex^2\).
    The weight of this vector, its \(H\) eigenvalue, is \(+1\)
    Further, this representation is simple, because if we start with some arbitrary vector then acting with \(E\) gives a vector along \(e_1\), and acting with \(F\) gives a vector along \(e_2\), spanning the whole space. So, this representation is isomorphic to \(V(1)\).
    
    Now consider the adjoint action of \(\specialLinearLie(2, \complex)\).
    Recall that this is the action of \(\specialLinearLie(2, \complex)\) on itself by the bracket.
    Then we may identify the representation space with \(\complex^3\) where
    \begin{equation}
        E = 
        \begin{bmatrix}
            1\\ 0\\ 0
        \end{bmatrix}
        , \quad H = 
        \begin{bmatrix}
            0\\ 1\\ 0
        \end{bmatrix}
        , \qand F = 
        \begin{bmatrix}
            0\\ 0\\ 1
        \end{bmatrix}
        .
    \end{equation}
    From the defining bracket relations we can see that we have
    \begin{equation*}
        \ad_E =
        \begin{bmatrix}
            0 & -2 & 0\\
            0 & 0 & 1\\
            0 & 0 & 0
        \end{bmatrix}
        , \quad
        \ad_H = 
        \begin{bmatrix}
            2 & 0 & 0\\
            0 & 0 & 0\\
            0 & 0 & -2
        \end{bmatrix}
        , \qand \ad_F = 
        \begin{bmatrix}
            0 & 0 & 0\\
            -1 & 0 & 0\\
            0 & 2 & 0
        \end{bmatrix}
        .
    \end{equation*}
    These matrices are such that \(\ad_X(Y) = \bracket{X}{Y}\).
    For example, we have
    \begin{equation}
        \ad_H(E) =
        \begin{bmatrix}
            2 & 0 & 0\\
            0 & 0 & 0\\
            0 & 0 & -2
        \end{bmatrix}
        \begin{bmatrix}
            1\\ 0\\ 0
        \end{bmatrix}
        =
        \begin{bmatrix}
            2\\ 0\\ 0
        \end{bmatrix}
        = 2E = \bracket{H}{E}.
    \end{equation}
    The eigenvectors of \(H\) are \(E\), \(H\), and \(F\), with eigenvalues \(+2\), \(0\), and \(-2\) respectively.
    Of these, only \(E\) has the property that \(E \cdot E = \ad_E(E) = \bracket{E}{E}\) vanishes, and thus \(E\) is a highest weight vector, and it has weight \(2\).
    Acting with \(F\) we have \(F \cdot E = \ad_F(E) = \bracket{F}{E} = -H\), and acting with \(F\) again we have \(F \cdot H = \ad_F(H) = \bracket{F}{H} = 2F\).
    Thus, \(E\) generates the whole space, showing that any submodule containing \(E\) must be all of \(\specialLinearLie(2, \complex)\), and a similar argument holds for \(H\) or \(F\).
    This shows that the module is simple.
    Then this module is isomorphic to \(V(2)\).
    
    \chapter{The Structure of Lie Algebras}
    In this chapter we will analyse the structure of general Lie algebras.
    We do this by looking at some special types of Lie algebras which we define here.
    
    \section{Centre}
    \begin{dfn}{}{}
        Let \(\lie{g}\) be a Lie algebra.
        Its \defineindex{centre}, \(\centre(\lie{g})\), is the ideal
        \begin{equation}
            \centre(\lie{g}) \coloneqq \{X \in \lie{g} \mid \bracket{X}{Y} = 0 \forall Y \in \lie{g}\}.
        \end{equation} 
    \end{dfn}
    
    When we interpret the bracket as a commutator the centre is exactly the collection of all elements which commute with all other elements of the Lie algebra when viewed as an associative algebra.
    In general we say that two elements commute if their bracket vanishes.
    This definition is then analogous to the centre of a group, which is all elements that commute with all other elements of the group.
    
    \begin{exm}{}{}
        We have that \(\centre(\generalLinearLie(n, \complex)) = \Span_{\complex}\{I_n\}\), this is just Schur's lemma (\cref{lma:schur}).
    \end{exm}
    
    \begin{lma}{}{}
        The centre of a Lie algebra is an ideal.
        \begin{proof}
            Let \(\lie{g}\) be a Lie algebra and \(\centre(\lie{g})\) its centre.
            If \(X, Y \in \centre(\lie{g})\) then we have
            \begin{equation}
                \bracket{X + Y}{Z} = \bracket{X}{Z} + \bracket{Y}{Z} = 0
            \end{equation}
            for all \(Z \in \lie{g}\), and so \(\centre(\lie{g})\) is closed under addition.
            Similarly, if \(\lambda \in \field\) and \(X \in \centre(\lie{g})\) then we have
            \begin{equation}
                \bracket{\lambda X}{Y} = \lambda \bracket{X}{Y} = 0
            \end{equation}
            for all \(Y \in \lie{g}\), so \(\centre(\lie{g})\) is closed under scalar multiplication.
            Thus, \(\centre(\lie{g})\) is a subspace of \(\lie{g}\).
            Now notice that if \(X \in \centre(\lie{g})\) and \(Y \in \lie{g}\) then for all \(Z \in \lie{g}\) we have
            \begin{equation}
                \bracket{\bracket{X}{Y}}{Z} = \bracket{0}{Z} = 0,
            \end{equation}
            and so \(\bracket{X}{Y} \in \centre(\lie{g})\) so \(\centre(\lie{g})\) is an ideal.
        \end{proof}
    \end{lma}
    
    Notice that a Lie algebra, \(\lie{g}\), is abelian if and only if we have \(\lie{g} = \centre(\lie{g})\).
    
    \section{Simple Lie Algebra}
    \begin{dfn}{}{def:simple lie alg}
        A Lie algebra, \(\lie{g}\), is \defineindex{simple} if it is nonabelian has no proper ideals.
    \end{dfn}
    
    We exclude abelian Lie algebras in part because this excludes the one-dimensional Lie algebra, which doesn't poses all of the properties that we want from simple Lie algebras.
    These properties will become clear later.
    
    \begin{exm}{}{}
        The Lie algebra \(\specialLinearLie(n, \complex)\) is simple.
    \end{exm}
    
    \section{Solvable and Nilpotent Lie Algebras}
    \begin{dfn}{Nilpotent Lie Algebra}{def:nilpotent lie alg}
        The \defineindex{lower central series} of a Lie algebra, \(\lie{g}\), is the series of subalgebras
        \begin{equation}
            \lie{g} = \lie{g}_0 \subseteq \lie{g}_1 \subseteq \lie{g}_2 \subseteq \dotsb.
        \end{equation}
        These are defined recursively by
        \begin{equation}
            \lie{g}_0 = \lie{g}, \quad \lie{g}_1 = \bracket{\lie{g}}{\lie{g}}, \qand \lie{g}_{k+1} = \bracket{\lie{g}}{\lie{g}_k}.
        \end{equation}
        
        A Lie algebra is \define{nilpotent}\index{nilpotent!Lie algebra} if its lower central series eventually terminates with \(0\).
        That is, there is some \(n \in \naturals\) such that \(\lie{g}_n = 0\).
    \end{dfn}
    
    \begin{exm}{}{}
        The Lie algebra \(\nilpotentLie(n, \complex)\) consisting of strictly upper triangular matrices is nilpotent.
        To see this note that the product of any two strictly upper triangular matrices has zero on both the diagonal and the superdiagonal.
        Then the product of any such matrix with a strictly upper triangular matrix has zero on the leading diagonal and the two diagonals above it.
        Continuing on like this we must eventually have all zeros, since the matrices are finite, and thus the lower central series terminates.
    \end{exm}
    
    \begin{lma}{}{}
        Each term in the lower central series is an ideal of \(\lie{g}\).
        \begin{proof}
            Since \(\bracket{\lie{g}}{\lie{g}}\) is defined to be the span of all elements of the form \(\bracket{X}{Y}\) with \(X, Y \in \lie{g}\) it is clearly a subspace of \(\lie{g}\).
            Further, since the bracket of two brackets is just a bracket of elements of \(\lie{g}\) it is again found in \(\bracket{\lie{g}}{\lie{g}}\).
            Thus, \(\bracket{\lie{g}}{\lie{g}}\) is a subalgebra of \(\lie{g}\).
            Now suppose that \(X, Y, Z \in \lie{g}\), then \(\bracket{X}{\bracket{Y}{Z}}\) is clearly an element of \(\bracket{\lie{g}}{\lie{g}}\), and by linearity this is also true if we replace \(\bracket{Y}{Z}\) with a linear combination of brackets, thus \(\bracket{\lie{g}}{\lie{g}}\) is an ideal of \(\lie{g}\).
            The same proof shows that \(\lie{g_2}\) is an ideal of \(\lie{g}_1\) and so on, and hence all of these are ideals of \(\lie{g}\) since an ideal of an ideal is again an ideal.
        \end{proof}
    \end{lma}
    
    \begin{dfn}{Solvable Lie Algebra}{def:solvable lie alg}
        The \defineindex{derived series} or \defineindex{upper central series} of a Lie algebra, \(\lie{g}\), is the series of subalgebras
        \begin{equation}
            \lie{g} = \lie{g}^0 \subseteq \lie{g}^1 \subseteq \lie{g}^2 \subseteq \dotsb.
        \end{equation}
        These are defined recursively by
        \begin{equation}
            \lie{g}_0 = \lie{g}, \quad \lie{g}^1 = \bracket{\lie{g}}{\lie{g}}, \qand \lie{g}^{k+1} = \bracket{\lie{g}^k}{\lie{g}^k}.
        \end{equation}
        The Lie subalgebra \(\lie{g}^1 = \lie{g}' = \bracket{\lie{g}}{\lie{g}}\) is sometimes called the \defineindex{derived subalgebra} of \(\lie{g}\).
        
        A Lie algebra is \defineindex{solvable} if its derived series eventually terminates with \(0\).
        That is, there is some \(n \in \naturals\) such that \(\lie{g}^n = 0\).
    \end{dfn}
    
    Note the difference between this definition and the lower central series, we have
    \begin{equation}
        \lie{g}_2 = \bracket{\lie{g}}{\lie{g}_1} = \bracket{\lie{g}}{\bracket{\lie{g}}{\lie{g}}}
    \end{equation}
    for the lower central series, and
    \begin{equation}
        \lie{g}^2 = \bracket{\lie{g}^1}{\lie{g}^1} = \bracket{\bracket{\lie{g}}{\lie{g}}}{\bracket{\lie{g}}{\lie{g}}}
    \end{equation}
    for the derived series.
    
    \begin{exm}{}{}
        The Lie algebra \(\borelLie(n, \complex)\) consisting of upper triangular matrices is nilpotent.
        To see this note that the commutator of two upper triangular matrices is strictly upper triangular, and the commutator of two strictly upper triangular matrices has zero on the leading diagonal and super diagonal, and the commutator of any two such matrices has three diagonals of zero, and so on.
        Thus, the derived series eventually terminates with 0.
    \end{exm}
    
    \begin{lma}{}{}
        Every nilpotent algebra is solvable.
        \begin{proof}
            Suppose that \(\lie{g}\) is a nilpotent Lie algebra.
            Then its upper central series terminates.
            This means that we eventually have
            \begin{equation}
                \bracket{\lie{g}}{\bracket{\lie{g}}{\bracket{\dotsb}{\lie{g}}}} = 0.
            \end{equation}
            This then necessitates that if the rightmost copy of \(\lie{g}\) is replaced with any subalgebra of \(\lie{g}\) we still have \(0\), and in particular we may replace it with some subalgebra given by iterated commutators to achieve an element of the derived series, and thus the derived series terminates.
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{lma:i and g/i solvable implies g solvable}
        If \(\lie{g}\) is a Lie algebra with an ideal \(\lie{i}\) such that \(\lie{i}\) is solvable and \(\lie{g}/\lie{i}\) is solvable then \(\lie{g}\) is solvable.
        \begin{proof}
            \Step{\((\lie{g}/\lie{i})^n \isomorphic (\lie{g}^n + \lie{i})/\lie{i}\)}
            We prove this by induction on \(n\).
            The base case, \(n = 0\), is that \(\lie{l}/\lie{i} \isomorphic (\lie{g} + \lie{i})/\lie{i}\).
            This is true because the second isomorphism theorem tells us that \((\lie{g} + \lie{i})/\lie{i} \isomorphic \lie{l}/(\lie{g} \cap \lie{i})\), and since \(\lie{i} \subseteq \lie{g}\) we have \(\lie{g} \cap \lie{i} = \lie{i}\), so we have \((\lie{g} + \lie{i})/\lie{i} \isomorphic \lie{l}/\lie{i}\).
            
            For the inductive step suppose the result holds for some \(k \in \naturals\), that is, \((\lie{g}/\lie{i})^k \isomorphic (\lie{g}^k + \lie{i})/\lie{i}\).
            Then we have
            \begin{alignat}{2}
                (\lie{g}/\lie{i})^{k + 1} &= ((\lie{g}/\lie{i})^k)' \qquad && \text{definition of derived subalgebra} \notag\\
                &\isomorphic ((\lie{g}^k + \lie{i})/\lie{i})' \qquad && \text{induction hypothesis}\\
                &\isomorphic ((\lie{g}^k + \lie{i})' + \lie{i})/\lie{i} \qquad && \text{base case}\\
                &\isomorphic (\lie{g}^{k + 1} + \lie{i}' + \lie{i})/\lie{i} \qquad && \lie{i}' \subseteq \lie{i}\\
                &= (\lie{g}^{k + 1} + \lie{i})/\lie{i}
            \end{alignat}
            and the result holds for all \(n \in \naturals\).
            
            \Step{\((\lie{g}^n)^m = \lie{g}^{n + m}\)}
            We prove this by induction on \(m\).
            For the base case we take \(m = 1\), then we have \((\lie{g}^n)^m = (\lie{g}^n)^1 = (\lie{g}^n)' = \lie{g}^{n+1}\) by definition.
            Now suppose that for some \(k \in \naturals\) we have \((\lie{g}^n)^k = \lie{g}^{n + k}\).
            Then consider \(k + 1\), we have \((\lie{g}^n)^{k + 1} = ((\lie{g}^n)^k)' = (\lie{g}^{n + k})' = \lie{g}^{n + k + 1}\), and so the result holds for all \(m \in \naturals\).
            
            \Step{\(\lie{g}\) is Solvable}
            By assumption \(\lie{i}\) and \(\lie{g}/\lie{i}\) are solvable, so there exist \(M, N \in \naturals\) such that \(\lie{i}^M = 0\) and \((\lie{g}/\lie{i})^N = 0\).
            We therefore have \(0 = (\lie{g}/\lie{i})^N \isomorphic (\lie{g}^N + \lie{i})/\lie{i}\).
            From this it follows that \(\lie{g}^N + \lie{i} \subseteq \lie{i}\), because this is the condition for a quotient to be zero.
            Thus, we must have \(\lie{g}^N \subseteq \lie{i}\).
            Taking the \(M\)th derived subalgebra of each side we have \((\lie{g}^N)^M \subseteq \lie{i}^M = 0\), and thus \(\lie{g}^{N + M} = (\lie{g}^N)^M = 0\), so \(\lie{g}\) is solvable.
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{}
        If \(\lie{g}\) is a Lie algebra with solvable ideals \(\lie{i}\) and \(\lie{j}\) then \(\lie{i} + \lie{j}\) is a solvable ideal of \(\lie{g}\).
        \begin{proof}
            Since \(\lie{i}\) and \(\lie{j}\) are solvable there exist \(M, N \in \naturals\) such that \(\lie{i}^N = 0\) and \(\lie{j}^M = 0\).
            By the second isomorphism theorem we have \((\lie{i} + \lie{j})/\lie{j} \isomorphic \lie{i}/(\lie{i} \cap \lie{j})\), and taking the \(M\)th derived subalgebra of each side we get \(((\lie{i} + \lie{j})/\lie{j})^M = (\lie{i}/(\lie{i} \cap \lie{j}))^M\).
            By the first step in the proof of \cref{lma:i and g/i solvable implies g solvable} we know that \((\lie{i} / (\lie{i} \cap \lie{j}))^M \isomorphic (\lie{i}^M + (\lie{i} \cap \lie{j}))/(\lie{i} \cap \lie{j}) = (0 + (\lie{i} \cap \lie{j}))/(\lie{i} \cap \lie{j}) = (\lie{i} \cap \lie{j})/(\lie{i} \cap \lie{j}) = 0\).
            Thus, \(((\lie{i} + \lie{j})/\lie{j})^M = 0\), so \((\lie{i} + \lie{j})/\lie{j}\) is solvable.
            Hence, by \cref{lma:i and g/i solvable implies g solvable} we know that \(\lie{i} + \lie{j}\) is solvable. 
        \end{proof}
    \end{lma}
    
    \begin{dfn}{Semisimple Lie Algebra}{def:semisimple lie alg}
        A Lie algebra, \(\lie{g}\), is semisimple if it contains no proper solvable ideals.
    \end{dfn}
    
    We immediately see that every simple Lie algebra is semisimple, since a simple Lie algebra contains no nonzero proper ideals, so it certainly doesn't contain any solvable ideals.
    Note that this is not true if we relax the definition of simple Lie algebras to allow abelian Lie algebras.
    
    \begin{lma}{}{}
        Let \(\lie{g}\) be a finite dimensional Lie algebra.
        Then there exists a maximal (with respect to inclusion) solvable ideal of \(\lie{g}\) called the \define{radical}\index{radical!of a Lie algebra} of \(\lie{g}\), denoted \(\rad \lie{g}\).
        \begin{proof}
            Let \(\{\lie{i}_{\alpha}\}_{\alpha \in A}\) be the collection of all solvable ideals.
            Define
            \begin{equation}
                \lie{i} = \sum_{\alpha \in A} \lie{i}_\alpha = \bigg\{\sum_{\alpha \in A} i_\alpha \mid i_\alpha \in \lie{i}_\alpha \text{ and } i_\alpha = 0 \text{ for all but finitely many } \alpha \in A\}.
            \end{equation}
            This is a subspace of \(\lie{g}\) since it is a sum of subspaces.
            
            We claim that we actually have
            \begin{equation}
                \lie{i} = \lie{i}_{\alpha_1} + \dotsb + \lie{i}_{\alpha_n}
            \end{equation}
            for some finite indexing subset \(\{\alpha_1, \dotsc, \alpha_n\} \subseteq A\).
            This is the case because \(\lie{g}\) is finite dimensional, so \(\lie{i}\) must also be finite dimensional.
            If \(\lie{i} = \lie{i}_{\alpha_1}\) for some \(\alpha_1 \in A\) then we are done.
            If this is not the case then we can find some \(\alpha_2 \in A\) such that \(\lie{i}_{\alpha_1} + \lie{i}_{\alpha_2} \ne \lie{i}_{\alpha_1}\).
            If \(\lie{i} = \lie{i}_{\alpha_1} + \lie{i}_{\alpha_2}\) we are done.
            Iterating this process we see that since we get a new subspace each time we add an \(\lie{i}_{\alpha_k}\) and we clearly have \(\lie{i}_{\alpha_1} + \dotsb \lie{i}_{\alpha_{k-1}} \subseteq \lie{i}_{\alpha_1} + \dotsb + \lie{i}_{\alpha_{k-1}} + \lie{i}_{\alpha_k}\) the dimension must increase at each step, and thus this process must terminate for \(\lie{i}\) to be finite dimensional.
            
            By construction any solvable ideal, \(\lie{j}\), can be found in \(\lie{i}\), since we have \(\lie{j} = \lie{i}_{\alpha}\) for some \(\alpha \in A\) and we therefore have terms of the form \(j = 0 + \dotsb + 0 + j + 0 + \dotsb + 0 \in \lie{i}\) for each \(j \in \lie{j}\).
            Thus, \(\lie{i}\) is a maximal subspace of \(\lie{g}\) containing all solvable ideals.
            
            We now need only show that \(\lie{i}\) is an ideal.
            That it is a subalgebra follows by linearity of the bracket and the fact that \(\lie{i}\) is defined as a sum of subalgebras.
            That it is an ideal follows because each ideal in the sum defining \(\lie{i}\) has the absorbing property, and so the whole sum has the absorbing property.
        \end{proof}
    \end{lma}
    
    This result tells us that \(\lie{g}\) is semisimple if and only if \(\rad \lie{g} = 0\).
    One thing we can say is that \(\lie{g}/\rad \lie{g}\) is semisimple.
    This is because an ideal of the quotient \(\lie{g}/\rad \lie{g}\) is of the form \(\lie{i}/\rad \lie{g}\) where \(\lie{i} \supseteq \rad \lie{g}\) is an ideal of \(\lie{i}\).
    Supposing that \(\lie{i}/\rad\lie{g}\) is solvable, and knowing that \(\rad\lie{g}\) is solvable we must have that \(\lie{i}\) is solvable, meaning that \(\lie{i} \subseteq \rad\lie{g}\).
    Thus, we have \(\lie{i} = \rad\lie{g}\), and so \(\lie{i}/\rad\lie{g} = \lie{i}/\lie{i} = 0\).
    Thus, there are no nontrivial proper solvable ideals of \(\lie{g}/\rad\lie{g}\), so \(\lie{g}/\rad\lie{g}\) is semisimple.
    
    \section{Engel's Theorem}
    Let \(V\) be a vector space.
    A \define{nilpotent}\index{nilpotent!endomorphism} of \(V\) is a linear map \(\varphi \colon V \to V\) such that there exists some \(N \in \naturals\) such that \(\varphi^N\) is the zero map.
    That is, \(\varphi^N(v) = 0\) for all \(v \in V\).
    
    Let \(\lie{g}\) be a Lie algebra.
    An element, \(X\), of \(\lie{g}\) is called \define{\(\symbf{\ad}\)-nilpotent}\index{ad-nilpotent@\(\ad\)-nilpotent}\index{nilpotent!element of a Lie algebra} if \(\ad_X\) is a nilpotent endomorphism of \(\lie{g}\).
    
    Before we can prove the big result of this section, Engel's theorem, we need a couple of lemmas.
    The proof of the first is quite a lot of algebra, manipulating various sums, the details of the proof aren't important, so I'd suggest skipping it.
    
    \begin{lma}{}{lma:power of the adjoint action}
        Let \(A\) be an associative algebra with the Lie algebra structure given by the commutator.
        Then for \(n \in \naturals\) and \(X, Y \in A\) we have
        \begin{equation}
            \ad_X^n(Y) = \sum_{k=0}^n (-1)^k \binom{n}{k} x^{n-k}yx^k.
        \end{equation}
        \begin{proof}
            The proof is by induction on \(n\).
            For \(n = 1\) we have
            \begin{align}
                \sum_{k=0}^1 (-1)^{k} \binom{2}{k} X^{1 - k}YX^k &= (-1)^0 \binom{2}{0}XY + (-1)^1 \binom{2}{1}YX\\
                &= XY - YX\\
                &= \bracket{X}{Y}\\
                &= \ad_X^1(Y).
            \end{align}
            Now suppose that the result holds for some \(m \in \naturals\).
            Consider the following calculation
            \begin{align}
                \ad_X^{m + 1}(Y) &= \bracket{x}{\ad_X^m(Y)}\\
                &= \bracket*{X}{\sum_{k=0}^m (-1)^k \binom{m}{k} X^{m-k}YX^k}\\
                &= \sum_{k=0}^m (-1)^k \binom{m}{k} \bracket{X}{X^{m-k}YX^k}\\
                &= \sum_{k=0}^m (-1)^k \binom{m}{k} (XX^{m-k}YX^k - X^{m-k}YX^kX)\\
                &= \sum_{k=0}^m (-1)^k \binom{m}{k} (X^{m-k+1}YX^k - X^{m-k}YX^{k+1})\\
                &= \sum_{k=0}^m (-1)^k \binom{m}{k} X^{m-k+1}YX^k - \sum_{k=0}^m (-1)^k \binom{m}{k} X^{m-k}YX^{k+1}. \notag
            \end{align}
            We can reindex the second sum so that \(k\) runs from \(1\) to \(m + 1\).
            To do so we replace each \(k\) in the second sum with \(k - 1\):
            \begin{equation}
                \sum_{k=0}^m (-1)^k \binom{m}{k} X^{m-k}YX^{k+1} = \sum_{k=1}^{m+1} (-1)^{k-1} \binom{m}{k-1} X^{m-k+1}YX^k. 
            \end{equation}
            Thus, we have
            \begingroup
            \allowdisplaybreaks
            \begin{align}
                \ad_X^{m+1}(Y) &= \sum_{k=0}^m (-1)^k \binom{m}{k} X^{m-k+1}YX^k\\
                &\quad - \sum_{k=1}^{m+1} (-1)^{k-1} \binom{m}{k-1} X^{m-k+1}YX^k \notag\\
                &= (-1)^0 \binom{m}{0} X^{m+1}YX^0 + \sum_{k=1}^m (-1)^k \binom{m}{k} X^{m-k+1}YX^k \notag\\
                &\quad- \sum_{k-1}^m (-1)^{k-1}\binom{m}{k-1}X^{m-k-1}YX^k - (-1)^m \binom{m}{m} X^0YX^{m+1} \notag\\
                &= X^{m+1}Y + \sum_{k=1}^m (-1)^k \binom{m}{k} X^{m-k+1}YX^k\\
                &\quad- \sum_{k-1}^m (-1)^{k-1}\binom{m}{k-1}X^{m-k-1}YX^k - (-1)^m YX^{m+1} \notag\\
                &= \sum_{k=1}^m \left( (-1)^k \binom{m}{k} + (-1)^k \binom{m}{k - 1} \right) X^{m-k+1}YX^k \notag\\
                &\quad+ X^{m+1}Y - (-1)^mYX^{m+1}\\
                &= \sum_{k=1}^m (-1)^k \binom{m + 1}{k} X^{m-k+1}YX^k + X^{m+1}Y - (-1)^mYX^{m+1}\notag
            \end{align}
            \endgroup
            where we've used the result
            \begin{equation}
                \binom{m}{k} + \binom{m}{k - 1} = \frac{m + 1}{k},
            \end{equation}
            which can be proved by manipulating some factorials.
            We can then recognise that the two terms not in the sum are just the \(k = 0\) term and the \(k = m + 1\) term, so that we have
            \begin{equation}
                \ad_X^{m+1}(Y) = \sum_{k=0}^{m+1} (-1)^k \binom{m+1}{k} X^{m + 1 - k} Y X^k.
            \end{equation}
            So, the result holds for all \(n \in \naturals\).
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{lma:nilpotent implies ad-nilpotent}
        Let \(V\) be a finite dimensional vector space and let \(X \in \generalLinearLie(V)\) be a nilpotent endomorphism.
        Then \(X\) is \(\ad\)-nilpotent.
        Further, if \(n \in \naturals\) is the smallest positive integer making \(X^n = 0\) then \(2n + 1\) is such that \(\ad_X^{2n + 1} = 0\).
        \begin{proof}
            Let \(X \in \generalLinearLie(V)\) be a nilpotent endomorphism.
            That is, there is some \(n \in \naturals\) such that \(X^n = 0\).
            Since \(\generalLinearLie(V)\) is a Lie algebra under the commutator we may apply the results of \cref{lma:power of the adjoint action}.
            For all \(X, Y \in \generalLinearLie(V)\) we have
            \begin{equation}
                \ad_X^{2n+1}(Y) = \sum_{k=0}^{2n+1} (-1)^k \binom{2n+1}{k} X^{2n + 1 - k}YX^k.
            \end{equation}
            This always vanishes, and we can show this by showing that every term must vanish:
            \begin{itemize}
                \item if \(k \ge n\) then the term contains \(X^k = 0\); or
                \item if \(k < n\) then \(2n + 1 - k > n\) then the term contains \(X^{2n + 1 - k} = 0\).
            \end{itemize}
            So, every term is zero, and the whole expression vanishes.
        \end{proof}
    \end{lma}
    
    \begin{thm}{}{thm:engel}
        Let \(V\) be a finite dimensional vector space over \(\field\) with \(V \ne 0\).
        Let \(\nilpotentLie\) be a subalgebra of \(\generalLinearLie(V)\) such that all elements of \(\nilpotentLie\) are nilpotent endomorphisms of \(V\).
        Then there exists some vector \(v \in V\) with \(v \ne 0\) such that \(\nilpotentLie \cdot v = 0\), that is, \(v \in \ker \varphi\) for all \(\varphi \in \nilpotentLie\).
        \begin{proof}
            The proof is by induction on \(\dim \nilpotentLie\).
            
            \Step{Base Case}
            The base case is \(\dim \nilpotentLie = 1\).
            In this case we know that \(\nilpotentLie = \field X\) for some (in fact, any) \(X \in \nilpotentLie \setminus \{0\}\).
            We know that \(X\) is nilpotent by assumption.
            So, there exists some minimal \(n \in \naturals\) such that \(X^n = 0\), and since \(X \ne 0\) we know that \(n \ge 2\).
            By the assumption that \(n\) is minimal we know that \(X^{n-1} \ne 0\).
            Recalling that \(X^{n-1}\) is an element of \(\generalLinearLie(V)\) this means that there exists some \(w \in V\) such that \(X^{n-1}(w) \ne 0\).
            However, we have that
            \begin{equation}
                X(X^{n-1}(w)) = X^n(W) = 0(w) = 0,
            \end{equation}
            and so \(X^{n-1}(w) \in \ker X\).
            Thus, \(v = X^{n-1}(w)\) is as needed, and we are done with the base case.
            
            \Step{Induction}
            Suppose that the result holds when \(\dim \nilpotentLie = k\) for some \(k \in \naturals\).
            We first show that there is an ideal of \(\nilpotentLie\) of codimension 1.
            That is, there is an ideal, \(\lie{l} \subseteq \nilpotentLie\) such that \(\dim \lie{l} = k - 1\).
            
            To do this we first suppose that \(\lie{l} \subseteq \nilpotentLie\) is a maximal proper subalgebra of \(\nilpotentLie\).
            This makes sense because
            \begin{itemize}
                \item \(\nilpotentLie\) definitely has subalgebras, any one-dimensional subspace is a subalgebra; and
                \item \(\dim \nilpotentLie\) is finite, and so the dimension of \(\lie{l}\) is bounded, and as such we can simply take all subalgebras and pick one with maximal dimension.
            \end{itemize}
            
            Since \(\nilpotentLie\) is a vector space and \(\lie{l} \subseteq \nilpotentLie\) is a subspace we can form the quotient \emph{vector space} \(\nilpotentLie/\lie{l}\).
            We do now know if this is a quotient \emph{Lie algebra}, since we only know that \(\lie{l}\) is a subalgebra, not that it's an ideal.
            Consider the adjoint action of \(\nilpotentLie\) on itself.
            We can restrict this to consider the action of \(\lie{l}\) on \(\nilpotentLie\), given by \(X \cdot Y = \bracket{X}{Y}\) for \(X \in \lie{l}\) and \(Y \in \nilpotentLie\).
            This makes \(\nilpotentLie\) an \(\lie{l}\)-module.
            Since \(\lie{l}\) is a subalgebra we have \(\bracket{\lie{l}}{\lie{l}} \subseteq \lie{l}\), which means that this action naturally gives the quotient vector space, \(\nilpotentLie/\lie{l}\), an \(\lie{l}\)-module structure given by
            \begin{equation}
                X \cdot (Y + \lie{l}) = \bracket{X}{Y} + \lie{l}
            \end{equation}
            for \(X \lie{l}\) and \(Y \in \nilpotentLie\).
            
            By assumption elements of \(\nilpotentLie\) are nilpotent, and thus by \cref{lma:nilpotent implies ad-nilpotent} they are also \(\ad\)-nilpotent.
            Hence, \(\ad(\lie{l}) = \{\ad_X \mid X \in \lie{l}\}\) consists of nilpotent endomorphisms in \(\generalLinearLie(\nilpotentLie)\), and \(\ad(\lie{l})\) is a Lie algebra under the commutator.
            Since \(\lie{l}\) is a proper subalgebra of \(\nilpotentLie\) we know that \(\dim \lie{l} < \dim \nilpotentLie\), and hence \(\dim \ad(\lie{l}) < \dim \nilpotentLie = k\).
            Hence, we can apply the inductive hypothesis to \(\ad(\lie{l})\), meaning that there is some \(\overbar{Y} \in \nilpotentLie/\lie{l}\) such that \(\overbar{Y} \ne 0\) and \(\lie{l} \cdot \overbar{Y} = 0\).
            
            Let \(Y \in \nilpotentLie\) be a representative of \(\overbar{Y}\).
            That is, \(\overbar{Y} = Y + \nilpotentLie\).
            Then we have \(\bracket{\lie{l}}{Y} \subseteq \lie{l}\) and \(\).
            This implies that \(\bracket{\lie{l} \oplus \field Y}{\lie{l} \oplus \field Y} \subseteq \lie{l} \oplus \field Y\).
            This means that \(\lie{l} \oplus \field \lie{l}\) is a subalgebra of \(\nilpotentLie\).
            Now, \(\dim(\lie{l} \oplus \field Y) = \dim(\lie{l}) + 1\), and so by the assumption that \(\lie{l}\) is a \emph{maximal} proper subalgebra we must have that \(\lie{l} \oplus \field Y = \nilpotentLie\).
            This proves that \(\dim \lie{l} = \dim(\nilpotentLie) - 1\).
            It also implies that \(\lie{l}\) is an ideal, since all elements in \(\nilpotentLie\) which are not in \(\lie{l}\) are just scalar multiples of \(Y\), and we've seen that \(\bracket{\lie{l}}{Y} \in \lie{l}\), so \(\lie{l}\) has the required absorbing property.
            
            Since \(\dim \lie{l} < \dim \nilpotentLie\) we can apply the induction hypothesis, and we know that there is some \(v \in \lie{l}\) with \(v \ne 0\) such that \(\lie{l} \cdot v = 0\).
            Let \(W \subseteq V\) be the subspace of all vectors with this property.
            Taking the decomposition \(\nilpotentLie = \lie{l} \oplus \field Y\) it is sufficient to show that there is some \(w \in W\) with \(w \ne 0\) such that \(Y(w) = 0\), since then we will certainly have \((X + Y)(w) = X(w) + Y(w) = 0 + 0 = 0\) for \(X \in \lie{l}\).
            To show that such a \(w\) exists take \(X \in \lie{l}\), then since the bracket in \(\generalLinearLie(V)\) is just the commutator we can evaluate at some \(w \in W\) and we have
            \begin{equation}
                XY(w) = YX(w) + \bracket{X}{Y}(w) = 0,
            \end{equation}
            where we've used the fact that \(X, \bracket{X}{Y} \in \lie{l}\) (since \(\lie{l}\) is an ideal) and thus \(X(w) = \bracket{X}{Y}(w) = 0\) by definition of \(W\).
            Therefore, \(Y(W) \subseteq W\).
            Since \(Y\) is a nilpotent endomorphism of \(V\) and preserves \(W\) it restricts to a nilpotent endomorphism of \(W\).
            Thus, there exists some \(w \in W\) with \(w \ne 0\) such that \(Y(w) = 0\).
        \end{proof}
    \end{thm}
    
    \subsection{Corollaries of Engel's Theorem}
    Before we can prove some corollaries of Engel's theorem we need a definition and a couple of lemmas.
    
    \begin{dfn}{Flag}{}
        Let \(V\) be a vector space.
        A \defineindex{flag}, \(V_{\bullet}\), in \(V\) is an ascending sequence of vector spaces
        \begin{equation}
            0 = V_0 \subsetneq V_1 \subsetneq \dotsb \subsetneq V_n = V.
        \end{equation}
        A flag is \define{complete}\index{complete flag} if \(\dim V_{i+1} = \dim V_i + 1\), or equivalently \(\dim(V_{i+1}/V_i) = 1\).
        
        For \(\field^n\) with basis \(\{e_1, \dotsc, e_n\}\) the \defineindex{standard flag} is the complete flag given by taking \(V_0 = 0\) and \(V_i = \Span_{\field}\{e_1, \dotsc, e_i\}\).
    \end{dfn}
    
    Each choice of a basis gives rise to a complete flag, although not uniquely, since \(\{e_1, e_2, e_3, \dotsc, e_n\}\) and \(\{e_1, 2e_2, e_3, \dotsc, e_n\}\) both give rise to the same standard flag.
    
    \begin{lma}{}{lma:flag vs upper triangular}
        Take \(V_{\bullet}\) to be the standard flag in \(\complex^n\) with basis \(\{e_1, \dotsc, e_n\}\).
        Then for \(X \in \generalLinear(n, \complex)\) we have
        \begin{itemize}
            \item \(X \in \borelLie(n, \complex)\) if and only if \(X(V_i) \subseteq V_i\); and
            \item \(X \in \nilpotentLie(n, \complex)\) if and only if \(X(V_i) \subseteq V_{i-1}\).
        \end{itemize}
        \begin{proof}
            This is just restating the definition of the (strictly) upper triangular matrices using the language of flags.
            We can identify \(V_i\) as the space of all column vectors which are zero after the \(i\)th row.
            Then if \(v \in V_i\) we know that \(X(v) \in V_i\) if and only if \(X(v)\) has zeros after the \(i\)th row, which is true exactly when \(X\) is upper triangular.
            If we replace \(X\) with a strictly upper triangular matrix then \(X(v)\) will have zeros after the \((i - 1)\)st row, and so will be in \(V_{i-1}\).
        \end{proof}
    \end{lma}
    
    We say that \(X \in \borelLie(n, \complex)\) preserves the flag and \(X \in \nilpotentLie(n, \complex)\) strictly preserves the flag.
    
    Note that each basis choice gives us a (not necessarily unique) algebra \(\borelLie(n, \complex)\) (called a Borel subalgebra), of matrices which are upper triangular when expressed in that basis.
    While this pairing of bases to Borel subalgebras is not a bijection the pairing of standard flags to Borel subalgebras is, since two bases give the same Borel subalgebra if and only if they are related by rescaling the basis elements, if and only if they correspond to the same standard flag.
    
    \begin{crl}{}{crl:nilpotent always corresponds to strictly upper triangular}
        Let \(V\) be a finite dimensional vector space with \(V \ne 0\).
        Let \(\nilpotentLie\) be a subalgebra of \(\generalLinearLie(V)\) such that all elements of \(\nilpotentLie\) are nilpotent endomorphisms of \(V\).
        Then there exists a basis of \(V\) such that all elements of \(\nilpotentLie\) correspond to strictly upper triangular matrices in this basis.
        \begin{proof}
            By \cref{lma:flag vs upper triangular} this result is equivalent to the statement that there exists a complete flag, \(V_{\bullet}\), such that \(\nilpotentLie(V_i) \subseteq V_{i-1}\), that is \(X(V_i) \subseteq V_{i - 1}\) for all \(X \in \nilpotentLie\).
            
            We can prove this by induction on \(\dim V\).
            The base case is \(\dim V = 1\).
            If \(X \colon V \to V\) is nilpotent, so that \(X^n = 0\) for some \(n \in \naturals\), then in any basis of \(V\) we can express \(X\) as \([X]_{\symcal{B}} = (b)\) for some \(b \in \complex\).
            Then the nilpotent condition is that \((b)^n = (b^n) = 0\), which is true if and only if \(b = 0\), and hence \([X]_{\symcal{B}} = (0)\), which is strictly upper triangular.
            
            For the inductive step suppose that for any vector space of dimension less than \(\dim V\) there is a complete flag which is completely preserved by elements of \(\nilpotentLie\).
            By Engel's theorem (\cref{thm:engel}) we know that there is some \(v_1 \in V\) such that \(v_1 \ne 0\) and \(\nilpotentLie v_1 = 0\) (that is, \(X(v_1) = 0\) for all \(X \in \nilpotentLie\)).
            Consider the flag
            \begin{equation}
                0 = V_0 \subsetneq V_1 = \complex v_1 \subsetneq V.
            \end{equation}
            Note that we're assuming that \(\dim V > \dim V_1 = 1\), else we would already be done by the base case.
            
            We now consider the quotient \(V/V_1\).
            This has \(\dim(V/V_1) = \dim V - \dim V_1 = \dim V - 1 \le \dim V\), and so the induction hypothesis applies.
            This tells us that there is a flag
            \begin{equation}
                0 = \overbar{V}_1 \subsetneq \overbar{V}_2 \subsetneq \dotsb \subsetneq \overbar{V}_{n-1} = V/V_1
            \end{equation}
            which is strictly preserved by \(\nilpotentLie\), so \(\nilpotentLie \overbar{V}_i \subseteq \overbar{V}_{i-1}\).
            We use the preimage of the \(\overbar{V}_i\) under the quotient map \(V \twoheadrightarrow V/V_1\) to extend our initial flag by defining
            \begin{equation}
                V_i = \{v \in V \mid v + V_1 \in \overbar{V}_i\}.
            \end{equation}
            Note that this agrees with our earlier definition of \(V_1\) since \(v + V_1 \in \overbar{V}_1 = 0\) if and only if \(v \in V_1\).
            So, we have a complete flag
            \begin{equation}
                0 = V_0 \subsetneq V_1 \subsetneq \dotsb \subsetneq V_{n-1} = V/V_1 \subsetneq V_n = V.
            \end{equation}
            We just need to show that this is strictly preserved by \(\nilpotentLie\).
            To see this note that if \(X \in \nilpotentLie\) then \(X(\overbar{V}_i) \subseteq \overbar{V}_{i-1}\).
            This means that \(X(V_i) \subseteq V_{i-1} + V_1 = V_{i-1}\).
            Thus, this flag is as required.
        \end{proof}
    \end{crl}
    
    We now have another lemma before we can prove another corollary of Engel's theorem, which is really a corollary of this corollary.
    
    \begin{lma}{}{lma:nilpotent when quotiented by centre implies nilpotent}
        Let \(\nilpotentLie\) be a Lie algebra such that \(\nilpotentLie/\centre(\nilpotentLie)\) is nilpotent.
        Then \(\nilpotentLie\) is nilpotent.
        \begin{proof}
            For \(\lie{g}\) a Lie algebra and \(\lie{i} \subseteq \lie{g}\) an ideal we have \((\lie{g}/\lie{i})_n = \lie{g}_n + \lie{i}/\lie{i}\).
            Taking \(\lie{g} = \nilpotentLie\) and \(\lie{i} = \centre(\nilpotentLie)\) we have \((\nilpotentLie_n + \centre(\nilpotentLie))/\centre(\nilpotentLie) = (\nilpotentLie/\centre(\nilpotentLie))_n\).
            By assumption \(\nilpotentLie/\centre(\nilpotentLie)\) is nilpotent, so there is some \(N \in \naturals\) such that \((\nilpotentLie/\centre(\nilpotentLie))_N = 0\).
            We therefore have \((\nilpotentLie_N + \centre(\nilpotentLie))/\centre(\nilpotentLie) = 0\), which is only possible if \(\nilpotentLie_N = 0\), which means that \(\nilpotentLie\) is nilpotent.
        \end{proof}
    \end{lma}
    
    This result is useful because we always have the adjoint action, \(\ad \colon \lie{g} \to \generalLinearLie(\lie{g})\), and by definition \(\ker \ad = \centre(\lie{g})\).
    Then using the first isomorphism theorem we have \(\ad(\lie{g}) \isomorphic \lie{g} / \centre(\lie{g})\).
    This means that we always have an embedding of \(\lie{g}/\centre(\lie{g})\) into \(\generalLinearLie(\lie{g})\) for any Lie algebra, nilpotent or not.
    This lets us study \(\lie{g}\) by studying \(\generalLinearLie(\lie{g})\), so long as we're prepared to set all central elements to zero.
    
    \begin{crl}{}{crl:ad-nilpotent implies nilpotent}
        Let \(\nilpotentLie\) be a finite dimensional Lie algebra.
        If every element of \(\nilpotentLie\) is \(\ad\)-nilpotent then \(\nilpotentLie\) is nilpotent.
        \begin{proof}
            By \cref{lma:nilpotent when quotiented by centre implies nilpotent} it is sufficient to prove that \(\nilpotentLie/\centre(\nilpotentLie)\).
            By the remark following \cref{lma:nilpotent when quotiented by centre implies nilpotent} we have that \(\nilpotentLie/\centre(\nilpotentLie) \isomorphic \ad(\nilpotentLie) \subseteq \generalLinearLie(\nilpotentLie)\).
            By \cref{crl:nilpotent always corresponds to strictly upper triangular} there exists a basis for \(\nilpotentLie\) such that elements of \(\ad(\nilpotentLie)\) are strictly upper triangular matrices in this basis.
            This implies that, in this basis, \(\ad(\nilpotentLie) \subseteq \nilpotentLie(n, \complex)\), and this implies that \(\nilpotentLie/\centre(\nilpotentLie)\) is nilpotent, so \(\nilpotentLie\) is nilpotent by \cref{lma:nilpotent when quotiented by centre implies nilpotent}.
        \end{proof}
    \end{crl}
    
    \section{Lie's Theorem}
    Given the notation setup in the statement of Engel's theorem (\cref{thm:engel}) we may state the conclusion as \enquote{there is a common eigenvector, \(v \in V\), for all endomorphisms \(X \in \nilpotentLie\) such that the eigenvalue of \(v\) for \(X\) is zero}.
    Lie's theorem is the analogue for solvable Lie algebras.
    It states that there is a common eigenvector, but does not force the eigenvalue to be zero.
    Let \(\lie{g} \subseteq \generalLinearLie(V)\) be some Lie algebra.
    Then \(v \in V\) is an eigenvector of all \(X \in \lie{g}\) if there exist scalars \(\alpha_X \in \field\) for each \(X \in \lie{g}\) such that \(Xv = \alpha_X v\).
    We can then view this as a map \(\alpha \colon \lie{g} \to \field\), \(X \mapsto \alpha_X\).
    Further, this map is linear since \(\alpha_{X + Y}\) is the eigenvalue of \(v\) under the operation \(X + Y\) and \((X + Y)v = Xv + Yv = \alpha_X v + \alpha_Y v\) so we can identify \(\alpha_{X + Y} = \alpha_X + \alpha_Y\), and similarly, \((\lambda X)v = \lambda(Xv) = \lambda \alpha_X v\) so \(\alpha_{\lambda X} = \lambda \alpha_X\).
    Thus, \(\alpha \in \lie{g}^*\).
    So, \(v \in V\) with \(v \ne 0\) is a common eigenvalue of all elements of \(\lie{g}\) if and only if there exists some \(\alpha \in \lie{g}^*\) such that \(Xv = \alpha(X)v\) for all \(X \in \lie{g}\).
    
    \begin{thm}{Lie}{thm:lie}
        Let \(V\) be a finite-dimensional vector space over an algebraically closed field of characteristic zero (so basically, \(\field = \complex\)), \(\field\), with \(V \ne 0\), and let \(\lie{s}\) a solvable subalgebra of \(\generalLinearLie(V)\).
        Then there exists a common eigenvector for all endomorphisms in \(\lie{s}\).
        \begin{proof}
            The steps taken in this proof are the same as in Enge's theorem (\cref{thm:engel}), but the justifications for why each step holds differ slightly.
            
            The proof is by induction on \(\dim \lie{s}\).
            For the base case, \(\dim \lie{s} = 1\), the result is trivial as \(\lie{s} = \complex X\) for some nonzero endomorphism \(X \in \generalLinearLie(V)\), and by the assumption that we work over an algebraically closed field \(X\) has an eigenvector, \(v \in V\) with \(v \ne 0\), so \(Xv = \alpha v\) for some \(\alpha \in \field\), and this is an eigenvector of all \(Y \in \lie{s}\) since \(Y = yX\) for some \(y \in \field\), since \(Yv = yXv = y\alpha v\), so \(v\) is an eigenvector of \(Y\) with eigenvalue \(y\alpha\).
            
            Now suppose that \(\dim\lie{s} > 1\) and that the hypothesis holds for all solvable Lie algebras of dimension less than \(\dim \lie{s}\).
            Since \(\lie{s}\) is solvable the derived subalgebra, \(\bracket{\lie{s}}{\lie{s}}\) is either zero or a proper ideal of \(\lie{s}\).
            Importantly, \(\bracket{\lie{s}}{\lie{s}} \ne \lie{s}\), since then the derived series would never terminate.
            If \(\bracket{\lie{s}}{\lie{s}} = 0\) then \(\lie{s}\) is abelian and \(\lie{s}/\bracket{\lie{s}}{\lie{s}} = \lie{s} / 0 = \lie{s}\) is abelian.
            If \(\bracket{\lie{s}}{\lie{s}}\) is a proper ideal of \(\lie{s}\) then \(\lie{s}/\bracket{\lie{s}}{\lie{s}}\) is abelian since for all \(X, Y \in \lie{s}\) we have
            \begin{equation}
                \bracket{X + \lie{s}'}{Y + \lie{s}'} = \bracket{X}{Y} + \lie{s}' = \lie{s}'
            \end{equation}
            where \(\lie{s}' = \bracket{\lie{s}}{\lie{s}}\) and \(\bracket{X}{Y} \in \lie{s}'\) gives us the final equality.
            This means that regardless of whether \(\lie{s}'\) is zero or a proper ideal \(\lie{s}/\lie{s}'\) is abelian and hence every subspace of \(\lie{s}/\lie{s}'\) is an ideal.
            
            Take some subspace of codimension 1, that is a subspace with dimension \(\dim(\lie{s}/\lie{s}') - 1\).
            Call this subspace \(\overbar{\nilpotentLie}\), and note that this is an ideal of \(\lie{s}/\lie{s}'\).
            Denote by \(\nilpotentLie\) the preimage of \(\overbar{\nilpotentLie}\) under the quotient map.
            This is an ideal of \(\lie{s}\) of codimension 1.
            This holds because the preimage of an ideal under a homomorphism is always an ideal and the projection map is surjective, so the dimensions match up as needed.
            By induction we may posit that there is some common eigenvector for all elements of \(\nilpotentLie\), since it has dimension strictly less than \(\dim\lie{s}\) and is solvable as a subalgebra of a solvable algebra.
            Then there exists some \(\alpha \in \nilpotentLie^*\) such that \(Xv = \alpha(X)v\) for all \(X \in \nilpotentLie\).
            
            Now consider the subspace \(W = \{w \in V \mid Xw = \alpha(X)w \forall X \in \nilpotentLie\}\).
            This subspace can be thought of as the space of all candidates for a common eigenvector of \(\lie{s}\), we just need to find some element of this subspace that is also an eigenvector of some vector spanning the missing dimension that \(\lie{s}\) has but \(\nilpotentLie\) doesn't.
            We know by the previous paragraph that \(W\) is nonempty, and thus by the following lemma (\cref{lma:lie's theorem lemma}) we know that \(W\) is an \(\lie{s}\)-submodule of \(V\).
            Choose some \(Y \in \lie{s} \setminus \nilpotentLie\), so \(\lie{s} = \nilpotentLie \oplus \complex Y\).
            Then \(Y(W) \subseteq W\), as otherwise \(W\) would not be a \(\lie{s}\)-submodule.
            Thus, there exists some \(w \in W\) with \(w \ne 0\) such that \(w\) is an eigenvector of \(Y\), and so \(w\) is an eigenvalue of all elements of \(\lie{s}\).
        \end{proof}
    \end{thm}
    
    \begin{lma}{}{lma:lie's theorem lemma}
        Work over an algebraically closed field of characteristic zero.
        Let \(\nilpotentLie\) be an ideal of a Lie algebra \(\lie{s}\) and let \(V\) be an \(\lie{s}\)-submodule and \(\alpha \in \lie{s}^*\).
        Let
        \begin{equation}
            W = \{v \in V \mid Xv = \alpha(X)v \forall X \in \nilpotentLie\}.
        \end{equation}
        Then \(W\) is an \(\lie{s}\)-submodule of \(V\).
        \begin{proof}
            We need to show that \(Y(W) \subseteq W\) for all \(Y \in \lie{s}\).
            If \(Y \in \nilpotentLie\) then this is clearly true since \(Y\) just acts on \(W\) by scaling by \(\alpha(Y)\).
            We need to show in general that \(Yw \in W\) for all \(w \in W\).
            This means we need to show that \(X(Yw) = \alpha(X)Yw\) for all \(X \in \nilpotentLie\).
            We can do this using the definition of how the bracket acts and the fact that \(w\) is an \(X\) eigenvector:
            \begin{align}
                X(Yw) &= Y(Xw) + \bracket{X}{Y}w\\
                &= Y(\alpha(X)w) + \alpha(\bracket{X}{Y})w\\
                &= \alpha(X)Yw + \alpha(\bracket{X}{Y})w.\label{eqn:bracket eigenvector}
            \end{align}
            Here we've used the fact that \(\bracket{X}{Y} \in \nilpotentLie\) since \(\nilpotentLie\) is an ideal of \(\lie{s}\), and so the action of the bracket is determined by \(\alpha\).
            We are then left to show that \(\alpha(\bracket{X}{Y}) = 0\).
            
            Let \(U\) be the subspace of \(V\) spanned by \(\{w, Yw, Y^2w, \dotsc\}\).
            Then \(Y(U)\) is the subspace spanned by \(\{Yw, Y^2w, Y^3w, \dotsc\}\), which is clearly a subspace of \(U\), so \(Y(U) \subseteq U\).
            We claim that \(U\) is also an \(\nilpotentLie\)-submodule of \(V\).
            That is, we claim that \(X(U) \subseteq U\) for all \(X \in \nilpotentLie\).
            Clearly we have \(X(w) = \alpha(X)w \in U\), and \cref{eqn:bracket eigenvector} shows that \(X(Yw)\) is a linear combination of \(Yw\) and \(w\), so is an element of \(U\).
            We take this as the base case for an induction proof where we wish now to show that \(X(Y^nw) \in U\) for some \(n \in \naturals\) and we assume that \(X(Y^kw) \in U\) for all \(k < n\).
            Then we have
            \begin{equation}
                XY^nw = YXY^{n-1}w + \bracket{X}{Y}Y^{n-1}w
            \end{equation}
            and since \(\nilpotentLie\) is an ideal and \(X \in \nilpotentLie\) we have \(\bracket{X}{Y} \in \nilpotentLie\), so by the induction hypothesis \(\bracket{X}{Y}Y^{n-1}w \in U\).
            Thus, \(XY^nw \in U\) as it's a linear combination of elements of \(U\).
            This shows that \(XY^nw\) is given by \(\alpha(X)Y^nw\) plus terms involving \(Y^kw\) with \(k < n\).
            Thus, there is a basis for \(U\) in which \(X\) is upper triangular, with the diagonals given by \(\alpha(X)\).
            In particular, we have that \(\tr(X|_U) = \alpha(X) \dim U\).
            The same is true fo \(\bracket{X}{Y} \in \nilpotentLie\), \(\tr(\bracket{X}{Y}|_U) = \alpha(\bracket{X}{Y}) \dim U\).
            However, the trace of a commutator must vanish by the cyclic property of the trace, and thus we have \(\alpha(\bracket{X}{Y}) = 0\).
        \end{proof}
    \end{lma}
    
    Note that we require characteristic zero to conclude from \(0 = \alpha(\bracket{X}{Y}) \dim U\) that \(\alpha(\bracket{X}{Y}) = 0\), since in a field of characteristic dividing \(\dim U\) this will hold with nonzero \(\alpha(\bracket{X}{Y})\).
    We need algebraic closure to guarantee the existence of an eigenvector in the one-dimensional base case.
    So, Lie's theorem pretty much holds only over \(\complex\), and a few other more \enquote{constructed} fields, such as \(\overbar{\rationals}\), the algebraic closure of \(\rationals\).
    
    \begin{crl}{}{}
        There exists a basis of \(V\) such that any solvable subalgebra \(\lie{s} \subseteq \generalLinearLie(V)\) is a subalgebra of \(\borelLie(n, \complex)\).
        \begin{proof}
            This is the same as \cref{crl:nilpotent always corresponds to strictly upper triangular} but with solvable and upper triangular in place of nilpotent and strictly upper triangular.
        \end{proof}
    \end{crl}
    
    What we have shown with Engel and Lie's theorems is the following:
    \begin{itemize}
        \item A nilpotent subalgebra of \(\generalLinearLie(V)\) is (up to isomorphism) a subalgebra of \(\nilpotentLie(n, \complex)\).
        \item A solvable subalgebra of \(\generalLinearLie(V)\) is (up to isomorphism) a subalgebra of \(\borelLie(n, \complex)\).
    \end{itemize}
    This is the sense in which \(\nilpotentLie(n, \complex)\) and \(\borelLie(n, \complex)\) are the maximal nilpotent and solvable subalgebras of \(\generalLinearLie(V)\).
    
    Further clarifying nilpotent and solvable subalgebras is hard to impossible, and so we shan't try.
    We move our focus to the classifiable semisimple Lie algebras.
    
    \chapter{Killing Form}
    In this chapter we develop one of the key tools in working with semisimple Lie algebras, the Killing form.
    Before we define the Killing form we'll have a short introduction to bilinear forms.
    
    \section{Bilinear Forms}
    \begin{dfn}{}{def:bilinear form}
        Let \(V\) be a \(\field\)-vector space.
        A \defineindex{bilinear form}, \(\eta\), is a map \(\eta \colon V \times V \to \field\) which is linear in both arguments.
        That is, for all \(u, v, w \in V\) and \(\lambda \in \field\) we have
        \begin{equation}
            \eta(u + \lambda v, w) = \eta(u, w) + \lambda \eta(v, w), \qqand \eta(u, v + \lambda w) = \eta(u, v) + \lambda \eta(u, w).
        \end{equation}
        
        A bilinear form, \(\eta\), is \define{symmetric}\index{symmetric bilinear form} if for all \(u, v \in V\) we have \(\eta(u, v) = \eta(v, u)\).
        
        A symmetric bilinear form is \define{non-degenerate}\index{non-degenerate bilinear form} if for all \(u \in V\) with \(u \ne 0\) there exists some \(v \in V\) such that \(\eta(u, v) \ne 0\).
    \end{dfn}
    
    Note that for \(\field = \reals\) non-degeneracy is equivalent to requiring that \(\eta(u, u) \ne 0\) for all \(u \ne 0\).
    The notion of a non-degenerate bilinear form can also be extended to non-symmetric bilinear forms, but we do not have need of this.
    We just have to repeat the definition with \(u\) and \(v\) swapped.
    
    Suppose that \(\eta \colon V \times V \to \field\) is a symmetric bilinear form.
    Then we can partially evaluate at some \(u \in V\) and consider a map \(\eta(u, -) \colon V \to \field\) given by \(v \mapsto \eta(u, v)\).
    This is linear since \(\eta\) is linear in the second argument, so \(\eta(u, -) \in V^*\).
    Likewise, \(\eta(-, v) \colon V \to \field\) defined by \(u \mapsto \eta(u, v)\) is linear and so \(\eta(-, v) \in V^*\).
    Of course, since \(\eta\) is symmetric these two maps are actually the same.
    
    So, each symmetric\footnote{a non-symmetric bilinear form gives rise to two such maps, and we have to choose one, so we lose the \enquote{canonical} part of this map.} bilinear form, \(\eta\), gives rise to a canonical linear map, \(\iota_\eta \colon V \to V^*\), given by \(\iota_\eta(u) = \eta(u, -)\).
    We can use this to restate the non-degeneracy condition as saying that \(\ker \iota_\eta = 0\) if and only if \(\eta\) is non-degenerate.
    To see this first let \(v \in \ker \iota_\eta\) so \(\iota_\eta(v) = 0\), which means \(\eta(v, -)\) is the zero map, so for all \(u \in V\) we have \(\eta(v, u) = 0\), so if \(\eta\) is non-degenerate we must have \(v = 0\) and vice-versa if \(\eta(v, -)\) is not the zero map then there exists some \(u \in V\) such that \(\eta(v, u) \ne 0\), and so \(\eta\) is non-degenerate.
    
    Further, in the finite dimensional case if \(\eta\) is non-degenerate then \(\iota_\eta\) is an isomorphism, since we showed above that \(\iota_\eta\) is an injection \(V \hookrightarrow V^*\), and it is a well-known fact that \(\dim V = \dim V^*\) for finite-dimensional \(V\), and so an injective linear map between spaces of the same dimension is automatically an isomorphism.
    
    So, a non-degenerate bilinear form gives a canonical isomorphism \(V \isomorphic V^*\).
    Note that we always have \emph{an} isomorphism \(V \isomorphic V^*\), it's just that in general there are many such isomorphisms and no reason to prefer one over another.
    This particular canonical isomorphism is important when our space is a Hilbert space, in which case this result is called the riesz representation theorem.
    
    \begin{dfn}{Radical}{}
        Let \(\eta\) be a bilinear form on \(V\).
        The \define{radical}\index{radical!of a bilinear form} is \(\rad \eta = \ker \iota_\kappa \subseteq V\).
    \end{dfn}
    
    \section{Killing Form}
    \begin{dfn}{}{}
        Let \(\lie{g}\) be a Lie algebra.
        Then the \defineindex{Killing form} on \(\lie{g}\) is the symmetric bilinear form
        \begin{align}
            \kappa \colon \lie{g} \times \lie{g} &\to \field\\
            (X, Y) &\mapsto \kappa(X, Y) = \tr(\ad_X \circ \ad_Y).
        \end{align}
    \end{dfn}
    
    Note that symmetry of the Killing form follows immediately from the cyclic property of the trace, namely that \(\tr(AB) = \tr(BA)\), and linearity follows immediately from the linearity of the trace and composition of endomorphisms.
    
    The most important property of the Killing form is arguably that it gives us a way to show that a finite-dimensional Lie algebra is semisimple.
    Recall that a Lie algebra is semisimple if it contains no proper solvable ideals.
    
    \begin{lma}{Associativity of the Killing Form}{lma:associativity of killing form}
        Let \(\lie{g}\) be a Lie algebra and \(\kappa\) the Killing form on \(\lie{g}\), then
        \begin{equation}
            \kappa(X, \bracket{Y}{Z}) = \kappa(\bracket{X}{Y}, Z)
        \end{equation}
        for all \(X, Y, Z \in \lie{g}\).
        \begin{proof}
            This follows by a quick calculation using the fact that \(\ad\) is a representation and some properties of the trace:
            \begin{align}
                \kappa(X, \bracket{Y}{Z}) &= \tr(\ad_X \circ \ad_{\bracket{Y}{Z}})\\
                &= \tr(\ad_X \circ \bracket{\ad_Y}{\ad_Z})\\
                &= \tr(\ad_X \circ (\ad_Y \circ \ad_Z - \ad_Z \circ \ad_Y))\\
                &= \tr(\ad_X \circ \ad_Y \circ \ad_Z - \ad_X \circ \ad_Z \circ \ad_Y)\\
                &= \tr(\ad_X \circ \ad_Y \circ \ad_Z - \ad_Y \circ \ad_X \circ \ad_Z)\\
                &= \tr((\ad_X \circ \ad_Y - \ad_Y \circ \ad_X) \circ \ad_Z)\\
                &= \tr(\bracket{\ad_X}{\ad_Y} \circ \ad_Z)\\
                &= \tr(\ad_{\bracket{X}{Y}} \circ \ad_Z)\\
                &= \kappa(\bracket{X}{Y}, Z). \qedhere
            \end{align}
        \end{proof}
    \end{lma}
    
    \begin{thm}{Cartan's First Criterion}{thm:cartans first criterion}
        A finite-dimensional Lie algebra, \(\lie{g}\), over \(\complex\) is solvable if and only if \(\kappa(X, Y) = 0\) for all \(X \in \lie{g}'\) and \(Y \in \lie{g}\).
        \begin{proof}
            Suppose that \(\lie{g}\) is solvable.
            Then Lie's theorem tells us that there is a basis, \(\symcal{B}\), for \(\lie{g}\) such that if \(W \in \lie{g}\) then \([\ad_W]_{\symcal{B}}\) is upper triangular.
            This implies that \([\bracket{\ad_X}{\ad_Y}]_{\symcal{B}} = \bracket{[\ad_X]_{\symcal{B}}}{[\ad_Y]_{\symcal{B}}}\) is \emph{strictly} upper triangular for \(X, Y \in \lie{g}\), since the commutator of upper triangular matrices is strictly upper triangular.
            This then implies that \(\kappa(\bracket{X}{Y}, Z) = \tr(\bracket{\ad_X}{\ad_Y}\circ \ad_Z) = 0\) using similar calculations to \cref{lma:associativity of killing form}.
            Note then that \(\bracket{X}{Y}\) is exactly an element of \(\lie{g}'\) and \(Z\) an element of \(\lie{g}\).
            
            Now suppose that \(X, Y, Z \in \lie{g}\) are such that \(\kappa(\bracket{X}{Y}, Z) = 0\).
            If \(L' = \bracket{\lie{g}}{\lie{g}}\) is nilpotent then \(L\) is solvable by Engel's theorem.
            Notice that if \(W = \bracket{X}{Y} \in \lie{g}'\) then \(\kappa(W, Z) = 0\).
            We can now use the Jordan decomposition to write
            \begin{equation}
                \ad_W = (\ad_W)_{\symrm{d}} + (\ad_W)_{\symrm{n}} \eqqcolon W_{\symrm{d}} + W_{\symrm{n}}
            \end{equation}
            where \(W_{\symrm{d}}\) is diagonalisable and \(W_{\symrm{n}}\) is nilpotent, meaning that in some basis \(W_{\symrm{n}}\) is upper triangular, and so has vanishing trace.
            Then we have
            \begin{equation}
                \tr(W\overbar{W}_{\symrm{d}}) = \tr((W_{\symrm{d}} + W_{\symrm{n}})\overbar{W}_{\symrm{d}}) = \tr(W_{\symrm{d}}\overbar{W}_{\symrm{d}}) + \tr(W_{\symrm{n}}\overbar{W}_{\symrm{d}}) = \tr(W_{\symrm{d}}\overbar{W}_{\symrm{d}})
            \end{equation}
            where we've used the fact that the product \(W_{\symrm{n}}\overbar{W}_{\symrm{d}}\) will be strictly upper triangular in whichever basis \(W_{\symrm{n}}\) is strictly upper triangular in, and as such has trace zero.
            Writing \(\lambda_i\) with \(i = 1, \dotsc, m\) for the eigenvalues of \(\ad_W\) we have
            \begin{equation}
                \tr(W\overbar{W}_{\symrm{d}}) = \tr(W_{\symrm{d}}\overbar{W}_{\symrm{d}}) = \abs{\lambda_1}^2 + \dotsb + \abs{\lambda_m}^2
            \end{equation}
            since \(W_{\symrm{d}}\) and \(\overbar{W}_{\symrm{d}}\) are diagonal with entries \(\lambda_i\) and \(\overbar{\lambda}_i\) as the \(i\)th diagonal entry respectively.
            We also have
            \begin{align}
                \tr(W\overbar{W}_{\symrm{d}}) &= \tr(\ad_W\circ (\ad_{\overbar{W}})_{\symrm{d}})\\
                &= \tr(\ad_{\bracket{X}{Y}} (\ad_{\overbar{W}})_{\symrm{d}})\\
                &= \tr(\ad_X \circ (\ad_{\bracket{Y}{\overbar{W}}})_{\symrm{d}})\\
                &= 0
            \end{align}
            where we've used similar calculations to the proof of \cref{lma:associativity of killing form} in the last step.
            
            So, we must have that \(\abs{\lambda_1}^2 + \dotsb + \abs{\lambda_m}^2 = 0\), which implies that \(\lambda_i = 0\) for all \(i = 1, \dotsc, m\).
            Thus, \(W_{\symrm{d}} = 0\) so \(\ad_W = W_{\symrm{n}}\), and hence \(W\) is nilpotent, because this shows it is \(\ad\)-nilpotent so we can apply \cref{crl:ad-nilpotent implies nilpotent}.
            Since \(W\) was chosen as an arbitrary element of \(\lie{g}'\) this shows that \(\lie{g}'\) is nilpotent, and hence that \(\lie{g}\) is solvable by Engel's theorem.
        \end{proof}
    \end{thm}
    
    \begin{lma}{}{}
        Every nonzero solvable ideal of a finite dimensional Lie algebra, \(\lie{g}\), contains a nonzero abelian ideal of \(\lie{g}\).
        \begin{proof}
            Let \(\lie{l}\) be a solvable ideal of \(\lie{g}\) such that \(\lie{l}^n = 0\).
            Consider \(\lie{l}^{n-1}\).
            This is such that \(\bracket{\lie{l}^{n-1}}{\lie{l}^{n-1}} = \lie{l}^n = 0\), and so \(\lie{l}^{n-1}\) is abelian, and it's also an ideal.
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{}
        Let \(\lie{s}\) be a Lie algebra.
        Then \(\lie{s}\) is solvable if and only if \(\ad(\lie{s}) \subseteq \generalLinearLie(\lie{s})\) is solvable.
        \begin{proof}
            Suppose \(\ad(\lie{s})\) is solvable.
            We have seen that \(\ad(\lie{s}) \isomorphic \lie{s}/\centre(\lie{s})\).
            The centre, \(\centre(\lie{s})\) is abelian, and thus solvable, and this isomorphism tells us that \(\lie{s}/\centre(\lie{s})\) is solvable, and so it follows by \cref{lma:i and g/i solvable implies g solvable} that \(\lie{s}\) is solvable.
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{lma:radical of killing form is ideal}
        Let \(\lie{g}\) be a Lie algebra and let \(\lie{s} = \rad\kappa\) be the radical of the Killing form.
        Then \(\lie{s}\) is a solveable ideal of \(\lie{g}\).
        \begin{proof}
            Recall that \(\lie{s} = \rad\kappa = \ker \iota_\kappa\) where \(\iota_\kappa \colon\lie{g} \to \lie{g}^*\) is the map \(X \mapsto \kappa(X, -)\).
            Suppose that \(X \in \lie{s}\) and \(Y \in \lie{g}\).
            Then we want to show that \(\bracket{Y}{X} \in \lie{s}\).
            That is, we want to show that \(\kappa(\bracket{Y}{X}, -)\) is the zero map, which means showing that \(\kappa(\bracket{Y}{X}, Z) = 0\) for all \(Z \in \lie{g}\).
            This is true by \cref{lma:associativity of killing form} because we have
            \begin{equation}
                \kappa(\bracket{Y}{X}, Z) = \kappa(Y, \bracket{X}{Z}) = 0
            \end{equation}
            with the last equality following because \(\lie{s}\) is solvable, which follows because \(\kappa(X, X') = 0\) for \(X, X' \in \lie{s}\) proving solvability by Cartan's first criterion (\cref{thm:cartans first criterion}).
        \end{proof}
    \end{lma}
    
    \begin{thm}{Cartan's Second Criterion}{}
        A Lie algebra, \(\lie{g}\), over \(\complex\) is semisimple if and only if the Killing form is non-degenerate.
        \begin{proof}
            Suppose that \(\lie{g}\) is semisimple, then we want to show that \(\kappa\) is non-degenerate.
            Since \(\lie{g}\) is semisimple it has no solveable ideals, and by \cref{lma:radical of killing form is ideal} is a solvable ideal, which means that we must have \(\lie{s} = 0\), and thus \(\kappa\) is non-degenerate as this shows that \(\iota_\kappa\) is an injection so the discussion following \cref{def:bilinear form} applies.
            
            Now suppose that \(\kappa\) is non-degenerate.
            Then \(\lie{s} = 0\).
            Let \(I \subseteq \lie{g}\) be a solveable ideal.
            By ... we know that \(I\) contains an abelian ideal, \(\lie{l}\).
            We will show that \(\lie{l} \subseteq \lie{s}\) and thus \(\lie{l} = 0\) and so \(I = 0\).
            Take elements \(X \in \lie{l}\) and \(Y \in \lie{g}\).
            We want to show that \(\kappa(X, Y) = 0\), since this implies that \(\lie{l} \subseteq \lie{s}\).
            To do this we consider the map \(\ad_X \circ \ad_Y \colon \lie{g} \to \lie{g}\), the trace of which is \(\kappa(X, Y)\).
            For any \(Z \in \lie{g}\) we always have that \(\bracket{X}{\bracket{Y}{Z}} \in \lie{l}\) because \(X \in \lie{l}\) and \(\lie{l}\) is an ideal.
            Further, since \(\lie{l}\) is abelian we have
            \begin{equation}
                \bracket{X}{\bracket{Y}{\bracket{X}{\bracket{Y}{Z}}}} = 0
            \end{equation}
            since \(X \in \lie{l}\) and \(\bracket{Y}{\bracket{X}{\bracket{Y}{Z}}} \in \lie{l}\) (since \(\bracket{X}{\bracket{Y}{Z}} \in \lie{l}\) and \(\lie{l}\) is an ideal), so this is indeed the bracket of two elements of the abelian ideal \(\lie{l}\) and so vanishes.
            We can then identify the operator \(\ad_X \circ \ad_Y\) in the above, as what we have here is
            \begin{equation}
                (\ad_X \circ \ad_Y)^2(Z) = 0.
            \end{equation}
            This holds for all \(Z \in \lie{g}\) and so we can conclude that \((\ad_X \circ \ad_Y)^2 = 0\).
            So, \(\ad_X\circ\ad_Y\) is a nilpotent operator.
            This means that \(\ad_X \circ \ad_Y\) has trace zero, since this is true of all nilpotent operators, and so we are done.
            This general fact follows because nilpotent operators are strictly upper triangular in some basis, and computing the trace in this basis must give zero, and trace is basis independent.
            An alternative proof of this general fact is that the minimal polynomial of \(T \colon V \to V\) if \(T^n = 0\) (and \(n\) is minimal in this respect) is \(m(t) = t^n\), and the roots of this are zero, and the roots of the minimal polynomial are the eigenvalues (without multiplicity) of the operator, and the trace is the sum of the eigenvalues (with multiplicity), and so here all the eigenvalues are zero and so their sum vanishes.
        \end{proof}
    \end{thm}
    
    
    
    
	
	% Appdendix
%	\appendixpage
%	\begin{appendices}
%	
%	\end{appendices}
	
	\backmatter
	\renewcommand{\glossaryname}{Acronyms}
	\printglossary[acronym]
	\printindex
\end{document}
