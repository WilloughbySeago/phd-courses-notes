% !TeX program = lualatex
\documentclass[fleqn]{NotesClass}

\strictpagecheck

\usepackage{csquotes}
\usepackage{enumitem}

\usepackage{tikz}
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz-external/]

\usepackage{tikz-cd}

\usepackage[pdfauthor={Willoughby Seago},pdftitle={Notes from Lie Theory Course},pdfkeywords={Lie theory, Lie algebra},pdfsubject={Lie algebras}]{hyperref}  % Should be loaded second last (cleveref last)
\colorlet{hyperrefcolor}{blue!60!black}
\hypersetup{colorlinks=true, linkcolor=hyperrefcolor, urlcolor=hyperrefcolor}
\usepackage[
capitalize,
nameinlink,
noabbrev
]{cleveref} % Should be loaded last

% My packages
\usepackage{NotesBoxes}
\usepackage{NotesMaths2}

\setmathfont[range={\int, \oint, \otimes, \oplus, \bigotimes, \bigoplus}]{Latin Modern Math}


% Highlight colour
\definecolor{highlight}{HTML}{710D78}
\definecolor{my blue}{HTML}{2A0D77}
\definecolor{my red}{HTML}{770D38}
\definecolor{my green}{HTML}{14770D}
\definecolor{my yellow}{HTML}{E7BB41}

% Title page info
\title{Lie Theory}
\author{Willoughby Seago}
\date{September 26th, 2024}
\subtitle{Notes from}
\subsubtitle{University of Glasgow}
\renewcommand{\abstracttext}{These are my notes from the master's course \emph{Lie Theory} taught at the University of Glasgow by Dr Dinakar Muthiah. I also found \href{https://www.youtube.com/playlist?list=PLVMgvCDIRy1zJ6iJHLC_lWgIwY4AhviZ-}{this video series}\footnote{\url{https://www.youtube.com/playlist?list=PLVMgvCDIRy1zJ6iJHLC_lWgIwY4AhviZ-}} from Michael Penn useful. These notes were last updated at \printtime{} on \today{}.}

\hyphenation{ei-gen-space ei-gen-spaces}

\AtBeginEnvironment{tikzcd}{\tikzexternaldisable}
\AtEndEnvironment{tikzcd}{\tikzexternalenable}

% Commands
% Maths
\renewcommand{\field}{\symbb{k}}
\newcommand{\cat}[1]{\symsfup{#1}}
\makeatletter
\newcommand{\c@egory}[1]{\symsfup{#1}}
\newcommand{\Vect}[1][\field]{#1\text{-}\c@egory{Vect}}
\newcommand{\LieAlg}[1][\field]{#1\text{-}\c@egory{LieAlg}}
\newcommand{\Rep}[1][\lie{g}]{#1\text{-}\c@egory{Rep}}
\makeatother
\DeclareMathOperator{\Hom}{Hom}
\DeclarePairedDelimiterX{\commutator}[2]{[}{]}{#1, #2}
\DeclarePairedDelimiterX{\bracket}[2]{[}{]}{#1, #2}
\newcommand{\isomorphic}{\cong}
\DeclareMathOperator{\Span}{span}
\newcommand{\id}{\symrm{id}}
\DeclareMathOperator{\Char}{char}
\RenewDocumentCommand{\matrices}{o m m}{%
    \IfNoValueTF{#1}{%
        \symrm{Mat}(#2, #3)
    }{%
        \symrm{Mat}(#1 \times #2, #3)
    }
}
\newcommand{\trans}{{\top}}
\newcommand{\hermit}{*}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\im}{im}
\newcommand{\ad}{\symrm{ad}}
\newcommand{\borelLie}{\symfrak{b}}
\newcommand{\nilpotentLie}{\symfrak{n}}
\DeclareMathOperator{\Der}{Der}
\DeclareMathOperator{\Perm}{Perm}
\newcommand{\centre}{\symfrak{z}}
\DeclareMathOperator{\rad}{rad}
\newcommand{\universalenveloping}{\symcal{U}}
\newcommand{\basis}{\symcal{B}}
\DeclareMathOperator{\res}{res}
\newcommand{\csa}{\symfrak{h}}

\begin{document}
	\frontmatter
	\titlepage
	\innertitlepage{}
	\tableofcontents
	% \listoffigures
	\mainmatter
	\chapter{Introduction}
	In this course we will study Lie algebras.
	The motivation for the definition of a Lie algebra is as a tool for studying Lie groups.
	Despite this we will rarely mention Lie groups, since the study of Lie groups requires a knowledge of differential geometry.
	We will stick to the study of Lie algebras, which can be defined purely algebraically.
	
	Roughly speaking, the storey is thus: Lie groups are both smooth manifolds and groups in a compatible way.
	By this, we mean that if \(G\) is a Lie group then the maps
	\begin{itemize}
		\item \(m \colon G \times G \to G\) given by \(m(x, y) = xy\)
	    \item \(i \colon G \to G\) given by \(i(g) = g^{-1}\)
	\end{itemize}
	are smooth and these maps satisfy the obvious requirements to make \(G\) a group.
    In fancy terms, a Lie group is a group object in the category of smooth manifolds.
    
    Lie groups were first used by Sophus Lie, who lends his name to these objects.
    Lie is pronounced Lee by the way, not lye.
    He was looking at the continuous symmetries possessed by the solutions to differential equations, and found that these naturally formed what we now know as a Lie group.
    The problem with studying Lie groups, apart from all of the topology, is that they are generally very non-linear.
    We can think of Lie algebras as being a linear approximation of Lie groups, given by expanding about the identity and discarding non-linear terms.
	
	To get a Lie algebra out of a Lie group we take the tangent space at the identity, \(T_eG\), which is a vector space.
	It turns out that the group commutator, \(\commutator{g}{h} = ghg^{-1}h^{-1}\), induces a natural operation on the tangent space, called the Lie bracket.
	A Lie algebra is then this tangent space equipped with this Lie bracket.
	
	It is possible to abstract the properties of this Lie bracket to define a Lie algebra without reference to a Lie group.
	It is also possible to go in reverse, given a Lie algebra there is always a corresponding Lie group.
	It should be noted that this assignment is not unique, in general many different Lie groups can have the same Lie algebra.
	The assignment does become invertible if we restrict ourselves to simply connected Lie groups, but then we're really getting into topology in a way that this course hopes to avoid.
	This assignment of a Lie algebra to a Lie group is functorial.
	
	\section{Notation}
	Throughout \(\field\) will denote a field.
	Often we will place further restrictions on \(\field\), such as being algebraically closed.
	Most of the time we'll be interested in the case \(\field = \complex\), with occasional use of \(\field = \reals\).
	Unless stated otherwise vector spaces are assumed to be vector spaces over \(\field\).
    
    We will denote by \(0\) the zero vector space (for a given field), \(\{0\}\), which consists of only the zero vector.
	
	If we have a category, \(\cat{C}\), we will denote by \(\cat{C}(A, B) = \Hom_{\cat{C}}(A, B)\) the set of all morphisms \(A \to B\), or simply \(\Hom(A, B)\) if \(\cat{C}\) is clear from context.
	This is just notation, we won't make much use of categories apart from borrowing some of the language.
	The main category we'll make use of is \(\Vect\), the category of vector spaces over \(\field\) and linear maps.
    
    We will mostly denote Lie algebras with lowercase fraktur letters, or so called \verb*|\mathfrak| letters.
    For reference, here's the alphabet in \verb*|\mathfrak|:
    \begin{equation*}
        \symfrak{a\,b\,c\,d\,e\,f\,g\,h\,i\,j\,k\,l\,m\,n\,o\,p\,q\,r\,s\,t\,u\,v\,w\,x\,y\,z}
    \end{equation*}
	
	\chapter{Direct Sums and Diagonalisation}
	Our goal in this section is to state a definition of a diagonalisable linear operator in such a basis free way.
	We will then generalise this to get an \enquote{almost diagonal form} for arbitrary linear operators when \(\field\) is algebraically closed.
	To this end we will recap some linear algebra in this first section.
	
	\section{Direct Sums}
	\subsection{Binary Direct Sums}
	\begin{dfn}{Binary Direct Sum}{}
		Let \(V\) be a vector space over \(\field\), and let \(U_1, U_2 \subseteq V\) be subspaces.
		Then we say that \(V = U_1 \oplus U_2\), that is \(V\) is the (internal) \defineindex{direct sum} of \(U_1\) and \(U_2\), if
		\begin{itemize}
			\item \(V = U_1 + U_2 \coloneq \{u_1 + u_2 \mid u_1 \in U_1 \text{ and } u_2 \in U_2\}\); and
			\item \(U_1 \cap U_2 = 0\).
		\end{itemize}
		
		If \(U_1\) and \(U_2\) are vector spaces over \(\field\) then we can construct a vector space \(V = U_1 \oplus U_2\), also over \(\field\), by defining
		\begin{equation}
			V = \{(u_1, u_2) \mid u_1 \in U_1 \text{ and } u_2 \in U_2\}
		\end{equation}
		and defining addition and scalar multiplication by
		\begin{equation}
			(u_1, u_2) + (u_1', u_2') = (u_1 + u_1', u_2 + u_2') \qqand \lambda (u_1, u_2) = (\lambda u_1, \lambda u_2)
		\end{equation}
        for all \(u_1, u_1' \in U_1\), \(u_2, u_2' \in U_2\), and \(\lambda \in \field\).
	\end{dfn}
	
    After constructing the external direct sum we may identify \(U_1\) with the subspace consisting of elements of the form \((u_1, 0)\) with \(u_1 \in U_1\), and \(U_2\) with the subspace of elements of the form \((0, u_2)\) with \(u_2 \in U_2\).
    Then the external direct sum coincides with the internal direct sum.
    For this reason we won't distinguish internal and external direct sums, making such identifications as necessary.
    
    The following lemma gives an alternative characterisation of the direct sum.
    
    \begin{lma}{}{lma:binary direct sum gives decompositon of vectors}
        If \(V = U_1 \oplus U_2\) then every vector \(v \in V\) can be written \emph{uniquely} as a sum \(v = u_1 + u_2\) with \(u_1 \in U_1\) and \(u_2 \in U_2\).
        Conversely, if every \(v\) has a unique decomposition as \(v = u_1 + u_2\) with \(u_1 \in U_1\) and \(u_2 \in U_2\) then \(V = U_1 \oplus U_2\).
        \begin{proof}
            Suppose that \(V = U_1 \oplus U_2\).
            From the definition of a direct sum we know that \(v \in V = U_1 + U_2\) and as such \(v = u_1 + u_2\) for some \(u_1 \in U_1\) and \(u_2 \in U_2\) because this is how elements of \(U_1 + U_2\) are defined.
            We need only prove uniqueness.
            Suppose that \(v = u_1 + u_2\) and \(v = u_1' + u_2'\) with \(u_1, u_1' \in U_1\) and \(u_2, u_2' \in U_2\) are two decompositions of \(v\).
            Then we have \(u_1 + u_2 = u_1' + u_2'\), which we can rearrange to get \((u_1 - u_1') + (u_2 - u_2') = 0\).
            This means that \(u_1 - u_1' = u_2' - u_2 \eqqcolon w\).
            Now, \(u_1 - u_1' \in U_1\), since it's a linear combination of elements of \(U_1\), and similarly \(u_2' - u_2 \in U_2\).
            Thus, \(w \in U_1 \cap U_2 = 0\) and so \(w = 0\).
            
            Suppose instead that every \(v \in V\) has a unique decomposition as \(v = u_1 + u_2\).
            Then clearly \(v\) corresponds to \((u_1, u_2) \in U_1 \oplus U_2\) and if \(v = u_1 + u_2\) and \(v' = u_1' + u_2'\) and \(\lambda \in \field\) then \(v + \lambda v' = (u_1 + u_2) + \lambda(u_1' + u_2')\) corresponds to \((u_1, u_2) + \lambda(u_1', u_2')\), but also \(v + \lambda v' = (u_1 + \lambda u_1') + (u_2 + \lambda u_2')\) corresponds to \((u_1 + \lambda u_1', u_2 + \lambda u_2')\).
            This shows that this correspondence defines a linear map.
            Clearly this correspondence is invertible, and thus we have an isomorphism \(V \isomorphic U_1 \oplus U_2\).
        \end{proof}
    \end{lma}
    
    Thus, the direct sum may be characterised as giving a unique decomposition of each vector into a pair of vectors from two subspaces with only the zero vector in common.
    
    \subsection{Finite Direct Sums}
    Direct sums of two vector spaces generalise to direct sums of a finite number of vector spaces in an obvious way.
    
    \begin{dfn}{Finite Direct Sum}{}
        Let \(V\) be a vector space over \(\field\), and let \(U_1, \dotsc, U_r \subseteq V\) be subspaces.
        Then we say that
        \begin{equation}
            V = U_1 \oplus \dotsb \oplus U_r = \bigoplus_{i=1}^r U_i,
        \end{equation}
        that is \(V\) is the (internal) \define{direct sum} of the \(U_i\), if
        \begin{itemize}
            \item \(V = U_1 + \dotsb + U_r \coloneq \{u_1 + \dotsb + u_r \mid u_i \in U_i \text{for all } i = 1, \dotsc, r\}\); and
            \item for all \(i = 1, \dotsc, r\) we have \(U_i \cap (U_1 + \dotsb + U_{i-1} + U_{i+1} + \dotsb + U_r) = 0\) where the sum is over all subspaces apart from \(U_i\).
        \end{itemize}
    \end{dfn}
    
    Note that we can define the external direct sum, but care has to be taken as if we define \(U_1 \oplus U_2 \oplus U_3\) to consist of elements of the form \((u_1, u_2, u_3)\) then this is not the same as defining \(U_1 \oplus (U_2 \oplus U_3)\) to consist of elements of the form \((u_1, (u_2, u_3))\) and \((U_1 \oplus U_2) \oplus U_3\) to consist of elements of the form \(((u_1, u_2), u_3)\).
    However these spaces are naturally (in the technical sense) isomorphic, and as such we will identify them with each other.
    A complicated way to put this is that the direct sum is associative up to natural isomorphism.
    An even more complicated way to put this, along with the fact that \(V \oplus 0 \isomorphic V \isomorphic 0 \oplus V\), is that \((\Vect, \oplus, 0)\) is a monoidal category.
    
    The same characterisation of the direct sum giving a unique decomposition of \(v \in V\) carries over to finite direct sums.
    
    \begin{lma}{}{}
        If \(V = U_1 \oplus \dotsb \oplus U_r\) then for all \(v \in V\) there exist unique \(u_i \in U_i\) such that \(v = u_1 + \dotsb + u_r\).
        \begin{proof}
            We proceed by induction on \(r\).
            The case \(r = 2\) is \cref{lma:binary direct sum gives decompositon of vectors}.
            Suppose that the result holds some \(k \ge 2\) and that \(V = U_1 \oplus \dotsb \oplus U_k \oplus U_{k+1}\).
            Take \(v \in V\).
            Writing \(V = (U_1 \oplus \dotsb \oplus U_k) \oplus U_{k+1}\) we see that there are unique vectors \(u \in U_1 \oplus \dotsb \oplus U_k\) and \(u_{k+1} \in U_{k+1}\) such that \(v = u + u_{k+1}\).
            By the induction hypothesis since \(U_1 \oplus \dotsb \oplus U_k\) is a \(k\)-fold direct sum we can uniquely decompose \(u\) as \(u = u_1 + \dotsb + u_k\).
            Then \(v = u_1 + \dotsb + u_k + u_{k+1}\) gives a unique decomposition of \(v\).
            Thus, by induction the result holds for any finite direct sum.
        \end{proof}
    \end{lma}
    
    \subsection{Arbitrary Direct Sums}
    We can further generalise direct sums to arbitrary collections of spaces.
    One thing we have to be cautious about is that infinite sums of vectors are not defined, and we get around this by considering sums over infinite sets, but requiring that only finitely many of the vectors in the sum are not zero.
    More formally, we can view a sequence \((u_i)_{i \in I}\) of vectors as a function \(u \colon I \to V\) with \(u(i) = u_i\) and then we require that \(u\) has finite support.
    
    \begin{dfn}{Arbitrary Direct Sums}{}
        Let \(V\) be a vector space over \(\field\), and let \(\{U_i\}_{i \in I}\) be an indexed family of subspaces, that is \(U_i \subseteq V\) is a subspace for all \(i \in I\).
        Then we say that
        \begin{equation}
            V = \bigoplus_{i \in I} U_i,
        \end{equation}
        that is \(V\) is the (internal) \define{direct sum} of the \(U_i\), if
        \begin{itemize}
            \item we have
            \begin{equation*}
                V = \sum_{i\in I} U_i \coloneq \bigg\{ \sum_{i \in I} u_i \,\bigg\vert\, u_i \in U_i \text{ and } u_i \ne 0 \text{ for only finitely many terms}\bigg\};
            \end{equation*}
            \item for all \(i \in I\) we have \(U_i \cap \sum_{j \in I \setminus \{i\}} U_j = 0\).
        \end{itemize}
    \end{dfn}
    
    We can define the external direct sum by taking a family of vector spaces, \(\{U_i\}_{i \in I}\), all over \(\field\), and defining
    \begin{equation}
        V = \bigoplus_{i \in I} U_i \coloneqq \{(u_i)_{i \in I} \mid u_i \text{ nonzero for only finitely many } i\} \subseteq \prod_{i \in I} U_i
    \end{equation}
    where
    \begin{equation}
        \prod_{i \in I} U_i = \{(u_i)_{i \in I}\}
    \end{equation}
    is the direct product of vector spaces (that is, the product in the category \(\Vect\)).
    Note that when \(I\) is finite the product and direct sum coincide.
    
    \begin{lma}{}{}
        If \(V = \bigoplus_{i \in I} U_i\) then for all \(v \in V\) there exist unique \(u_i \in U_i\) such that \(v = \sum_{i \in I} u_i\) with only finitely many of the \(u_i\) being nonzero.
        \begin{proof}
            The proof is essentially the same as before, but now we work with formally infinite sums in which most terms are zero, and all we have to do is check that at each stage this is still the case, which it is, since adding two such sums cannot result in infinitely many nonzero terms.
        \end{proof}
    \end{lma}
    
    \begin{exm}{}{}
        Let \(V = \field[x]\) be the vector space of polynomials in \(x\) with coefficients in \(\field\).
        That is
        \begin{equation}
            V = \Bigg\{ \sum_{i = 0}^N a_i x^i \,\Bigg\vert\, N \in \naturals \text{ and } a_i \in \field \Bigg\}.
        \end{equation}
        Addition is just addition of polynomials, and scaling is simply scaling each coefficient, which we can phrase as multiplying by the corresponding constant polynomial.
        We have the subspaces
        \begin{equation}
            U_i = \Span \{x^i\} \coloneq \{a x^i \mid a \in \field\}
        \end{equation}
        and it's easy to see that
        \begin{equation}
            V = \bigoplus_{i \in \naturals} U_i.
        \end{equation}
        Specifically, if we have a polynomial, \(f(x) \in \field[x]\), then
        \begin{equation}
            f(x) = a_0 + a_1x + a_2 x^2 + \dotsb + a_N x^N
        \end{equation}
        and \(a_0 \in U_0\), \(a_1 x \in U_1\), \(a_2 x^2 \in U_2\), and so on up to \(a_N x^N \in U_N\), so  we get a finite decomposition of \(f(x)\), and clearly this is unique, showing that \(\field[x]\) is indeed this direct sum as claimed.
        
        \begin{rmk}
            There's actually more structure here.
            First, we can multiply polynomials, so we have an algebra (see \cref{def:algebra}), not just a vector space.
            Define \(\deg f(t)\) to be the highest power of \(t\) appearing in \(f\) with a nonzero coefficient.
            Then \(U_i\) consists of all homogenous polynomials of degree \(i\) (homogenous meaning each term has the same degree).
            Further, since \(x^n x^m = x^{n + m}\) we have \(\deg (x^n x^m) = n + m\), and so \(U_iU_j \coloneq \{u_i u_j \mid u_i \in U_i \text{ and } u_j \in U_j\} = U_{i + j}\).
            When we have a decomposition like this and \(U_i U_j \subseteq U_{i + j}\) we say that \(V\) is an \(\naturals\)-graded algebra.
        \end{rmk}
    \end{exm}
    
    \begin{remark}{}{}
        The most general definition of the direct sum is as the coproduct in the category \(\Vect\).
        Recall (if you know anything about categories) that the coproduct \(V = \bigoplus_{i \in I} U_i\) comes equipped with inclusion maps \(\iota_i \colon U_i \hookrightarrow V\) which are such that if we have a family of maps \(f_i \colon U_i \to W\) for some other vector space \(W\) then there is a unique linear map \(f \colon V \to W\) such that \(f\iota_i = f_i\).
        This is the universal property of the coproduct, and can be summarised as the following diagram commuting for all \(j \in I\):
        \begin{equation}
            \tikzexternaldisable
            \begin{tikzcd}
                U_j \arrow[r, hookrightarrow, "\iota_j"] \arrow[dr, "f_j"'] & \displaystyle \bigoplus_{i \in I} U_i \arrow[d, dashed, "f", "\exists!"', pos=0.4]\\
                & W\mathrlap{.}
            \end{tikzcd}
            \tikzexternalenable
        \end{equation}
        
        The inclusion map \(\iota_i \colon U_i \hookrightarrow V\) is what allows us to identify \(U_i\) in the external direct sum with the subspace \(\iota_i(U_i) \subseteq V\), and since \(\iota_i\) is injective we have \(U_i \isomorphic \iota_i(U_i)\).
    \end{remark}
    
    \section{Diagonalisable Operators and the Direct Sum Decomposition}
    In this section we'll give the standard definition of a diagonalisable operator, which should be familiar.
    We'll then give an equivalent definition which makes no mention of a basis.
    
    Let \(V\) be a finite dimensional vector space over \(\field\) with basis \(\basis = \{v_1, \dotsc, v_n\}\).
    Recall that if \(T \colon V \to V\) is a linear operator then its matrix in basis \(\basis\) is the matrix
    \begin{equation}
        [T]_{\basis} \coloneq 
        \begin{bmatrix}
            \uparrow & & \uparrow \\
            Tv_1 & \dots & Tv_n \\
            \downarrow & & \downarrow
        \end{bmatrix}
        =
        \begin{bmatrix}
            c_{11} & c_{12} & \dots & c_{1n}\\
            c_{21} & c_{22} & \dots & c_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            c_{n1} & c_{n2} & \dots & c_{nn}
        \end{bmatrix}
    \end{equation}
    where \(c_{ij}\) is the coefficient of \(v_i\) in \(Tv_j\) when expressed in the basis \(\basis\).
    That is, the \(i\)th column of \([T]_{\basis}\) is \(Tv_i\) as a column vector in basis \(\basis\).
    
    \begin{dfn}{Diagonalisable Operator}{}
        Let \(V\) be a finite dimensional vector space over \(\field\).
        Then a linear operator, \(T \colon V \to V\), is \defineindex{diagonalisable} if there exists some basis, \(\basis\), in which the matrix \([T]_{\basis}\) is diagonal.
    \end{dfn}
    
    Notice that if \(T \colon V \to V\) is a linear operator diagonalised by the basis \(\basis = \{v_1, \dotsc, v_n\}\) then
    \begin{equation}
        [T]_{\basis} = 
        \begin{bmatrix}
            \lambda_1 & & \\
            & \ddots & \\
            & & \lambda_n
        \end{bmatrix}
    \end{equation} 
    for some \(\lambda_i \in \field\).
    We can also express the basis vectors \(v_i\) in this basis, they are simply the standard basis, in which \([v_i]_{\basis} = e_i\) is the column vector with 1 in the \(i\)th position and 0 everywhere else.
    Then, for example, we have
    \begin{equation}
        [Tv_1]_{\basis} = [T]_{\basis}[v_1]_{\basis} = 
        \begin{bmatrix}
            \lambda_1 & & \\
            & \ddots & \\
            & & \lambda_n
        \end{bmatrix}
        \begin{bmatrix}
            1\\ \vdots\\ 0
        \end{bmatrix}
        =
        \begin{bmatrix}
            \lambda_1\\ \vdots\\ 0
        \end{bmatrix}
        = \lambda_1
        \begin{bmatrix}
            1\\ \vdots\\ 0
        \end{bmatrix}
        = \lambda_1 [v_1]_{\basis}.
    \end{equation}
    Thus, we have \(Tv_1 = \lambda v_1\), that is, \(v_1\) is an eigenvector of \(T\) with eigenvalue \(\lambda_1\).
    In general, \(v_i\) will be an eigenvector of \(T\) with eigenvalue \(\lambda_i\).
    
    Now make the following observations.
    We may define the subspaces \(U_i = \Span\{v_i\}\).
    The fact that \(V = \bigoplus_{i=1}^n U_i\) follows immediately from (and is equivalent to) the fact that \(\basis\) is a basis.
    Each subspace \(U_i\) is \(T\)-\defineindex{invariant}, meaning that \(T(U_i) \subseteq U_i\), which simply means that if \(u \in U_i\) then \(Tu \in U_i\) also, so it's not possible to leave \(U_i\) just by the action of \(T\).
    This is clear in this case because \(T\) acts on each subspace, \(U_i\), by scalar multiplication, specifically, by multiplication by \(\lambda_i\).
    Because of this it makes sense to consider the restricted linear map \(T|_{U_i} \colon U_i \to U_i\), defined by \(T|_{U_i}(u) = T(u)\) for \(u \in U_i\).
    By definition \(T(u) = \lambda_i u\) for \(u \in U_i\), and thus we have \(T = \lambda_i \id_{U_i}\) where \(\id_W \colon W \to W\) is the identity linear map.
    
    \subsection{Basis Independent Definition}
    We can now move towards making a basis independent definition of diagonalisability.
    To do so we need the notion of an eigenspace.
    
    \begin{dfn}{Eigenspace}{}
        Let \(\field\) be a field and let \(V\) be a vector space over \(V\).
        Let \(T \colon V \to V\) be a linear operator.
        Then for \(\alpha \in \field\) define the \defineindex{eigenspace} corresponding to \(\alpha\) to be
        \begin{equation}
            W_\alpha \coloneq \{w \in V \mid T(w) = \alpha w\}.
        \end{equation}
    \end{dfn}
    
    Note that \(W_\alpha\) can alternatively be characterised as
    \begin{equation}
        W_\alpha = \{w \in V \mid (T - \alpha)(w) = 0\} = \ker(T - \alpha)
    \end{equation}
    where we perform the common abuse of notation writing \(T - \alpha\) when we mean \(T - \alpha \id_V\).
    
    For example, in the case of a diagonalisable operator where the eigenvalues are all distinct, as discussed at the end of the previous section, the subspaces \(U_i\) are exactly the eigenspaces \(W_{\lambda_i}\).
    Also, \(W_{\alpha} = 0\) if \(\alpha \ne \lambda_i\) for any \(i\).
    This is a general fact, if \(\alpha\) is not an eigenvalue then there will be no \(w \in V\) satisfying \(T(w) = \alpha w\).
    
    The problem is that eigenvalues needn't be distinct, in fact, eigenvalues are distinct if and only if the nonzero eigenspaces have dimension 1.
    Regardless of this problem, we can still perform the direct sum decomposition from the last part, replacing the \(U_i\) with the eigenspaces \(W_\alpha\).
    In fact, this gives us an equivalent definition of diagonalisability without reference to a basis.
    
    \begin{dfn}{Diagonalisable Operator}{}
        Let \(V\) be a finite dimensional vector space over \(\field\).
        Then a linear operator, \(T \colon V \to V\), is \defineindex{diagonalisable} if 
        \begin{equation}
            V = \bigoplus_{\alpha \in \field} W_\alpha.
        \end{equation}
    \end{dfn}
    
    \subsection{Decomposition Theorem}
    We now ask if there is a generalisation of this decomposition to not-necessarily-diagonalisable operators.
    We'll start with an example.
    
    \begin{exm}{Non-diagonalisable Operator}{exm:E not diagonalisable}
        Consider the case \(\field = \complex\), \(V = \complex^2\), and
        \begin{equation}
            T = 
            \begin{bmatrix}
                0 & 1\\
                0 & 0
            \end{bmatrix}
        \end{equation}
        in the standard basis
        \begin{equation}
            e_1 = 
            \begin{bmatrix}
                1\\ 0
            \end{bmatrix},
            \qqand e_2 = 
            \begin{bmatrix}
                0\\ 1
            \end{bmatrix}
            .
        \end{equation}
        To find the eigenvalues we solve the characteristic polynomial, which is
        \begin{equation}
            \chi_T(t) = \det(\lambda - T) = \det
            \begin{bmatrix}
                0 & 1\\
                0 & 0
            \end{bmatrix}
            = \lambda^2.
        \end{equation}
        The only solution to \(\lambda^2 = 0\) is \(\lambda = 0\), and thus we have one repeated eigenvalue.
        This means that \(T\) is not diagonalisable.
        For \(\alpha \ne 0\) we have \(W_\alpha = 0\), and we have
        \begin{equation}
            W_0 = \{w \in V \mid Tw = 0\} = \ker(T - 0) = \ker T = \Span\{e_1\}.
        \end{equation}
        This follows simply by considering the action of \(T\) on the basis vectors, \(Te_1 = 0\) and \(Te_2 = e_1 \ne 0\).
        This means that
        \begin{equation}
            \bigoplus_{\alpha \in \complex} W_{\alpha} = W_0 = \Span\{e_1\} \ne \complex^2.
        \end{equation}
        
        From this calculation we see that \(T^2e_2 = 0\), and so if we instead consider the operator \(T^2\) then the eigenspaces of this cover all of \(\complex^2\), so this suggests that we should look not just at \(T\), but also powers of \(T\) in our decomposition.
    \end{exm}
    
    \begin{remark}{}{}
        The matrix
        \begin{equation}
            T = 
            \begin{bmatrix}
                0 & 1\\
                0 & 0
            \end{bmatrix}
            = E
        \end{equation}
        is important for the Lie algebra \(\specialLinearLie(2, \complex)\), which is one of the most important Lie algebras.
        The fact that this matrix is not diagonalisable is important in that it effects whether \(E\) commutes with other matrices.
        Recall that if two matrices can be simultaneously diagonalised then they commute, an important fact in quantum mechanics.
    \end{remark}
    
    Following on from the idea that we should look at powers of \(T\) we replace the eigenspace with the following definition.
    
    \begin{dfn}{Generalised Eigenspace}{}
        Let \(\field\) be a field and let \(T\) be a vector space over \(\field\).
        Let \(T \colon V \to V\) be a linear operator.
        Then for \(\alpha \in \field\) define the \defineindex{generalised eigenspace} corresponding to \(\alpha\) to be
        \begin{equation}
            V_\alpha \coloneq \{w \in V \mid (T - \alpha)^N(w) = 0 \text{ for some } N \in \naturals\}.
        \end{equation}
    \end{dfn}
    
    We can then often make a decomposition into a direct sum of generalised eigenspaces.
    There's just one hitch, this only works if \(\field\) is algebraically closed.
    
    \begin{dfn}{Algebraically Closed Field}{}
        A field, \(\field\), is algebraically closed if every nonconstant polynomial \(f(t) \in \field[t]\) has a root, \(\alpha \in \field\), such that \(f(\alpha) = 0\).
    \end{dfn}
    
    The complex numbers, \(\complex\), are algebraically closed, this is the fundamental theorem of algebra.
    The real numbers, \(\reals\), are not algebraically closed, for example \(t^2 + 1\) has no (real) roots.
    
    The following proof relies on three standard results, which we state without proof.
    \begin{thm}{Cayley--Hamilton}{}
        If \(\chi_T(t) = t^n + a_{n-1}t^{n-1} + \dotsb + a_1t + a_0\) is the characteristic polynomial of the linear operator \(T \colon V \to V\) then the linear operator
        \begin{equation}
            T^n + a_{n-1}T^{n-1} + \dotsb + a_1T + a_0\id_V \colon V \to V
        \end{equation}
        is the zero map.
        That is, \(T\) satisfies it's own characteristic polynomial, \(\chi_T(T) = 0\).
    \end{thm}
    
    \begin{lma}{B\'ezout's Lemma}{}
        If \(R\) is a principal ideal domain and \(x, y \in R\) have greatest common divisor \(d\) then there exist \(a, b \in R\) such that \(ax + by = d\).
    \end{lma}
    
    \begin{lma}{}{}
        If \(\field\) is a field then \(\field[x]\) is a PID, and in particular B\'ezout's lemma applies to polynomials.
    \end{lma}
    
    \begin{thm}{Decomposition Theorem}{thm:decomposition}
        Let \(\field\) be an algebraically closed field, \(V\) a finite dimensional vector space, and \(T \colon V \to V\) a linear operator.
        Then
        \begin{equation}
            V = \bigoplus_{\alpha \in \field} V_\alpha 
        \end{equation}
        where \(V_\alpha\) is the generalised eigenspace associated with \(\alpha\) and only finitely many of the \(V_\alpha\) are nonzero.
        \begin{proof}
            We start by showing that \(V_\alpha \cap \sum_{\beta \in \field \setminus \{\alpha\}} V_\beta = 0\).
            Take \(v V_\alpha \cap \sum_{\beta \in \field \setminus \alpha} V_\beta\), then we have \(v \in V_\alpha\) and \(v \in V_{\beta_1} + \dotsb + V_{\beta_r}\) for some \(\beta_1, \dotsc, \beta_r \in \field \setminus \{\alpha\}\).
            This means that there exist some \(N, N_1, \dotsc, N_r \in \naturals\) such that
            \begin{equation}
                (T - \alpha)^Nv = (T - \beta_1)^{N_1} \dotsm (T - \beta_r)^{N_r} v = 0.
            \end{equation}
            Since \(\alpha \ne \beta_i\) we know that the greatest common factor of \((t - \alpha)^N\) and \((t - \beta_1)^{N_1} \dotsm (t - \beta_r)^{N_r}\) is 1.
            Thus, by B\'ezout's lemma there exist polynomials \(f(t), g(t) \in \field[t]\) such that
            \begin{equation}
                f(t)(t - \alpha)^N + g(t)(t - \beta_1)^{N_1} \dotsm (t - \beta_r)^{N_r} = 1.
            \end{equation}
            Evaluating at \(T\) and applying this to \(v\) we then have
            \begin{equation}
                f(T)(T - \alpha)^Nv + g(T)(T - \beta_1)^{N_1}v \dotsm (T - \beta_r)^{N_r}v = v,
            \end{equation}
            and we know that these operators acting on \(v\) both give zero, so the left hand side vanishes, and thus \(v = 0\), and so \(V_\alpha \cap \sum_{\beta \in \field \setminus \{\alpha\}} V_\beta = 0\).
            
            We now show that \(\sum_{\alpha \in \field} V_\alpha = V\).
            Factorise the characteristic polynomial, \(\chi_T(t)\), as follows
            \begin{equation}
                \chi_T(t) = (t - \alpha_1)^{N_1} \dotsm (t - \alpha_s)^{N_s},
            \end{equation}
            with \(\alpha_i \ne \alpha_j\) for \(i \ne j\).
            Note that the existence of such a factorisation relies on \(\field\) being algebraically closed.
            We claim that \(v \in V_{\alpha_1} + \dotsb + V_{\alpha_s}\).
            This can be proven by induction on \(s\).
            The basis case, \(s = 1\), has the characteristic polynomial factorise as
            \begin{equation}
                \chi_T(t) = (t - \alpha_1)^{N_1}.
            \end{equation}
            Then by the Cayley--Hamilton theorem we have
            \begin{equation}
                (T - \alpha_1)^{N_1}v = 0
            \end{equation}
            for all \(v \in V\), and thus \(V_{\alpha_1} = V\).
            For the inductive step suppose that \(s > 1\) and the statement is true for \(s - 1\).
            The highest common factor of \((t - \alpha_s)^{N_s}\) and \((t - \alpha_1)^{N_1} \dotsm (t - \alpha_{s-1})^{N_{s-1}}\) is 1.
            Thus, by B\'ezout's lemma there exist \(f(t), g(t) \in \field[t]\) such that
            \begin{equation}
                f(t)(t - \alpha_s)^{N_s} + g(t)(t - \alpha_1)^{N_1} \dotsm (t - \alpha_{s-1})^{N_{s-1}} = 1.
            \end{equation}
            Evaluating at \(T\) and applying the map to \(v \in V\) we have
            \begin{equation}
                f(T)(T - \alpha_s)^{N_s}v + g(T)(T - \alpha_1)^{N_1} \dotsm (T - \alpha_{s-1})^{N_{s-1}}v = v.
            \end{equation}
            Define
            \begin{equation}
                v' = f(T)(T - \alpha_s)^{N_s}v, \qqand v_s = g(T)(T - \alpha_1)^{N_1} \dotsm (T - \alpha_{s-1})^{N_{s-1}}v,
            \end{equation}
            so we have \(v = v' + v_s\).
            Note that we have \((T - \alpha_s)^{N_s}v_s = 0\) and \((T - \alpha_1)^{N_1} \dotsm (T - \alpha_{s-1})^{N_{s-1}}v' = 0\).
            This is simply the Cayley--Hamilton theorem applied to the linear map \(T\) restricted to the subspaces corresponding to \(v_s\) and \(v'\) respectively.
            Define
            \begin{equation}
                V' = \{u \in V \mid (T - \alpha_1)^{N_1} \dotsm (T - \alpha_{s-1})^{N_{s-1}}u = 0\}.
            \end{equation}
            Then \(v_s \in V_{\alpha_s}\) and \(v' \in V'\).
            The characteristic polynomial of the restricted map, \(T|_{V'}\), has all of its roots in \(\{\alpha_1, \dotsc, \alpha_{s-1}\}\).
            By induction, we therefore have \(v' = v_1 + \dotsb + v_{s-1}\) with \(v_i \in V_{\alpha_i}' = V' \cap V_{\alpha_i}\).
            Thus, \(v = v_1 + \dotsb + v_{s-1} + v_s in V_{\alpha_1} + \dotsb + V_{\alpha_s}\).
            Since \(v \in V\) was arbitrary this shows that \(V = V_{\alpha_1} + \dotsb + V_{\alpha_s}\), proving that \(V = V_{\alpha_1} \oplus \dotsb \oplus V_{\alpha_s}\).
            This gives the final result by realising that if \(\beta \ne \alpha_i\) then \(V_\beta = 0\) since \(V_\beta \cap (V_{\alpha_1} + \dotsb + V_{\alpha_s}) = 0\) so \(V_\beta \cap V = 0\), so \(V_\beta = 0\).
            Thus, \(V = \bigoplus_{\alpha \in \field} V_{\alpha}\) and \(V_{\alpha}\) is only nonzero for the finite index set \(\{\alpha_1, \dotsc, \alpha_s\}\).
        \end{proof}
    \end{thm}
    
    Note that both conditions, the finite dimension of \(V\) and the algebraic closure of \(\field\), are required here.
    Dropping either condition will result in cases where this decomposition doesn't exist.
    
    \begin{exm}{}{}
        Consider again the example of \(\field = \complex\), \(V = \complex^2\), and
        \begin{equation}
            T = 
            \begin{bmatrix}
                0 & 1\\
                0 & 0
            \end{bmatrix}
            .
        \end{equation}
        As we saw before (\cref{exm:E not diagonalisable}) \(T\) is not diagonalisable, and \(\bigoplus_{\alpha \in \complex} W_\alpha = W_0 = \Span\{e_1\} \ne \complex^2\).
        We also saw there that \(T^2e_2 = 0\), meaning that when considering generalised eigenspaces we also have \(e_2 \in V_0\), and thus
        \begin{equation}
            \bigoplus_{\alpha \in \complex} V_\alpha = V_0 = \complex^2.
        \end{equation}
    \end{exm}
    
    \begin{dfn}{Jordan Block and Jordan Normal Form}{}
        A \defineindex{Jordan block} is a matrix of the form
        \begin{equation}
            \begin{bmatrix}
                \alpha & 1 \\
                & \alpha & 1 \\
                & & \ddots & \ddots\\
                & & & \alpha & 1\\
                & & & & \alpha
            \end{bmatrix}
            .
        \end{equation}
        That is, we have some \(\alpha \in \field\) on the diagonal and 1s above the diagonal, and 0 everywhere else.
        
        A matrix, \(X\), is a sum of Jordan blocks, or is in \defineindex{Jordan normal form} if it is given by the block diagonal matrix
        \begin{equation}
            X = 
            \begin{bmatrix}
                J_1\\
                & \ddots \\
                & & J_r
            \end{bmatrix}
        \end{equation}
        with the \(J_i\) Jordan blocks.
    \end{dfn}
    
    \begin{crl}{Existence of Jordan Normal Form}{}
        Let \(\field\) be an algebraically closed field, let \(V\) be a finite dimensional vector space, and let \(T \colon V \to V\) be a linear map.
        There exists a basis of \(V\) such that \([T]_{\basis}\) is in Jordan normal form.
        \begin{proof}
            Consider the decomposition of \(V\) into a sum of generalised eigenspaces of \(T\).
            Each generalised eigenspace is invariant under \(T\), and thus if \(v_\alpha \in V_\alpha\) then \(Tv_\alpha = c_1v_1 + \dotsb c_rv_r\) where \(\{v_1, \dotsc, v_r\}\) is a basis for \(V_\alpha\).
            We can further choose this basis by scaling and taking linear combinations untile \(Tv_{i} = \alpha v_i + v_{i+1}\), which can be shown by induction on the size of the dimension of \(V_\alpha\).
            Then \(T\) acts on \(V_\alpha\) in this basis as a Jordan block, and so \(T\) acts on \(V\), in the basis formed by the union of these bases, as a matrix in Jordan normal form.
        \end{proof}
    \end{crl}
    
    \chapter{Lie Algebras}
    \section{Definition and Remarks}
    \begin{dfn}{Lie Algebra}{}
        Let \(\field\) be a field.
        A \defineindex{Lie algebra} over \(\field\) is a vector space, \(\lie{g}\), over \(\field\) equipped with a bilinear map,
        \begin{align}
            \bracket{-}{-} \colon \lie{g} \times \lie{g} &\to \lie{g}\\
            (X, Y) &\mapsto \bracket{X}{Y},
        \end{align}
        called the Lie bracket, or simply the bracket.
        This must satisfy two properties:
        \begin{enumerate}
            \item \defineindex{alternating}: for all \(X \in \lie{g}\) we have \(\bracket{X}{X} = 0\);
            \item \defineindex{Jacobi identity}: for all \(X, Y, Z \in \lie{g}\) we have
            \begin{equation}
                \bracket{X}{\bracket{Y}{Z}} + \bracket{Y}{\bracket{Z}{X}} + \bracket{Z}{\bracket{X}{Y}} = 0.
            \end{equation}
        \end{enumerate}
    \end{dfn}
    
    Bilinearity means that for all \(\lambda \in \field\) and \(X, Y, Z \in \lie{g}\) we have
    \begin{align}
        \bracket{X + \lambda Y}{Z} &= \bracket{X}{Z} + \lambda \bracket{Y}{Z},\\
        \bracket{X}{Y + \lambda Z} &= \bracket{X}{Y} + \lambda \bracket{X}{Z}.
    \end{align}
    
    We say that \(\bracket{-}{-}\) is antisymmetric if \(\bracket{X}{Y} = -\bracket{Y}{X}\).
    It turns out that this property is almost the same as, but not quite, the alternating property, and as such the definition of a Lie algebra is often given with antisymmetry in place of alternativity, the catch being that over fields of characteristic 2 these aren't equivalent.
    Alternativity is the \enquote{correct} condition, but if we're only looking at \(\field = \complex, \reals\), as is often the case, then there's little harm in taking antisymmetry as the defining condition.
    
    \begin{lma}{}{}
        The Lie bracket is antisymmetric.
        \begin{proof}
            Let \(\lie{g}\) be a Lie algebra and take \(X, Y \in \lie{g}\).
            Consider the bracket of \(X + Y\) with itself.
            On the one hand, alternativity tells us that \(\bracket{X + Y}{X + Y} = 0\), and on the other hand using bilinearity and alternativity we have
            \begin{align}
                \bracket{X + Y}{X + Y} &= \bracket{X}{X} + \bracket{X}{Y} + \bracket{Y}{X} + \bracket{Y}{Y}\\
                &= \bracket{X}{Y} + \bracket{Y}{X}
            \end{align}
            Now, if this is to vanish we must have
            \begin{equation}
                \bracket{X}{Y} = -\bracket{Y}{X}. \qedhere
            \end{equation}
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{}
        Over a field of characteristic other than 2 antisymmetry of a bilinear bracket implies alternativity.
        \begin{proof}
            Let \(\lie{g}\) be a vector space over \(\field\) with \(\Char \field \ne 2\), equipped with an \emph{antisymmetric} bracket, \(\bracket{-}{-} \colon \lie{g} \times \lie{g} \to \lie{g}\).
            Antisymmetry means that \(\bracket{X}{Y} = -\bracket{Y}{X}\) for all \(X, Y \in \lie{g}\).
            In particular, we are free to take \(X = Y\), then we have \(\bracket{X}{X} = -\bracket{X}{X}\).
            Rearranging this gives
            \begin{equation}
                \bracket{X}{X} + \bracket{X}{X} = 2 \bracket{X}{X} = 0.
            \end{equation}
            Now, in a field with \(2 \ne 0\) (that is, in a field of characteristic other than 2) this immediately implies \(\bracket{X}{X} = 0\).
        \end{proof}
    \end{lma}
    
    This distinction is subtle, and ultimately not that important since we'll mostly concern ourselves with \(\field = \complex\), which has characteristic 0 (and \(\Char \reals = 0\) also).
    
    The Jacobi identity is a bit weird the first time you see it.
    There are a couple of ways to think about it that help to remember the definition.
    The first is to notice that the second and third term are cyclic permutations of the first, so it's often shortened to
    \begin{equation}
        \bracket{X}{\bracket{Y}{Z}} + \text{cycilc permutations} = 0.
    \end{equation}
    The second is to write it like
    \begin{equation}
        \bracket{X}{\bracket{Y}{Z}} = \bracket{\bracket{X}{Y}}{Z} + \bracket{Y}{\bracket{X}{Z}},
    \end{equation}
    which is an equivalent form.
    This doesn't look much simpler, but fix some \(X \in \lie{g}\) and define a linear map \(D \colon \lie{g} \to \lie{g}\) by \(D(Y) = \bracket{X}{Y}\).
    Then this becomes
    \begin{equation}
        D(\bracket{Y}{Z}) = \bracket{D(Y)}{Z} + \bracket{Y}{D(Z)}.
    \end{equation}
    To make this even simpler, write \(A \cdot B\) in place of \(\bracket{A}{B}\), and we have
    \begin{equation}
        D(Y \cdot Z) = D(Y) \cdot Z + Y \cdot D(Z)
    \end{equation}
    and we see that this is a version of the product rule, or Leibniz rule.
    So \(D\) acts a bit like a derivative.
    The fancy way to say this is that the adjoint representation of \(\lie{g}\) acts on \(\lie{g}\) by derivations.
    A \defineindex{derivation} is just any linear map on an algebra satisfying the Leibniz rule with respect to the product of that algebra, which for a Lie group is the Lie bracket.
    The \enquote{adjoint representation} (\cref{def:adjoint rep}) is simply \(\lie{g}\) acting on itself where \(X\) acts on \(Y\) by \(X \cdot Y \mapsto \bracket{X}{Y}\).
    
    Now\footnote{I've brought this forwards in the notes, it feels wrong to define objects and not move on to morphisms immediately.} that we've defined Lie algebras, an algebraic object, we should define maps between them.
    The appropriate maps will be \enquote{structure preserving}, in the same way that a group homomorphism is structure preserving (it preserves the multiplicative structure) and a linear map is structure preserving (it preserves addition and scalar multiplication).
    The structure that we have to preserve here is the Lie bracket, as well as the underlying vector space structure of the Lie algebra.
    
    \begin{dfn}{Homomorphism}{}
        Let \(\lie{g}\) be a Lie algebra with bracket \(\bracket{-}{-}_{\lie{g}}\), and let \(\lie{h}\) be a Lie algebra with bracket \(\bracket{-}{-}_{\lie{h}}\).
        Then a \defineindex{Lie algebra homomorphism}, \(\varphi \colon \lie{g} \to \lie{h}\), is a linear map which preserves the bracket, meaning that for all \(X, Y \in \lie{g}\) we have
        \begin{equation}
            \varphi(\bracket{X}{Y}_{\lie{g}}) = \bracket{\varphi(X)}{\varphi(Y)}_{\lie{h}}.
        \end{equation}
        An invertible Lie algebra homomorphism is a \defineindex{Lie algebra isomorphism}.
    \end{dfn}
    
    We'll usually just say \enquote{homomorphism}, or simply \enquote{morphism}, rather than \enquote{Lie algebra homomorphism}, and likewise we'll just speak of an \enquote{isomorphism} rather than a \enquote{Lie algebra isomorphism}.
    We will writhe \(\lie{g} \isomorphic \lie{h}\) if there is a Lie algebra isomorphism \(\lie{g} \to \lie{h}\).
    For notational simplicity we'll usually write \(\bracket{-}{-}\) for the bracket of both \(\lie{g}\) and \(\lie{h}\), allowing context (i.e., what elements we put into it) to make clear which we mean.
    
    We now prove the standard results about morphisms.
    
    \begin{lma}{}{}
        If \(\varphi \colon \lie{g} \to \lie{h}\) is a Lie algebra isomorphism then \(\varphi^{-1} \colon \lie{h} \to \lie{g}\) is a Lie algebra isomorphism.
        \begin{proof}
            We need to show that for all \(X', Y' \in \lie{h}\) we have
            \begin{equation}
                \varphi^{-1}(\bracket{X'}{Y'}_{\lie{h}}) = \bracket{\varphi^{-1}(X')}{\varphi^{-1}(Y')}_{\lie{g}}.
            \end{equation}
            To do so note that \(\varphi^{-1}\) is an invertible linear map, and as such is a bijection.
            This means that there exist unique \(X, Y \in \lie{g}\) such that \(\varphi(X) = X'\) and \(\varphi(Y) = Y'\).
            Thus, we may write the left hand side as
            \begin{equation}
                \varphi^{-1}(\bracket{X'}{Y'}_{\lie{h}}) = \varphi^{-1}(\bracket{\varphi(X)}{\varphi(Y)}_{\lie{h}}).
            \end{equation}
            Using the fact that \(\varphi\) is an isomorphism, and hence is a homomorphism, we can pull the \(\varphi\) out of the bracket on the right, to give
            \begin{equation}
                \varphi^{-1}(\bracket{X'}{Y'}_{\lie{h}}) = \varphi^{-1}(\varphi(\bracket{X}{Y}_{\lie{g}})).
            \end{equation}
            Then, since by definition \(\varphi^{-1} \circ \varphi = \id_{\lie{g}}\) we have
            \begin{equation}
                \varphi^{-1}(\bracket{X'}{Y'}_{\lie{h}}) = \bracket{X}{Y}_{\lie{g}}.
            \end{equation}
            We can then invert \(X' = \varphi(X)\) and \(Y' = \varphi(Y)\) to write \(X = \varphi^{-1}(X')\) and \(Y = \varphi^{-1}(Y')\).
            This lets us write this as
            \begin{equation}
                \varphi^{-1}(\bracket{X'}{Y'}_{\lie{h}}) = \bracket{\varphi^{-1}(X')}{\varphi^{-1}(Y')}_{\lie{g}},
            \end{equation}
            which is the result we wanted.
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{}
        The composite of Lie algebra homomorphisms is a Lie algebra homomorphism.
        \begin{proof}
            Let \(\lie{g}\), \(\lie{h}\), and \(\lie{l}\) be Lie algebras with brackets \(\bracket{-}{-}_{\lie{g}}\), \(\bracket{-}{-}_{\lie{h}}\), and \(\bracket{-}{-}_{\lie{l}}\) respectively.
            Let \(\varphi \colon \lie{g} \to \lie{h}\) and \(\psi \colon \lie{h} \to \lie{l}\) be Lie algebra homomorphisms.
            Then we may consider \(\psi \circ \varphi \colon \lie{g} \to \lie{l}\).
            We wish to show that this is a Lie algebra homomorphism.
            
            First, note that \(\psi\) and \(\varphi\) are linear, so their composite is too.
            We then need only show that for all \(X, Y \in \lie{g}\) we have
            \begin{equation}
                (\psi \circ \varphi)(\bracket{X}{Y}_{\lie{g}}) = \bracket{(\psi \circ \varphi)(X)}{(\psi \circ \varphi)(Y)}_{\lie{l}}.
            \end{equation}
            Starting with the left-hand side we can use the definition of composition to write
            \begin{equation}
                (\psi \circ \varphi)(\bracket{X}{Y}_{\lie{g}}) = \psi(\varphi(\bracket{X}{Y}_{\lie{g}})).
            \end{equation}
            Using the fact that \(\varphi\) is a Lie algebra homomorphism into \(\lie{h}\) we have \(\varphi(\bracket{X}{Y}_{\lie{g}}) = \bracket{\varphi(X)}{\varphi(Y)}_{\lie{h}}\), so we have
            \begin{equation}
                (\psi \circ \varphi)(\bracket{X}{Y}_{\lie{g}}) = \psi(\bracket{\varphi(X)}{\varphi(Y)}_{\lie{h}}).
            \end{equation}
            Now, using the fact that \(\psi\) is a Lie algebra homomorphism into \(\lie{l}\) we have \(\psi(\bracket{X'}{Y'}_{\lie{h}}) = \bracket{\psi(X')}{\psi(Y')}_{\lie{l}}\) for any \(X', Y' \in \lie{h}\), and in particular this is true when \(X' = \varphi(X)\) and \(Y' = \varphi(Y)\), giving
            \begin{equation}
                (\psi \circ \varphi)(\bracket{X}{Y}_{\lie{g}}) = \bracket{\psi(\varphi(X))}{\psi(\varphi(Y))}_{\lie{l}}.
            \end{equation}
            Finally, using the definition of composition again, we get our result,
            \begin{equation}
                (\psi \circ \varphi)(\bracket{X}{Y}_{\lie{g}}) = \bracket{(\psi \circ \varphi)(X)}{(\psi \circ \varphi)(Y)}_{\lie{l}}. \qedhere
            \end{equation}
        \end{proof}
    \end{lma}
    
    Noting that function composition is associative we have the following result.
    
    \begin{crl}{}{}
        For a field \(\field\) there is a category \(\LieAlg\) whose objects are Lie algebras over \(\field\) and whose morphisms are Lie algebra homomorphisms.
    \end{crl}
    
    The objects of this category, Lie algebras, are vector spaces, and the morphisms are linear maps.
    This means that \(\LieAlg\) is a subcategory of \(\Vect\), and as such inherits a lot of its structure from \(\Vect\).
    For example, the coproduct (direct sum) of Lie algebras is just the coproduct (direct sum) of the underlying vector spaces, equipped with an appropriate bracket.
    That is, given Lie algebras \(\lie{g}\) and \(\lie{h}\) we can equip the vector space \(\lie{g} \oplus \lie{h}\) with a Lie algebra structure by defining
    \begin{equation}
        \bracket{X_1 + Y_1}{X_2 + Y_2}_{\lie{g} \oplus \lie{h}} = \bracket{X_1}{X_2}_{\lie{g}} + \bracket{Y_1}{Y_2}_{\lie{h}}
    \end{equation} 
    where \(X_1 + Y_1\) and \(X_2 + Y_2\) are two arbitrary elements of \(\lie{g} \oplus \lie{h}\) decomposed into \(X_1, X_2 \in \lie{g}\) and \(Y_1, Y_2 \in \lie{h}\).
    The alternativity and Jacobi identity for \(\lie{g}\) and \(\lie{h}\) apply directly when brackets in \(\lie{g} \oplus \lie{h}\) are written as on the right-hand side here, and so these properties also hold for the bracket on \(\lie{g} \oplus \lie{h}\).
    This gives the external direct sum.
    We can similarly give an internal direct sum by saying that if \(\lie{g}\) is a Lie algebra with subalgebras \(\lie{h}_1\) and \(\lie{h}_2\) then \(\lie{g} = \lie{h}_1 \oplus \lie{h}_2\) if \(\lie{g} = \lie{h}_1 \oplus \lie{h}_2\) as vector spaces and for all \(X \in \lie{h}_1\) and \(Y \in \lie{h}_2\) we have \(\bracket{X}{Y} = 0\).
    The idea here is that the two parts of the direct sum should not \enquote{interact}.
    For vector spaces this simply means that they should be disjoint except for 0, and for Lie algebras this means that elements from the two parts should commute (which is what we say when the bracket vanishes, since often the bracket is literally a commutator).
        
    \section{Subalgebras, Ideals, and Quotients}
    To state these definitions it's useful to abuse the notation slightly as follows.
    Let \(U, V \subseteq \lie{g}\) be subspaces of a Lie algebra.
    Define the subspace
    \begin{equation}
        \bracket{U}{V} = \Span\{\bracket{u}{v} \mid u \in U \text{ and } v \in V\}.
    \end{equation}
    
    \begin{dfn}{Subalgebra}{}
        Let \(\lie{g}\) be a Lie algebra.
        A \defineindex{Lie subalgebra} (or just \defineindex{subalgebra}), \(\lie{h}\), is a subspace which is closed under the Lie bracket.
        That is, \(\bracket{\lie{h}}{\lie{h}} \subseteq \lie{h}\), or equivalently for all \(h, h' \in \lie{h}\) we have \(\bracket{h}{h'} \in \lie{h}\).
    \end{dfn}
    
    The notion of a Lie subalgebra is the Lie algebra analogue of a subset of a set, subspace of a vector space, subgroup of a group, or subring of a ring.
    We can't form a quotient by an arbitrary subset, subgroup or subring, we need a set generated by an equivalence relation, a normal subgroup, or an ideal.
    This is exactly the case with Lie algebras as well.
    
    Note that a Lie subalgebra, \(\lie{h}\), is necessarily a Lie algebra in its own right after restricting the bracket to \(\lie{h}\).
    This follows immediately because the requirements for a Lie algebra are universally quantified so remain true after restricting to a subspace.
    
    \begin{dfn}{Ideal}{}
        Let \(\lie{g}\) be a Lie algebra.
        An \defineindex{ideal} of \(\lie{g}\) is a Lie subalgebra, \(\lie{i}\), such that \(\bracket{\lie{i}}{\lie{g}} \subseteq \lie{i}\), or equivalently for all \(X \in \lie{g}\) and \(I \in \lie{i}\) we have \(\bracket{X}{I} \in \lie{i}\).
    \end{dfn}
    
    Note that there's no notion of left- or right-ideals, any ideal is two sided since \(\bracket{\lie{h}}{\lie{h}'} = \bracket{\lie{h}}{\lie{h}'}\), as elements of the two differ only by a sign and the fact that these are subspaces means that all the elements that differ only by a sign are also included.
    
    With ideals we can define quotients.
    
    \begin{dfn}{Quotient}{}
        Let \(\lie{g}\) be a Lie algebra, and let \(\lie{i} \subseteq \lie{g}\) be an ideal.
        Then the \defineindex{quotient} vector space \(\lie{g}/\lie{i}\) is a Lie algebra when we define the bracket by
        \begin{equation}
            \bracket{X + \lie{i}}{Y + \lie{i}} = \bracket{X}{Y} + \lie{i}.
        \end{equation}
    \end{dfn}
    
    Note that in the definition of the bracket of \(\lie{g}/\lie{i}\) the bracket \(\bracket{X + \lie{i}}{Y + \lie{i}}\) is computed in \(\lie{g}/\lie{i}\), and the bracket \(\bracket{X}{Y}\) is computed in \(\lie{g}\).
    
    \begin{lma}{}{}
        The quotient algebra as defined above is a Lie algebra.
        \begin{proof}
            Let \(\lie{g}\) be a Lie algebra and let \(\lie{i}\) be an ideal.
            We have four things to show:
            \begin{enumerate}
                \item The bracket on \(\lie{g}/\lie{i}\) is well-defined;
                \item The bracket on \(\lie{g}/\lie{i}\) is bilinear;
                \item The bracket on \(\lie{g}/\lie{i}\) is alternating;
                \item The bracket on \(\lie{g}/\lie{i}\) satisfies the Jacobi identity.
            \end{enumerate}
            The first relies on \(\lie{i}\) being an ideal, and the others are simply an exercise in algebra.
            
            \Step{Well-Defined}
            Let \(X, X' \in \lie{g}\) be such that \(X + \lie{i} = X' + \lie{i}\).
            Recall that this means \(X - X' \in \lie{i}\).
            Then for all \(Y \in \lie{g}\) we have
            \begin{equation}
                \bracket{X + \lie{i}}{Y + \lie{i}} = \bracket{X}{Y} + \lie{i}
            \end{equation}
            and
            \begin{equation}
                \bracket{X' + \lie{i}}{Y + \lie{i}} = \bracket{X'}{Y} + \lie{i}.
            \end{equation}
            We need to show that these are equal.
            This means we need to show that
            \begin{equation}
                \bracket{X}{Y} - \bracket{X'}{Y} \in \lie{i}.
            \end{equation}
            This is necessarily true however, since bilinearity gives us
            \begin{equation}
                \bracket{X}{Y} - \bracket{X'}{Y} = \bracket{X - X'}{Y}
            \end{equation}
            which is in \(\lie{i}\) since \(X - X' \in \lie{i}\) and \(\lie{i}\) is an ideal.
            
            \Step{Bilinear}
            We will demonstrate linearity in the first argument.
            Linearity in the second argument then follows from a similar argument, or in a field of characteristic other than 2 it follows from linearity in the first argument and alternativity, which implies anticommutativity.
            Let \(X, X', Y \in \lie{g}\), then
            \begin{align}
                \bracket{(X + \lie{i}) + (X' + \lie{i})}{Y + \lie{i}} &= \bracket{(X + X') + \lie{i}}{Y + \lie{i}}\\
                &= \bracket{X + X'}{Y} + \lie{i}\\
                &= (\bracket{X}{Y} + \bracket{X'}{Y}) + \lie{i}\\
                &= (\bracket{X}{Y} + \lie{i}) + (\bracket{X'}{Y} + \lie{i}).
            \end{align}
            Further, take \(\lambda \in \field\) and we have
            \begin{align}
                \bracket{(\lambda X) + \lie{i}}{Y + \lie{i}} &= \bracket{\lambda X}{Y} + \lie{i}\\
                &= (\lambda \bracket{X}{Y}) + \lie{i}\\
                &= \lambda(\bracket{X}{Y} + \lie{i}).
            \end{align}
            Thus, the bracket is linear in the first argument.
            
            \Step{Alternating}
            Let \(X \in \lie{g}\), then
            \begin{equation}
                \bracket{X + \lie{i}}{X + \lie{i}} = \bracket{X}{X} + \lie{i} = 0 + \lie{i} = \lie{i} = 0
            \end{equation}
            since \(\lie{i} = 0 + \lie{i}\) is the zero vector in the quotient space \(\lie{g}/\lie{i}\).
            
            \Step{Jacobi Identity}
            Let \(X, Y, Z \in \lie{g}\),
            \begin{equation}
                \bracket{X + \lie{i}}{\bracket{Y + \lie{i}}{Z + \lie{i}}} = \bracket{X + \lie{i}}{\bracket{Y}{Z} + \lie{i}} = \bracket{X}{\bracket{Y}{Z}} + \lie{i}.
            \end{equation}
            Doing this with the other terms in the Jacobi identity we see that we end up with the left hand side of the Jacobi identity in \(\lie{g}/\lie{i}\) being
            \begin{equation}
                (\bracket{X}{\bracket{Y}{Z}} + \lie{i}) + (\bracket{Y}{\bracket{Z}{X}} + \lie{i}) + (\bracket{Z}{\bracket{X}{Y}} + \lie{i}).
            \end{equation}
            Recall that addition in the quotient space is defined by \((X + \lie{i}) + (Y + \lie{i}) = (X + Y) + \lie{i}\), and so this becomes
            \begin{equation}
                (\bracket{X}{\bracket{Y}{Z}} + \bracket{Y}{\bracket{Z}{X}} + \bracket{Z}{\bracket{X}{Y}}) + \lie{i} = 0 + \lie{i} = 0
            \end{equation}
            where we've used the Jacobi identity in \(\lie{g}\), and then identified \(0 + \lie{i}\) as the zero vector in the quotient \(\lie{g}/\lie{i}\).
        \end{proof}
    \end{lma}
    
    \section{Examples of Lie Algebras}
    We've done quite a lot now without ever actually looking at any examples of a Lie algebra, so let's change that.
    
    \subsection{Abelian Lie Algebra}
    We start with the simplest example, where the bracket always vanishes.
    
    \begin{dfn}{Abelian Lie Algebra}{}
        A Lie algebra, \(\lie{g}\), is \define{abelian}\index{abelian Lie algebra} if \(\bracket{\lie{g}}{\lie{g}} = 0\), that is, if \(\bracket{X}{Y} = 0\) for all \(X, Y \in \lie{g}\).
    \end{dfn}
    
    The terminology here, abelian, should not be confused with this terminology referring to groups.
    An abelian Lie algebra is not one in which the bracket is \enquote{commutative} (although it is, since every bracket is the 0 we do technically have \(\bracket{X}{Y} = \bracket{Y}{X}\)).
    Instead, this terminology is inherited from the Lie group, an abelian Lie group will give us an abelian Lie algebra.
    
    Note that, up to isomorphism, there is one abelian Lie algebra of each dimension for each field.
    Any vector space can be made into an abelian Lie algebra by equipping it with the trivial bracket, and any two such Lie algebras are isomorphic so long as the underlying vector spaces are.
    For this reason abelian Lie algebras on their own are \enquote{boring}, we only look at them when they arise naturally as subalgebras of nonabelian Lie algebras.
    
    \subsection{Low Dimension}
    In an attempt to classify Lie algebras (something we'll return to later) one might start with dimension.
    Let's do that.
    For dimension 0 there is one Lie algebra, the zero vector space with the trivial bracket.
    
    For dimension 1 (and a fixed field) there is one Lie algebra (up to isomorphism), the one-dimensional space equipped with the trivial bracket.
    Note that there cannot be a nonabelian one-dimensional Lie algebra.
    Suppose that \(\lie{g}\) was such a Lie algebra, then \(\lie{g} = \field X = \Span\{X\}\) for some \(X\).
    Then if \(Y, Z \in \lie{g}\) we know that \(Y = \lambda X\) and \(Z = \mu X\), and then
    \begin{equation}
        \bracket{Y}{Z} = \bracket{\lambda X}{\mu X} = \lambda \mu \bracket{X}{X} = \lambda \mu \cdot 0 = 0.
    \end{equation}
    Thus, all one-dimensional Lie algebras are necessarily abelian.
    
    For dimension 2 (and a fixed field) we of course have the abelian Lie algebra, but there is another.
    Suppose that \(\lie{g}\) is two-dimensional with basis \(\{X, Y\}\).
    Assuming that \(\lie{g}\) is nonabelian the bracket \(\bracket{X}{Y} = -\bracket{Y}{X}\) must be nonzero.
    This means that \(\bracket{\lie{g}}{\lie{g}}\) is a one-dimensional subspace of \(\lie{g}\), since every element of \(\bracket{\lie{g}}{\lie{g}}\) is a linear combination of brackets of elements of \(\lie{g}\), but all nonzero such brackets are multiples of \(\bracket{X}{Y}\).
    Further, we can rescale \(X\) so that \(\bracket{X}{Y} = Y\), and this is the only 2-dimensional non-abelian Lie algebra up to isomorphism (for a fixed field).
    So, there are two Lie algebras of dimension 2 up to isomorphism.
    
    In three dimensions one can make similar arguments, and it turns out that there are infinitely many isomorphism classes of three-dimensional Lie algebra, but we can still classify them.
    Here are the isomorphism classes over \(\complex\) (or any algebraically closed field of characteristic 0):
    \begin{itemize}
        \item The abelian Lie algebra of dimension 3.
        \item \(\lie{g} = \Span\{X, Y, Z\}\) with \(\bracket{X}{Y} = Z\) and \(\bracket{X}{Z} = \bracket{Y}{Z} = 0\)\footnote{This is the three-dimensional \defineindex{Heisenberg algebra}.}.
        \item \(\lie{g} = \Span\{X, Y, Z\}\) with \(\bracket{X}{Y} = Y\) and \(\bracket{X}{Z} = \bracket{X}{Z} = 0\).
        \item \(\lie{g}_b = \Span\{X, Y, Z\}\) with \(\bracket{X}{Y} = Y\), \(\bracket{Y}{Z} = bZ\), and \(\bracket{Y}{Z} = 0\) for \(b \in \complex^{\times}\).
        Note that \(\lie{g}_b \isomorphic \lie{g}_{b'}\) if and only if \(b = b'\) or \(b = 1/b'\).
        \item \(\lie{g} = \Span\{X, Y, Z\}\) with \(\bracket{X}{Y} = Y\), \(\bracket{X}{Z} = Y + Z\), and \(\bracket{Y}{Z} = 0\).
        \item \(\lie{g} = \Span\{X, Y, Z\}\) with \(\bracket{X}{Y} = 2X\), \(\bracket{X}{Z} = -2Z\), and \(\bracket{Y}{Z} = X\).
    \end{itemize}
    Of these, by far the most important is the last one, which we'll later see is the Lie algebra known as \(\specialLinearLie(2, \complex)\).
    
    If we classify over \(\reals\) instead then we get another familiar example of a Lie algebra, \(\reals^3\) equipped with the bracket given by the cross-product.
    
    Hopefully, these examples are enough to convince you that a full classification for Lie algebras of dimension 4 is, while maybe possible, probably quite challenging.
    
    All hope is not lost, we do have, as we'll see later, a classification of a particularly nice type of Lie algebra, called simple Lie algebras, over \(\complex\).
    
    \subsection{Associative Algebras}
    \begin{dfn}{Algebra}{def:algebra}
        An \defineindex{algebra} is a vector space, \(A\), equipped with a bilinear product, \(\cdot \colon A \times A \to A\).
        An \defineindex{associative algebra} is an algebra for which the product is associative.
    \end{dfn}
    
    Note that Lie algebras are algebras in the above sense, but they are generally not associative.
    Examples of associative algebras include:
    \begin{itemize}
        \item \(n \times n\) matrices with matrix multiplication.
        \item \(\reals\), \(\complex\), or \(\quaternions\) (the quaternions) with their usual multiplication are associative algebras over \(\reals\).
        In fact, these are \emph{division} algebras: an algebra, \(D\), is a division algebra if for any \(a, b \in D\) with \(b \ne 0\) there is exactly one \(x \in D\) with \(a = bx\) and exactly one \(y \in D\) with \(a = yb\).
        It turns out that these are the only finite-dimensional associative division algebras over \(\reals\).
        If we drop the associativity condition then we also get \(\symbb{O}\), the octonions.
    \end{itemize}
    
    Our interest in associative algebras is mostly the following.
    
    \begin{lma}{}{}
        If \(A\) is an associative algebra then defining the bracket by \(\commutator{X}{Y} = XY - YX\) defines a Lie algebra.
        \begin{proof}
            First note that we clearly have
            \begin{equation}
                \commutator{X}{X} = XX - XX = 0.
            \end{equation}
            The commutator is bilinear, here we show linearity in the second argument, taking \(X, Y, Z \in A\) and \(\lambda \in \field\)
            \begin{align}
                \commutator{X}{Y + \lambda Z} &= X(Y + \lambda Z) - (Y + \lambda Z)X\\
                &= XY + \lambda XZ - YX - \lambda ZX\\
                &= XY - YX + \lambda (XZ - ZX)\\
                &= \commutator{X}{Y} + \lambda \commutator{X}{Z}.
            \end{align}
            The Jacobi identity follows by some algebra.
            First note that
            \begin{align}
                \commutator{X}{\commutator{Y}{Z}} &= \commutator{X}{YZ - ZY}\\
                &= X(YZ - ZY) - (YZ - ZY)X\\
                &= XYZ - XZY - YZX + ZYX.
            \end{align}
            Then, using the fact that the other terms are simply linear combinations of these, we have that the Jacobi relation is reduced to
            \begin{multline}
                XYZ - XZY - YZX + ZYX + YZX - YXZ - ZXY + XZY\\
                + ZXY - ZYX - XYZ + YXZ
            \end{multline}
            and these terms all cancel to give zero.
        \end{proof}
    \end{lma}
    
    When the Lie bracket can be written as \(\commutator{X}{Y} = XY - YX\) we call it the \defineindex{commutator}, because it is a measure of the failure of \(A\) to be a commutative algebra.
    This terminology is also often used for the Lie bracket in general, but we'll try to avoid it in the general case.
    
    \subsection{Classical Lie Algebras}
    The complex classical Lie algebras are some classes of Lie algebras defined with similar definitions.
    These were first of interest because they correspond to particularly common Lie groups.
    First, some notation.
    
    \begin{ntn}{}{}
        Let \(\matrices{n}{\field}\) denote the set of \(n \times n\) matrices with entries in \(\field\).
        
        For \(A \in \matrices{n}{\field}\) denote the transpose by \(A^{\trans}\).
        
        For \(A \in \matrices{n}{\field}\) with \(\field = \reals, \complex\) denote the complex conjugate by \(\overbar{A}\).
        
        For \(A \in \matrices{n}{\field}\) with \(\field = \reals, \complex\) denote the Hermitian conjugate by \(A^{\hermit}\), recall that \(A^{\hermit} = \overbar{A}^{\trans} = \overline{A^{\trans}}\)
        
        \begin{wrn}
            Physicists will denote the complex conjugate by \(A^*\), and the Hermitian conjugate by \(A^{\dagger}\).
        \end{wrn}
    \end{ntn}
    
    \begin{dfn}{General Linear Lie Algebra}{}
        Let \(V\) be a vector space, and define
        \begin{equation}
            \generalLinearLie(V) = \End V \coloneq \{T \colon V \to V \mid T \text{ is linear}\} = \Vect(V, V).
        \end{equation}
        This is an associative algebra under composition of linear maps, and as such \(\generalLinearLie(V)\) is a Lie algebra under the commutator.
    \end{dfn}
    
    Typically we write \(\generalLinearLie(V)\) when thinking of the Lie algebra, and \(\End V\) when thinking of the associative algebra.
    
    When \(V\) is finite dimensional, say \(\dim V = n\), we can fix a basis and then each linear map corresponds to an \(n \times n\) matrix with entries in \(\field\).
    This leads us to make the following definition.
    \begin{equation}
        \generalLinearLie(n, \field) \coloneq \matrices{n}{\field} \isomorphic \generalLinearLie(V).
    \end{equation}
    Note that if \(\dim V = n\) then \(V \isomorphic \field^n\) and \(\generalLinearLie(n, \field) = \generalLinearLie(\field^n)\).
    
    The following are some of the subalgebras of \(\generalLinearLie(n, \complex)\) viewed as a \emph{real} vector space.
    This is important, not all of these are closed under multiplication by arbitrary complex numbers (in particular, if \(A\) is skew-Hermitian then \(iA\) is Hermitian).
    \begin{itemize}
        \item \(\generalLinearLie(n, \reals) = \matrices{n}{\reals}\), the general linear Lie algebra.
        \item \(\specialLinearLie(n, \reals) \coloneq \{A \in \generalLinearLie(n, \reals) \mid \tr A = 0\}\), the \defineindex{special linear Lie algebra};
        \item \(\orthogonalLie(n, \reals) = \specialOrthogonalLie(n, \reals) \coloneq \{A \in \generalLinearLie(n, \reals) \mid A^{\trans} + A = 0\}\), the \defineindex{orthogonal Lie algebra} and \defineindex{special orthogonal Lie algebra}.
        Note that these two are equal but given different names because they both come from important Lie groups which are not equal.
        Note that \(A^{\trans} + A = 0\) means \(A\) is antisymmetric.
        \item \(\unitaryLie(n) \coloneq \{A \in \generalLinearLie(n, \complex) \mid A^{\hermit} + A = 0\}\), the \defineindex{unitary Lie algebra}.
        Note that \(A^{\hermit} + A = 0\) means \(A\) is skew-Hermitian.
        \item \(\specialUnitaryLie(n) \coloneq \{A \in \generalLinearLie(n, \complex) \mid \tr A = 0 \text{ and } A^* + A = 0\}\), the \defineindex{special unitary Lie algebra};
        \item \(\symplecticLie(n, \reals) \coloneq \{A \in \generalLinearLie(2n, \reals) \mid A^\trans J_n + J_n A = 0\}\), the \defineindex{symplectic Lie algebra}, where \(J_n \in \matrices{2n}{\reals}\) is the block matrix
        \begin{equation}
            \begin{bmatrix}
                0 & I_n\\
                -I_n & 0
            \end{bmatrix}
        \end{equation}
        where \(I_n \in \matrices{n}{\reals}\) is the \(n \times n\) identity matrix.
        The matrix \(J_n\) (or rather, the symmetric bilinear form it represents) is called a \defineindex{symplectic form}.
    \end{itemize}
    
    \begin{wrn}
        Physicists like operators to be Hermitian, for quantum mechanics, so they will define things with a few factors of \(i\) different so that \(\unitaryLie(n)\) and \(\specialUnitaryLie(n)\) consist of Hermitian matrices, as opposed to \emph{skew-}Hermitian matrices.
        This distinction unfortunately causes these factors of \(i\) to propagate through many formulae, so be careful when looking things up that the source uses the correct convention.
    \end{wrn}
    
    \begin{wrn}
        Notation differs for the symplectic Lie algebra (and group), some authors denote it \(\symplecticLie(2n)\), it's the \(D_n\) vs \(D_{2n}\) debate all over again.
    \end{wrn}
    
    The names here are all derived from the names of the associated Lie groups, \(\generalLinear(n, \reals)\) the general linear group, \(\specialLinear(n, \reals)\) the special linear group, \(\orthogonal(n, \reals)\) the orthogonal group, \(\specialOrthogonal(n, \reals)\) the special orthogonal group, \(\unitary(n)\) the unitary group, \(\specialUnitary(n)\) the special unitary group, and various different but related symplectic Lie groups.
    
    These are defined as follows:
    \begin{itemize}
        \item \(\generalLinear(n, \reals) \coloneq \{A \in \matrices{n}{\reals} \mid \det A \ne 0\}\);
        \item \(\specialLinearLie(n, \reals) \coloneq \{A \in \generalLinear(n, \reals) \mid \det A = 1\}\);
        \item \(\orthogonal(n, \reals) \coloneq \{A \in \generalLinear(n, \reals) \mid A^{\trans}A = I_n\}\);
        \item \(\specialOrthogonal(n, \reals) \coloneq \{A \in \generalLinear(n, \reals) \mid A^{\trans}A = I_n \text{ and } \det A = 1\}\).
        \item \(\unitary(n) \coloneq \{A \in \generalLinear(n, \complex) \mid U^{\hermit} U = I_n\}\);
        \item \(\specialUnitary(n) \coloneq \{A \in \generalLinear(n, \complex) \mid U^{\hermit}U = I_n \text{ and } \det U = 1\}\);
        \item \(\symplectic(n, \reals) \coloneq \{A \in \generalLinear(2n, \reals) \mid A^{\trans}J_nA = J_n\}\).
    \end{itemize}
    
    These definitions are all about preserving some structure on \(\reals^n\) (or \(\reals^{2n}\) for the symplectic group).
    \begin{itemize}
        \item The general linear group preserves the vector space structure, including dimension.
        \item The special linear group preserves volues.
        \item The orthogonal group preserves angles.
        \item The special orthogonal group preserves the inner product on \(\reals^n\).
        \item The (special) unitary group is a complex analogue of the (special) orthogonal group.
        \item The symplectic group preserves \(J_n\).
    \end{itemize}
    
    Note that \enquote{special} means that we impose the condition \(\det A = 1\).
    In the case of the Lie algebras this becomes the condition that \(\tr A = 0\).
    This is the reason that \(\orthogonalLie(n, \reals) = \specialOrthogonalLie(n, \reals)\), the requirement that \(A\) is antisymmetric means that the diagonal of \(A\) is zero, so it automatically has trace zero.
    In terms of Lie groups, the reason is that both \(\orthogonal(n, \reals)\) and \(\specialOrthogonal(n, \reals)\) are very similar, in particular, \(\specialOrthogonal(n, \reals) \isomorphic \orthogonal(n, \reals)/(\integers/2\integers)\) where \(\integers/2\integers\) here is \(\{\pm I_n\}\), so \(\specialOrthogonal(n, \reals)\) preserves everything that \(\orthogonal(n, \reals)\) preserves, and also preserves orientation.
    This similarity means that \(\orthogonal(n, \reals)\) has two connected components, one preserving orientation, corresponding to the subgroup \(\specialOrthogonal(n, \reals)\), and one reversing orientation.
    Then the Lie algebra corresponds only to the component of the Lie group connected to the identity, which is \(\specialOrthogonal(n, \reals)\) in both cases, so the Lie algebras are the same.
    
    In some of these cases, but not all, it is possible to extend the field to \(\complex\), giving
    \begin{itemize}
        \item \(\generalLinearLie(n, \complex) \coloneq \matrices{n}{\complex}\);
        \item \(\orthogonalLie(n, \complex) \coloneq \specialOrthogonalLie(n, \complex) = \{A \in \generalLinearLie(n, \complex) \mid A^{\trans} + A\} = 0\).
        \item \(\symplecticLie(n, \complex) \coloneq \{A \in \generalLinearLie(2n, \complex) \mid A^{\trans}J_n + J_n A = 0\}\).
    \end{itemize}
    
    Showing that the classical Lie algebras are indeed Lie algebras, specifically subalgebras of \(\generalLinearLie(N, \field)\), requires a little bit of work.
    Mostly we have to show that they are closed under the bracket, which is the commutator in all cases.
    There are a few tricks though.
    The first is that \(\specialLinearLie(n, \field) = \ker \tr\), when we view \(\tr \colon \generalLinearLie(n, \field) \to \generalLinearLie(n, \field)\) as a linear operator, and the kernel of a linear operator is always a subspace.
    We also have to show that the commutator of two traceless matrices is again traceless for \(\specialLinearLie(n, \field)\) to be closed under the bracket:
    \begin{equation}
        \tr(\commutator{X}{Y}) = \tr(XY - YX) = \tr(XY) - \tr(YX) = \tr(XY) - \tr(XY) = 0
    \end{equation}
    where we've used the fact that the trace of a product is invariant under cyclic permutations of that product (henceforth, \enquote{the trace is cyclic}).
    Notice that this doesn't actually use that \(X\) and \(Y\) are traceless, the trace of a commutator is always zero.
    This means that if \(X \in \generalLinearLie(n, \field)\) and \(Y \in \specialLinearLie(n, \field)\) we still have \(\tr(\bracket{X}{Y}) = 0\), and thus \(\bracket{X}{Y} \in \specialLinearLie(n, \field)\), meaning that \(\specialLinearLie(n, \field)\) is an ideal of \(\generalLinearLie(n, \field)\)
    
    \subsubsection{Dimensions}
    An important exercise is to compute the dimensions of the classical Lie algebras.
    We'll do it for the real ones.
    \begin{itemize}
        \item \(\dim \generalLinearLie(n, \reals) = n^2\), since an arbitrary element of \(\generalLinearLie(n, \reals)\) is an \(n \times n\) matrix, which is parametrised by \(n^2\) entries.
        A basis for \(\generalLinearLie(n, \reals)\) is
        \begin{equation}
            \{E_{ij} \mid i = 1, \dotsc, n \text{ and } j = 1, \dotsc, n\}
        \end{equation}
        where \(E_{ij}\) has 1 in the \(i\)th row and \(j\)th column and \(0\) everywhere else.
        Note that
        \begin{equation}
            \commutator{E_{ij}}{E_{kl}} = E_{ij}E_{kl} - E_{kl}E_{ij} = \delta_{ij}E_{il} - \delta_{il}E_{kj}.
        \end{equation}
        \item \(\dim \specialLinearLie(n, \reals) = n^2 - 1\), since setting \(\tr A = 0\) fixes one element on the diagonal, say the last element, by the requirement that if \(A = (a_{ij})\) then \(a_{11} + \dotsb + a_{n-1,n-1} = -a_{nn}\) to get \(\tr A = a_{11} + \dotsb + a_{n-1,n-1} + a_{nn} = 0\).
        Alternatively, note that \(\specialLinearLie(n, \reals) = \ker \tr\), and \(\im \tr = \reals\) and so by the rank-nullity theorem we have
        \begin{equation}
            \dim \generalLinearLie(n, \reals) = \dim(\ker \tr) + \dim(\im \tr) = \dim \specialLinearLie(n, \reals) + 1.
        \end{equation}
        The result then follows from this and our calculation of \(\dim \generalLinearLie(n, \reals) = n^2\).
        \item \(\dim \specialOrthogonalLie(n, \reals) = n(n - 1)/2\), requiring that \(A\) be antisymmetric means that \(A\) is zero on the diagonal, and below the diagonal is fixed by above the diagonal.
        The above diagonal elements form a triangle with a base of \(n - 1\) elements, and thus the number of entries above the diagonal is the \((n-1)\)st triangle number, \(T_{n-1} = n(n-1)/2\) (\(T_n = n(n + 1)/2\)).
        We can check this by identifying that \(\specialOrthogonal(3)\) corresponds to rotations in three dimensions, and this is a three dimensional group, since any rotation is specified by either a) three Euler angles, or b) an angle and an axis of rotation (requiring two numbers to fix the direction, the third component fixed by requiring it to be a unit vector).
        Note that \(\dim \specialOrthogonalLie(3) = 3\) is a coincidence, we have \(\dim \specialOrthogonalLie(2) = 1\) (rotations in two dimensions are specified by just an angle) and \(\dim \specialOrthogonalLie(4) = 6\).
        Of course, here we're using the fact that a Lie group and its associated Lie algebra have the same dimension, this is because the tangent space has the same dimension as the manifold (assuming the manifold is connected, which is always the case for at least one Lie group corresponding to a given Lie algebra).
        \item \(\dim \unitaryLie(n) = n^2\), note that we are talking about the dimension as a real vector space, despite the entries being complex numbers.
        This is the dimension because an arbitrary \(n \times n\) matrix with entries in \(\complex\) has \(2n^2\) real parameters (one for the real part and one for the imaginary part of each entry).
        Requiring that the matrix is skew-Hermitian means that each entry on the diagonal must be equal to its conjugate, so it must have zero imaginary part (fixing \(n\) parameters), then the entries below the diagonal are fixed by the entries above the diagonal (fixing \(2 n(n - 1)/2 = n^2 - n\) parameters).
        Thus, the dimension is \(2n^2 - n - (n^2 - n) = n^2\).
        \item \(\dim \specialUnitaryLie(n) = n^2 - 1\), since any matrix in \(\specialUnitaryLie(n)\) is in \(\unitaryLie(n)\), and setting its trace to zero fixes the final entry on the diagonal (which is purely real).
        \item \(\dim \symplecticLie(n) = 2n^2 + n\), a general \(2n \times 2n\) matrix with entries in \(\reals\) has \((2n)^2 = 4n^2\) parameters.
        Write this as a block matrix,
        \begin{equation}
            A = 
            \begin{bmatrix}
                X & Y\\
                Z & W
            \end{bmatrix}
            , \qquad A^{\trans} = 
            \begin{bmatrix}
                X^{\trans} & Z^{\trans}\\
                Y^{\trans} & W^{\trans}
            \end{bmatrix}
        \end{equation}
        with \(X, Y, Z, W \in \matrices{n}{\reals}\).
        Then computing the defining relationship, \(A^{\trans}J_n + J_nA = 0\), we have
        \begin{equation}
            \begin{bmatrix}
                Z^{\trans} - Z & -W - X^{\trans}\\
                W^{\trans} + X & Y - Y^{\trans}
            \end{bmatrix}
            =
            \begin{bmatrix}
                0 & 0\\
                0 & 0
            \end{bmatrix}
            .
        \end{equation}
        We see that this fixes \(Z = Z^{\trans}\), \(Y = Y^{\trans}\), and \(W = -X^{\trans}\).
        Requiring that \(Z\) be symmetric means that the elements below the diagonal are fixed, removing \(n(n-1)/2\) free parameters.
        Similarly, requiring that \(Y\) is symmetric removes \(n(n-1)/2\) parameters.
        Since \(X\) determines \(W\) we also fix another \(n^2\) parameters.
        Thus, the number of remaining free parameters is
        \begin{equation}
            4n^2 - \left( \frac{1}{2}n(n - 1) + \frac{1}{2}n(n - 1) + n^2 \right) = 2n^2 + n.
        \end{equation}
    \end{itemize}
    
    \subsubsection{\texorpdfstring{\(\specialLinearLie(2, \complex)\)}{sl(2, C)}}
    As we've said a couple of times already the most important Lie algebra is \(\specialLinearLie(2, \complex)\), which is a Lie algebra over \(\complex\).
    A general element of this algebra is a \(2 \times 2\) matrix with complex entries such that the trace vanishes, we can write such an element as
    \begin{equation}
        \begin{bmatrix}
            a & b\\
            c & -a
        \end{bmatrix}
    \end{equation}
    for \(a, b, c \in \complex\).
    This shows that \(\dim \specialLinearLie(2, \complex) = 3\).
    
    An explicit basis for \(\specialLinearLie(2, \complex)\) is
    \begin{equation}
        \quad H = 
        \begin{bmatrix}
            1 & 0\\
            0 & -1
        \end{bmatrix}
        , \qquad
        E = 
        \begin{bmatrix}
            0 & 1\\
            0 & 0
        \end{bmatrix}
        , \qqand F = 
        \begin{bmatrix}
            0 & 0\\
            1 & 0
        \end{bmatrix}
        .
    \end{equation}
    Then
    \begin{equation}
        \begin{bmatrix}
            a & b\\
            c & -a
        \end{bmatrix}
        = aH + bE + cF.
    \end{equation}
    
    A simple calculation of commutators shows that we have the relations
    \begin{equation}
        \commutator{H}{E} = 2E, \qquad \commutator{H}{F} = -2F, \qqand \commutator{E}{F} = H.
    \end{equation}
    
    Later we will work more abstractly, rather than defining \(\specialLinearLie(2, \complex)\) to be \(2 \times 2\) complex traceless matrices we'll define \(\specialLinearLie(2, \complex)\) to be a three dimensional Lie algebra with basis \(\{H, E, F\}\) satisfying the above bracket relations.
    Note that this doesn't require the bracket to be the commutator, in fact, the commutator isn't even defined in this abstract setting since we don't have a notion of multiplication\footnote{It turns out that every Lie algebra, \(\lie{g}\), can be embedded into an associative algebra, \(\symcal{U}(\lie{g})\) in which the Lie bracket on \(\lie{g}\) coincides with the commutator in \(\symcal{U}(\lie{g})\). This larger algebra is called the \defineindex{universal enveloping algebra}. Formally, this algebra can be constructed by taking the tensor algebra, \(T(\lie{g}) = \field \oplus \lie{g} \oplus (\lie{g} \otimes \lie{g}) \oplus (\lie{g} \otimes \lie{g} \otimes \lie{g}) \oplus \dotsb\), and defining the quotient \(\symcal{U}(\lie{g}) = T(\lie{g})/I\) where \(I\) is the two sided ideal generated by elements of the form \(a \otimes b - b \otimes a - \commutator{a}{b} \in \lie{g} \oplus (\lie{g} \otimes \lie{g}) \subseteq T(\lie{g})\). A general element of this ideal is thus of the form \(c \otimes d \otimes \dotsb \otimes (a \otimes b - b \otimes a - \bracket{a}{b}) \otimes e \otimes f \otimes \dotsb\) with \(a, b, c, d, e, f \in \lie{g}\). Note that the product of two elements of \(T(\lie{g})\) is their tensor product, although we usually write \(a \otimes b = ab\).} with which to form, say, \(HE\).
    Once we do this we see that the matrices above form a two-dimensional representation (\cref{def:representation}) of \(\specialLinearLie(2, \complex)\), called the defining representation, which has an obvious action on \(\complex^2\).
    
    An important observation here is that if we define the map\footnote{This is the so called adjoint representation (\cref{def:adjoint rep}).} \(\ad_H \colon \specialLinearLie(2, \complex) \to \specialLinearLie(2, \complex)\) by \(\ad_H(X) = \commutator{H}{X}\) then we see that this map has \(E\) and \(F\) as eigenvectors, with eigenvalues \(2\) and \(-2\) respectively.
    The other eigenvector of \(\ad_H\) is \(H\) itself, since \(\ad_H(H) = \commutator{H}{H} = 0 = 0H\).
    Then we have the eigensspaces \(V_0 = \complex H = \lie{h}\), \(V_2 = \complex E\), and \(V_{-2} = \complex F\), giving the decomposition
    \begin{equation}
        \specialLinearLie(2, \complex) = \complex H \oplus \complex E \oplus \complex F = \lie{h} \oplus V_2 \oplus V_{-2} = \lie{h} \oplus \bigoplus_{\alpha \in \complex \setminus \{0\}} V_{\alpha}.
    \end{equation}
    
    The important thing here is that we get \(\lie{h}\), which is a special subalgebra called the Cartan subalgebra which we'll see a lot later, and we get the generalised eigenspaces \(V_{\alpha}\).
    We'll see later\footnote{so ignore any terminology here that is unfamiliar} that replacing the generalised eigenspaces with something called root spaces makes this construction generalise to finite dimensional semisimple (\cref{def:semisimple lie alg}) Lie algebras over \(\complex\), and further that the subalgebras appearing in this decomposition are all simple \cref{def:simple lie alg}, so a classification of simple Lie algebras extends to a classification of semisimple Lie algebras.
    This requires replacing the indexing set \(\complex \setminus \{0\}\) with the root space \(\lie{h}^* \setminus \{0\}\), or rather we've already done that, it's just that \(\dim \lie{h} = 1\) so \(\lie{h}^* \isomorphic \lie{h}\) and \(\dim \lie{h}^* = 1\) so \(\lie{h}^* \isomorphic \complex\).
    
    While this last paragraph is beyond what we're ready for yet it's important to start seeing the recurring pattern of this type of decomposition now.
    The fact that \(\specialLinearLie(2, \complex)\) is the simplest example of such a decomposition is why it's so important.
    In particular, every finite dimension semisimple Lie algebra over \(\complex\) of dimension at least 3 contains (many) copies of \(\specialLinearLie(2, \complex)\).
    
    \subsection{Other Subalgebras of \texorpdfstring{\(\generalLinearLie(n, \field)\)}{gl(n, k)}}
    There are two further subalgebras of \(\generalLinearLie(n, \field)\) which are worth mentioning.
    \begin{dfn}{Triangular Matrices}{}
        Denote by \(\borelLie(n, \field) \subseteq \generalLinearLie(n, \field)\) the subalgebra of upper triangular matrices, and by \(\nilpotentLie(n, \field) \subseteq \generalLinearLie(n, \field)\) the subalgebra of \emph{strictly} upper triangular matrices (meaning the diagonal is all zeros).
    \end{dfn}
    
    \begin{remark}{}{}
        Here \(\borelLie\) stands for \enquote{Borel} because \(\borelLie(n, \field)\) is a \defineindex{Borel subalgebra}, meaning it's a maximal solvable subalgebra (\cref{def:solvable lie alg}).
        The \(\nilpotentLie\) stands for \enquote{nilpotent} because \(\nilpotentLie(n, \field)\) is a (maximal) nilpotent algebra (\cref{def:nilpotent lie alg})
        
        These properties ultimately come from the fact that taking nested commutators of (strictly) upper triangular matrices will eventually result in zero.
        The difference in how the commutators are nested is the distinction between solvable (e.g., \(\bracket{\bracket{-}{-}}{\bracket{-}{-}}\)) and nilpotent (e.g., \(\bracket{-}{\bracket{-}{-}}\)).
    \end{remark}
    
    It turns out that \(\nilpotentLie(n, \field)\) is an ideal of \(\borelLie(n, \field)\), since the product of an upper triangular matrix and a strictly upper triangular matrix is strictly upper triangular, and so the commutator is too.
    This means we can consider the quotient \(\lie{g} = \borelLie(n, \field)/\nilpotentLie(n, \field)\).
    Two upper triangular matrices in \(\borelLie(n, \field)\) are identified in \(\lie{g}\) if their difference is a strictly upper triangular matrix in \(\nilpotentLie(n, \field)\).
    This means that they must have the same diagonal.
    Suppose then that \(X, Y \in \borelLie(n, \field)\) have the same diagonal, so \(X_{ii} = Y_{ii}\) for all \(i = 1, \dotsc, n\).
    By definition of upper triangular we also know that \(X_{ij} = Y_{ij} = 0\) if \(i > j\).
    Basic matrix multiplication tells us an element of the diagonal of \(XY\) is
    \begin{equation}
        (XY)_{ii} = \sum_{j=1}^{n} X_{ij} Y_{ji}.
    \end{equation}
    If \(i > j\) then \(X_{ij} = 0\), and if \(i < j\) then \(Y_{ji} = 0\), so the only nonzero diagonal term comes from \(X_{ii} Y_{ii}\).
    The same analysis can be applied to \(YX\) and we see that the only nonzero diagonal term is \((YX)_{ii} = Y_{ii} X_{ii}\).
    In the commutator these terms cancel out, and as such \(\commutator{X}{Y} \in \nilpotentLie(n, \field)\) for all \(X, Y \in \borelLie(n, \field)\).
    This means that in the quotient \(\lie{g}\) we identify \(\commutator{X}{Y}\) with \(\commutator{X}{Y} + \nilpotentLie(n, \field) = 0 + \nilpotentLie(n, \field)\), which is zero, and so this quotient is abelian.
    
    Note that as \(\field\)-vector spaces \(\dim \borelLie(n, \field) = n(n + 1)/2 = T_n\) and \(\dim \borelLie(n, \field) = n(n - 1)/2 = T_{n-1}\).
    Thus, the quotient \(\lie{g}\) has dimension \(T_n - T_{n-1} = n\).
    
    If \(A\) is an algebra with then another important Lie subalgebra of \(\generalLinearLie(A)\) is \(\Der A\), the algebra of derivations of \(A\).
    Recall that a derivation is a linear map \(D \colon A \to A\) such that
    \begin{equation}
        D(ab) = aD(b) + D(a)b
    \end{equation}
    for all \(a, b \in A\).
    To show that this is a subalgebra of \(\generalLinearLie(A)\) we need to show that \(\Der A\) is closed under the commutator, that is, that the commutator of two derivations is a derivation.
    This follows from the following calculation with \(D_1, D_2 \in \Der A\) and \(a, b \in A\):
    \begingroup
    \allowdisplaybreaks
    \begin{align}
        \commutator{D_1}{D_2}(ab) &= (D_1D_2 - D_2D_1)(ab)\\
        &= D_1(D_2(ab)) - D_2(D_1(ab)) \notag\\
        &= D_1(aD_2(b) + D_2(a)b) - D_2(aD_1(b) - D_1(a)b) \notag\\
        &= D_1(aD_2(b)) + D_1(D_2(a)b) - D_2(aD_1(b)) - D_2(D_1(a)b) \notag\\
        &= aD_1(D_2(b)) + D_1(a)D_2(b) + D_2(a)D_1(b) + D_1(D_2(a))b \notag\\
        &\quad- aD_2(D_1(b)) - D_2(a)D_1(b) - D_1(a)D_2(b) - D_2(D_1(a))b \notag\\
        &= aD_1(D_2(b)) + D_1(D_2(a))b - aD_2(D_1(b)) - D_2(D_1(a))b \notag\\
        &= a\{D_1(D_2(b)) - D_2(D_1(b))\} + \{D_1(D_2(a)) - D_2(D_1(a))\}b \notag\\
        &= a(D_1D_2 - D_2D_1)(b) + (D_1D_2 - D_2D_1)(a)b \notag\\
        &= a\commutator{D_1}{D_2}(b) + \commutator{D_1}{D_2}(a)b. \notag
    \end{align}
    \endgroup
    This shows that \(\commutator{D_1}{D_2}\) is a derivation, so \(\Der A\) is closed under the commutator.
    
    \chapter{Representation Theory}
    \section{Definition}
    \begin{dfn}{Representation}{def:representation}
        Let \(\lie{g}\) be a Lie algebra over a field, \(\field\).
        A \defineindex{representation} of \(\lie{g}\) is a pair, \((V, \rho)\), where \(V\) is a vector space over \(\field\) and \(\rho \colon \lie{g} \to \generalLinearLie(V)\) is a Lie algebra homomorphism.
    \end{dfn}
    
    The idea behind this definition is that \(\lie{g}\) is acting on \(V\) by specifying an endomorphism \(\varphi_X = \rho(X) \in \End V = \generalLinearLie(V)\) for each \(X \in \lie{g}\).
    Then \(X\) acts on \(v \in V\) by \(x \mapsto \varphi_X(V)\).
        
    This should be compared to the idea of a group action, which can similarly be specified by a map \(\rho \colon G \to \Perm(X)\) where \(\Perm(X)\) is the group of permutations of elements of \(X\).
    If you're familiar with group representations then the idea here is the same as a group representation \((V, \rho)\) with \(\rho \colon G \to \generalLinear(V)\) a group homomorphism.
    
    \begin{exm}{Trivial Representation}{}
        Let \(\lie{g}\) be a Lie algebra over \(\field\) and \(V\) any vector space over \(\field\).
        Then the homomorphism \(\rho_{\symrm{triv}} \colon \lie{g} \to \generalLinearLie(V)\) defined by \(\rho_{\symrm{triv}}(X) = 0\) is the \defineindex{trivial representation} of \(\lie{g}\) on \(V\).
    \end{exm}
    
    Note that the\footnote{We say \enquote{the} trivial representation, because \(V\) really isn't important in this case. When \(V\) is not specified the trivial representation is typically taken to be carried by a one-dimensional vector space.} trivial Lie algebra representation acts as zero, whereas a trivial group representation acts as the identity.
    This is consistent since \(\e^0 = 1\), so we recover the trivial action of the corresponding Lie group from the trivial action of the Lie algebra.
    
    One of the most important examples of a representation is when the Lie algebra acts on itself in the obvious way, through the bracket.
    
    \begin{dfn}{Adjoint Representation}{def:adjoint rep}
        Let \(\lie{g}\) be a Lie algebra.
        There is a Lie algebra representation called the \defineindex{adjoint representation}, \((\lie{g}, \ad)\) where \(\ad \colon \lie{g} \to \generalLinearLie(\lie{g})\) is defined by \(\ad(X) = \ad_X\) where \(\ad_X \in \generalLinearLie(\lie{g})\) is the linear map \(\ad_X \colon \lie{g} \to \lie{g}\) defined by \(\ad_X(Y) = \bracket{X}{Y}\).
    \end{dfn}
    
    This is related to the adjoint action of a the corresponding Lie group on itself, which is by conjugation.
    
    \begin{dfn}{Modules}{}
        Let \(\lie{g}\) be a Lie algebra over a field, \(\field\).
        A (left) \define{\(\lie{g}\)-module}\index{g-module@\(\lie{g}\)-module}\index{module}, \(V\), is a vector space over \(\field\) equipped with a bilinear map \({-} \cdot {-} \colon \lie{g} \times V\) such that for all \(X, Y \in \lie{g}\) and \(v \in V\) we have
        \begin{equation}
            \bracket{X}{Y} \cdot v = X \cdot (Y \cdot v) - Y \cdot (X \cdot v).
        \end{equation}
    \end{dfn}
    
    If you're familiar with the idea of an \(R\)-module for \(R\) a ring then the idea is the same here.
    An \(R\)-module is an abelian group equipped with an \(R\)-action, and a \(\lie{g}\)-module is a vector space equipped with a \(\lie{g}\)-action.
    
    \begin{prp}{}{}
        A Lie algebra representation of \(\lie{g}\) carries the same information as a \(\lie{g}\)-module.
        \begin{proof}
            Let \((V, \rho)\) be a representation of \(\lie{g}\).
            Then we may define a \(\lie{g}\)-action on \(V\) by \(X \cdot v \coloneqq \rho(X)(v)\).
            This is linear in the first argument, \(X\), because \(\rho\) is linear, and linear in the second argument, \(v\), because \(\rho(X) \in \generalLinearLie(V)\) is linear.
            Thus, this is a bilinear map.
            We further have
            \begin{align}
                \bracket{X}{Y} \cdot v &= \rho(\bracket{X}{Y})(v)\\
                &= \bracket{\rho(X)}{\rho(Y)}(v)\\
                &= \rho(X)(\rho(Y)(v)) - \rho(Y)(\rho(X)(v))\\
                &= X \cdot (Y \cdot v) - Y \cdot (X \cdot v)
            \end{align}
            where we've used the fact that \(\rho\) is a homomorphism and in \(\generalLinearLie(V)\) the Lie bracket is just the commutator.
            
            Conversely, if \(V\) is a \(\lie{g}\)-module then we may define a homomorphism \(\rho \colon \lie{g} \to \generalLinearLie(V)\) by \(X \mapsto X \cdot {-}\) which acts by \(v \mapsto X \cdot v\).
            This is a linear map \(V \to V\) because the \(\lie{g}\)-action is bilinear in the second argument, and it's a Lie algebra homomorphism because by definition the Lie bracket acts as a commutator, which is the Lie bracket in \(\generalLinearLie(V)\).
            
            Thus, every representation of \(\lie{g}\) uniquely specifies a \(\lie{g}\)-module and vice versa.
        \end{proof} 
    \end{prp}
    
    The \enquote{modern} treatment of representations is mostly via the notion of modules, and we shall prefer this method.
    
    \subsection{Morphism of Representations}
    \begin{dfn}{}{}
        Let \(V_1\) and \(V_2\) be \(\lie{g}\)-modules for some Lie algebra \(\lie{g}\).
        A \define{morphism}\index{morphism!of g-modules@morphism!of \(\lie{g}\)-modules} of \(\lie{g}\)-modules is then a linear map \(\varphi \colon V_1 \to V_2\) such that for all \(X \in \lie{g}\) and \(v \in V_1\) we have
        \begin{equation}
            \varphi(X \cdot v) = X \cdot v.
        \end{equation}
        We also call \(\varphi\) an \defineindex{equivariant map}.
        
        An invertible \(\lie{g}\)-module homomorphism is a \(\lie{g}\)-module isomorhpism.
    \end{dfn}
    
    Writing \(X \cdot {-}\) for the map \(v \mapsto X \cdot v\) the definition above is equivalent to the commutativity of the following square for all \(X \in \lie{g}\):
    \begin{equation}
        \tikzexternaldisable
        \begin{tikzcd}
            V_1 \arrow[r, "\varphi"] \arrow[d, "X \cdot {-}"'] & V_2 \arrow[d, "X \cdot {-}"]\\
            V_1 \arrow[r, "\varphi"'] & V_2\mathrlap{.}
        \end{tikzcd}
        \tikzexternalenable
    \end{equation}
    Note that \(X \cdot {-}\) on the left is the action of \(X\) on \(V_1\), whereas on the right it's the action of \(X\) on \(V_2\).
    We get away with the same notation by looking at the domain of the action to specify which we mean.
    
    Now that we have a notion of morphisms between representations we can form a category.
    Specifically, \(\Rep\) is the category whose objects are \(\lie{g}\)-modules (or equivalently representations of \(\lie{g}\)) and whose morhpisms are morphisms of representations.
    Note that formally the category with representations and the category with \(\lie{g}\)-modules are different, but they are isomorphic, which is why we don't need to distinguish between them.
    The isomorphism is given by the functor sending \((V, \rho)\) to the \(\lie{g}\)-module \(V\) with the action given by \(X \cdot v = \rho(X)(v)\).
    
    \begin{lma}{}{}
        If \(\varphi \colon V_1 \to V_2\) is an isomorphism of \(\lie{g}\)-modules then \(\varphi^{-1} \colon V_1 \to V_2\) is also.
        \begin{proof}
            We need to show that for all \(X \in \lie{g}\) and \(v \in V_2\) we have
            \begin{equation}
                \varphi^{-1}(X \cdot v) = X \cdot \varphi^{-1}(v).
            \end{equation}
            To do so note that \(\varphi^{-1}\) is an invertible linear map, and as such is a bijection.
            This means that there exists unique \(v' \in V_1\) such that \(\varphi(v') = v\) and \(v' = \varphi^{-1}(v)\).
            Thus, we may write the left hand side as
            \begin{equation}
                \varphi^{-1}(X \cdot v) = \varphi^{-1}(X \cdot \varphi(v')).
            \end{equation}
            Using the fact that \(\varphi\) is a morphism of \(\lie{g}\)-modules we have
            \begin{equation}
                \varphi^{-1}(X \cdot v) = \varphi^{-1}(\varphi(X \cdot v')) = X \cdot v' = X \cdot \varphi^{-1}(v). \qedhere
            \end{equation}
        \end{proof}
    \end{lma}
    
    \begin{ntn}{}{}
        If \(M\) and \(N\) are \(\lie{g}\)-modules over \(\field\) we write
        \begin{align}
            \Hom_{\field}(M, N) &= \Hom(M, N) = \{T \colon M \to N \mid T \text{ is linear}\}\\
            &= \Vect(M, N)
        \end{align}
        and
        \begin{align}
            \Hom_{\lie{g}}(M, N) &= \{T \colon M \to N \mid T \text{ is a } \lie{g} \text{-module homomorphism}\} \notag\\
            &= \Rep(M, N).
        \end{align}
        Similarly, we write \(\End_{\field}(M) = \End(M) = \Hom(M, M)\) and \(\End_{\lie{g}}(M) = \Hom_{\lie{g}}(M, M)\).
    \end{ntn}
    
    \section{Simple Modules}
    \begin{dfn}{Submodule}{}
        Let \(\lie{g}\) be a Lie algebra over \(\field\) and let \(V\) be a \(\lie{g}\)-module.
        A subspace, \(W \subseteq V\), is a \defineindex{submodule} if it is a \(\lie{g}\)-module under the restricted action of \(\lie{g}\).
        Equivalently, \(W\) is a submodule if \(X \cdot w \in W\) for all \(X \in \lie{g}\) and for all \(w \in W\).
        
        A \defineindex{proper submodule} of \(V\) is a submodule that is neither \(0\) nor \(V\).
    \end{dfn}
    
    \begin{lma}{}{}
        Let \(\varphi \colon M \to N\) be a \(\lie{g}\)-module homomorphism.
        Then \(\ker \varphi\) is a submodule of \(M\) and \(\im \varphi\) is a submodule of \(N\).
        \begin{proof}
            We already know that \(\ker \varphi\) and \(\im \varphi\) are subspaces of \(M\) and \(N\) respectively, so it remains only to show that they are submodules, that they are closed under the action of \(\lie{g}\).
            Let \(v \in \ker \varphi\), then \(\varphi(v) = 0\).
            From this we know that
            \begin{equation}
                \varphi(X \cdot v) = X \cdot \varphi(v) = X \cdot 0 = 0
            \end{equation}
            and so \(X \cdot v \in \ker \varphi\).
            Now, let \(w \in \im \varphi\), then there exists some \(w' \in M\) with \(\varphi(w') = w\).
            We then have
            \begin{equation}
                X \cdot w = X \cdot \varphi(w') = \varphi(X \cdot w')
            \end{equation}
            which means \(X \cdot w \in \im \varphi\).
        \end{proof}
    \end{lma}
    
    \begin{dfn}{Simple Module}{}
        A \define{simple \(\lie{g}\)-module}\index{simple module} is a \(\lie{g}\)-module with no proper submodules.
        The corresponding representation is called an \defineindex{irreducible representation}, often shortened to \defineindex{irrep}.
    \end{dfn}
    
    The simple \(\lie{g}\)-modules turn out to be the most important.
    Often, but not always (but always over \(\complex\)) it is possible to write any module, \(V\), as a direct sum of simple \(\lie{g}\)-modules.
    Because of this one of the first goals when studying representations of a given Lie algebra is to classify all simple \(\lie{g}\)-modules.
    This is often possible, but not always easy.
    
    \begin{lma}{Schur's Lemma}{lma:schur}
        Let \(V\) be a finite-dimensional \(\lie{g}\)-module over an algebraically closed field, \(\field\).
        Then eery \(\lie{g}\)-module endomorphism of \(V\) is a multiple of the identity and \(\End_{\lie{g}}(V) \isomorphic \field\).
        \begin{proof}
            Let \(\varphi \colon V \to V\) be a \(\lie{g}\)-module endomorphism.
            Then \(\varphi\) is a linear map.
            Since \(\field\) is assumed to be algebraically closed we know that this linear map has an eigenvalue, call it \(\lambda\) and let \(v \in V\) be the associated eigenvector.
            We know that \(v \in \ker \varphi'\) by construction.
            The kernel is a submodule, and since it's not zero (it contains \(v\)) and \(V\) is simple the kernel must be all of \(V\).
            Thus, \(\varphi'(w) = 0\) for all \(w \in V\), and so \(\varphi - \lambda \id\) is the zero map, and so \(\varphi = \lambda \id\) is just a scalar multiple of the identity.
        \end{proof}
    \end{lma}
    
    Note that from this we can immediately conclude that the only \(\lie{g}\)-module endomorphisms are invertible (if \(\lambda \ne 0\)) or the zero map (\(\lambda = 0\)).
    
    \begin{lma}{}{}
        If \(V\) and \(W\) are non-isomorphic simple \(\lie{g}\)-modules then \(\Hom_{\lie{g}}(V, W) = 0\).
        \begin{proof}
            Suppose that \(\varphi \in \Hom_{\lie{g}}(V, W)\).
            That is, \(\varphi \colon V \to W\) is a \(\lie{g}\)-module homomorphism.
            Then we may consider the submodule \(\ker \varphi \subseteq V\).
            Supposing that \(\varphi \ne 0\) there must be some \(v \in V\) such that \(\varphi(v) \ne 0\), thus \(\ker \varphi\) is not the whole of \(V\), and as a submodule of the simple module \(V\) it must be that \(\ker \varphi = 0\), so \(\varphi\) is injective.
            Now consider the submodule \(\im \varphi \subseteq W\).
            We know that \(\im \varphi \ne 0\) since \(0 \ne \varphi(v) \in \im \varphi\).
            Thus, since \(W\) is simple, \(\im \varphi\) must be all of \(W\), so \(\varphi\) is surjective.
            This leaves us with two cases, either \(\varphi\) is a injective and surjective, and hence an isomorphism, or \(\varphi\) is the zero map.
            By assumption there are no isomorphisms \(V \to W\), and further \(\varphi \ne 0\).
            Thus, there are no non-trivial morphisms \(V \to W\) as claimed.
        \end{proof}
    \end{lma}
    
    \subsection{New Modules From Old}
    \begin{lma}{}{}
        Let \(\lie{m}\) and \(\lie{l}\) be Lie algebras, and let \(V\) and \(W\) be simple \(\lie{m}\)- and \(\lie{l}\)-modules, respectively.
        Then \(V \otimes W\) is made into an \((\lie{m} \oplus \lie{l})\)-module by extending the action
        \begin{equation}
            (X, Y) \cdot (v \otimes w) = (X \cdot v) \otimes w + v \otimes (Y \cdot w)
        \end{equation}
        by linearity to non-simple tensors.
        Note that \(X \in \lie{m}\), \(Y \in \lie{l}\), \(v \in V\) and \(w \in W\).
        \begin{proof}
            We first show that this action is bilinear.
            Linearity in the first argument follows from the calculation
            \begin{align}
                (\lambda(X, Y)) \cdot (v \otimes w) &= (\lambda X, \lambda Y) \cdot (v \otimes w)\\
                &= (\lambda X \cdot w) \otimes v + w \otimes (\lambda Y \cdot w)\\
                &= \lambda ((X \cdot w) \otimes v) + \lambda (w \otimes (Y \cdot w))\\
                &= \lambda ((X \cdot w) \otimes v + w \otimes (Y \cdot w))\\
                &= \lambda ((X, Y) \cdot (v \otimes w))
            \end{align}
            and
            \begin{align}
                &((X, Y) + (X', Y')) \cdot (v \otimes w) = (X + X', Y + X') \cdot (v \otimes w)\\
                &= ((X + X') \cdot v) \otimes w + v \otimes ((Y + Y') \cdot w)\\
                &= (X \cdot v + X' \cdot v) \otimes w + v \otimes (Y \cdot w + Y' \cdot w)\\
                &= (X \cdot v) \otimes w + (X' \cdot v) \otimes w + v \otimes (Y \cdot w) + v \otimes (Y' \cdot w)\\
                &= (X \cdot v) \otimes w + v \otimes (Y \cdot w) + (X' \cdot v) \otimes w + v \otimes (Y' \cdot w)\\
                &= (X, Y) \cdot (v \otimes w) + (X', Y') \cdot (v \otimes w).
            \end{align}
            Linearity in the second argument may be demonstrated by a similar argument.
            We can also demonstrate that brackets act as they must:
            \begin{align}
                &\bracket{(X, Y)}{(X', Y')} \cdot (v \otimes w) = (\bracket{X}{X'}, \bracket{Y}{Y'}) \cdot (v \otimes w)\\
                &= (\bracket{X}{X'} \cdot v) \otimes w + v \otimes (\bracket{Y}{Y'} \cdot w)\\
                &= (X \cdot (X' \cdot v) - X' \cdot (X \cdot v)) \otimes w + v \otimes (Y \cdot (Y' \cdot w) + Y' \cdot (Y \cdot w)) \notag\\
                &= (X \cdot (X' \cdot v)) \otimes w - (X' \cdot (X \cdot v)) \otimes w\\
                &\qquad\qquad+ v \otimes (Y \cdot (Y' \cdot w)) - v \otimes (Y' \cdot (Y \cdot w)) \notag\\
                &= (X \cdot (X' \cdot v)) \otimes w + v \otimes (Y \cdot (Y' \cdot w))\\
                &\qquad\qquad- (X' \cdot (X \cdot v)) \otimes w - v \otimes (Y' \cdot (Y \cdot w)) \notag\\
                &= (X, Y) \cdot ((X', Y') \cdot (v \otimes w)) - (X', Y') \cdot ((X, Y) \cdot (v \otimes w)).\notag
            \end{align}
            Thus, this is indeed an \((\lie{m} \oplus \lie{l})\)-module.
        \end{proof}
    \end{lma}
    
    The goal of the next couple of lemmas is to prove \cref{prp:tensor product of simple modules is simple}, that the tensor product of simple modules is again simple.
    
    \begin{lma}{}{lma:smallest submodule of V+V}
        Let \(V\) be a simple \(\lie{m}\)-module and \(v_1, v_2 \in V\) such that \(v_1\) is not proportional to \(v_2\).
        Then the smallest submodule of \(V \oplus V\) containing \((v_1, v_2)\) is \(V \oplus V\).
        \begin{proof}
            Let \(U \subseteq V \oplus V\) be the minimal submodule (with respect to inclusion) containing \((v_1, v_2)\).
            Consider the inclusion map \(i \colon U \hookrightarrow V \oplus V\) and the projection maps \(p_1, p_2 \colon V \oplus V \to V\).
            These are all module homomorphisms, and so the composites \(p_1 \circ i, p_2 \circ i \colon U \to V\) are also homomorphisms.
            Since \(V\) is simple and \(U\) is nonzero and \(p_1 \circ i\) is a nonzero map this map must be surjective.
            The kernel of \(p_1 \circ i\) is contained in the kernel of \(p_1\), which is \(0 \oplus V\).
            Clearly \(\{0\} \oplus \isomorphic V\) is simple, and as such the kernel of \(p_1 \circ i\) is either \(0\), in which case \(U \isomorphic V\), or it's \(0 \oplus V\), in which case \(U = V \oplus V\).
            Suppose that the kernel of \(p_1 \circ i\) is zero, then \(p_1 \circ i \colon U \to V\) is an isomorphism, and we can denote its inverse by \(\varphi \colon V \to U\).
            The map \(p_1 \circ i \colon U \to V\) will also be an isomorphism in this case, and thus we can consider the composite \(p_2 \circ i \circ \varphi\), which is an isomorphism \(V \to V\).
            Since \(V\) is simple we know by Schur's lemma (\cref{lma:schur}) that \(p_2 \circ i \circ \varphi\) is a multiple of the identity, so \(p_2 \circ i \circ \varphi(v_1) = \lambda v_1\) for some \(\lambda \in \field\).
            However, we have \(p_2 \circ i \circ \varphi(v_1) = p_2 \circ i(v_1, v_2) = p_2(v_1, v_2) = v_2\), contradicting the assumption that \(v_2\) is not proportional to \(v_1\).
            Thus, we are left with the case where \(U = V \oplus V\).
        \end{proof}
    \end{lma}
    
    The following lemma is then a special case of the result of \cref{prp:tensor product of simple modules is simple}.
    
    \begin{lma}{}{lma:tensor product of simple modules is simple if it contains a pure tensor}
        Let \(V\) be a simple \(\lie{m}\)-module and \(W\) a simple \(\lie{l}\)-module.
        The smallest \((\lie{m} \oplus \lie{l})\)-submodule of \(V \otimes W\) containing a pure tensor \(v \otimes w \ne 0\) is \(V \otimes W\).
        \begin{proof}
            The module \(V \otimes W\) is spanned by vectors of the form \(v' \otimes w'\) for \(v' \in V\) and \(w' \in W\).
            Therefore, it suffices to show that \(v' \otimes w'\) is contained in the smallest submodule, \(U\), of \(V \otimes W\) containing \(v \otimes w\), and then necessarily \(U\) will contain all elements of \(V \otimes W\).
            Linearity of the \(\lie{g}\)-action tells us that \(0 \cdot v = 0\) for all \(v \in V\), and thus we have
            \begin{equation}
                (0, Y) \cdot (v \otimes w) = (0 \cdot v) \otimes w + v \otimes (Y \cdot w) = 0 \otimes w + v \otimes (Y \cdot w) = v \otimes (Y \cdot w).
            \end{equation}
            This is true for all \(Y \in \lie{l} \subseteq \lie{m} \oplus \lie{l}\).
            Now, we can generate a submodule in this way starting with \(v \otimes w\) and acting with \(Y \in \lie{l}\) as above.
            Doing so we generate a submodule of \(W\) which is not zero, and thus must be isomorphic to all of \(W\).
            This submodule is thus \(\field v \otimes W\) where \(\field v = \Span\{v\}\).
            Thus, \(v \otimes w' \in U\) for any \(w' \in W\).
            Similarly, one can show \(v' \otimes w \in U\) for any \(v' \in V\).
            Thus, \(v' \otimes w' \in U\).
        \end{proof}
    \end{lma}
    
    We are now ready to prove our result.
    
    \begin{prp}{}{prp:tensor product of simple modules is simple}
        Let \(V\) be a simple \(\lie{m}\)-module and \(W\) a simple \(\lie{l}\)-module.
        Then \(V \otimes W\) is a simple \((\lie{m} \oplus \lie{l})\)-module.
        \begin{proof}
            Let \(u \in V \otimes W\) be such that \(u \ne 0\).
            Let \(U\) be the smallest \((\lie{m} \oplus \lie{l})\)-submodule of \(V \otimes W\) containing \(u\).
            We want to show that \(U = V \otimes W\).
            If we know that \(0 \ne v \otimes w \in U\) then the result follows from \cref{lma:tensor product of simple modules is simple if it contains a pure tensor}.
            As an element of \(V \otimes W\) we can express \(u\) as a sum of simple tensors,
            \begin{equation}
                u = \sum_{i=1}^k v_i \otimes w_i.
            \end{equation}
            Note that we don't need scalar factors since we can just absorb them into the definition of the \(v_i\).
            We may further assume that the \(v_i\) are linearly independent, and that no pair of \(w_i\)s is proportional, since if \(w_i = \alpha w_j\) then \(v_i \otimes w_i + v_j \otimes w_j = (v_i + \alpha v_j) \otimes w_i\).
            It is also possible to find some element of \(U\) in place of \(u\) such that \(k\) is minimal in making the above sum true.
            If \(k = 1\) then \(u\) is a simple tensor and we're done.
            Suppose then that \(k > 1\).
            Since the \(v_i\) are linearly independent we can define an injective \(\lie{l}\)-module homomorphism \(\psi \colon W \oplus \dotsb \oplus W \to V \otimes W\) by
            \begin{equation}
                (w_1', \dotsc, w_k') \mapsto \sum_{i=1}^k v_i \otimes w_i'.
            \end{equation}
            Then \(u\) is the image of some \(\vv{w} = (w_1, \dotsc, w_k)\).
            If \(\vv{w}'\) is some element of the smallest \(\lie{l}\)-submodule \(W'\) of \(W \oplus \dotsb \oplus W\) containing \(\vv{w}\) then \(\psi(\vv{w}') \in U\).
            Consider the projection of \(W'\) onto the first two terms, \(W \oplus W \oplus 0 \oplus \dotsb\).
            This is an \(\lie{l}\)-submodule of \(W \oplus W\) and contains \((w_1, w_2)\).
            We have assumed that \(w_1\) and \(w_2\) are not proportional, and thus by \cref{lma:smallest submodule of V+V} this projection must be the whole of \(W \oplus W\).
            In particular, \((w_1, 0)\) is in the image of the projection.
            Let \(\vv{w}' \in W'\) be some element projecting to \((w_1, 0)\).
            Then \(\vv{w}' = (w_1, 0, w_2', w_3', \dotsc, w_k')\).
            This then gives us an element in \(U\) that can be formed by summing \(k - 1\) vectors of the form \(v_i \otimes w_i'\), which contradicts the minimality assumption of \(k\), and thus we must have \(k = 1\) and we are done.
        \end{proof}
    \end{prp}
    
    \begin{lma}{}{}
        Let \(V\) and \(W\) be finite-dimensional \(\lie{g}\)-modules and let \(\Hom_{\field}(V, W)\) be the space of linear maps \(V \to W\).
        Equipping this space with the \(\lie{g}\)-action
        \begin{equation}
            (X \cdot f)(v) = X \cdot f(v) - f(X \cdot v) \qquad X \in \lie{g}, v \in V, \text{ and } f \in \Hom_{\field}(V, W)
        \end{equation}
        defines a \(\lie{g}\)-module.
        \begin{proof}
            Linearity is immediate because \(f\) is linear so we may pull out any scalar factors and pull apart any sums.
            We need only show that the bracket acts as required:
            \begin{align}
                &(\bracket{X}{Y} \cdot f)(v) = \bracket{X}{Y} \cdot f(v) - f(\bracket{X}{Y} \cdot f)\\
                &= X \cdot (Y \cdot f(v)) - Y \cdot (X \cdot f(v)) - f(X \cdot (Y \cdot v) - Y \cdot (X \cdot v)) \notag\\
                &= X \cdot (Y \cdot f(v)) - Y \cdot (X \cdot f(v)) - f(X \cdot (Y \cdot v)) + f(Y \cdot (X \cdot v)) \notag\\
                &= X \cdot (Y \cdot f(v)) - f(X \cdot (Y \cdot v)) - (Y \cdot (X \cdot f(v)) - f(Y \cdot (X \cdot v))) \notag\\
                &= (X \cdot (Y \cdot f))(v) - (Y \cdot (X \cdot f))(v).
            \end{align}
            This holds for all \(v \in V\), and thus
            \begin{equation}
                \bracket{X}{Y} \cdot f = X \cdot (Y \cdot f) - Y \cdot (X \cdot f)
            \end{equation}
            as required.
        \end{proof}
    \end{lma}
    
    Since the sum of two \(\lie{g}\)-module homomorphisms is again a \(\lie{g}\)-module homomorphism this shows that \(\Hom_{\lie{g}}(V, W)\) is a \(\lie{g}\)-submodule of \(\Hom_{\field}(V, W)\), and hence \(\Hom_{\lie{g}}(V, W)\) is a \(\lie{g}\)-module in its own right.
    A fancy way of putting this is that \(\Rep\) is enriched over itself.
    
    \section{Representation Theory of \texorpdfstring{\(\specialLinearLie(2, \complex)\)}{sl(2, C)}}
    Recall that the complex Lie algebra \(\specialLinearLie(2, \complex)\) can be defined as the span of \(\{E, F, H\}\) with the bracket defined by
    \begin{equation}
        \bracket{H}{E} = 2E, \quad \bracket{H}{F} = -2F, \qand \bracket{E}{F} = H.
    \end{equation}
    These relations make \(H\) a \defineindex{semisimple element}, meaning that the linear map \(X \mapsto \bracket{H}{X}\) is diagonalisable.
    We can see this is the case because the bracket's definition above shows that \(E\) and \(F\) are eigenvectors of the map \(\bracket{H}{-}\), with eigenvalues \(+2\) and \(-2\) respectively, and alternativity tells us that \(\bracket{H}{H} = 0\), so \(H\) is an eigenvector of \(\bracket{H}{-}\) with eigenvalue \(0\).
    
    Let \(V\) be a finite-dimensional \(\specialLinearLie(2, \complex)\)-module.
    Then we can decompose \(V\) into generalised eigenspaces of \(H\):
    \begin{equation}
        V = \bigoplus_{\alpha \in \complex} V_\alpha
    \end{equation}
    where
    \begin{equation}
        V_\alpha = \{v \in V \mid (H - \alpha)^N \cdot v = 0 \text{ for some } N \in \naturals\}.
    \end{equation}
    We call the \(V_\alpha\) the \define{\(\symbf{\alpha}\)-weight spaces}\index{weight space} for \(H\).
    The \define{weight vectors}\index{weight vector} of \(V\) are those \(v \in V\) such that \(v \in V_\alpha\) for some \(\alpha \in \complex\).
    We call \(\alpha\) the \defineindex{weight} of such a \(v\).
    Note that not all vectors are weight vectors, any \(v \in V\) can be written as \(\sum_{\alpha \in \complex} v_\alpha\) with \(v_\alpha \in V_\alpha\), and so \(v\) is a weight vector only if \(v_\alpha = 0\) for all but one value of \(\alpha\).
    
    \begin{lma}{}{lma:e vecs of H are still e vecs after E and F act}
        If \(v\) is an eigenvector of \(H\) with eigenvalue \(\alpha\) then \(E \cdot v\) (\(F \cdot v\)) is an eigenvector of \(H\) with eigenvalue \(\alpha + 2\) (\(\alpha - 2\)).
        \begin{proof}
            We know that \(H \cdot v = \alpha v\).
            Using the action of a Lie bracket we have
            \begin{align}
                \bracket{H}{E} \cdot v &= H \cdot (E \cdot v) - E \cdot (H \cdot v)\\
                &= H \cdot (E \cdot v) - E \cdot (\alpha v)\\
                &= H \cdot (E \cdot v) - \alpha E\cdot v.
            \end{align}
            Using the defining relations of \(\specialLinearLie(2, \complex)\) we have
            \begin{equation}
                \bracket{H}{E} = 2E \implies 2E \cdot v = H \cdot (E \cdot v) - \alpha E \cdot v.
            \end{equation}
            Rearranging this we have
            \begin{equation}
                H \cdot (E \cdot v) = 2E \cdot v + \alpha E \cdot v = (\alpha + 2)(E \cdot v),
            \end{equation}
            and thus \(E \cdot v\) is an eigenvector of \(H\) with eigenvalue \(\alpha + 2\).
            The proof for \(F \cdot v\) is analogous.
        \end{proof}
    \end{lma}
    
    This extends to the following result about generalised eigenspaces.
    
    \begin{lma}{}{}
        If \(v \in V_\alpha\) is a weight vector of weight \(\alpha\) then \(E \cdot v\) (\(F \cdot v\)) is a weight vector of weight \(\alpha + 2\) (\(\alpha - 2\)).
%        \begin{proof}
            % TODO
%        \end{proof}
    \end{lma}
    
    If \(V_\alpha \ne 0\) but \(V_{\alpha + 2} = 0\) (\(V_{\alpha - 2} = 0\)) then this implies that \(E \cdot v = 0\) (\(F \cdot v\)) for all \(v \in V_\alpha\).
    We call \(v \in V\) a \defineindex{highest weight} (\defineindex{lowest weight}) if \(E \cdot v = 0\) (\(F \cdot v = 0\)) and \(H \cdot v = \alpha v\).
    
    \begin{lma}{}{}
        Let \(V\) be a finite-dimensional \(\specialLinearLie(2, \complex)\)-module.
        Then \(V\) contains a highest weight vector, \(v_0\).
        Setting \(v_{-1} = 0\) and \(v_i = \frac{1}{i!} F^i \cdot v_0\) for \(i \ge 0\) if \(v_0\) has weight \(\alpha\) then
        \begin{equation}
            H \cdot v_i = (\alpha - 2i)v_i, \quad E \cdot v_i  (\alpha - i + 1) v_{i-1}, \qand F \cdot v_i = (i + 1) v_{i+1}.
        \end{equation}
        \begin{proof}
            \Step{\(H \cdot v_i\)}
            First note that \(H \cdot v_0 = \alpha v_0\) so the \(i = 0\) case holds.
            We proceed by induction.
            For the base case, \(i = 1\), we have \(v_1 = F \cdot v_0\).
            We then have \(H \cdot (F \cdot v_0) = (\alpha - 2)(F \cdot v_0)\) by \cref{lma:e vecs of H are still e vecs after E and F act}, and so \(H \cdot v_1 = (\alpha - 2 \cdot 1)v_1\).
            Now suppose that the result holds for some \(k \in \naturals\), that is, we have \(H \cdot v_k = (\alpha - 2k) v_k\).
            Consider \(H \cdot v_{k+1}\):
            \begin{align}
                H \cdot v_{k+1} &= H \cdot \left( \frac{1}{k + 1}F \cdot v_k \right)\\
                &= \frac{1}{k + 1}H \cdot (F \cdot v_k)\\
                &= \frac{1}{k + 1} (\bracket{H}{F} \cdot v_k + F \cdot (H \cdot v_k))\\
                &= \frac{1}{k + 1} (-2F \cdot v_k + F \cdot (\alpha - 2k)v_k)\\
                &= \frac{1}{k + 1} (-2 + (\alpha - 2k))(F \cdot v_k)\\
                &= (\alpha - 2(k + 1)) v_{k + 1}.
            \end{align}
            Thus, by induction, the result holds for all \(i \in \naturals\).
            
            \Step{\(E \cdot v_i\)}
            First note that \(E \cdot v_0 = 0 = (\alpha - 0 + 1)v_{0-1}\) as \(v_{-1} = 0\), so the \(i = 0\) case holds.
            We again proceed by induction.
            for the base case, \(i = 1\), we have \(v_1 = F \cdot v_0\).
            We then have
            \begin{align}
                E \cdot v_1 &= E \cdot (F \cdot v_0)\\
                &= \underbrace{\bracket{E}{F}}_{=H} \cdot v_0 + F \cdot \underbrace{(E \cdot v_0)}_{=0}\\
                &= H \cdot v_0\\
                &= \alpha v_0\\
                &= (\alpha - 1 + 1)v_{1 - 1}.
            \end{align}
            Now suppose that the result holds for some \(k \in \naturals\), that is, we have \(E \cdot v_k = (\alpha - k + 1)v_{k - 1}\).
            Consider \(E \cdot v_{k+1}\):
            \begin{align}
                E \cdot v_{k+1} &= \frac{1}{k + 1}E \cdot (F \cdot v_k)\\
                &= \frac{1}{k + 1}(\bracket{E}{F} \cdot v_k + F \cdot (E \cdot v_k))\\
                &= \frac{1}{k + 1}(H \cdot v_k + (\alpha - k + 1)F \cdot v_{k-1})\\
                &= \frac{1}{k + 1}((\alpha - 2k)v_k + (\alpha - k + 1)(k + 1)v_k)\\
                &= \text{\textcolor{red}{Something isn't working here}}\\ % TODO: finish this proof
                &= (\alpha - k)v_k\\
                &= (\alpha - (k + 1) + 1)v_k
            \end{align}
            
            \Step{\(F \cdot v_i = (i + 1)v_{i+1}\)}
            This is evident from the definition of \(v_i\).
        \end{proof}
    \end{lma}
    
    Since we have \(H \cdot v_i = (\alpha - 2i)v_i\) the vectors \(v_i\) are all in different eigenspaces of \(H\), and thus are linearly independent.
    Since, by assumption, \(V\) is finite-dimensional there must therefore be some \(N \in \naturals\) such that \(v_N \ne 0\) but \(v_{N + 1} = 0\).
    Considering the action of \(E\) according to the above result we have \(0 = E \cdot 0 = E \cdot v_{N+1} = (\alpha - N)v_N\) and since \(v_N \ne 0\) we must therefore have \(\alpha - N = 0\), so \(\alpha = N\), and thus the only possible eigenvalues of \(H\) are non-negative integers.
    
    Further assume that \(V\) is a simple module, and that \(V \ne 0\).
    Then by simplicity the submodule \(\Span\{v_0, \dotsc, v_N\}\) must be all of \(V\), that is, \(\{v_0, \dotsc, v_N\}\) provides a basis for \(V\), and \(\dim V = N + 1\).
    Further, the weight spaces are \(V_{N-2i}\) for \(i = 0, \dotsc, N\), and each is one-dimensional, being spanned by \(v_i\), that is \(V_{N - 2i} = \complex v_i\).
    
    Combining these results we have proven the following.
    
    \begin{thm}{Classification of Finite Dimensional Simple \(\specialLinearLie(2, \complex)\)-Modules}{thm:sl2 reps classification}
        Let \(V\) be a finite-dimensional simple \(\specialLinearLie(2, \complex)\)-module with a highest weight vector of weight \(\alpha\).
        Then
        \begin{enumerate}
            \item \(\alpha\) is a non-negative integer, \(N\);
            \item \(\dim V = N + 1\) and the nonzero weight spaces of \(V\) are \(V_N, V_{N-2}, \dotsc, V_{-N}\), each of which is one-dimensional.
        \end{enumerate}
        Conversely, for any positive integer, \(N\), there exists a unique simple \(\specialLinearLie(2, \complex)\)-module \(V(N)\) with a highest weight vector of weight \(N\).
    \end{thm}
    
    This leads to the following picture of \(\specialLinearLie(2, \complex)\) representations:
    \begin{equation}
        \label{eqn:sl2 rep picture}
        \hfil
        \tikzexternaldisable
        \begin{tikzcd}
            0 \\
            V_{N} \arrow[u, "E"] \arrow[d, phantom, "\vdots"] \arrow[loop right, looseness=15, "H"]\\
            V_{\alpha + 2} \arrow[d, shift left, "F"] \arrow[loop right, looseness=10, "H"]\\
            V_{\alpha} \arrow[d, shift left, "F"] \arrow[u, shift left, "E"] \arrow[loop right, looseness=15, "H"]\\
            V_{\alpha - 2} \arrow[u, shift left, "E"] \arrow[loop right, looseness=10, "H"]\\
            V_{-N} \arrow[d, "F"] \arrow[u, phantom, "\vdots"] \arrow[loop right, looseness=10, "H"]\\
            0
        \end{tikzcd}
        \tikzexternalenable
    \end{equation}
    
    Consider the defining \(\specialLinearLie(2, \complex)\) module \(\complex^2\) where \(H\), \(E\), and \(F\) act as the matrices
    \begin{equation}
        H =
        \begin{bmatrix}
            1 & 0\\
            0 & -1
        \end{bmatrix}
        , \quad E = 
        \begin{bmatrix}
            0 & 1\\
            0 & 0
        \end{bmatrix}
        , \qand F = 
        \begin{bmatrix}
            0 & 0\\
            1 & 0
        \end{bmatrix}
        .
    \end{equation}
    The eigenvectors of \(H\) are
    \begin{equation}
        e_1 = 
        \begin{bmatrix}
            1\\ 0
        \end{bmatrix}
        \qqand
        e_2 =  
        \begin{bmatrix}
            0\\ 1
        \end{bmatrix}
        .
    \end{equation}
    These have eigenvalues \(+1\) and \(-1\) respectively.
    Of these only the first vanishes when we act with \(E\), giving
    \begin{equation}
        \begin{bmatrix}
            0\\ 1
        \end{bmatrix}
        .
    \end{equation}
    This means that the first eigenvector is a highest weight vector of \(\complex^2\).
    The weight of this vector, its \(H\) eigenvalue, is \(+1\)
    Further, this representation is simple, because if we start with some arbitrary vector then acting with \(E\) gives a vector along \(e_1\), and acting with \(F\) gives a vector along \(e_2\), spanning the whole space. So, this representation is isomorphic to \(V(1)\).
    
    Now consider the adjoint action of \(\specialLinearLie(2, \complex)\).
    Recall that this is the action of \(\specialLinearLie(2, \complex)\) on itself by the bracket.
    Then we may identify the representation space with \(\complex^3\) where
    \begin{equation}
        E = 
        \begin{bmatrix}
            1\\ 0\\ 0
        \end{bmatrix}
        , \quad H = 
        \begin{bmatrix}
            0\\ 1\\ 0
        \end{bmatrix}
        , \qand F = 
        \begin{bmatrix}
            0\\ 0\\ 1
        \end{bmatrix}
        .
    \end{equation}
    From the defining bracket relations we can see that we have
    \begin{equation*}
        \ad_E =
        \begin{bmatrix}
            0 & -2 & 0\\
            0 & 0 & 1\\
            0 & 0 & 0
        \end{bmatrix}
        , \quad
        \ad_H = 
        \begin{bmatrix}
            2 & 0 & 0\\
            0 & 0 & 0\\
            0 & 0 & -2
        \end{bmatrix}
        , \qand \ad_F = 
        \begin{bmatrix}
            0 & 0 & 0\\
            -1 & 0 & 0\\
            0 & 2 & 0
        \end{bmatrix}
        .
    \end{equation*}
    These matrices are such that \(\ad_X(Y) = \bracket{X}{Y}\).
    For example, we have
    \begin{equation}
        \ad_H(E) =
        \begin{bmatrix}
            2 & 0 & 0\\
            0 & 0 & 0\\
            0 & 0 & -2
        \end{bmatrix}
        \begin{bmatrix}
            1\\ 0\\ 0
        \end{bmatrix}
        =
        \begin{bmatrix}
            2\\ 0\\ 0
        \end{bmatrix}
        = 2E = \bracket{H}{E}.
    \end{equation}
    The eigenvectors of \(H\) are \(E\), \(H\), and \(F\), with eigenvalues \(+2\), \(0\), and \(-2\) respectively.
    Of these, only \(E\) has the property that \(E \cdot E = \ad_E(E) = \bracket{E}{E}\) vanishes, and thus \(E\) is a highest weight vector, and it has weight \(2\).
    Acting with \(F\) we have \(F \cdot E = \ad_F(E) = \bracket{F}{E} = -H\), and acting with \(F\) again we have \(F \cdot H = \ad_F(H) = \bracket{F}{H} = 2F\).
    Thus, \(E\) generates the whole space, showing that any submodule containing \(E\) must be all of \(\specialLinearLie(2, \complex)\), and a similar argument holds for \(H\) or \(F\).
    This shows that the module is simple.
    Then this module is isomorphic to \(V(2)\).
    
    \chapter{The Structure of Lie Algebras}
    In this chapter we will analyse the structure of general Lie algebras.
    We do this by looking at some special types of Lie algebras which we define here.
    
    \section{Centre}
    \begin{dfn}{}{}
        Let \(\lie{g}\) be a Lie algebra.
        Its \defineindex{centre}, \(\centre(\lie{g})\), is the ideal
        \begin{equation}
            \centre(\lie{g}) \coloneqq \{X \in \lie{g} \mid \bracket{X}{Y} = 0 \forall Y \in \lie{g}\}.
        \end{equation} 
    \end{dfn}
    
    When we interpret the bracket as a commutator the centre is exactly the collection of all elements which commute with all other elements of the Lie algebra when viewed as an associative algebra.
    In general we say that two elements commute if their bracket vanishes.
    This definition is then analogous to the centre of a group, which is all elements that commute with all other elements of the group.
    
    \begin{exm}{}{}
        We have that \(\centre(\generalLinearLie(n, \complex)) = \Span_{\complex}\{I_n\}\), this is just Schur's lemma (\cref{lma:schur}).
    \end{exm}
    
    \begin{lma}{}{}
        The centre of a Lie algebra is an ideal.
        \begin{proof}
            Let \(\lie{g}\) be a Lie algebra and \(\centre(\lie{g})\) its centre.
            If \(X, Y \in \centre(\lie{g})\) then we have
            \begin{equation}
                \bracket{X + Y}{Z} = \bracket{X}{Z} + \bracket{Y}{Z} = 0
            \end{equation}
            for all \(Z \in \lie{g}\), and so \(\centre(\lie{g})\) is closed under addition.
            Similarly, if \(\lambda \in \field\) and \(X \in \centre(\lie{g})\) then we have
            \begin{equation}
                \bracket{\lambda X}{Y} = \lambda \bracket{X}{Y} = 0
            \end{equation}
            for all \(Y \in \lie{g}\), so \(\centre(\lie{g})\) is closed under scalar multiplication.
            Thus, \(\centre(\lie{g})\) is a subspace of \(\lie{g}\).
            Now notice that if \(X \in \centre(\lie{g})\) and \(Y \in \lie{g}\) then for all \(Z \in \lie{g}\) we have
            \begin{equation}
                \bracket{\bracket{X}{Y}}{Z} = \bracket{0}{Z} = 0,
            \end{equation}
            and so \(\bracket{X}{Y} \in \centre(\lie{g})\) so \(\centre(\lie{g})\) is an ideal.
        \end{proof}
    \end{lma}
    
    Notice that a Lie algebra, \(\lie{g}\), is abelian if and only if we have \(\lie{g} = \centre(\lie{g})\).
    
    \section{Simple Lie Algebra}
    \begin{dfn}{}{def:simple lie alg}
        A Lie algebra, \(\lie{g}\), is \defineindex{simple} if it is nonabelian has no proper ideals.
    \end{dfn}
    
    We exclude abelian Lie algebras in part because this excludes the one-dimensional Lie algebra, which doesn't poses all of the properties that we want from simple Lie algebras.
    These properties will become clear later.
    
    \begin{exm}{}{}
        The Lie algebra \(\specialLinearLie(n, \complex)\) is simple.
    \end{exm}
    
    \section{Solvable and Nilpotent Lie Algebras}
    \begin{dfn}{Nilpotent Lie Algebra}{def:nilpotent lie alg}
        The \defineindex{lower central series} of a Lie algebra, \(\lie{g}\), is the series of subalgebras
        \begin{equation}
            \lie{g} = \lie{g}_0 \subseteq \lie{g}_1 \subseteq \lie{g}_2 \subseteq \dotsb.
        \end{equation}
        These are defined recursively by
        \begin{equation}
            \lie{g}_0 = \lie{g}, \quad \lie{g}_1 = \bracket{\lie{g}}{\lie{g}}, \qand \lie{g}_{k+1} = \bracket{\lie{g}}{\lie{g}_k}.
        \end{equation}
        
        A Lie algebra is \define{nilpotent}\index{nilpotent!Lie algebra} if its lower central series eventually terminates with \(0\).
        That is, there is some \(n \in \naturals\) such that \(\lie{g}_n = 0\).
    \end{dfn}
    
    \begin{exm}{}{}
        The Lie algebra \(\nilpotentLie(n, \complex)\) consisting of strictly upper triangular matrices is nilpotent.
        To see this note that the product of any two strictly upper triangular matrices has zero on both the diagonal and the superdiagonal.
        Then the product of any such matrix with a strictly upper triangular matrix has zero on the leading diagonal and the two diagonals above it.
        Continuing on like this we must eventually have all zeros, since the matrices are finite, and thus the lower central series terminates.
    \end{exm}
    
    \begin{lma}{}{}
        Each term in the lower central series is an ideal of \(\lie{g}\).
        \begin{proof}
            Since \(\bracket{\lie{g}}{\lie{g}}\) is defined to be the span of all elements of the form \(\bracket{X}{Y}\) with \(X, Y \in \lie{g}\) it is clearly a subspace of \(\lie{g}\).
            Further, since the bracket of two brackets is just a bracket of elements of \(\lie{g}\) it is again found in \(\bracket{\lie{g}}{\lie{g}}\).
            Thus, \(\bracket{\lie{g}}{\lie{g}}\) is a subalgebra of \(\lie{g}\).
            Now suppose that \(X, Y, Z \in \lie{g}\), then \(\bracket{X}{\bracket{Y}{Z}}\) is clearly an element of \(\bracket{\lie{g}}{\lie{g}}\), and by linearity this is also true if we replace \(\bracket{Y}{Z}\) with a linear combination of brackets, thus \(\bracket{\lie{g}}{\lie{g}}\) is an ideal of \(\lie{g}\).
            The same proof shows that \(\lie{g_2}\) is an ideal of \(\lie{g}_1\) and so on, and hence all of these are ideals of \(\lie{g}\) since an ideal of an ideal is again an ideal.
        \end{proof}
    \end{lma}
    
    \begin{dfn}{Solvable Lie Algebra}{def:solvable lie alg}
        The \defineindex{derived series} or \defineindex{upper central series} of a Lie algebra, \(\lie{g}\), is the series of subalgebras
        \begin{equation}
            \lie{g} = \lie{g}^0 \subseteq \lie{g}^1 \subseteq \lie{g}^2 \subseteq \dotsb.
        \end{equation}
        These are defined recursively by
        \begin{equation}
            \lie{g}_0 = \lie{g}, \quad \lie{g}^1 = \bracket{\lie{g}}{\lie{g}}, \qand \lie{g}^{k+1} = \bracket{\lie{g}^k}{\lie{g}^k}.
        \end{equation}
        The Lie subalgebra \(\lie{g}^1 = \lie{g}' = \bracket{\lie{g}}{\lie{g}}\) is sometimes called the \defineindex{derived subalgebra} of \(\lie{g}\).
        
        A Lie algebra is \defineindex{solvable} if its derived series eventually terminates with \(0\).
        That is, there is some \(n \in \naturals\) such that \(\lie{g}^n = 0\).
    \end{dfn}
    
    Note the difference between this definition and the lower central series, we have
    \begin{equation}
        \lie{g}_2 = \bracket{\lie{g}}{\lie{g}_1} = \bracket{\lie{g}}{\bracket{\lie{g}}{\lie{g}}}
    \end{equation}
    for the lower central series, and
    \begin{equation}
        \lie{g}^2 = \bracket{\lie{g}^1}{\lie{g}^1} = \bracket{\bracket{\lie{g}}{\lie{g}}}{\bracket{\lie{g}}{\lie{g}}}
    \end{equation}
    for the derived series.
    
    \begin{exm}{}{}
        The Lie algebra \(\borelLie(n, \complex)\) consisting of upper triangular matrices is nilpotent.
        To see this note that the commutator of two upper triangular matrices is strictly upper triangular, and the commutator of two strictly upper triangular matrices has zero on the leading diagonal and super diagonal, and the commutator of any two such matrices has three diagonals of zero, and so on.
        Thus, the derived series eventually terminates with 0.
    \end{exm}
    
    \begin{lma}{}{}
        Every nilpotent algebra is solvable.
        \begin{proof}
            Suppose that \(\lie{g}\) is a nilpotent Lie algebra.
            Then its upper central series terminates.
            This means that we eventually have
            \begin{equation}
                \bracket{\lie{g}}{\bracket{\lie{g}}{\bracket{\dotsb}{\lie{g}}}} = 0.
            \end{equation}
            This then necessitates that if the rightmost copy of \(\lie{g}\) is replaced with any subalgebra of \(\lie{g}\) we still have \(0\), and in particular we may replace it with some subalgebra given by iterated commutators to achieve an element of the derived series, and thus the derived series terminates.
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{lma:i and g/i solvable implies g solvable}
        If \(\lie{g}\) is a Lie algebra with an ideal \(\lie{i}\) such that \(\lie{i}\) is solvable and \(\lie{g}/\lie{i}\) is solvable then \(\lie{g}\) is solvable.
        \begin{proof}
            \Step{\((\lie{g}/\lie{i})^n \isomorphic (\lie{g}^n + \lie{i})/\lie{i}\)}
            We prove this by induction on \(n\).
            The base case, \(n = 0\), is that \(\lie{l}/\lie{i} \isomorphic (\lie{g} + \lie{i})/\lie{i}\).
            This is true because the second isomorphism theorem tells us that \((\lie{g} + \lie{i})/\lie{i} \isomorphic \lie{l}/(\lie{g} \cap \lie{i})\), and since \(\lie{i} \subseteq \lie{g}\) we have \(\lie{g} \cap \lie{i} = \lie{i}\), so we have \((\lie{g} + \lie{i})/\lie{i} \isomorphic \lie{l}/\lie{i}\).
            
            For the inductive step suppose the result holds for some \(k \in \naturals\), that is, \((\lie{g}/\lie{i})^k \isomorphic (\lie{g}^k + \lie{i})/\lie{i}\).
            Then we have
            \begin{alignat}{2}
                (\lie{g}/\lie{i})^{k + 1} &= ((\lie{g}/\lie{i})^k)' \qquad && \text{definition of derived subalgebra} \notag\\
                &\isomorphic ((\lie{g}^k + \lie{i})/\lie{i})' \qquad && \text{induction hypothesis}\\
                &\isomorphic ((\lie{g}^k + \lie{i})' + \lie{i})/\lie{i} \qquad && \text{base case}\\
                &\isomorphic (\lie{g}^{k + 1} + \lie{i}' + \lie{i})/\lie{i} \qquad && \lie{i}' \subseteq \lie{i}\\
                &= (\lie{g}^{k + 1} + \lie{i})/\lie{i}
            \end{alignat}
            and the result holds for all \(n \in \naturals\).
            
            \Step{\((\lie{g}^n)^m = \lie{g}^{n + m}\)}
            We prove this by induction on \(m\).
            For the base case we take \(m = 1\), then we have \((\lie{g}^n)^m = (\lie{g}^n)^1 = (\lie{g}^n)' = \lie{g}^{n+1}\) by definition.
            Now suppose that for some \(k \in \naturals\) we have \((\lie{g}^n)^k = \lie{g}^{n + k}\).
            Then consider \(k + 1\), we have \((\lie{g}^n)^{k + 1} = ((\lie{g}^n)^k)' = (\lie{g}^{n + k})' = \lie{g}^{n + k + 1}\), and so the result holds for all \(m \in \naturals\).
            
            \Step{\(\lie{g}\) is Solvable}
            By assumption \(\lie{i}\) and \(\lie{g}/\lie{i}\) are solvable, so there exist \(M, N \in \naturals\) such that \(\lie{i}^M = 0\) and \((\lie{g}/\lie{i})^N = 0\).
            We therefore have \(0 = (\lie{g}/\lie{i})^N \isomorphic (\lie{g}^N + \lie{i})/\lie{i}\).
            From this it follows that \(\lie{g}^N + \lie{i} \subseteq \lie{i}\), because this is the condition for a quotient to be zero.
            Thus, we must have \(\lie{g}^N \subseteq \lie{i}\).
            Taking the \(M\)th derived subalgebra of each side we have \((\lie{g}^N)^M \subseteq \lie{i}^M = 0\), and thus \(\lie{g}^{N + M} = (\lie{g}^N)^M = 0\), so \(\lie{g}\) is solvable.
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{}
        If \(\lie{g}\) is a Lie algebra with solvable ideals \(\lie{i}\) and \(\lie{j}\) then \(\lie{i} + \lie{j}\) is a solvable ideal of \(\lie{g}\).
        \begin{proof}
            Since \(\lie{i}\) and \(\lie{j}\) are solvable there exist \(M, N \in \naturals\) such that \(\lie{i}^N = 0\) and \(\lie{j}^M = 0\).
            By the second isomorphism theorem we have \((\lie{i} + \lie{j})/\lie{j} \isomorphic \lie{i}/(\lie{i} \cap \lie{j})\), and taking the \(M\)th derived subalgebra of each side we get \(((\lie{i} + \lie{j})/\lie{j})^M = (\lie{i}/(\lie{i} \cap \lie{j}))^M\).
            By the first step in the proof of \cref{lma:i and g/i solvable implies g solvable} we know that \((\lie{i} / (\lie{i} \cap \lie{j}))^M \isomorphic (\lie{i}^M + (\lie{i} \cap \lie{j}))/(\lie{i} \cap \lie{j}) = (0 + (\lie{i} \cap \lie{j}))/(\lie{i} \cap \lie{j}) = (\lie{i} \cap \lie{j})/(\lie{i} \cap \lie{j}) = 0\).
            Thus, \(((\lie{i} + \lie{j})/\lie{j})^M = 0\), so \((\lie{i} + \lie{j})/\lie{j}\) is solvable.
            Hence, by \cref{lma:i and g/i solvable implies g solvable} we know that \(\lie{i} + \lie{j}\) is solvable. 
        \end{proof}
    \end{lma}
    
    \begin{dfn}{Semisimple Lie Algebra}{def:semisimple lie alg}
        A Lie algebra, \(\lie{g}\), is semisimple if it contains no proper solvable ideals.
    \end{dfn}
    
    We immediately see that every simple Lie algebra is semisimple, since a simple Lie algebra contains no nonzero proper ideals, so it certainly doesn't contain any solvable ideals.
    Note that this is not true if we relax the definition of simple Lie algebras to allow abelian Lie algebras.
    
    \begin{lma}{}{}
        Let \(\lie{g}\) be a finite dimensional Lie algebra.
        Then there exists a maximal (with respect to inclusion) solvable ideal of \(\lie{g}\) called the \define{radical}\index{radical!of a Lie algebra} of \(\lie{g}\), denoted \(\rad \lie{g}\).
        \begin{proof}
            Let \(\{\lie{i}_{\alpha}\}_{\alpha \in A}\) be the collection of all solvable ideals.
            Define
            \begin{equation}
                \lie{i} = \sum_{\alpha \in A} \lie{i}_\alpha = \bigg\{\sum_{\alpha \in A} i_\alpha \mid i_\alpha \in \lie{i}_\alpha \text{ and } i_\alpha = 0 \text{ for all but finitely many } \alpha \in A\}.
            \end{equation}
            This is a subspace of \(\lie{g}\) since it is a sum of subspaces.
            
            We claim that we actually have
            \begin{equation}
                \lie{i} = \lie{i}_{\alpha_1} + \dotsb + \lie{i}_{\alpha_n}
            \end{equation}
            for some finite indexing subset \(\{\alpha_1, \dotsc, \alpha_n\} \subseteq A\).
            This is the case because \(\lie{g}\) is finite dimensional, so \(\lie{i}\) must also be finite dimensional.
            If \(\lie{i} = \lie{i}_{\alpha_1}\) for some \(\alpha_1 \in A\) then we are done.
            If this is not the case then we can find some \(\alpha_2 \in A\) such that \(\lie{i}_{\alpha_1} + \lie{i}_{\alpha_2} \ne \lie{i}_{\alpha_1}\).
            If \(\lie{i} = \lie{i}_{\alpha_1} + \lie{i}_{\alpha_2}\) we are done.
            Iterating this process we see that since we get a new subspace each time we add an \(\lie{i}_{\alpha_k}\) and we clearly have \(\lie{i}_{\alpha_1} + \dotsb \lie{i}_{\alpha_{k-1}} \subseteq \lie{i}_{\alpha_1} + \dotsb + \lie{i}_{\alpha_{k-1}} + \lie{i}_{\alpha_k}\) the dimension must increase at each step, and thus this process must terminate for \(\lie{i}\) to be finite dimensional.
            
            By construction any solvable ideal, \(\lie{j}\), can be found in \(\lie{i}\), since we have \(\lie{j} = \lie{i}_{\alpha}\) for some \(\alpha \in A\) and we therefore have terms of the form \(j = 0 + \dotsb + 0 + j + 0 + \dotsb + 0 \in \lie{i}\) for each \(j \in \lie{j}\).
            Thus, \(\lie{i}\) is a maximal subspace of \(\lie{g}\) containing all solvable ideals.
            
            We now need only show that \(\lie{i}\) is an ideal.
            That it is a subalgebra follows by linearity of the bracket and the fact that \(\lie{i}\) is defined as a sum of subalgebras.
            That it is an ideal follows because each ideal in the sum defining \(\lie{i}\) has the absorbing property, and so the whole sum has the absorbing property.
        \end{proof}
    \end{lma}
    
    This result tells us that \(\lie{g}\) is semisimple if and only if \(\rad \lie{g} = 0\).
    One thing we can say is that \(\lie{g}/\rad \lie{g}\) is semisimple.
    This is because an ideal of the quotient \(\lie{g}/\rad \lie{g}\) is of the form \(\lie{i}/\rad \lie{g}\) where \(\lie{i} \supseteq \rad \lie{g}\) is an ideal of \(\lie{i}\).
    Supposing that \(\lie{i}/\rad\lie{g}\) is solvable, and knowing that \(\rad\lie{g}\) is solvable we must have that \(\lie{i}\) is solvable, meaning that \(\lie{i} \subseteq \rad\lie{g}\).
    Thus, we have \(\lie{i} = \rad\lie{g}\), and so \(\lie{i}/\rad\lie{g} = \lie{i}/\lie{i} = 0\).
    Thus, there are no nontrivial proper solvable ideals of \(\lie{g}/\rad\lie{g}\), so \(\lie{g}/\rad\lie{g}\) is semisimple.
    
    \section{Engel's Theorem}
    Let \(V\) be a vector space.
    A \define{nilpotent}\index{nilpotent!endomorphism} of \(V\) is a linear map \(\varphi \colon V \to V\) such that there exists some \(N \in \naturals\) such that \(\varphi^N\) is the zero map.
    That is, \(\varphi^N(v) = 0\) for all \(v \in V\).
    
    Let \(\lie{g}\) be a Lie algebra.
    An element, \(X\), of \(\lie{g}\) is called \define{\(\symbf{\ad}\)-nilpotent}\index{ad-nilpotent@\(\ad\)-nilpotent}\index{nilpotent!element of a Lie algebra} if \(\ad_X\) is a nilpotent endomorphism of \(\lie{g}\).
    
    Before we can prove the big result of this section, Engel's theorem, we need a couple of lemmas.
    The proof of the first is quite a lot of algebra, manipulating various sums, the details of the proof aren't important, so I'd suggest skipping it.
    
    \begin{lma}{}{lma:power of the adjoint action}
        Let \(A\) be an associative algebra with the Lie algebra structure given by the commutator.
        Then for \(n \in \naturals\) and \(X, Y \in A\) we have
        \begin{equation}
            \ad_X^n(Y) = \sum_{k=0}^n (-1)^k \binom{n}{k} x^{n-k}yx^k.
        \end{equation}
        \begin{proof}
            The proof is by induction on \(n\).
            For \(n = 1\) we have
            \begin{align}
                \sum_{k=0}^1 (-1)^{k} \binom{2}{k} X^{1 - k}YX^k &= (-1)^0 \binom{2}{0}XY + (-1)^1 \binom{2}{1}YX\\
                &= XY - YX\\
                &= \bracket{X}{Y}\\
                &= \ad_X^1(Y).
            \end{align}
            Now suppose that the result holds for some \(m \in \naturals\).
            Consider the following calculation
            \begin{align}
                \ad_X^{m + 1}(Y) &= \bracket{x}{\ad_X^m(Y)}\\
                &= \bracket*{X}{\sum_{k=0}^m (-1)^k \binom{m}{k} X^{m-k}YX^k}\\
                &= \sum_{k=0}^m (-1)^k \binom{m}{k} \bracket{X}{X^{m-k}YX^k}\\
                &= \sum_{k=0}^m (-1)^k \binom{m}{k} (XX^{m-k}YX^k - X^{m-k}YX^kX)\\
                &= \sum_{k=0}^m (-1)^k \binom{m}{k} (X^{m-k+1}YX^k - X^{m-k}YX^{k+1})\\
                &= \sum_{k=0}^m (-1)^k \binom{m}{k} X^{m-k+1}YX^k - \sum_{k=0}^m (-1)^k \binom{m}{k} X^{m-k}YX^{k+1}. \notag
            \end{align}
            We can reindex the second sum so that \(k\) runs from \(1\) to \(m + 1\).
            To do so we replace each \(k\) in the second sum with \(k - 1\):
            \begin{equation}
                \sum_{k=0}^m (-1)^k \binom{m}{k} X^{m-k}YX^{k+1} = \sum_{k=1}^{m+1} (-1)^{k-1} \binom{m}{k-1} X^{m-k+1}YX^k. 
            \end{equation}
            Thus, we have
            \begingroup
            \allowdisplaybreaks
            \begin{align}
                \ad_X^{m+1}(Y) &= \sum_{k=0}^m (-1)^k \binom{m}{k} X^{m-k+1}YX^k\\
                &\quad - \sum_{k=1}^{m+1} (-1)^{k-1} \binom{m}{k-1} X^{m-k+1}YX^k \notag\\
                &= (-1)^0 \binom{m}{0} X^{m+1}YX^0 + \sum_{k=1}^m (-1)^k \binom{m}{k} X^{m-k+1}YX^k \notag\\
                &\quad- \sum_{k-1}^m (-1)^{k-1}\binom{m}{k-1}X^{m-k-1}YX^k - (-1)^m \binom{m}{m} X^0YX^{m+1} \notag\\
                &= X^{m+1}Y + \sum_{k=1}^m (-1)^k \binom{m}{k} X^{m-k+1}YX^k\\
                &\quad- \sum_{k-1}^m (-1)^{k-1}\binom{m}{k-1}X^{m-k-1}YX^k - (-1)^m YX^{m+1} \notag\\
                &= \sum_{k=1}^m \left( (-1)^k \binom{m}{k} + (-1)^k \binom{m}{k - 1} \right) X^{m-k+1}YX^k \notag\\
                &\quad+ X^{m+1}Y - (-1)^mYX^{m+1}\\
                &= \sum_{k=1}^m (-1)^k \binom{m + 1}{k} X^{m-k+1}YX^k + X^{m+1}Y - (-1)^mYX^{m+1}\notag
            \end{align}
            \endgroup
            where we've used the result
            \begin{equation}
                \binom{m}{k} + \binom{m}{k - 1} = \frac{m + 1}{k},
            \end{equation}
            which can be proved by manipulating some factorials.
            We can then recognise that the two terms not in the sum are just the \(k = 0\) term and the \(k = m + 1\) term, so that we have
            \begin{equation}
                \ad_X^{m+1}(Y) = \sum_{k=0}^{m+1} (-1)^k \binom{m+1}{k} X^{m + 1 - k} Y X^k.
            \end{equation}
            So, the result holds for all \(n \in \naturals\).
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{lma:nilpotent implies ad-nilpotent}
        Let \(V\) be a finite dimensional vector space and let \(X \in \generalLinearLie(V)\) be a nilpotent endomorphism.
        Then \(X\) is \(\ad\)-nilpotent.
        Further, if \(n \in \naturals\) is the smallest positive integer making \(X^n = 0\) then \(2n + 1\) is such that \(\ad_X^{2n + 1} = 0\).
        \begin{proof}
            Let \(X \in \generalLinearLie(V)\) be a nilpotent endomorphism.
            That is, there is some \(n \in \naturals\) such that \(X^n = 0\).
            Since \(\generalLinearLie(V)\) is a Lie algebra under the commutator we may apply the results of \cref{lma:power of the adjoint action}.
            For all \(X, Y \in \generalLinearLie(V)\) we have
            \begin{equation}
                \ad_X^{2n+1}(Y) = \sum_{k=0}^{2n+1} (-1)^k \binom{2n+1}{k} X^{2n + 1 - k}YX^k.
            \end{equation}
            This always vanishes, and we can show this by showing that every term must vanish:
            \begin{itemize}
                \item if \(k \ge n\) then the term contains \(X^k = 0\); or
                \item if \(k < n\) then \(2n + 1 - k > n\) then the term contains \(X^{2n + 1 - k} = 0\).
            \end{itemize}
            So, every term is zero, and the whole expression vanishes.
        \end{proof}
    \end{lma}
    
    \begin{thm}{}{thm:engel}
        Let \(V\) be a finite dimensional vector space over \(\field\) with \(V \ne 0\).
        Let \(\nilpotentLie\) be a subalgebra of \(\generalLinearLie(V)\) such that all elements of \(\nilpotentLie\) are nilpotent endomorphisms of \(V\).
        Then there exists some vector \(v \in V\) with \(v \ne 0\) such that \(\nilpotentLie \cdot v = 0\), that is, \(v \in \ker \varphi\) for all \(\varphi \in \nilpotentLie\).
        \begin{proof}
            The proof is by induction on \(\dim \nilpotentLie\).
            
            \Step{Base Case}
            The base case is \(\dim \nilpotentLie = 1\).
            In this case we know that \(\nilpotentLie = \field X\) for some (in fact, any) \(X \in \nilpotentLie \setminus \{0\}\).
            We know that \(X\) is nilpotent by assumption.
            So, there exists some minimal \(n \in \naturals\) such that \(X^n = 0\), and since \(X \ne 0\) we know that \(n \ge 2\).
            By the assumption that \(n\) is minimal we know that \(X^{n-1} \ne 0\).
            Recalling that \(X^{n-1}\) is an element of \(\generalLinearLie(V)\) this means that there exists some \(w \in V\) such that \(X^{n-1}(w) \ne 0\).
            However, we have that
            \begin{equation}
                X(X^{n-1}(w)) = X^n(W) = 0(w) = 0,
            \end{equation}
            and so \(X^{n-1}(w) \in \ker X\).
            Thus, \(v = X^{n-1}(w)\) is as needed, and we are done with the base case.
            
            \Step{Induction}
            Suppose that the result holds when \(\dim \nilpotentLie = k\) for some \(k \in \naturals\).
            We first show that there is an ideal of \(\nilpotentLie\) of codimension 1.
            That is, there is an ideal, \(\lie{l} \subseteq \nilpotentLie\) such that \(\dim \lie{l} = k - 1\).
            
            To do this we first suppose that \(\lie{l} \subseteq \nilpotentLie\) is a maximal proper subalgebra of \(\nilpotentLie\).
            This makes sense because
            \begin{itemize}
                \item \(\nilpotentLie\) definitely has subalgebras, any one-dimensional subspace is a subalgebra; and
                \item \(\dim \nilpotentLie\) is finite, and so the dimension of \(\lie{l}\) is bounded, and as such we can simply take all subalgebras and pick one with maximal dimension.
            \end{itemize}
            
            Since \(\nilpotentLie\) is a vector space and \(\lie{l} \subseteq \nilpotentLie\) is a subspace we can form the quotient \emph{vector space} \(\nilpotentLie/\lie{l}\).
            We do now know if this is a quotient \emph{Lie algebra}, since we only know that \(\lie{l}\) is a subalgebra, not that it's an ideal.
            Consider the adjoint action of \(\nilpotentLie\) on itself.
            We can restrict this to consider the action of \(\lie{l}\) on \(\nilpotentLie\), given by \(X \cdot Y = \bracket{X}{Y}\) for \(X \in \lie{l}\) and \(Y \in \nilpotentLie\).
            This makes \(\nilpotentLie\) an \(\lie{l}\)-module.
            Since \(\lie{l}\) is a subalgebra we have \(\bracket{\lie{l}}{\lie{l}} \subseteq \lie{l}\), which means that this action naturally gives the quotient vector space, \(\nilpotentLie/\lie{l}\), an \(\lie{l}\)-module structure given by
            \begin{equation}
                X \cdot (Y + \lie{l}) = \bracket{X}{Y} + \lie{l}
            \end{equation}
            for \(X \lie{l}\) and \(Y \in \nilpotentLie\).
            
            By assumption elements of \(\nilpotentLie\) are nilpotent, and thus by \cref{lma:nilpotent implies ad-nilpotent} they are also \(\ad\)-nilpotent.
            Hence, \(\ad(\lie{l}) = \{\ad_X \mid X \in \lie{l}\}\) consists of nilpotent endomorphisms in \(\generalLinearLie(\nilpotentLie)\), and \(\ad(\lie{l})\) is a Lie algebra under the commutator.
            Since \(\lie{l}\) is a proper subalgebra of \(\nilpotentLie\) we know that \(\dim \lie{l} < \dim \nilpotentLie\), and hence \(\dim \ad(\lie{l}) < \dim \nilpotentLie = k\).
            Hence, we can apply the inductive hypothesis to \(\ad(\lie{l})\), meaning that there is some \(\overbar{Y} \in \nilpotentLie/\lie{l}\) such that \(\overbar{Y} \ne 0\) and \(\lie{l} \cdot \overbar{Y} = 0\).
            
            Let \(Y \in \nilpotentLie\) be a representative of \(\overbar{Y}\).
            That is, \(\overbar{Y} = Y + \nilpotentLie\).
            Then we have \(\bracket{\lie{l}}{Y} \subseteq \lie{l}\) and \(\).
            This implies that \(\bracket{\lie{l} \oplus \field Y}{\lie{l} \oplus \field Y} \subseteq \lie{l} \oplus \field Y\).
            This means that \(\lie{l} \oplus \field \lie{l}\) is a subalgebra of \(\nilpotentLie\).
            Now, \(\dim(\lie{l} \oplus \field Y) = \dim(\lie{l}) + 1\), and so by the assumption that \(\lie{l}\) is a \emph{maximal} proper subalgebra we must have that \(\lie{l} \oplus \field Y = \nilpotentLie\).
            This proves that \(\dim \lie{l} = \dim(\nilpotentLie) - 1\).
            It also implies that \(\lie{l}\) is an ideal, since all elements in \(\nilpotentLie\) which are not in \(\lie{l}\) are just scalar multiples of \(Y\), and we've seen that \(\bracket{\lie{l}}{Y} \in \lie{l}\), so \(\lie{l}\) has the required absorbing property.
            
            Since \(\dim \lie{l} < \dim \nilpotentLie\) we can apply the induction hypothesis, and we know that there is some \(v \in \lie{l}\) with \(v \ne 0\) such that \(\lie{l} \cdot v = 0\).
            Let \(W \subseteq V\) be the subspace of all vectors with this property.
            Taking the decomposition \(\nilpotentLie = \lie{l} \oplus \field Y\) it is sufficient to show that there is some \(w \in W\) with \(w \ne 0\) such that \(Y(w) = 0\), since then we will certainly have \((X + Y)(w) = X(w) + Y(w) = 0 + 0 = 0\) for \(X \in \lie{l}\).
            To show that such a \(w\) exists take \(X \in \lie{l}\), then since the bracket in \(\generalLinearLie(V)\) is just the commutator we can evaluate at some \(w \in W\) and we have
            \begin{equation}
                XY(w) = YX(w) + \bracket{X}{Y}(w) = 0,
            \end{equation}
            where we've used the fact that \(X, \bracket{X}{Y} \in \lie{l}\) (since \(\lie{l}\) is an ideal) and thus \(X(w) = \bracket{X}{Y}(w) = 0\) by definition of \(W\).
            Therefore, \(Y(W) \subseteq W\).
            Since \(Y\) is a nilpotent endomorphism of \(V\) and preserves \(W\) it restricts to a nilpotent endomorphism of \(W\).
            Thus, there exists some \(w \in W\) with \(w \ne 0\) such that \(Y(w) = 0\).
        \end{proof}
    \end{thm}
    
    \subsection{Corollaries of Engel's Theorem}
    Before we can prove some corollaries of Engel's theorem we need a definition and a couple of lemmas.
    
    \begin{dfn}{Flag}{}
        Let \(V\) be a vector space.
        A \defineindex{flag}, \(V_{\bullet}\), in \(V\) is an ascending sequence of vector spaces
        \begin{equation}
            0 = V_0 \subsetneq V_1 \subsetneq \dotsb \subsetneq V_n = V.
        \end{equation}
        A flag is \define{complete}\index{complete flag} if \(\dim V_{i+1} = \dim V_i + 1\), or equivalently \(\dim(V_{i+1}/V_i) = 1\).
        
        For \(\field^n\) with basis \(\{e_1, \dotsc, e_n\}\) the \defineindex{standard flag} is the complete flag given by taking \(V_0 = 0\) and \(V_i = \Span_{\field}\{e_1, \dotsc, e_i\}\).
    \end{dfn}
    
    Each choice of a basis gives rise to a complete flag, although not uniquely, since \(\{e_1, e_2, e_3, \dotsc, e_n\}\) and \(\{e_1, 2e_2, e_3, \dotsc, e_n\}\) both give rise to the same standard flag.
    
    \begin{lma}{}{lma:flag vs upper triangular}
        Take \(V_{\bullet}\) to be the standard flag in \(\complex^n\) with basis \(\{e_1, \dotsc, e_n\}\).
        Then for \(X \in \generalLinear(n, \complex)\) we have
        \begin{itemize}
            \item \(X \in \borelLie(n, \complex)\) if and only if \(X(V_i) \subseteq V_i\); and
            \item \(X \in \nilpotentLie(n, \complex)\) if and only if \(X(V_i) \subseteq V_{i-1}\).
        \end{itemize}
        \begin{proof}
            This is just restating the definition of the (strictly) upper triangular matrices using the language of flags.
            We can identify \(V_i\) as the space of all column vectors which are zero after the \(i\)th row.
            Then if \(v \in V_i\) we know that \(X(v) \in V_i\) if and only if \(X(v)\) has zeros after the \(i\)th row, which is true exactly when \(X\) is upper triangular.
            If we replace \(X\) with a strictly upper triangular matrix then \(X(v)\) will have zeros after the \((i - 1)\)st row, and so will be in \(V_{i-1}\).
        \end{proof}
    \end{lma}
    
    We say that \(X \in \borelLie(n, \complex)\) preserves the flag and \(X \in \nilpotentLie(n, \complex)\) strictly preserves the flag.
    
    Note that each basis choice gives us a (not necessarily unique) algebra \(\borelLie(n, \complex)\) (called a Borel subalgebra), of matrices which are upper triangular when expressed in that basis.
    While this pairing of bases to Borel subalgebras is not a bijection the pairing of standard flags to Borel subalgebras is, since two bases give the same Borel subalgebra if and only if they are related by rescaling the basis elements, if and only if they correspond to the same standard flag.
    
    \begin{crl}{}{crl:nilpotent always corresponds to strictly upper triangular}
        Let \(V\) be a finite dimensional vector space with \(V \ne 0\).
        Let \(\nilpotentLie\) be a subalgebra of \(\generalLinearLie(V)\) such that all elements of \(\nilpotentLie\) are nilpotent endomorphisms of \(V\).
        Then there exists a basis of \(V\) such that all elements of \(\nilpotentLie\) correspond to strictly upper triangular matrices in this basis.
        \begin{proof}
            By \cref{lma:flag vs upper triangular} this result is equivalent to the statement that there exists a complete flag, \(V_{\bullet}\), such that \(\nilpotentLie(V_i) \subseteq V_{i-1}\), that is \(X(V_i) \subseteq V_{i - 1}\) for all \(X \in \nilpotentLie\).
            
            We can prove this by induction on \(\dim V\).
            The base case is \(\dim V = 1\).
            If \(X \colon V \to V\) is nilpotent, so that \(X^n = 0\) for some \(n \in \naturals\), then in any basis of \(V\) we can express \(X\) as \([X]_{\basis} = (b)\) for some \(b \in \complex\).
            Then the nilpotent condition is that \((b)^n = (b^n) = 0\), which is true if and only if \(b = 0\), and hence \([X]_{\basis} = (0)\), which is strictly upper triangular.
            
            For the inductive step suppose that for any vector space of dimension less than \(\dim V\) there is a complete flag which is completely preserved by elements of \(\nilpotentLie\).
            By Engel's theorem (\cref{thm:engel}) we know that there is some \(v_1 \in V\) such that \(v_1 \ne 0\) and \(\nilpotentLie v_1 = 0\) (that is, \(X(v_1) = 0\) for all \(X \in \nilpotentLie\)).
            Consider the flag
            \begin{equation}
                0 = V_0 \subsetneq V_1 = \complex v_1 \subsetneq V.
            \end{equation}
            Note that we're assuming that \(\dim V > \dim V_1 = 1\), else we would already be done by the base case.
            
            We now consider the quotient \(V/V_1\).
            This has \(\dim(V/V_1) = \dim V - \dim V_1 = \dim V - 1 \le \dim V\), and so the induction hypothesis applies.
            This tells us that there is a flag
            \begin{equation}
                0 = \overbar{V}_1 \subsetneq \overbar{V}_2 \subsetneq \dotsb \subsetneq \overbar{V}_{n-1} = V/V_1
            \end{equation}
            which is strictly preserved by \(\nilpotentLie\), so \(\nilpotentLie \overbar{V}_i \subseteq \overbar{V}_{i-1}\).
            We use the preimage of the \(\overbar{V}_i\) under the quotient map \(V \twoheadrightarrow V/V_1\) to extend our initial flag by defining
            \begin{equation}
                V_i = \{v \in V \mid v + V_1 \in \overbar{V}_i\}.
            \end{equation}
            Note that this agrees with our earlier definition of \(V_1\) since \(v + V_1 \in \overbar{V}_1 = 0\) if and only if \(v \in V_1\).
            So, we have a complete flag
            \begin{equation}
                0 = V_0 \subsetneq V_1 \subsetneq \dotsb \subsetneq V_{n-1} = V/V_1 \subsetneq V_n = V.
            \end{equation}
            We just need to show that this is strictly preserved by \(\nilpotentLie\).
            To see this note that if \(X \in \nilpotentLie\) then \(X(\overbar{V}_i) \subseteq \overbar{V}_{i-1}\).
            This means that \(X(V_i) \subseteq V_{i-1} + V_1 = V_{i-1}\).
            Thus, this flag is as required.
        \end{proof}
    \end{crl}
    
    We now have another lemma before we can prove another corollary of Engel's theorem, which is really a corollary of this corollary.
    
    \begin{lma}{}{lma:nilpotent when quotiented by centre implies nilpotent}
        Let \(\nilpotentLie\) be a Lie algebra such that \(\nilpotentLie/\centre(\nilpotentLie)\) is nilpotent.
        Then \(\nilpotentLie\) is nilpotent.
        \begin{proof}
            For \(\lie{g}\) a Lie algebra and \(\lie{i} \subseteq \lie{g}\) an ideal we have \((\lie{g}/\lie{i})_n = \lie{g}_n + \lie{i}/\lie{i}\).
            Taking \(\lie{g} = \nilpotentLie\) and \(\lie{i} = \centre(\nilpotentLie)\) we have \((\nilpotentLie_n + \centre(\nilpotentLie))/\centre(\nilpotentLie) = (\nilpotentLie/\centre(\nilpotentLie))_n\).
            By assumption \(\nilpotentLie/\centre(\nilpotentLie)\) is nilpotent, so there is some \(N \in \naturals\) such that \((\nilpotentLie/\centre(\nilpotentLie))_N = 0\).
            We therefore have \((\nilpotentLie_N + \centre(\nilpotentLie))/\centre(\nilpotentLie) = 0\), which is only possible if \(\nilpotentLie_N = 0\), which means that \(\nilpotentLie\) is nilpotent.
        \end{proof}
    \end{lma}
    
    This result is useful because we always have the adjoint action, \(\ad \colon \lie{g} \to \generalLinearLie(\lie{g})\), and by definition \(\ker \ad = \centre(\lie{g})\).
    Then using the first isomorphism theorem we have \(\ad(\lie{g}) \isomorphic \lie{g} / \centre(\lie{g})\).
    This means that we always have an embedding of \(\lie{g}/\centre(\lie{g})\) into \(\generalLinearLie(\lie{g})\) for any Lie algebra, nilpotent or not.
    This lets us study \(\lie{g}\) by studying \(\generalLinearLie(\lie{g})\), so long as we're prepared to set all central elements to zero.
    
    \begin{crl}{}{crl:ad-nilpotent implies nilpotent}
        Let \(\nilpotentLie\) be a finite dimensional Lie algebra.
        If every element of \(\nilpotentLie\) is \(\ad\)-nilpotent then \(\nilpotentLie\) is nilpotent.
        \begin{proof}
            By \cref{lma:nilpotent when quotiented by centre implies nilpotent} it is sufficient to prove that \(\nilpotentLie/\centre(\nilpotentLie)\).
            By the remark following \cref{lma:nilpotent when quotiented by centre implies nilpotent} we have that \(\nilpotentLie/\centre(\nilpotentLie) \isomorphic \ad(\nilpotentLie) \subseteq \generalLinearLie(\nilpotentLie)\).
            By \cref{crl:nilpotent always corresponds to strictly upper triangular} there exists a basis for \(\nilpotentLie\) such that elements of \(\ad(\nilpotentLie)\) are strictly upper triangular matrices in this basis.
            This implies that, in this basis, \(\ad(\nilpotentLie) \subseteq \nilpotentLie(n, \complex)\), and this implies that \(\nilpotentLie/\centre(\nilpotentLie)\) is nilpotent, so \(\nilpotentLie\) is nilpotent by \cref{lma:nilpotent when quotiented by centre implies nilpotent}.
        \end{proof}
    \end{crl}
    
    \section{Lie's Theorem}
    Given the notation setup in the statement of Engel's theorem (\cref{thm:engel}) we may state the conclusion as \enquote{there is a common eigenvector, \(v \in V\), for all endomorphisms \(X \in \nilpotentLie\) such that the eigenvalue of \(v\) for \(X\) is zero}.
    Lie's theorem is the analogue for solvable Lie algebras.
    It states that there is a common eigenvector, but does not force the eigenvalue to be zero.
    Let \(\lie{g} \subseteq \generalLinearLie(V)\) be some Lie algebra.
    Then \(v \in V\) is an eigenvector of all \(X \in \lie{g}\) if there exist scalars \(\alpha_X \in \field\) for each \(X \in \lie{g}\) such that \(Xv = \alpha_X v\).
    We can then view this as a map \(\alpha \colon \lie{g} \to \field\), \(X \mapsto \alpha_X\).
    Further, this map is linear since \(\alpha_{X + Y}\) is the eigenvalue of \(v\) under the operation \(X + Y\) and \((X + Y)v = Xv + Yv = \alpha_X v + \alpha_Y v\) so we can identify \(\alpha_{X + Y} = \alpha_X + \alpha_Y\), and similarly, \((\lambda X)v = \lambda(Xv) = \lambda \alpha_X v\) so \(\alpha_{\lambda X} = \lambda \alpha_X\).
    Thus, \(\alpha \in \lie{g}^*\).
    So, \(v \in V\) with \(v \ne 0\) is a common eigenvalue of all elements of \(\lie{g}\) if and only if there exists some \(\alpha \in \lie{g}^*\) such that \(Xv = \alpha(X)v\) for all \(X \in \lie{g}\).
    
    \begin{thm}{Lie}{thm:lie}
        Let \(V\) be a finite-dimensional vector space over an algebraically closed field of characteristic zero (so basically, \(\field = \complex\)), \(\field\), with \(V \ne 0\), and let \(\lie{s}\) a solvable subalgebra of \(\generalLinearLie(V)\).
        Then there exists a common eigenvector for all endomorphisms in \(\lie{s}\).
        \begin{proof}
            The steps taken in this proof are the same as in Enge's theorem (\cref{thm:engel}), but the justifications for why each step holds differ slightly.
            
            The proof is by induction on \(\dim \lie{s}\).
            For the base case, \(\dim \lie{s} = 1\), the result is trivial as \(\lie{s} = \complex X\) for some nonzero endomorphism \(X \in \generalLinearLie(V)\), and by the assumption that we work over an algebraically closed field \(X\) has an eigenvector, \(v \in V\) with \(v \ne 0\), so \(Xv = \alpha v\) for some \(\alpha \in \field\), and this is an eigenvector of all \(Y \in \lie{s}\) since \(Y = yX\) for some \(y \in \field\), since \(Yv = yXv = y\alpha v\), so \(v\) is an eigenvector of \(Y\) with eigenvalue \(y\alpha\).
            
            Now suppose that \(\dim\lie{s} > 1\) and that the hypothesis holds for all solvable Lie algebras of dimension less than \(\dim \lie{s}\).
            Since \(\lie{s}\) is solvable the derived subalgebra, \(\bracket{\lie{s}}{\lie{s}}\) is either zero or a proper ideal of \(\lie{s}\).
            Importantly, \(\bracket{\lie{s}}{\lie{s}} \ne \lie{s}\), since then the derived series would never terminate.
            If \(\bracket{\lie{s}}{\lie{s}} = 0\) then \(\lie{s}\) is abelian and \(\lie{s}/\bracket{\lie{s}}{\lie{s}} = \lie{s} / 0 = \lie{s}\) is abelian.
            If \(\bracket{\lie{s}}{\lie{s}}\) is a proper ideal of \(\lie{s}\) then \(\lie{s}/\bracket{\lie{s}}{\lie{s}}\) is abelian since for all \(X, Y \in \lie{s}\) we have
            \begin{equation}
                \bracket{X + \lie{s}'}{Y + \lie{s}'} = \bracket{X}{Y} + \lie{s}' = \lie{s}'
            \end{equation}
            where \(\lie{s}' = \bracket{\lie{s}}{\lie{s}}\) and \(\bracket{X}{Y} \in \lie{s}'\) gives us the final equality.
            This means that regardless of whether \(\lie{s}'\) is zero or a proper ideal \(\lie{s}/\lie{s}'\) is abelian and hence every subspace of \(\lie{s}/\lie{s}'\) is an ideal.
            
            Take some subspace of codimension 1, that is a subspace with dimension \(\dim(\lie{s}/\lie{s}') - 1\).
            Call this subspace \(\overbar{\nilpotentLie}\), and note that this is an ideal of \(\lie{s}/\lie{s}'\).
            Denote by \(\nilpotentLie\) the preimage of \(\overbar{\nilpotentLie}\) under the quotient map.
            This is an ideal of \(\lie{s}\) of codimension 1.
            This holds because the preimage of an ideal under a homomorphism is always an ideal and the projection map is surjective, so the dimensions match up as needed.
            By induction we may posit that there is some common eigenvector for all elements of \(\nilpotentLie\), since it has dimension strictly less than \(\dim\lie{s}\) and is solvable as a subalgebra of a solvable algebra.
            Then there exists some \(\alpha \in \nilpotentLie^*\) such that \(Xv = \alpha(X)v\) for all \(X \in \nilpotentLie\).
            
            Now consider the subspace \(W = \{w \in V \mid Xw = \alpha(X)w \forall X \in \nilpotentLie\}\).
            This subspace can be thought of as the space of all candidates for a common eigenvector of \(\lie{s}\), we just need to find some element of this subspace that is also an eigenvector of some vector spanning the missing dimension that \(\lie{s}\) has but \(\nilpotentLie\) doesn't.
            We know by the previous paragraph that \(W\) is nonempty, and thus by the following lemma (\cref{lma:lie's theorem lemma}) we know that \(W\) is an \(\lie{s}\)-submodule of \(V\).
            Choose some \(Y \in \lie{s} \setminus \nilpotentLie\), so \(\lie{s} = \nilpotentLie \oplus \complex Y\).
            Then \(Y(W) \subseteq W\), as otherwise \(W\) would not be a \(\lie{s}\)-submodule.
            Thus, there exists some \(w \in W\) with \(w \ne 0\) such that \(w\) is an eigenvector of \(Y\), and so \(w\) is an eigenvalue of all elements of \(\lie{s}\).
        \end{proof}
    \end{thm}
    
    \begin{lma}{}{lma:lie's theorem lemma}
        Work over an algebraically closed field of characteristic zero.
        Let \(\nilpotentLie\) be an ideal of a Lie algebra \(\lie{s}\) and let \(V\) be an \(\lie{s}\)-submodule and \(\alpha \in \lie{s}^*\).
        Let
        \begin{equation}
            W = \{v \in V \mid Xv = \alpha(X)v \forall X \in \nilpotentLie\}.
        \end{equation}
        Then \(W\) is an \(\lie{s}\)-submodule of \(V\).
        \begin{proof}
            We need to show that \(Y(W) \subseteq W\) for all \(Y \in \lie{s}\).
            If \(Y \in \nilpotentLie\) then this is clearly true since \(Y\) just acts on \(W\) by scaling by \(\alpha(Y)\).
            We need to show in general that \(Yw \in W\) for all \(w \in W\).
            This means we need to show that \(X(Yw) = \alpha(X)Yw\) for all \(X \in \nilpotentLie\).
            We can do this using the definition of how the bracket acts and the fact that \(w\) is an \(X\) eigenvector:
            \begin{align}
                X(Yw) &= Y(Xw) + \bracket{X}{Y}w\\
                &= Y(\alpha(X)w) + \alpha(\bracket{X}{Y})w\\
                &= \alpha(X)Yw + \alpha(\bracket{X}{Y})w.\label{eqn:bracket eigenvector}
            \end{align}
            Here we've used the fact that \(\bracket{X}{Y} \in \nilpotentLie\) since \(\nilpotentLie\) is an ideal of \(\lie{s}\), and so the action of the bracket is determined by \(\alpha\).
            We are then left to show that \(\alpha(\bracket{X}{Y}) = 0\).
            
            Let \(U\) be the subspace of \(V\) spanned by \(\{w, Yw, Y^2w, \dotsc\}\).
            Then \(Y(U)\) is the subspace spanned by \(\{Yw, Y^2w, Y^3w, \dotsc\}\), which is clearly a subspace of \(U\), so \(Y(U) \subseteq U\).
            We claim that \(U\) is also an \(\nilpotentLie\)-submodule of \(V\).
            That is, we claim that \(X(U) \subseteq U\) for all \(X \in \nilpotentLie\).
            Clearly we have \(X(w) = \alpha(X)w \in U\), and \cref{eqn:bracket eigenvector} shows that \(X(Yw)\) is a linear combination of \(Yw\) and \(w\), so is an element of \(U\).
            We take this as the base case for an induction proof where we wish now to show that \(X(Y^nw) \in U\) for some \(n \in \naturals\) and we assume that \(X(Y^kw) \in U\) for all \(k < n\).
            Then we have
            \begin{equation}
                XY^nw = YXY^{n-1}w + \bracket{X}{Y}Y^{n-1}w
            \end{equation}
            and since \(\nilpotentLie\) is an ideal and \(X \in \nilpotentLie\) we have \(\bracket{X}{Y} \in \nilpotentLie\), so by the induction hypothesis \(\bracket{X}{Y}Y^{n-1}w \in U\).
            Thus, \(XY^nw \in U\) as it's a linear combination of elements of \(U\).
            This shows that \(XY^nw\) is given by \(\alpha(X)Y^nw\) plus terms involving \(Y^kw\) with \(k < n\).
            Thus, there is a basis for \(U\) in which \(X\) is upper triangular, with the diagonals given by \(\alpha(X)\).
            In particular, we have that \(\tr(X|_U) = \alpha(X) \dim U\).
            The same is true fo \(\bracket{X}{Y} \in \nilpotentLie\), \(\tr(\bracket{X}{Y}|_U) = \alpha(\bracket{X}{Y}) \dim U\).
            However, the trace of a commutator must vanish by the cyclic property of the trace, and thus we have \(\alpha(\bracket{X}{Y}) = 0\).
        \end{proof}
    \end{lma}
    
    Note that we require characteristic zero to conclude from \(0 = \alpha(\bracket{X}{Y}) \dim U\) that \(\alpha(\bracket{X}{Y}) = 0\), since in a field of characteristic dividing \(\dim U\) this will hold with nonzero \(\alpha(\bracket{X}{Y})\).
    We need algebraic closure to guarantee the existence of an eigenvector in the one-dimensional base case.
    So, Lie's theorem pretty much holds only over \(\complex\), and a few other more \enquote{constructed} fields, such as \(\overbar{\rationals}\), the algebraic closure of \(\rationals\).
    
    \begin{crl}{}{crl:solvable subalgebra consists of simple endomorphisms}
        There exists a basis of \(V\) such that any solvable subalgebra \(\lie{s} \subseteq \generalLinearLie(V)\) is a subalgebra of \(\borelLie(n, \complex)\).
        \begin{proof}
            This is the same as \cref{crl:nilpotent always corresponds to strictly upper triangular} but with solvable and upper triangular in place of nilpotent and strictly upper triangular.
        \end{proof}
    \end{crl}
    
    What we have shown with Engel and Lie's theorems is the following:
    \begin{itemize}
        \item A nilpotent subalgebra of \(\generalLinearLie(V)\) is (up to isomorphism) a subalgebra of \(\nilpotentLie(n, \complex)\).
        \item A solvable subalgebra of \(\generalLinearLie(V)\) is (up to isomorphism) a subalgebra of \(\borelLie(n, \complex)\).
    \end{itemize}
    This is the sense in which \(\nilpotentLie(n, \complex)\) and \(\borelLie(n, \complex)\) are the maximal nilpotent and solvable subalgebras of \(\generalLinearLie(V)\).
    
    Further clarifying nilpotent and solvable subalgebras is hard to impossible, and so we shan't try.
    We move our focus to the classifiable semisimple Lie algebras.
    
    \chapter{Killing Form}
    In this chapter we develop one of the key tools in working with semisimple Lie algebras, the Killing form.
    Before we define the Killing form we'll have a short introduction to bilinear forms.
    
    \section{Bilinear Forms}
    \begin{dfn}{}{def:bilinear form}
        Let \(V\) be a \(\field\)-vector space.
        A \defineindex{bilinear form}, \(\eta\), is a map \(\eta \colon V \times V \to \field\) which is linear in both arguments.
        That is, for all \(u, v, w \in V\) and \(\lambda \in \field\) we have
        \begin{equation}
            \eta(u + \lambda v, w) = \eta(u, w) + \lambda \eta(v, w), \qqand \eta(u, v + \lambda w) = \eta(u, v) + \lambda \eta(u, w).
        \end{equation}
        
        A bilinear form, \(\eta\), is \define{symmetric}\index{symmetric bilinear form} if for all \(u, v \in V\) we have \(\eta(u, v) = \eta(v, u)\).
        
        A symmetric bilinear form is \define{non-degenerate}\index{non-degenerate bilinear form} if for all \(u \in V\) with \(u \ne 0\) there exists some \(v \in V\) such that \(\eta(u, v) \ne 0\).
    \end{dfn}
    
    Note that for \(\field = \reals\) non-degeneracy is equivalent to requiring that \(\eta(u, u) \ne 0\) for all \(u \ne 0\).
    The notion of a non-degenerate bilinear form can also be extended to non-symmetric bilinear forms, but we do not have need of this.
    We just have to repeat the definition with \(u\) and \(v\) swapped.
    
    Suppose that \(\eta \colon V \times V \to \field\) is a symmetric bilinear form.
    Then we can partially evaluate at some \(u \in V\) and consider a map \(\eta(u, -) \colon V \to \field\) given by \(v \mapsto \eta(u, v)\).
    This is linear since \(\eta\) is linear in the second argument, so \(\eta(u, -) \in V^*\).
    Likewise, \(\eta(-, v) \colon V \to \field\) defined by \(u \mapsto \eta(u, v)\) is linear and so \(\eta(-, v) \in V^*\).
    Of course, since \(\eta\) is symmetric these two maps are actually the same.
    
    So, each symmetric\footnote{a non-symmetric bilinear form gives rise to two such maps, and we have to choose one, so we lose the \enquote{canonical} part of this map.} bilinear form, \(\eta\), gives rise to a canonical linear map, \(\iota_\eta \colon V \to V^*\), given by \(\iota_\eta(u) = \eta(u, -)\).
    We can use this to restate the non-degeneracy condition as saying that \(\ker \iota_\eta = 0\) if and only if \(\eta\) is non-degenerate.
    To see this first let \(v \in \ker \iota_\eta\) so \(\iota_\eta(v) = 0\), which means \(\eta(v, -)\) is the zero map, so for all \(u \in V\) we have \(\eta(v, u) = 0\), so if \(\eta\) is non-degenerate we must have \(v = 0\) and vice-versa if \(\eta(v, -)\) is not the zero map then there exists some \(u \in V\) such that \(\eta(v, u) \ne 0\), and so \(\eta\) is non-degenerate.
    
    Further, in the finite dimensional case if \(\eta\) is non-degenerate then \(\iota_\eta\) is an isomorphism, since we showed above that \(\iota_\eta\) is an injection \(V \hookrightarrow V^*\), and it is a well-known fact that \(\dim V = \dim V^*\) for finite-dimensional \(V\), and so an injective linear map between spaces of the same dimension is automatically an isomorphism.
    
    So, a non-degenerate bilinear form gives a canonical isomorphism \(V \isomorphic V^*\).
    Note that we always have \emph{an} isomorphism \(V \isomorphic V^*\), it's just that in general there are many such isomorphisms and no reason to prefer one over another.
    This particular canonical isomorphism is important when our space is a Hilbert space, in which case this result is called the riesz representation theorem.
    
    \begin{dfn}{Radical}{}
        Let \(\eta\) be a bilinear form on \(V\).
        The \define{radical}\index{radical!of a bilinear form} is \(\rad \eta = \ker \iota_\kappa \subseteq V\).
    \end{dfn}
    
    \section{Killing Form}
    \begin{dfn}{}{}
        Let \(\lie{g}\) be a Lie algebra.
        Then the \defineindex{Killing form} on \(\lie{g}\) is the symmetric bilinear form
        \begin{align}
            \kappa \colon \lie{g} \times \lie{g} &\to \field\\
            (X, Y) &\mapsto \kappa(X, Y) = \tr(\ad_X \circ \ad_Y).
        \end{align}
    \end{dfn}
    
    Note that symmetry of the Killing form follows immediately from the cyclic property of the trace, namely that \(\tr(AB) = \tr(BA)\), and linearity follows immediately from the linearity of the trace and composition of endomorphisms.
    
    The most important property of the Killing form is arguably that it gives us a way to show that a finite-dimensional Lie algebra is semisimple.
    Recall that a Lie algebra is semisimple if it contains no proper solvable ideals.
    
    \begin{lma}{Associativity of the Killing Form}{lma:associativity of killing form}
        Let \(\lie{g}\) be a Lie algebra and \(\kappa\) the Killing form on \(\lie{g}\), then
        \begin{equation}
            \kappa(X, \bracket{Y}{Z}) = \kappa(\bracket{X}{Y}, Z)
        \end{equation}
        for all \(X, Y, Z \in \lie{g}\).
        \begin{proof}
            This follows by a quick calculation using the fact that \(\ad\) is a representation and some properties of the trace:
            \begin{align}
                \kappa(X, \bracket{Y}{Z}) &= \tr(\ad_X \circ \ad_{\bracket{Y}{Z}})\\
                &= \tr(\ad_X \circ \bracket{\ad_Y}{\ad_Z})\\
                &= \tr(\ad_X \circ (\ad_Y \circ \ad_Z - \ad_Z \circ \ad_Y))\\
                &= \tr(\ad_X \circ \ad_Y \circ \ad_Z - \ad_X \circ \ad_Z \circ \ad_Y)\\
                &= \tr(\ad_X \circ \ad_Y \circ \ad_Z - \ad_Y \circ \ad_X \circ \ad_Z)\\
                &= \tr((\ad_X \circ \ad_Y - \ad_Y \circ \ad_X) \circ \ad_Z)\\
                &= \tr(\bracket{\ad_X}{\ad_Y} \circ \ad_Z)\\
                &= \tr(\ad_{\bracket{X}{Y}} \circ \ad_Z)\\
                &= \kappa(\bracket{X}{Y}, Z). \qedhere
            \end{align}
        \end{proof}
    \end{lma}
    
    \begin{thm}{Cartan's First Criterion}{thm:cartans first criterion}
        A finite-dimensional Lie algebra, \(\lie{g}\), over \(\complex\) is solvable if and only if \(\kappa(X, Y) = 0\) for all \(X \in \lie{g}'\) and \(Y \in \lie{g}\).
        \begin{proof}
            Suppose that \(\lie{g}\) is solvable.
            Then Lie's theorem tells us that there is a basis, \(\basis\), for \(\lie{g}\) such that if \(W \in \lie{g}\) then \([\ad_W]_{\basis}\) is upper triangular.
            This implies that \([\bracket{\ad_X}{\ad_Y}]_{\basis} = \bracket{[\ad_X]_{\basis}}{[\ad_Y]_{\basis}}\) is \emph{strictly} upper triangular for \(X, Y \in \lie{g}\), since the commutator of upper triangular matrices is strictly upper triangular.
            This then implies that \(\kappa(\bracket{X}{Y}, Z) = \tr(\bracket{\ad_X}{\ad_Y}\circ \ad_Z) = 0\) using similar calculations to \cref{lma:associativity of killing form}.
            Note then that \(\bracket{X}{Y}\) is exactly an element of \(\lie{g}'\) and \(Z\) an element of \(\lie{g}\).
            
            Now suppose that \(X, Y, Z \in \lie{g}\) are such that \(\kappa(\bracket{X}{Y}, Z) = 0\).
            If \(L' = \bracket{\lie{g}}{\lie{g}}\) is nilpotent then \(L\) is solvable by Engel's theorem.
            Notice that if \(W = \bracket{X}{Y} \in \lie{g}'\) then \(\kappa(W, Z) = 0\).
            We can now use the Jordan decomposition to write
            \begin{equation}
                \ad_W = (\ad_W)_{\symrm{d}} + (\ad_W)_{\symrm{n}} \eqqcolon W_{\symrm{d}} + W_{\symrm{n}}
            \end{equation}
            where \(W_{\symrm{d}}\) is diagonalisable and \(W_{\symrm{n}}\) is nilpotent, meaning that in some basis \(W_{\symrm{n}}\) is upper triangular, and so has vanishing trace.
            Then we have
            \begin{equation}
                \tr(W\overbar{W}_{\symrm{d}}) = \tr((W_{\symrm{d}} + W_{\symrm{n}})\overbar{W}_{\symrm{d}}) = \tr(W_{\symrm{d}}\overbar{W}_{\symrm{d}}) + \tr(W_{\symrm{n}}\overbar{W}_{\symrm{d}}) = \tr(W_{\symrm{d}}\overbar{W}_{\symrm{d}})
            \end{equation}
            where we've used the fact that the product \(W_{\symrm{n}}\overbar{W}_{\symrm{d}}\) will be strictly upper triangular in whichever basis \(W_{\symrm{n}}\) is strictly upper triangular in, and as such has trace zero.
            Writing \(\lambda_i\) with \(i = 1, \dotsc, m\) for the eigenvalues of \(\ad_W\) we have
            \begin{equation}
                \tr(W\overbar{W}_{\symrm{d}}) = \tr(W_{\symrm{d}}\overbar{W}_{\symrm{d}}) = \abs{\lambda_1}^2 + \dotsb + \abs{\lambda_m}^2
            \end{equation}
            since \(W_{\symrm{d}}\) and \(\overbar{W}_{\symrm{d}}\) are diagonal with entries \(\lambda_i\) and \(\overbar{\lambda}_i\) as the \(i\)th diagonal entry respectively.
            We also have
            \begin{align}
                \tr(W\overbar{W}_{\symrm{d}}) &= \tr(\ad_W\circ (\ad_{\overbar{W}})_{\symrm{d}})\\
                &= \tr(\ad_{\bracket{X}{Y}} (\ad_{\overbar{W}})_{\symrm{d}})\\
                &= \tr(\ad_X \circ (\ad_{\bracket{Y}{\overbar{W}}})_{\symrm{d}})\\
                &= 0
            \end{align}
            where we've used similar calculations to the proof of \cref{lma:associativity of killing form} in the last step.
            
            So, we must have that \(\abs{\lambda_1}^2 + \dotsb + \abs{\lambda_m}^2 = 0\), which implies that \(\lambda_i = 0\) for all \(i = 1, \dotsc, m\).
            Thus, \(W_{\symrm{d}} = 0\) so \(\ad_W = W_{\symrm{n}}\), and hence \(W\) is nilpotent, because this shows it is \(\ad\)-nilpotent so we can apply \cref{crl:ad-nilpotent implies nilpotent}.
            Since \(W\) was chosen as an arbitrary element of \(\lie{g}'\) this shows that \(\lie{g}'\) is nilpotent, and hence that \(\lie{g}\) is solvable by Engel's theorem.
        \end{proof}
    \end{thm}
    
    \begin{lma}{}{}
        Every nonzero solvable ideal of a finite dimensional Lie algebra, \(\lie{g}\), contains a nonzero abelian ideal of \(\lie{g}\).
        \begin{proof}
            Let \(\lie{l}\) be a solvable ideal of \(\lie{g}\) such that \(\lie{l}^n = 0\).
            Consider \(\lie{l}^{n-1}\).
            This is such that \(\bracket{\lie{l}^{n-1}}{\lie{l}^{n-1}} = \lie{l}^n = 0\), and so \(\lie{l}^{n-1}\) is abelian, and it's also an ideal.
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{}
        Let \(\lie{s}\) be a Lie algebra.
        Then \(\lie{s}\) is solvable if and only if \(\ad(\lie{s}) \subseteq \generalLinearLie(\lie{s})\) is solvable.
        \begin{proof}
            Suppose \(\ad(\lie{s})\) is solvable.
            We have seen that \(\ad(\lie{s}) \isomorphic \lie{s}/\centre(\lie{s})\).
            The centre, \(\centre(\lie{s})\) is abelian, and thus solvable, and this isomorphism tells us that \(\lie{s}/\centre(\lie{s})\) is solvable, and so it follows by \cref{lma:i and g/i solvable implies g solvable} that \(\lie{s}\) is solvable.
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{lma:radical of killing form is ideal}
        Let \(\lie{g}\) be a Lie algebra and let \(\lie{s} = \rad\kappa\) be the radical of the Killing form.
        Then \(\lie{s}\) is a solveable ideal of \(\lie{g}\).
        \begin{proof}
            Recall that \(\lie{s} = \rad\kappa = \ker \iota_\kappa\) where \(\iota_\kappa \colon\lie{g} \to \lie{g}^*\) is the map \(X \mapsto \kappa(X, -)\).
            Suppose that \(X \in \lie{s}\) and \(Y \in \lie{g}\).
            Then we want to show that \(\bracket{Y}{X} \in \lie{s}\).
            That is, we want to show that \(\kappa(\bracket{Y}{X}, -)\) is the zero map, which means showing that \(\kappa(\bracket{Y}{X}, Z) = 0\) for all \(Z \in \lie{g}\).
            This is true by \cref{lma:associativity of killing form} because we have
            \begin{equation}
                \kappa(\bracket{Y}{X}, Z) = \kappa(Y, \bracket{X}{Z}) = 0
            \end{equation}
            with the last equality following because \(\lie{s}\) is solvable, which follows because \(\kappa(X, X') = 0\) for \(X, X' \in \lie{s}\) proving solvability by Cartan's first criterion (\cref{thm:cartans first criterion}).
        \end{proof}
    \end{lma}
    
    \begin{thm}{Cartan's Second Criterion}{thm:cartans second criterion}
        A Lie algebra, \(\lie{g}\), over \(\complex\) is semisimple if and only if the Killing form is non-degenerate.
        \begin{proof}
            Suppose that \(\lie{g}\) is semisimple, then we want to show that \(\kappa\) is non-degenerate.
            Since \(\lie{g}\) is semisimple it has no solveable ideals, and by \cref{lma:radical of killing form is ideal} is a solvable ideal, which means that we must have \(\lie{s} = 0\), and thus \(\kappa\) is non-degenerate as this shows that \(\iota_\kappa\) is an injection so the discussion following \cref{def:bilinear form} applies.
            
            Now suppose that \(\kappa\) is non-degenerate.
            Then \(\lie{s} = 0\).
            Let \(I \subseteq \lie{g}\) be a solveable ideal.
            By ... we know that \(I\) contains an abelian ideal, \(\lie{l}\).
            We will show that \(\lie{l} \subseteq \lie{s}\) and thus \(\lie{l} = 0\) and so \(I = 0\).
            Take elements \(X \in \lie{l}\) and \(Y \in \lie{g}\).
            We want to show that \(\kappa(X, Y) = 0\), since this implies that \(\lie{l} \subseteq \lie{s}\).
            To do this we consider the map \(\ad_X \circ \ad_Y \colon \lie{g} \to \lie{g}\), the trace of which is \(\kappa(X, Y)\).
            For any \(Z \in \lie{g}\) we always have that \(\bracket{X}{\bracket{Y}{Z}} \in \lie{l}\) because \(X \in \lie{l}\) and \(\lie{l}\) is an ideal.
            Further, since \(\lie{l}\) is abelian we have
            \begin{equation}
                \bracket{X}{\bracket{Y}{\bracket{X}{\bracket{Y}{Z}}}} = 0
            \end{equation}
            since \(X \in \lie{l}\) and \(\bracket{Y}{\bracket{X}{\bracket{Y}{Z}}} \in \lie{l}\) (since \(\bracket{X}{\bracket{Y}{Z}} \in \lie{l}\) and \(\lie{l}\) is an ideal), so this is indeed the bracket of two elements of the abelian ideal \(\lie{l}\) and so vanishes.
            We can then identify the operator \(\ad_X \circ \ad_Y\) in the above, as what we have here is
            \begin{equation}
                (\ad_X \circ \ad_Y)^2(Z) = 0.
            \end{equation}
            This holds for all \(Z \in \lie{g}\) and so we can conclude that \((\ad_X \circ \ad_Y)^2 = 0\).
            So, \(\ad_X\circ\ad_Y\) is a nilpotent operator.
            This means that \(\ad_X \circ \ad_Y\) has trace zero, since this is true of all nilpotent operators, and so we are done.
            This general fact follows because nilpotent operators are strictly upper triangular in some basis, and computing the trace in this basis must give zero, and trace is basis independent.
            An alternative proof of this general fact is that the minimal polynomial of \(T \colon V \to V\) if \(T^n = 0\) (and \(n\) is minimal in this respect) is \(m(t) = t^n\), and the roots of this are zero, and the roots of the minimal polynomial are the eigenvalues (without multiplicity) of the operator, and the trace is the sum of the eigenvalues (with multiplicity), and so here all the eigenvalues are zero and so their sum vanishes.
        \end{proof}
    \end{thm}
    
    \begin{thm}{}{}
        Let \(\lie{g}\) be a finite dimensional semisimple Lie algebra.
        Then there is a unique (up to isomorphisms of summands and the order of terms in the sum) decomposition
        \begin{equation}
            \lie{g} = \lie{g}_0 \oplus \lie{g}_1 \oplus \dotsb \oplus \lie{g}_k
        \end{equation}
        with \(\lie{g}_i\) simple Lie algebras.
        \begin{proof}
            \Step{Existence}
            Suppose \(\lie{g}\) is a semisimple Lie algebra.
            If such a decomposition exists then the \(\lie{g}_i\) will be ideals of \(\lie{g}\) which pairwise commute, that is \(\bracket{\lie{g}_i}{\lie{g}_j} = 0\) for \(i \ne j\).
            Another way to put this is that if \(i \ne j\) then \(\ad(\lie{g}_i)\) vanishes on \(\lie{g}_j\) and vice versa.
            Very concretely, \(\ad_X\), for \(X \in \lie{g}_i\), will admit a basis such that it is block diagonal and the blocks corresponding to \(\lie{g}_j\) (\(j \ne i\)) will be zero.
            In other words, the subspace \(\lie{g}_i\) will be perpendicular to the subspace \(\lie{g}_j\) with respect to the Killing form, \(\kappa\).
            
            This gives us a way to start to construct such a decomposition.
            Let \(\lie{l}\) be some proper ideal, that is \(\lie{l} \ne \lie{g}, 0\).
            Define
            \begin{equation}
                \lie{l}^{\perp} \coloneq \{X \in \lie{g} \mid \kappa(X, Y) = 0 \forall Y \in \lie{l}\}.
            \end{equation}
            This is the space of all \(X\) which are perpendicular to all elements of \(\lie{l}\).
            This is an ideal, since if \(X \in \lie{l}^{\perp}\), \(Y \in \lie{l}\), and \(Z \in \lie{g}\) then
            \begin{equation}
                \kappa(\bracket{X}{Z}, Y) = \kappa(X, \bracket{Z}{Y}) = 0
            \end{equation}
            having used the associativity of the Killing form (\cref{lma:associativity of killing form}) and then the fact that \(\lie{l}\) is an ideal and \(Y \in \lie{l}\) so \(\bracket{Z}{Y} \in \lie{l}\) and since \(X \in \lie{l}^{\perp}\) it must be that \(\kappa(X, \bracket{Z}{Y})\) vanishes.
            
            The intersection of ideals is again an ideal, and so \(\lie{l} \cap \lie{l}^{\perp}\) is an ideal of \(\lie{g}\).
            Further, \(\kappa\) vanishes on \(\lie{l} \cap \lie{l}^{\perp}\) since if \(X, Y \in \lie{l} \cap \lie{l}^{\perp}\) then, in particular, \(X \in \lie{l}^{\perp}\) and \(Y \in \lie{l}\) so by the definition of \(\lie{l}^{\perp}\) we must have that \(\kappa(X, Y) = 0\).
            This implies that \(\lie{l} \cap \lie{l}^{\perp}\) is solveable by Cartan's criterion (\cref{thm:cartans second criterion}).
            ***
            The semisimplicity of \(\lie{g}\) therefore requires that \(\lie{l} \cap \lie{l}^{\perp} = 0\), note that we can't have \(\lie{l} \cap \lie{l}^{\perp} = \lie{g}\) because \(\lie{l} \subsetneq \lie{g}\).
            From this we can see that \(\lie{l} + \lie{l}^{\perp} = \lie{l} \oplus \lie{l}^{\perp}\).
            
            We claim that \(\lie{l} \oplus \lie{l}^{\perp} = \lie{g}\).
            This follows by an argument on dimensions.
            Clearly, \(\lie{l} \oplus \lie{l}^{\perp} \subseteq \lie{g}\), and so we get the result if we can show that \(\dim \lie{l} + \dim \lie{l}^{\perp} = \dim \lie{g}\).
            We get this in turn by noting that \(\lie{l}^{\perp} = \ker(\iota_\kappa|_{\lie{l}})\) and that \(\iota_\kappa|_{\lie{l}} \in \lie{l}^*\).
            Since all of our vector spaces are finite dimensional we have \(\dim \lie{l}^* = \dim \lie{l}\), thus we have that \(\dim \lie{l} + \dim \lie{l}^{\perp} = \dim \lie{l}^* + \dim \lie{l}^{\perp} = \dim \lie{g}^*\), which follows since any linear map \(\lie{g} \to \complex\) may be written as a linear map on \(\lie{l}\) plus a linear map on \(\lie{g}\) which vanishes on \(\lie{l}\), which is exactly what \(\lie{l}^* + \lie{l}^{\perp}\) gives us.
            Finally, we have \(\dim \lie{g}^* = \dim\lie{g}\), and so \(\lie{l} \oplus \lie{l}^{\perp} = \lie{g}\).
            
            If \(\lie{l}\) and \(\lie{l}^{\perp}\) are simple we are done.
            If this is not the case then we can recurse on any non-simple term.
            Note that \(\lie{l}\) and \(\lie{l}^{\perp}\) are ideals of a semisimple Lie algebra and so are themselves semisimple.
            
            The dimensions of \(\lie{l}\) and \(\lie{l}^{\perp}\) are strictly smaller than the dimension of \(\lie{g}\).
            Because the dimensions are finite decrease with each recursion this process must terminate eventually.
            It will terminate only when all terms appearing in the sums are simple, and then \(\lie{g}\) will be a sum of all of these simple Lie algebras.
            
            \Step{Uniqueness}
            Suppose that \(\lie{h}\) is a simple ideal of \(\lie{g}\).
            We claim that \(\lie{h} = \lie{g}_i\) for some \(i\).
            The space \(\bracket{\lie{g}}{\lie{h}}\) is an ideal of \(\lie{h}\).
            It is nonzero since the centre of \(\lie{h}\) is trivial.
            Since \(\lie{h}\) is assumed to be simple it must then be that \(\bracket{\lie{g}}{\lie{h}} = \lie{h}\).
            Now consider the decomposition
            \begin{equation}
                \lie{g} = \lie{g}_1 \oplus \lie{g}_2 \oplus \dotsb \oplus \lie{g}_k.
            \end{equation}
            Taking the bracket with \(\lie{h}\) we have
            \begin{equation}
                \bracket{\lie{g}}{\lie{h}} = \bracket{\lie{g}_1}{\lie{h}} \oplus \bracket{\lie{g}_2}{\lie{h}} \oplus \dotsb \oplus \bracket{\lie{g}_k}{\lie{h}} = \lie{h}.
            \end{equation}
            Since each bracket is either \(0\) or \(\lie{h}\) by the same logic as above there must be a unique \(\lie{i}\) such that \(\bracket{\lie{g}_i}{\lie{h}} = \lie{h}\).
            Since each \(\lie{g}_i\) is simple and \(\bracket{\lie{g}_i}{\lie{h}} = \lie{h} \ne 0\) we must have that \(\lie{h} = \bracket{\lie{g}_i}{\lie{h}} = \lie{g}_i\).
            Thus, every ideal of \(\lie{g}\) corresponds uniquely to some \(\lie{g}_i\), and clearly the \(\lie{g}_i\) are ideals of \(\lie{g}\), and so the decomposition is uniquely determined by the simple ideals of \(\lie{g}\), again, up to order in the sum and isomorphism of each \(\lie{g}_i\).
        \end{proof}
    \end{thm}
    
    The idea of this result is that a semisimple Lie algebra is determined by its simple ideals.
    Further, we've shown that if \(\lie{g}\) is semisimple then \(\bracket{\lie{g}}{\lie{g}} = \lie{g}\).
    In particular, this means that semisimple Lie algebras are neither solveable or nilpotent.
    
    \chapter{Complete Reducibility}
    \section{Definition and Statement of Weyl's Theorem}
    \begin{dfn}{Complete Reducibility}{}
        Let \(\lie{g}\) be a Lie algebra over \(\field\).
        A \(\lie{g}\)-module, \(V\), is \defineindex{completely reducible} if there exists simple \(\lie{g}\)-modules \(V_i\) such that
        \begin{equation}
            V = V_1 \oplus V_2 \oplus \dotsb \oplus V_n
        \end{equation}
        for some \(n \in \naturals\).
    \end{dfn}
    
    Note that in such a decomposition \(X \in \lie{g}\) acts on \(\bigoplus_i V_i\) by
    \begin{equation}
        X(v_1, v_2, \dotsc, v_n) = (Xv_1, \dotsc, Xv_n)
    \end{equation}
    where the action \(Xv_i\) is that of the \(\lie{g}\)-module structure of \(V_i\).
    
    There is an obvious inclusion
    \begin{align}
        V_i &\hookrightarrow \bigoplus_{i=1}^n V_i\\
        v &\mapsto (0, \dotsc, 0, \underbrace{v}_{\mathclap{i\text{th place}}}, 0, \dotsc, 0).
    \end{align}
    We write \(v_1 + \dotsb + v_n\) for \((v_1, \dotsc, v_n)\).
    With this the above inclusion is then even more obvious, it's just \(v \mapsto 0 + \dotsb + 0 + v + 0 + \dotsb + 0 = v\), where the zeros are implicit in the final expression.
    
    Note that, unlike the decomposition of a semisimple Lie algebra, the decomposition of a completely reducible \(\lie{g}\)-module need not be unique.
    For example, if we take \(\lie{g} = 0\) then any vector space is trivially a \(\lie{g}\)-module, and all subspaces of dimension 1 are simple submodules.
    Then any decomposition of a vector space is a decomposition into simple \(\lie{g}\)-modules, and such decompositions are not unique (for dimension at least 2).
    
    Our goal of this chapter is to prove the following theorem, but before we can do that we'll spend some time developing a few lemmas and some extra theory.
    
    \begin{thm}{Weyl's Theorem}{}
        Let \(\lie{g}\) be a semisimple Lie algebra over \(\complex\).
        Then every finite-dimensional representation of \(\lie{g}\) is completely reducible.
    \end{thm}
    
    This result is as strong as possible.
    If \(\lie{g}\) is not semisimple then this result doesn't hold.
    If the representation is not finite dimensional then this result doesn't hold.
    For this second case simply take any infinite dimensional vector space as a \(0\)-module, such a space does not have a decomposition as a \emph{finite} sum of one-dimensional spaces, so is not completely reducible.
    
    Note that since we can always decompose a semisimple Lie algebra, \(\lie{g}\), as a direct sum of simple Lie algebras it is sufficient to prove Weyl's theorem for a simple Lie algebra.
    
    \section{Complements}
    \begin{lma}{}{lma:reducible iff comlements}
        A finite dimensional \(\lie{g}\)-module, \(V\), is completely reducible if and only if every submodule, \(W \subseteq V\), admits a complement, \(W'\), which is a submodule \(W' \subseteq V\) such that \(V = W \oplus W'\).
        \begin{proof}
            \Step{Complete Reducibility Implies Complements}
            Suppose that \(V\) is a completely reducible \(\lie{g}\)-module.
            Then we can write \(V = V_1 \oplus \dotsb \oplus V_n\).
            Let \(W \subseteq V\) be a submodule of \(V\).
            Then \(W\) has the decomposition
            \begin{equation}
                W = (W \cap V_1) \oplus \dotsb \oplus (W \cap V_n).
            \end{equation}
            Since each \(V_i\) is a simple module and \(W \cap V_i \subseteq V_i\) is a submodule of \(V_i\) we have that either \(W \cap V_i = 0\) or \(W \cap V_i = V_i\).
            Let \(\{i_1, \dotsc, i_k\}\) be the set of indices for which \(W \cap V_i = V_i\), then we can write the decomposition of \(W\) as
            \begin{equation}
                W = V_{i_1} \oplus \dotsb \oplus V_{i_k}.
            \end{equation}
            Let \(\{j_1, \dotsc, j_\ell\}\) be the set of indices for which \(W \cap V_j = 0\), and let
            \begin{equation}
                W' = V_{j_1} \oplus \dotsb \oplus V_{j_\ell}.
            \end{equation}
            Then we have
            \begin{equation}
                W \oplus W' = V_{i_1} \oplus \dotsb \oplus V_{i_k} \oplus V_{j_1} \oplus \dotsb \oplus V_{j_\ell} = V
            \end{equation}
            and so \(W'\) is the complement of \(W\).
            
            \Step{Complements Imply Complete Reducibility}
            Suppose that \(V\) is a \(\lie{g}\)-module such that every submodule, \(W \subseteq V\), admits a complement, \(W'\).
            If \(V\) is simple then it is trivially a direct sum of simple modules, so we're done.
            If \(V\) is not simple then we induct on \(\dim V\).
            For \(\dim V = 1\) then \(V\) is simple and so we're done.
            Now suppose that having complements implies complete reducibility for all \(\lie{g}\)-modules of dimension less than \(\dim V\).
            By assumption \(V\) is not simple, so we may find some proper submodule \(W \subsetneq V\).
            Take \(W' \subseteq V\) such that \(W \oplus W' = V\), such a complement exists by assumption.
            Since \(W\) was chosen to be proper we know that \(\dim W, \dim W' < \dim V\), and thus \(W\) and \(W'\) both have decompositions into sums of simple \(\lie{g}\)-modules,
            \begin{equation}
                W = W_1 \oplus \dotsb \oplus W_n, \qand W' = W_1' \oplus \dotsb \oplus W_m'.
            \end{equation}
            Note that \(W_i'\) need not be the complement to \(W_1\).
            Then we have
            \begin{equation}
                V = W \oplus W' = W_1 \oplus \dotsb \oplus W_n \oplus W_1' \oplus \dotsb \oplus W_m',
            \end{equation}
            and so \(V\) is a direct sum of simple \(\lie{g}\)-modules as required.
        \end{proof}
    \end{lma}
    
    This result fails when \(\lie{g}\) is not semisimple.
    For example, taking \(\lie{g}\) to be the one-dimensional Lie algebra spanned by \(X\) we can define a two-dimensional \(\lie{g}\)-module \(V = \Span\{e_1, e_2\}\) by \(Xe_1 = 0\) and \(Xe_1 = 0\).
    Then \(W = \complex e_1\) is a \(\lie{g}\)-submodule of \(V\), since it's invariant under the action of \(X\) (just mapping everything in \(W\) to \(0 \in W\)).
    Any complementary subspace to \(W\) as a vector space must be of the form \(W' = \complex (\alpha e_1 + e_2)\) for some fixed value of \(\alpha \in \complex\) (we are taken the span of \(\alpha e_1 + e_2\), so we're free to choose the overall scaling to have the coefficient of \(e_2\) be \(1\)).
    This is the only way that \(W + W'\) can contain \(e_2\).
    However, \(X(\alpha e_1 + e_2) = \alpha X e_1 + X e_2 = 0 + e_1 = e_1\), and so \(W'\) is not a \(\lie{g}\)-submodule.
    
    Another example of this failing is when we take \(\lie{g} = \nilpotentLie(3, \complex)\), which is not semisimple, and we take \(V = \Span\{e_1, e_2, e_3\}\) to be the standard representation of a subalgebra of \(\generalLinearLie(3, \complex)\), so \(e_i\) are just the standard column vector basis vectors.
    Elements of \(\nilpotentLie(3, \complex)\) are of the form
    \begin{equation}
        \begin{bmatrix}
            0 & * & *\\
            0 & 0 & *\\
            0 & 0 & 0
        \end{bmatrix}
        .
    \end{equation}
    The fact that the first column of any nilpotent matrix must be zero means that any \(X \in \nilpotentLie(3, \complex)\) acts trivially on \(e_1\), we have \(X e_1 = 0\).
    This means that \(W = \complex e_1\) is a submodule of \(V\), since every element of \(W\) just maps to \(0 \in W\).
    However, there is no \(\lie{g}\)-submodule, \(W'\), such that \(V = W \oplus W'\), since if such a submodule existed we could take \(u = \alpha e_1 + \beta e_2 + \gamma e_3 \in W'\) and then since \(W \cap W' = 0\) we must have that either \(\beta\) or \(\gamma\) is non-zero in any such vector.
    For non-zero \(\beta\) we have
    \begin{equation}
        \begin{bmatrix}
            0 & 1 & 0\\
            0 & 0 & 0\\
            0 & 0 & 0
        \end{bmatrix}
        \begin{bmatrix}
            \alpha\\ \beta\\ \gamma
        \end{bmatrix}
        =
        \begin{bmatrix}
            \beta\\ 0\\ 0
        \end{bmatrix}
        = \beta e_1
    \end{equation}
    and for non-zero \(\gamma\) we have
    \begin{equation}
        \begin{bmatrix}
            0 & 0 & 1\\
            0 & 0 & 0\\
            0 & 0 & 0
        \end{bmatrix}
        \begin{bmatrix}
            \alpha\\ \beta\\ \gamma
        \end{bmatrix}
        =
        \begin{bmatrix}
            \gamma\\ 0\\ 0
        \end{bmatrix}
        = \gamma e_1.
    \end{equation}
    For either case we have left \(W'\) and ended up with an element of \(W\), and as such it cannot be that \(W \oplus W' = V\).
    
    \section{Faithful Modules}
    \begin{dfn}{}{}
        Let \((V, \rho)\) be a representation of a Lie algebra, \(\lie{g}\).
        We say that this is a \define{faithful representation} or that the corresponding \(\lie{g}\)-module, \(V\), is a \defineindex{faithful module} if \(\rho \colon \lie{g} \to \generalLinearLie(V)\) is injective.
    \end{dfn}
    
    The idea of a faithful representation is that in mapping \(\lie{g}\) to a Lie-algebra of operators on \(V\) we don't lose any information by mapping two elements of \(\lie{g}\) to the same operator on \(V\).
    
    \begin{lma}{}{lma:simple lie alg rep faithful or trivial}
        Let \(\lie{g}\) be a simple Lie algebra and \(V\) a \(\lie{g}\)-module.
        Then \(V\) is either a faithful module, or a trivial module.
        \begin{proof}
            Let \((V, \rho)\) be an irreducible representation (that is, \(V\) is a simple \(\lie{g}\)-module with the action given by \(\rho\)).
            Then \(\ker \rho \subseteq \lie{g}\) is an ideal of \(\lie{g}\) since \(\ker \rho\) is a subspace of \(\lie{g}\) by basic linear algebra, and if \(X \in \ker \rho\) and \(Y \in \lie{g}\) we have
            \begin{equation}
                \rho(\bracket{X}{Y})(v) = \bracket{\rho(X)}{\rho(Y)}(v) = \bracket{0}{\rho(Y)}(v) = 0(v) = 0
            \end{equation}
            for all \(v \in V\) and so \(\bracket{X}{Y} \in \ker \rho\).
            
            Since \(\lie{g}\) is assumed to be simple we must therefore have either \(\ker \rho = 0\), in which case \(\rho\) is injective, or \(\ker \rho = \lie{g}\), in which case \(\rho\) is the zero map.
            This corresponds to \(V\) being a faithful or trivial \(\lie{g}\)-module respectively.
        \end{proof}
    \end{lma}
    
    \begin{crl}{}{}
        If \(\lie{g}\) is a simple Lie algebra and \(V\) is a non-trivial \(\lie{g}\)-module then \(\dim V \ge \sqrt{\dim \lie{g}}\).
        \begin{proof}
            Let \((\rho, V)\) be the corresponding representation, which must be faithful, since we assume it is not trivial.
            Then \(\rho \colon \lie{g} \to \generalLinearLie(V)\) is injective, and so \(\lie{g}\) may be realised as a subalgebra of \(\generalLinearLie(V)\), and \(\dim \generalLinearLie(V) = (\dim V)^2\) and since a subalgebra must have at most the dimension of the full algebra we find that \(\dim \lie{g} \le (\dim V)^2\).
        \end{proof}
    \end{crl}
    
    For example, any non-trivial \(\specialLinearLie(n, \complex)\) representation must have dimension at least \(\sqrt{n^2 - 1}\).
    
    The intuition here is that a faithful module (which any non-trivial module of a simple Lie algebra is) must have \enquote{enough room} to fit all of the elements of \(\lie{g}\) in, and it turns out that enough room is \(\sqrt{\dim \lie{g}}\) dimensions.
    My first assumption would be that enough room would be \(\dim \lie{g}\) dimensions, but the bracket relations between elements of \(\lie{g}\) reduces the degrees of freedom required, and so we don't quite need one dimension for each dimension of \(\lie{g}\).
    
    \section{Casimir Operators}
    \textit{In this section I diverge slightly from the material of the lectures.
    This course never introduces the notion of the universal enveloping algebra, which I believe is a mistake, and so I will take a slightly more general approach here of introducing this universal enveloping algebra and then a general Casimir element, of which the quadratic Casimir is just one example, before restricting to representations of Casimir elements, which brings us back in line with the content of the course.}
    
    \begin{dfn}{Enveloping Algebra}{}
        Let \(\lie{g}\) be a Lie algebra over \(\field\).
        An \defineindex{enveloping algebra} of \(\lie{g}\) is a pair \((\varphi, U)\) where \(U\) is a unital associative algebra over \(\field\) and \(\varphi \colon \lie{g} \to U\) is a Lie algebra homomorphism, where the Lie algebra structure of \(U\) is that of the commutator, \(\bracket{X}{Y} = XY - YX\) for \(X, Y \in U\).
    \end{dfn}
    
    Note the similarity to the definition of a representation, \((\rho, V)\), where \(\rho \colon \lie{g} \to \generalLinearLie(V)\) is a Lie algebra homomorphism, where \(\generalLinearLie(V)\) is the Lie algebra of \(\End V\) equipped with the commutator as a bracket.
    What this means is that representations, \((\rho, V)\), correspond to enveloping algebras \((\rho, \End V)\).
    
    There is a distinguished enveloping algebra for each Lie algebra, called the universal enveloping algebra.
    As the name suggests it may be defined via a universal property.
    It is, in a sense, the most universal enveloping algebra because any other enveloping algebra factors through it.
    
    \begin{dfn}{Universal Enveloping Algebra}{}
        Let \(\lie{g}\) be a Lie algebra.
        The \defineindex{universal enveloping algebra} of \(\lie{g}\) is the enveloping algebra \((\Phi, \universalenveloping(\lie{g}))\) satisfying the universal property that for any enveloping algebra, \((\varphi, U)\), of \(\lie{g}\) there is a unique homomorphism of associative algebras \(f \colon \universalenveloping(\lie{g}) \to U\) such that \(\varphi = f \circ \Phi\).
        That is, the following diagram commutes
        \begin{equation}
            \begin{tikzcd}
                \lie{g} \arrow[r, "\varphi"] \arrow[d, "\Phi"'] & U\\
                \universalenveloping(\lie{g}) \arrow[ur, dashed, "\exists! f"']
            \end{tikzcd}
        \end{equation}
        commutes.
    \end{dfn}
    
    Note that the above definition doesn't really tell us what \(\universalenveloping(\lie{g})\) is, just what it does in relation to other enveloping algebras.
    In fact, this definition doesn't even guarantee the existence of such an object.
    This is usually the case with objects defined via a universal property.
    We shall now prove that the universal enveloping algebra always exists and is unique (up to isomorphism) when it does.
    
    \begin{lma}{}{}
        The universal enveloping algebra exists for any Lie algebra.
        \begin{proof}
            Let \(T(\lie{g})\) be the tensor algebra of \(\lie{g}\).
            This may variously be defined as the free unital associative algebra on some basis, \(\{e_1, e_2, \dotsc\}\) of \(\lie{g}\), or by
            \begin{equation}
                T(\lie{g}) = \field \oplus \lie{g} \oplus (\lie{g} \otimes \lie{g}) \oplus (\lie{g} \otimes \lie{g} \otimes \lie{g}) \oplus \dotsb.
            \end{equation}
            Let \(I\) be the two-sided ideal of \(T(\lie{g})\) generated by the elements \(e_ie_j - e_je_i - \bracket{e_i}{e_j}\).
            Then \(\universalenveloping(\lie{g}) = T(\lie{g})/I\) has the desired properties of the universal enveloping algebra if we define \(\Phi \colon \lie{g} \to T(\lie{g})/I\) by taking \(\Phi(e_i) = e_i + I\) and extending linearly to all of \(\lie{g}\).
            
            We first show that \((\Phi, T(\lie{g})/I)\) is an enveloping algebra.
            To do so we just need to check that \(\Phi\) is a Lie algebra homomorphism.
            Since \(\Phi\) is defined by linearly extending a mapping of a basis it is sufficient to check on a basis that \(\Phi\) is a Lie algebra homomorphism.
            That is, we need to check that \(\Phi(\bracket{e_i}{e_j}) = \bracket{\Phi(e_i)}{\Phi(e_j)}\).
            This is true because
            \begin{equation}
                \Phi(\bracket{e_i}{e_j}) - \bracket{\Phi(e_i)}{\Phi(e_j)} = \bracket{e_i}{e_j} - e_ie_j - e_je_i + I = 0 + I.
            \end{equation}
            Since \(0 + I\) is zero in \(T(\lie{g})/I\) this shows that we have the desired equality.
            
            We now show that \((\Phi, T(\lie{g})/I)\) is universal.
            Let \((\varphi, U)\) be some other enveloping algebra.
            An element of \(T(\lie{g})\) is a finite sum of a finite product of \(e_i\)s.
            Define the map \(\tilde{f} \colon T(\lie{g}) \to U\) by defining
            \begin{equation}
                f\left( {\textstyle \prod_{i=1}^k e_{i_j} }\right) = {\textstyle \prod_{i=1}^k} \varphi(e_{i_j})
            \end{equation}
            and extending linearly over sums.
            This is, by construction, a homomorphism of unital associative algebras.
            We further have
            \begin{equation}
                f(\bracket{e_i}{e_j} - e_ie_j - e_je_i) = \varphi(\bracket{e_i}{e_j}) - \varphi(e_i)\varphi(e_j) - \varphi(e_j)\varphi(e_i) = 0
            \end{equation}
            where the last equality follows because \(\varphi \colon \lie{g} \to U\) is a Lie algebra homomorphism.
            Then \(I \subseteq \ker \tilde{f}\), and so the map \(f \colon \universalenveloping(\lie{g}) \to U\) given by \(f(X + I) = \tilde{f}(X)\) is well-defined by definition of the quotient and is a homomorphism of associative unital algebras.
            
            Uniqueness of \(f\) is immediate when we realise that at no point in this construction were we free to make any assumptions.
        \end{proof}
    \end{lma}
    
    
    \begin{lma}{}{}
        The universal enveloping algebra is unique up to isomorphism.
        \begin{proof}
            Let \((\Phi_1, U_1)\) and \((\Phi_2, U_2)\) be universal enveloping algebras of \(\lie{g}\).
            Then by the universal properties of these two objects we have unique unital associative algebra homomorphisms \(f_{ij} \colon U_i \to U_j\) such that \(\Phi_i = f_{ij} \circ \Phi_j\) for \(i, j \in \{1, 2\}\).
            Since \(\Phi_i = \id_{U_i} \circ \Phi_i\) uniqueness imposes that \(f_{ii} = \id_{U_i}\).
            We also have \(\Phi_i = f_{ij} \circ \Phi_j = f_{ij} \circ f_{ji} \circ \Phi_i\), since \(\Phi_j = f_{ji} \circ \Phi_i\), and thus by uniqueness \(f_{ij} \circ f_{ji} = \id_{U_i}\).
            This means that \(f_{12}\) is invertible, and so is an isomorphism (of unital associative algebras) giving us the required uniqueness of universal enveloping algebras up to isomorphism.
        \end{proof}
    \end{lma}
    
    One can also set up the category of enveloping algebras, the objects of which are enveloping algebras and a morphism \((\varphi, U) \to (\psi, V)\) is a homomorphism of unital associative algebras, \(f \colon U \to V\), such that \(\psi = f \circ \varphi\).
    Then the universal enveloping algebra is the initial object in this category and is unique by abstract nonsense.
	
    The idea of the universal enveloping algebra is that it is the freest unital associative algebra which, when equipped with the commutator as a bracket, contains \(\lie{g}\) as a Lie-subalgebra.
    This is pretty much all we'll actually need.
    
    \begin{dfn}{Casimir Element}{}
        Let \(\lie{g}\) be a 
        A \defineindex{Casimir element}, \(\Omega\), is a distinguished element of the centre of \(\universalenveloping(\lie{g})\).
    \end{dfn}
    
    That is, \(\Omega\) is just some element of the universal enveloping algebra of \(\lie{g}\) which is such that it commutes with all elements of \(\universalenveloping(\lie{g})\) (commuting with respect to the associative product in the normal way, so it's commutator with any element of \(\universalenveloping(\lie{g})\) is always zero).
    
    It turns out that there's a way to construct such an element that works for any finite-dimensional Lie algebra.
    Before we can do this we need the notion of a dual basis.
    
    Let \(V\) be a finite-dimensional vector space with basis \(\basis = \{v_1, \dotsc, v_n\}\).
    Then any \(v \in V\) may uniquely be expressed as a sum
    \begin{equation}
        v = \sum_{i=1}^n a_i w_i
    \end{equation}
    with \(a_i \in \complex\).
    This defines \(n\) linear maps \(\lambda_i \colon V \to \complex\) given by \(\lambda_i(v) = a_i\).
    These maps form a basis, \(\{\lambda_1, \dotsc, \lambda_n\}\), for \(V^*\).
    We say that this is the dual basis in \(V^*\) to \(\basis\), by which we mean that
    \begin{equation}
        \lambda_i(v_j) = \delta_{ij}.
    \end{equation}
    
    Now suppose that \(\eta \colon V \times V \to \complex\) is a non-degenerate symmetric bilinear form.
    Then we can use this to define a second basis, \(\symcal{C} = \{u_1, \dotsc, u_n\}\) of \(V\), which is such that \(\eta(v_i, u_j) = \delta_{ij}\).
    Confusingly, we also call \(\symcal{C}\) the dual basis to \(\basis\), but now \(\symcal{C}\) is a basis for \(V\), not \(V^*\).
    
    \begin{lma}{}{}
        Given a finite-dimensional vector space, \(V\), with basis \(\basis\), and a non-degenerate symmetric bilinear form, \(\eta \colon V\times V \to \complex\), there is a unique dual basis, \(\symcal{C}\), such that \(\eta(v_i, u_j) = \delta_{ij}\) for all \(v_i \in \basis\) and \(u_j \in \symcal{C}\).
        \begin{proof}
            Non-degeneracy of \(\eta\) means that we have an isomorphism \(\iota_\eta \colon V \to V^*\).
            We may then define \(u_i = \iota_\eta^{-1}(\lambda_i)\) where \(\{\lambda_i\}\) is the dual basis to \(\basis\) in \(V^*\).
            Then
            \begin{multline}
                \eta(v_i, u_j) = \eta(u_j, v_i) = \iota_\eta(u_j)(v_i) = \iota_\eta(\iota_\eta^{-1}(\lambda_j))(v_i)\\
                = \id_{V^*}(\lambda_j)(v_i) = \lambda_j(v_i) = \delta_{ji} = \delta_{ij}.  
            \end{multline}
            This proves that \(\symcal{C} = \{u_1, \dotsc, u_n\}\) has the desired property.
            Uniqueness follows by noting that the above chain of equalities only holds for this choice of \(u_j\).
        \end{proof}
    \end{lma}
    
    One way of stating the above construction of \(\symcal{C}\) is that \(\symcal{C}\) is the pullback of the dual basis to \(\basis\) along \(\iota_\eta\).
    This is just fancy talk for saying that we're using the inverse of the map \(\iota_\eta\) to take something defined in \(V^*\) to something defined in \(V\).
    
    \begin{dfn}{Quadratic Casimir}{}
        Let \(\lie{g}\) be a finite-dimensional Lie algebra, and let \(\basis = \{X_1, \dotsc, X_n\}\) be a basis for \(\lie{g}\).
        Let \(\symcal{C} = \{Y_1, \dotsc, Y_n\}\) be the dual basis for \(\lie{g}\) corresponding to some non-degenerate symmetric bilinear form, \(\eta\).
        If \(\lie{g}\) is semisimple then we will assume \(\eta = \kappa\) is the Killing form, but it is always possible to find such an \(\eta\) in any case.
        The \defineindex{quadratic Casimir element}, \(\Omega\), is the Casimir element defined by
        \begin{equation}
            \Omega_2 = \sum_{i=1}^{n} X_i Y_i.
        \end{equation}
    \end{dfn}
     
    Note that this sum, and in particular the product \(X_i Y_i\), is all done in \(\universalenveloping(\lie{g})\).
    That \(\Omega_2\) is a Casimir element can be shown by an explicit calculation, or it can be seen by the following argument chaining together linear maps.
    This argument also proves that \(\Omega_2\) is independent of the choice of basis.
    \begin{itemize}
        \item There is a linear map \(\field \to \End \lie{g}\) given by sending \(\lambda \in \field\) to the map \(X \mapsto \lambda X\), that is the map which is just scaling by \(\lambda\).
        \item For \(\lie{g}\) finite dimensional there is a canonical isomorphism of vector spaces\footnote{We write \(\End_{\field}\lie{g}\) to mean the vector space of linear maps \(\lie{g} \to \lie{g}\), rather than the set of Lie algebra homomorphisms \(\lie{g} \to \lie{g}\), which we denote \(\End\lie{g}\).} \(\lie{g} \otimes \lie{g} \to \End_{\field} \lie{g}\) given by \((X, \lambda) \mapsto (Y \mapsto (\lambda(Y)X))\), that is \((X, \lambda)\) maps to the linear map given by scaling \(X\) by \(\lambda(Y) \in \complex\).
        \item For \(\lie{g}\) finite dimensional there is a canonical isomorphism of vector spaces \(\iota_\eta \colon \lie{g}^* \to \lie{g}\)
        \item There is a linear map \(\lie{g} \otimes \lie{g} \to \universalenveloping(\lie{g})\) given by \(X \otimes Y \mapsto XY\).
    \end{itemize}
    Combining these we have
    \begin{equation}
        \field \hookrightarrow \End_{\field} \lie{g} \xrightarrow{\sim} \lie{g} \otimes \lie{g}^* \xrightarrow{\sim} \lie{g} \otimes \lie{g} \to \universalenveloping(\lie{g}).
    \end{equation}
    Now consider where \(1 \in \field\) maps under this chain of linear maps.
    \begin{itemize}
        \item \(1 \mapsto \id_{\lie{g}}\) since scaling by 1 is the identity.
        \item \(\id_{\lie{g}} \mapsto \sum_{i=1}^n X_i \otimes \lambda_i\), this is the completeness relation and works for any choice of basis, \(\{X_i\}\), and its dual, \(\{\lambda_i\}\).
        \item \(\lambda_i \mapsto Y_i\) so \(X_i \otimes \lambda_i \mapsto X_i \otimes Y_i\) and \(\sum_{i=1}^n X_i \otimes \lambda_i \mapsto \sum_{i=1}^n X_i \otimes Y_i\).
        \item \(X_i \otimes Y_i\) maps to \(X_i Y_i\) so \(\sum_{i=1}^n X_i \otimes Y_i \mapsto \sum_{i=1}^n X_i Y_i = \Omega_2\).
    \end{itemize}
    Since \(1 \in \field\) is \(\lie{g}\)-invariant and commutes with all elements the same is true of its image under this composite algebra morphism, and the image is exactly the quadratic Casimir, \(\Omega_2\).
    
    \subsection{Casimir According to the Course}
    We now give the definition of the representation of \(\Omega\), which is what the course calls \emph{the} Casimir operator.
    
    Let \((\rho, V)\) be a representation of a simple Lie algebra, \(\lie{g}\).
    Then \(V\) is either faithful or trivial.
    Trivial modules aren't interesting, so we'll assume \(V\) is faithful.
    There is an obvious generalisation of the Killing form, namely
    \begin{align}
        \beta_V \colon \lie{g} \times \lie{g} &\to \complex\\
        (X, Y) &\mapsto \beta_V(X, Y) = \tr_V(\rho(X)\rho(Y))
    \end{align}
    where \(\tr_V\) is the trace of an operator on \(V\).
    Note that the Killing form is then simply \(\kappa = \beta_{\lie{g}}\) where we're considering the representation \((\ad, \lie{g})\).
    Many of the properties of the Killing form, such as bilinearity and associativity, hold for \(\beta_V\) without much modification to the proofs.
    The fact that \(\lie{g}\) is simple means that by similar proof to Cartan's criterion we also know that \(\beta_V\) is non-degenerate.
    
    We now define \enquote{the Casimir} as the course defines it.
    
    \begin{dfn}{}{}
        Let \(V\) be a faithful \(\lie{g}\)-module, and let \((\rho, V)\) be the corresponding representation.
        We define the \defineindex{Casimir} of \(V\) to be
        \begin{equation}
            \Omega_V = \sum_{i=1}^n \rho(X_i)\rho(Y_i)
        \end{equation}
        where \(\{X_1, \dotsc, X_n\}\) is some basis of \(\lie{g}\) and \(\{Y_1, \dotsc, Y_n\}\) is the dual basis in \(\lie{g}\) corresponding to the symmetric non-degenerate associative bilinear form \(\beta_V\).
    \end{dfn}
     
    That is, \(\Omega_V = \tilde{\rho}(\Omega_2)\) with our more general definition of the quadratic Casimir, where \(\tilde{\rho} \colon \universalenveloping(\lie{g}) \to \generalLinearLie(V)\) is the unital associative algebra homomorphism corresponding to the Lie algebra homomorhpism \(\rho\) under the universal property of \(\universalenveloping(\lie{g})\) with \((\rho, \End V)\) viewed as an enveloping algebra.
    
    That \(\Omega_V\) commutes with all elements of \(\End V\) follows immediately from the fact that \(\Omega_2\) commutes with all elements of \(\universalenveloping(\lie{g})\).
    It can also be proven by a calculation from the definition.
    First take \(X \in \lie{g}\) and let \(\{X_i\}\) be a basis for the \(n\)-dimensional \(\lie{g}\).
    Write
    \begin{equation}
        \bracket{X}{X_i} = \sum_{j=1}^n a_{ij}X_j
    \end{equation}
    for some \(a_{ij} \in \complex\).
    This is just expressing \(\bracket{X}{X_i} \in \lie{g}\) as a linear combination of basis vectors, \(X_i\).
    Similarly, write
    \begin{equation}
        \bracket{X}{Y_i} = \sum_{j=1}^n b_{ij}Y_j
    \end{equation}
    where \(\{Y_i\}\) is the dual basis to \(\{X_i\}\) corresponding to some symmetric non-degenerate associative bilinear form, \(\beta_V\).
    Then we have
    \begin{align}
        a_{ij} &= \sum_{k=1}^n a_{ik} \delta_{kj}\\
        &= \sum_{k=1}^n a_{ik} \beta_V(X_k, Y_j)\\
        &= \beta_V\left( \sum_{k=1}^n a_{ik}X_k, Y_j \right)\\
        &= \beta_V(\bracket{X}{X_i}, Y_j)\\
        &= \beta_V(-\bracket{X_i}{X}, Y_j)\\
        &= \beta_V(-X_i, \bracket{X}{Y_j})\\
        &= \beta_V\left( -X_i, \sum_{k=1}^n b_{jk}Y_k \right)\\
        &= \sum_{k=1}^n -b_{jk} \beta_V(X_i, Y_k)\\
        &= \sum_{k=1}^n -b_{jk} \delta_{ik}\\
        &= -b_{ji}.
    \end{align}
    Now we can use the fact that in \(\End V\) (and more generally in any algebra for which the bracket is a commutator) we have
    \begin{equation}
        \bracket{XY}{Z} = \bracket{X}{Z}Y + X\bracket{Y}{Z},
    \end{equation}
    which can be shown by expanding these brackets as commutators.
    This gives us
    \begin{align}
        \bracket{\Omega_V}{\rho(X)} &= \bracket*{\sum_{i=1}^n \rho(X_i)\rho(Y_i)}{\rho(X)}\\
        &= \sum_{i=1}^n (\bracket{\rho(X_i)}{\rho(X)}\rho(Y_i) + \rho(X_i)\bracket{\rho(Y_i)}{\rho(X)})\\
        &= \sum_{i=1}^n (\rho(\bracket{X_i}{X}) \rho(Y_i) + \rho(X_i) \rho(\bracket{Y_i}{X}))\\
        &= \sum_{i=1}^n (\rho(-\bracket{X}{X_i}) \rho(Y_i) + \rho(X_i) \rho(-\bracket{X}{Y_i}))\\
        &= -\sum_{i=1}^n \left( \rho\left( \sum_{j=1}^n a_{ij}X_j \right) \rho(Y_i) + \rho(X_i) \rho\left( \sum_{j=1}^n b_{ij}Y_j \right) \right)\\
        &= -\sum_{i=1}^n \sum_{j=1}^n (a_{ij}\rho(X_j)\rho(Y_i) + b_{ij}\rho(X_i)\rho(Y_j))\\
        &= 0
    \end{align}
    where the last equality follows since \(a_{ij} = -b_{ji}\) so the two terms in the sum cancel.
    
    Note that we have
    \begin{align}
        \tr(\Omega_V) &= \tr\left( \sum_{i=1}^n \rho(X_i)\rho(Y_i) \right)\\
        &= \sum_{i=1}^n \tr(\rho(X_i)\rho(Y_i))\\
        &= \sum_{i=1}^n \beta_V(X_i, Y_i)\\
        &= \sum_{i=1}^n \delta_{ii}\\
        &= \dim \lie{g}.
    \end{align}
    
    \section{Proving Weyl's Theorem}
    \Cref{lma:reducible iff comlements} implies that if \(V\) is a \(\lie{g}\)-module then it is sufficient to show that each \(\lie{g}\)-submodule, \(W\), admits a complementary \(\lie{g}\)-submodule, \(W'\).
    To do this we'll make use of the decomposition theorem (\cref{thm:decomposition}).
    Recall that this states that for a finite dimensional vector space, \(V\), and some endomorphism \(X \in \End_{\complex}(V)\) we have the decomposition
    \begin{equation}
        V = \bigoplus_{\alpha \in \complex} V_\alpha
    \end{equation}
    where
    \begin{equation}
        V_\alpha = \{v \in V \mid (X - \alpha)^Nv = 0 \text{ for some } N \in \naturals\}
    \end{equation}
    is the generalised eigenspace of \(V\) associated with \(X\).
    
    If in addition \(V\) is a \(\lie{g}\)-module and \(X \in \End_{\lie{g}}(V)\) then each \(V_\alpha\) is a \(\lie{g}\)-submodule since the action of \(\lie{g}\) on \(V\) commutes with the \(\lie{g}\)-module endomorphism, \(X\) (and also the identity), and thus leaves each \(V_\alpha\) invariant.
    That is, \(Y \cdot (X - \alpha)^Nv = 0\) if and only if \((X - \alpha)^N(Y \cdot v) = 0\).
    
    \begin{lma}{}{lma:one dimensional rep trivial}
        If \(\lie{g}\) is a semisimple Lie algebra and \(V\) is a one-dimensional \(\lie{g}\)-module then \(V\) is trivial.
        \begin{proof}
            Let \(\rho \colon \lie{g} \to \generalLinearLie(V)\) be the corresponding Lie algebra representation.
            For \(V\) one-dimensional, so \(V \isomorphic \complex\), then \(\generalLinearLie(V) \isomorphic \generalLinearLie(\complex) = \complex\), the only linear maps \(\complex \to \complex\) are scalar multiplication.
            The image \(\rho(\lie{g})\) is a Lie subalgebra of \(\generalLinearLie(V)\), and so in this case \(\rho(\lie{g})\) is a Lie subalgebra of \(\complex\), and thus is abelian.
            Then \(X, Y \in \lie{g}\) act on \(V\) by scalar multiplication, say \(X \cdot v = xv\) and \(Y \cdot v = yv\) for some \(x, y \in \complex\) and \(v \in V\).
            Then we have \(X \cdot (Y \cdot v) = xyv = yxv = Y \cdot (X \cdot v)\) so \(X \cdot (Y \cdot v) - Y \cdot (X \cdot v) = 0\) which means that we must have \(\bracket{X}{Y} \cdot v = 0\).
            Thus, we have that \(\bracket{\lie{g}}{\lie{g}}\) must act trivially on \(V\), but \(\lie{g} = \bracket{\lie{g}}{\lie{g}}\) for a semisimple Lie algebra, so \(\lie{g}\) acts trivially on \(V\).
            This last fact follows because \(\bracket{\lie{g}}{\lie{g}}\) is a solvable subalgebra of \(\lie{g}\) and therefore for semisimple \(\lie{g}\) must be either 0 or \(\lie{g}\), and it's not 0 unless \(\lie{g}\) is abelian, which is assumed not to be the case for a semisimple Lie algebra.
        \end{proof}
    \end{lma}
    
    Before we prove Weyl's theorem for all simple Lie algebras we prove a special case, and then the proof of the full theorem is just reduction to this special case.
    
    \begin{prp}{}{prp:weyls theorem for codimension 1}
        Let \(V\) be a \(\lie{g}\)-module and \(W\) a submodule of codimension one.
        Then, there exists a \(\lie{g}\)-submodule, \(W'\), of dimension one with \(W \oplus W' = V\).
        \begin{proof}
            The proof is by induction on \(\dim V\).
            The base case, \(\dim V = 1\), is trivial since any submodule of codimension one is of dimension \(1 - 1 = 0\) so the only codimension one submodule is \(0\), and we trivially have \(\dim V = 1\) and \(0 \oplus V = V\).
            
            For the inductive step assume that any \(\lie{g}\)-module of dimension less than \(\dim V \) admits complementary submodules for all submodules of codimension 1.
            Let \(W\) be a \(\lie{g}\)-submodule of \(V\) of codimension 1.
            We know that \(V\) is either a faithful representation or trivial (\cref{lma:simple lie alg rep faithful or trivial}).
            If \(V\) is trivial then any dimension 1 \(\lie{g}\)-submodule (which is any dimension 1 subspace) will work for \(W'\).
            Suppose \(V\) is faithful.
            We can use the Casimir, \(\Omega_V \in \End_{\lie{g}} V\), to generate \(W'\).
            To do this we consider the decomposition
            \begin{equation}
                V = \bigoplus_{\alpha \in \complex} V_\alpha
            \end{equation}
            where \(V_\alpha = \{v \in V \mid (\Omega_V - \alpha)^N(v) = 0 \text{ for some } N \in \naturals\}\) is the generalised eigenspace associated with \(\Omega_V\).
            We know that \(\Omega_V\) preserves \(W\), that is \(\Omega_V(W) \subseteq W\), since \(\lie{g}\) preserves \(W\), and \(\Omega_V\) acts as a sum of (products) of elements of \(\lie{g}\).
            We can therefore consider the restriction of \(\Omega_V\) to \(W\), \(\Omega_V|_W \colon W \to W\) where \(\Omega_V|_W(w) = \Omega_V(w)\), and \(\Omega_V|_W \in \End_{\lie{g}}W\).
            We then get a decomposition
            \begin{equation}
                W = \bigoplus_{\alpha \in \complex} W_\alpha
            \end{equation}
            where
            \begin{equation}
                W_\alpha = \{w \in W \mid (\Omega_V|_W - \alpha)^N(w) = 0 \text{ for some } N \in \naturals\}
            \end{equation}
            is the generalised eigenspace associated with \(\Omega_V|_W\).
            From these definitions we can immediately see that \(W_\alpha\) consists of all elements of \(W\) for which \((\Omega_V - \alpha)^N(w)\) is eventually \(0\) for some \(N \in \naturals\).
            That is, \(W_\alpha = W \cap V_\alpha\).
            
            \begin{clm}{}{}
                With the notation as above we have \(V_\alpha = W_\alpha\) for all \(\alpha \ne 0\) and \(\dim (V_0/W_0) = 1\).
                \begin{proof}
                    Each \(V_\alpha\) is a \(\lie{g}\)-module and \(W_\alpha\) is a \(\lie{g}\)-submodule of \(V_\alpha\).
                    Thus, the quotient \(V_\alpha/W_\alpha\) is a \(\lie{g}\)-module.
                    Further, we have
                    \begin{equation}
                        V/W = \bigoplus_{\alpha \in \complex} V_\alpha/W_\alpha.
                    \end{equation}
                    Since \(\dim V/W = 1\) (because \(W\) is of codimension 1 by assumption) there is therefore exactly one \(\alpha \in \complex\) for which \(V_\alpha/W_\alpha \ne 0\) and for that value of \(\alpha\) we have \(V_\alpha/W_\alpha = 1\).
                    This holds because the dimension of the sum is the sum of the dimensions.
                    We then only need to show that \(\alpha = 0\) is this distinguished value.
                    This follows because the quotients \(V_\alpha/W_\alpha = V_\alpha/(V_\alpha \cap W)\) are such that \(\lie{g}\) acts trivially on these, since these quotients have dimension at most one.
                    Then \(\Omega_V\) also acts trivially on the quotients since \(\Omega_V\) acts as a sum of products of elements of \(\lie{g}\).
                    However, by definition \(\Omega_V\) also acts on \(V_\alpha\) by scalar multiplication by \(\alpha\) and this carries across into the quotient, so it must be that \(\alpha = 0\) for our non-zero quotient.
                \end{proof}
            \end{clm}
            
            To complete the proof from here it is sufficient to show that \(\dim V_0 < \dim V\), since \(W_0 \subseteq V_0\) is a submodule of codimension one and this will allow us to apply the inductive hypothesis.
            We know that \(\dim V_0 < \dim V\) because \(\tr(\Omega_V) = \dim \lie{g} \ne 0\) so \(\Omega_V\) is not the zero operator, so there is at least one \(\alpha \in \complex\setminus\{0\}\) for which \(V_\alpha \ne 0\) and thus \(V_0\) cannot be all of \(V\).
        \end{proof}
    \end{prp}
    
    We're now almost ready to prove Weyl's theorem for any simple \(\lie{g}\)-module.
    In order to do this we just need a few more details.
    If \(V\) and \(W\) is a \(\lie{g}\)-module then so is \(\Hom_{\complex}(V, W)\), which has its vector space structure defined pointwise and the \(\lie{g}\)-module structure defined by taking \(X \in \lie{g}\) to act on \(f \in \Hom_{\complex}(V, W)\) by defining \(X \cdot f\) to be the map
    \begin{equation}
        \label{eqn:natural action on homC}
        (X \cdot f)(v) = X \cdot f(v) - f(X \cdot v)
    \end{equation}
    for all \(v \in V\).
    
    \begin{dfn}{Invariants}{}
        If \(U\) is a \(\lie{g}\)-module denote by \(U^{\lie{g}}\) the \(\lie{g}\)-submodule consisting of \(\lie{g}\)-invariant elements, that is \(u \in U^{\lie{g}}\) if and only if \(X \cdot u = 0\) for all \(X \in \lie{g}\).
    \end{dfn}
    
    \begin{lma}{}{}
        Let \(V\) and \(W\) be \(\lie{g}\)-modules.
        Then \(\Hom_{\complex}(V, W)^{\lie{g}} = \Hom_{\lie{g}}(V, W)\).
        That is, the \(\lie{g}\)-equivaraint maps are precisely those which are fixed under the action of \(\lie{g}\).
        \begin{proof}
            Suppose that \(f \in \Hom_{\lie{g}}(V, W)\), that is \(f \colon V \to W\) is linear and \(X \cdot f(v) = f(X \cdot v)\) for all \(X \in \lie{g}\) and \(v \in V\).
            Then by definition \(X\) acts trivially on \(f\) since the right hand side of \cref{eqn:natural action on homC} vanishes.
            Suppose instead that \(f \in \Hom_{\complex}(V, W)^{\lie{g}}\), so that \(X \cdot f = 0\) for all \(X \in \lie{g}\).
            Then the left hand side of \cref{eqn:natural action on homC} vanishes and it follows that \(X \cdot f(v) = f(X \cdot v)\), so \(f\) is \(\lie{g}\)-equivariant, so \(f \in \Hom_{\lie{g}}(V, W)\).
        \end{proof}
    \end{lma}
    
    We are now ready to prove Weyl's Theorem for simple Lie algebras.
    
    \begin{thm}{Weyl}{thm:weyl}
        Let \(\lie{g}\) be a simple Lie algebra over \(\complex\).
        Then every finite-dimensional representation of \(\lie{g}\) is completely reducible.
        \begin{proof}
            Let \(V\) be a \(\lie{g}\)-module with \(W \subseteq V\) a \(\lie{g}\)-submodule.
            Then \(\Hom_{\complex}(V, W)\) is a \(\lie{g}\)-module.
            Define
            \begin{equation}
                U = \{f \in \Hom_{\complex}(V, W) \mid f|_W = \lambda \id_W \text{ for some } \lambda \in \complex\}.
            \end{equation}
            That is, \(U\) is the subspace of \(\Hom_{\complex}(V, W)\) consisting of linear maps which restrict to scalar multiplication on \(W\).
            Let
            \begin{equation}
                U_0 = \{f \in \Hom_{\complex}(V, W) \mid f|_W = 0\}
            \end{equation}
            be the subspace of \(U\) of linear maps which \(V \to W\) which restrict to the zero map on \(W\).
            
            \begin{clm}{}{}
                With notation as above \(U\) and \(U_0\) are \(\lie{g}\)-submodules of \(\Hom_{\complex}(V, W)\).
                \begin{proof}
                    We need to show that \(U\) and \(U_0\) are closed under the action of \(\lie{g}\).
                    Let \(X \in \lie{g}\) and \(f \in U\).
                    Then \(X\) acts on \(f\) according to \cref{eqn:natural action on homC}.
                    That is
                    \begin{equation}
                        (X \cdot f)(v) = X \cdot f(v) - f(X \cdot v)
                    \end{equation}
                    for all \(v \in V\).
                    We need to show that \(X \cdot f \in U\), that is, we want to show that \((X \cdot f) |_W = \lambda \id_W\) for some \(\lambda \in \complex\).
                    This is simple because if \(v = w \in W\) in the above we have
                    \begin{equation}
                        (X \cdot f)|_W(w) = (X \cdot f)(w) = X \cdot f(w) - f(X \cdot w)
                    \end{equation}
                    and since \(W\) is a submodule we know that \(X \cdot w \in W\) so both applications of \(f\) above are to some element of \(W\), and so both are just scalar multiplication by some \(\lambda \in \complex\), and thus
                    \begin{equation}
                        (X \cdot f)|_W(w) = X \cdot (\lambda w) - \lambda(X \cdot w) = 0
                    \end{equation}
                    using linearity of the action of \(X\).
                    This shows that \(X \cdot f \in U\) as \(X \cdot f\) simply acts as \(0 \id_W\).
                    This also proves that \(U_0\) is a submodule, it's just the special case where \(\lambda = 0\) in the above equations.
                \end{proof}
            \end{clm}
            
            \begin{clm}{}{}
                With notation as above \(\dim U/U_0 = 1\).
                \begin{proof}
                    Consider the restriction map
                    \begin{align}
                        \res \colon U &\to \Hom_{\complex}(W, V)\\
                        f &\mapsto f|_W.
                    \end{align}
                    By definition of \(U\) we know that \(\res f = \lambda \id_W\) where \(\lambda \in \complex\), and certainly we hit every \(\lambda \in \complex\) in this way, so \(\res(U) \isomorphic \complex\).
                    We also have that \(\ker(\res)\) consists of those maps which map to \(0 \id_W\), which is exactly \(\ker(\res) = U_0\).
                    Thus, by the rank-nullity theorem we have that
                    \begin{equation}
                        \dim U = \dim(\res(U)) + \dim(\ker(\res)) = 1 + \dim U_0
                    \end{equation}
                    and so \(\dim U/U_0 = 1\).
                \end{proof}
            \end{clm}
            
            By the special case of Weyl's theorem (\cref{prp:weyls theorem for codimension 1}) we know that \(U_0\) admits a complement \(U_0'\) such that \(U_0 \oplus U_0' = U\).
            Now take some \(\varphi \in U_0'\), then we know that \(\res \varphi = \varphi|_W = c \id_W\) for some \(c \in \complex\).
            Since \(\varphi \in U_0'\) we know that \(\varphi \in U_0\) only if \(\varphi = 0\).
            Assuming that \(\varphi \ne 0\) we therefore have that \(c \ne 0\), and so we can rescale \(\varphi\) to \(\frac{1}{c}\varphi\) which has \(\res(\frac{1}{c}\varphi) = \id_W\).
            Therefore without loss of generality we may assume that any nonzero \(\varphi \in U_0'\) is such that \(\res(\varphi) = \id_W\).
            
            \begin{clm}{}{}
                With the notation as above we have \(\varphi \in \Hom_{\lie{g}}\).
                \begin{proof}
                    We know that \(\dim U_0' = 1\), and thus the action of \(\lie{g}\) on \(U_0'\) is trivial (\cref{lma:one dimensional rep trivial}).
                    Thus, for all \(X \in \lie{g}\) we have \(X \cdot \varphi(v) = \varphi(X \cdot v)\), which is \cref{eqn:natural action on homC} defining the action of \(X\) on \(\Hom_{\complex}(V, W)\) when the action is trivial.
                    This result is exactly what it means for \(\varphi\) to be a \(\lie{g}\)-module homomorphism.
                \end{proof}
            \end{clm}
            
            This allows us to construct the following sequence of \(\lie{g}\)-modules linked by \(\lie{g}\)-module homomorphisms:
            \begin{equation}
                W \hookrightarrow V \xrightarrow{\varphi} W.
            \end{equation}
            Further, by definition we know that embedding \(W\) in \(V\) then applying \(\varphi\) is the same sa applying the identity map since \(\res(\varphi) = \id_W\).
            The existence of such a chain here tells us that \(V = W \oplus \ker \varphi\) and thus we have found a complement for \(W\) and so \(V\) is completely reducible by \cref{lma:reducible iff comlements}.
        \end{proof}
    \end{thm}
    
    Here we've used the fact that a short exact sequence
    \begin{equation}
        \label{eqn:short exact sequence}
        0 \to A \xrightarrow{f} B \xrightarrow{g} C \to 0
    \end{equation}
    splits (meaning that the map \(B \to C\) has a right inverse, \(h \colon C \to B\), which we've shown is the case for \(\varphi\) here) if and only if \(B = C/A\).
    This is a general result that holds for vector spaces and more generally for \(R\)-modules.
    It follows then that \(B \isomorphic A \oplus C\) because we can explicitly construct an isomorphism \(A \oplus C \to B\) by \((a, c) \mapsto f(a) + h(c)\).
    Note that a sequence
    \begin{equation}
        \dotsb \to X \xrightarrow{f} Y \xrightarrow{g} Z \to \dotsb
    \end{equation}
    is exact at \(Y\) if \(f(X) = \ker g\), which for a short exact sequence, such as \cref{eqn:short exact sequence}, implies that \(f\) is injective and \(g\) is surjective.
    
    Note that the choice of \(U_0'\) is not unique, since in the proof of \cref{prp:weyls theorem for codimension 1} any one-dimensional subspace we choose will work.
    
    It remains only to show that this result generalises to semisimple Lie algebras.
    To do this first note that if \(V\) is an \((\lie{m} \oplus \lie{l})\)-module then defining \(X \cdot f = (X, 0) \cdot f\) for all \(f \in \Hom_{\ell}(W, V)\) makes \(\Hom_{\ell}(W, V)\) into a \(\lie{m}\)-module.
    
    \begin{crl}{Weyl for Semisimple Lie Algebras}{}
        Let \(\lie{g}\) be a semisimple Lie algebra over \(\complex\).
        Then every finite-dimensional representation of \(\lie{g}\) is completely reducible.
        \begin{proof}
            Recall that any semisimple Lie algebra, \(\lie{g}\), can be decomposed as a sum of simple Lie algebras, \(\lie{g}_i\):
            \begin{equation}
                \lie{g} = \lie{g}_1 \oplus \dotsb \oplus \lie{g}_k.
            \end{equation}
            We prove this result by induction on \(k\).
            The case of \(k = 1\) simply has \(\lie{g} = \lie{g}_1\), which means that \(\lie{g}\) is simple, so this is just Weyl's theorem (\cref{thm:weyl}).
            Suppose then that every semisimple Lie algebra which decomposes into fewer than \(k\) simple Lie algebras has only completely reducible finite-dimensional representations over \(\complex\).
            
            Let \(\lie{g}\) be a semisimple Lie group decomposing as \(\lie{g} = \lie{g}_1 \oplus \lie{m}\) where \(\lie{g}_1\) is simple and \(\lie{m} = \lie{g}_2 \oplus \dotsb \oplus \lie{g}_k\) where the \(\lie{g}_i\) are also simple.
            Let \(V\) be a finite-dimensional \(\lie{g}\)-module.
            Using the simplicity of \(\lie{g}_1\) we can decompose \(V\) as a direct sum \(V = V_1^{n_1} \oplus \dotsb \oplus V_r^{n_r}\) where the \(V_i\) are simple, pairwise non-isomorphic \(\lie{g}_1\)-modules.
            Define a map
            \begin{align}
                \Hom_{\lie{g}_1}(V_i, V) \otimes V_i &\to V\\
                (\varphi, v) &\mapsto \varphi(v).
            \end{align}
            We know by the discussion preceding this corollary that \(\Hom_{\lie{g}_1}(V_i, V)\) is an \(\lie{m}\)-module.
            Thus, \(\Hom_{\lie{g}_1}(V_i, V) \otimes V_i\) is a \(\lie{m}\) module, and \(V\) is a \(\lie{g}_1\)-module, because it is a \(\lie{g}\)-module and \(\lie{g}_1 \subseteq \lie{g}\).
            Then \(\Hom_{\lie{g}_1}(V_i, V) \otimes V_1\) is an \((\lie{m} \oplus \lie{g}_1)\)-module, and \(\lie{m} \oplus \lie{g}_1 \isomorphic \lie{g}_1 \oplus \lie{m} = \lie{g}\), so \(\Hom_{\lie{g}_1}(V_i, V) \otimes V\) is a \(\lie{g}\)-module.
            The map defined above then extends to an isomorphism of \(\lie{g}\)-modules
            \begin{equation}
                (\Hom_{\lie{g}_1}(V_1, V) \otimes V_1) \oplus \dotsb \oplus (\Hom_{\lie{g}_1}(V_r, V) \otimes V_r) \to V.
            \end{equation}
            Since each \(\Hom_{\lie{g}_1}(V_i, V)\) is a completely reducible \(\lie{m}\)-module by the induction hypothesis we can decompose each \(\Hom_{\lie{g}_1}(V_i, V) \otimes V_i\) as a direct sum of simple \(\lie{g}\)-modules.
        \end{proof}
    \end{crl}
    
    \chapter{Cartan Subalgebras}
    \section{Representation Theory of \texorpdfstring{\(\specialLinearLie_2\)}{sl2}}
    We start this section with a recap of \(\specialLinearLie_2 = \specialLinearLie(2, \complex)\) representation theory.
    We saw in \cref{thm:sl2 reps classification} that any simple \(\specialLinearLie_2\)-module is isomorphic to an \(\specialLinearLie_2\)-module, \(V(n)\), for some \(n \in \integers\) such that there exists some highest weight vector \(v_0 \in V(n)_n\) where
    \begin{equation}
        V(n)_k = \{v \in V(n) \mid Hv = kv\}
    \end{equation}
    for integer \(k\) with \(-n \le k \le n\).
    By \enquote{highest weight} we mean that \(Hv_0 = nv_0\) and \(Ev_0 = 0\).
    The dimension of \(V(n)\) is \(\dim V(n) = n + 1\), with a basis given by \(\{v_i = F^i v_0/i! \mid i = 0, 1, \dotsc, n\}\).
    This leads to the picture in \cref{eqn:sl2 rep picture} where the \(\specialLinearLie_2\) generators act as \(E \colon V(n)_k \to V(n)_{k+2}\), \(H \colon V(n)_k \to V(n)_k\), and \(F \colon V(n)_k \to V(n)_{k-2}\), where we define \(V(n)_{n+2} = 0 = V(n)_{-n-2}\).
    
    Since \(\specialLinearLie_2\) is simple the work of the previous chapter tells us that any finite-dimensional \(\specialLinearLie_2\)-module, \(V\), admits a decomposition as
    \begin{equation}
        V \isomorphic \bigoplus_{i \ge 0} V(i)^{\oplus n_i}
    \end{equation}
    where the \defineindex{multiplicity} is \(n_i \in \integers_{\ge 0}\) with \(n_i = 0\) for \(i \gg 0\).
    These \(n_i\) are uniquely determined by \(V\), and can be computed as \(n_i = \dim(V_i \cap \ker E)\).
    That is, we get one \(V(i)\) submodule for each basis vector in \(V_i\) (that is, an eigenvector of \(H\) with eigenvalue \(i\)) that also vanishes under the action of \(E\), so the highest weight vectors of \(V\) determine \(n_i\).
    
    We now list some special properties of \(H\) and which we will seek to generalise to other simple Lie algebras.
    \begin{itemize}
        \item If \(V\) is a finite dimensional \(\specialLinearLie_2\)-module then \(H\) acts diagonalisably on \(V\), this is an immediate consequence of the direct sum decomposition of \(V\) being into eigenspaces of \(H\).
        In fact, one can take \enquote{diagonalisable} to mean that there is an eigenbasis of the operator corresponding to such a decomposition.
        \item All of \(H\)'s eigenvalues are integers.
        \item For any integer \(\alpha \in \integers\) we have \(\dim V_\alpha = \dim V_{-\alpha}\), since the generalised \(H\)-eigenspace of \(\alpha\) and \(-\alpha\) coincide.
    \end{itemize}
    These last two facts are particularly nice because \(\specialLinearLie_2\)-modules tend to arise in lots of areas of maths, such as Hodge theory and the cohomology of K\"ahler manifolds.
    
    \subsection{Representation Theory of \texorpdfstring{\(\specialLinearLie_3\)}{sl3}}
    Now consider \(\specialLinearLie_3 = \specialLinearLie(3, \complex)\), which is also a simple Lie algebra, consisting of traceless \(3 \times 3\) matrices over \(\complex\).
    There is no one equivalent \enquote{\(H\)} for \(\specialLinearLie_3\).
    Instead, we can consider
    \begin{equation}
        H_1 = 
        \begin{bmatrix}
            1 & 0 & 0\\
            0 & -1 & 0\\
            0 & 0 & 0
        \end{bmatrix}
        , \qqand H_2 =
        \begin{bmatrix}
            0 & 0 & 0\\
            0 & 1 & 0\\
            0 & 0 & -1
        \end{bmatrix}
        .
    \end{equation}
    More specifically, we want to consider \(\csa = \Span_{\complex}\{H_1, H_2\}\) which consists of all traceless diagonal \(3 \times 3\) matrices over \(\complex\).
    Compare this to the case of \(\specialLinearLie_2\) where considering the same set but for \(2 \times 2\) matrices would mean considering \(\csa = \Span_{\complex}\{H\}\).
    We can generalise the definition of a weight space from \(\specialLinearLie_2\) to \(\specialLinearLie_3\) by defining a weight space to be a the common eigenspace of all \(H \in \csa\).
    Then any finite-dimensional \(\specialLinearLie_3\)-module, \(V\), has a decomposition into weight-spaces with multiplicity, just as in the \(\specialLinearLie_2\) case.
    
    We will follow this generalisation through for arbitrary simple finite-dimensional Lie algebras over \(\complex\), and we shall see that we get the same result each time.
    We'll also see that there are generalisations of \(E\) and \(F\), as well as the hidden integrality of the eigenvalues of \(H\) which lead us to define something called a root system, which then leads to a full classification of all finite-dimensional simple Lie algebras over \(\complex\).
    
    \subsection{Preliminary Definitions}
    \begin{dfn}{}{}
        Let \(V\) be a finite-dimensional vector space over \(\complex\).
        We say that \(A \in \End_{\complex}V\) is \defineindex{nilpotent} if there exists \(n \in \naturals\) such that \(A^n = 0\).
        We say that \(A\) is \defineindex{semisimple} if it is diagonalisable.
    \end{dfn}
    
    For example, for any finite-dimensional \(\specialLinearLie_2\)-module \(E\) and \(F\) are nilpotent, since applying them repeatedly we always end up with the zero module, and \(H\) is semisimple since we've shown that \(H\) is diagonalisable.
    The benefit of the \enquote{semisimple} wording over \enquote{diagonalisable} is that it reduces the emphasis of matrices, which is generally a good thing for more advanced linear algebra where we aim to talk about linear maps rather than matrices.
    
    \begin{prp}{}{}
        Let \(V\) be a finite-dimensional vector space over \(\complex\) and let \(A \in \End_{\complex}V\).
        Then there exists a unique decomposition \(A = A_{\symrm{s}} + A_{\symrm{n}}\) with \(A_{\symrm{s}}\) semisimple, \(A_{\symrm{n}}\) nilpotent, and \(\bracket{A_{\symrm{s}}}{A_{\symrm{n}}} = 0\).
        \begin{proof}
            Since we work over \(\complex\) we know that \(V\) decomposes into generalised-eigenspaces of \(A\) as
            \begin{equation}
                V = \bigoplus_{\alpha \in \complex} V_\alpha
            \end{equation}
            where
            \begin{equation}
                V_\alpha = \{v \in V \mid (A - \alpha)^Nv = 0 \text{ for some } N \in \naturals\}.
            \end{equation}
            We can then define \(A_{\symrm{s}}\) and \(A_{\symrm{n}}\) by their action on each generalised eigenspace.
            In particular, take
            \begin{equation}
                A_{\symrm{s}} = \sum_{\alpha \in \complex} \alpha \id_{V_\alpha}
            \end{equation}
            so \(A_{\symrm{s}}\) acts on \(V_\alpha\) by multiplication by \(\alpha\).
            Define \(A_{\symrm{n}}\) to be what is left over,
            \begin{equation}
                A_{\symrm{n}} = \sum_{\alpha \in \complex} (A|_{V_\alpha} - \alpha \id_{V_\alpha}).
            \end{equation}
            Then by construction \(A = A_{\symrm{s}} + A_{\symrm{n}}\) and \(A_{\symrm{s}}\) acts on each generalised eigenspace by scalar multiplication so it's action commutes with that of \(A_{\symrm{n}}\).
            
            We do not prove uniqueness.
            % TODO: Maybe prove uniqueness
        \end{proof}
    \end{prp}
    
    Note that this works over \(\complex\), since it's algebraically closed, guaranteeing the generalised eigenspace decomposition exists.
    It does not work in general over non-algebraically closed fields, it only works for the special cases where the field contains the eigenvalues.
    
    \subsection{Cartan Subalgebra}
    \begin{dfn}{}{}
        Let \(\lie{g}\) be a Lie algebra over \(\complex\).
        We say that \(X \in \lie{g}\) is nilpotent (semisimple), or if we need to distinguish \(\ad\)-nilpotent (\(\ad\)-semisimple) if \(\ad_X\) is a nilpotent (semisimple) element of \(\End_{\complex} \lie{g}\).
    \end{dfn}
    
    \begin{dfn}{Cartan Subalgebra}{}
        Let \(\lie{g}\) be a Lie algebra.
        A \defineindex{Cartan subalgebra} is a subalgebra \(\csa \subseteq \lie{g}\) containing only semisimple elements and which is maximal with respect to this property.
        That is, if \(\csa' \subseteq \lie{g}\) is also a subalgebra consisting of semisimple elements and \(\csa \subseteq \csa'\) then \(\csa = \csa'\).
        
        We call the dimension of a Cartan subalgebra, \(\csa\), the \defineindex{rank} of \(\lie{g}\).
    \end{dfn}
    
    \begin{lma}{}{}
        Every finite-dimensional Lie algebra has a Cartan subalgebra.
        \begin{proof}
            The zero space is always a subalgebra, and the zero element is semisimple.
            This shows that every Lie algebra has a subalgebra consisting of semisimple elements.
            Since we assume a finite number of dimensions it is always possible to take some maximal such subalgebra and this will then be a Cartan subalgebra.
        \end{proof}
    \end{lma}
    
    Note that we say \emph{a} Cartan subalgebra, Cartan subalgebras are not necessarily unique.
    However, when there are multiple Cartan subalgebras they are all conjugate, and as such it's common to talk of \enquote{the} Cartan subalgebra.
    
    One property of Cartan subalgebras is that they are always abelian.
    This is simply a special case of the following.
    
    \begin{lma}{}{lma:subalgebra of semisimple elements is abelian}
        Let \(\lie{g}\) be a Lie algebra.
        Any subalgebra, \(\lie{h} \subseteq \lie{g}\), consisting of semisimple elements is abelian.
        \begin{proof}
            Consider some \(X \in \lie{h}\).
            Since \(\lie{h}\) is a subalgebra of \(\lie{g}\) it is closed under the bracket, and thus we can restrict the adjoint action of \(X\) on \(\lie{g}\) to a map \(\ad_X \colon \lie{h} \to \lie{h}\).
            Our goal is to show that \(\lie{h}\) is abelian, which we can do by showing that \(\ad_X = 0\) when restricted to \(\lie{h}\).
            By assumption \(\lie{h}\) consists of semisimple elements so \(\ad_X\) is semisimple.
            Therefore it suffices to show that all eigenvalues of \(\ad_X\) are zero, since then in the basis in which it is diagonal it has only zeros on the diagonal, and thus is just the zero matrix.
            More generally, if all eigenvalues vanish then the operator is also nilpotent, and semisimplicity plus nilpotency imply that the operator is zero.
            
            Let \(Y \in \lie{h}\) be an eigenvector of \(\ad_X\).
            Then there exists some \(a \in \complex\) such that \(\ad_X(Y) = aY\).
            Note that \(Y\) is also an eigenvector of \(\ad_Y\) since \(\ad_Y(Y)= \bracket{Y}{Y} = 0\), so the eigenvalue is \(0\).
            We may extend this to some basis of eigenvectors of \(Y\), \(\{X_1, \dotsc, X_n\}\), with \(X_1 = Y\).
            Then by definition there exist some \(\alpha_i \in \complex\) such that \(\ad_Y(X_i) = \alpha_iX_i\), and in particular we must have that \(\alpha_1 = 0\).
            Now expand \(X\) in this eigenbasis, which lets us write
            \begin{equation}
                X = \sum_{i=1}^n u_iX_i
            \end{equation}
            for some \(u_i \in \complex\).
            A direct computation in this basis then shows that
            \begin{align}
                \ad_X(Y) &= \bracket{X}{Y}\\
                &= -\bracket{Y}{X}\\
                &= -\bracket*{Y}{\sum_{i=1}^n u_i X_i}\\
                &= -\sum_{i=1}^n u_i \bracket{Y}{X_i}\\
                &= -\sum_{i=1}^n u_i \ad_Y(X_i)\\
                &= -\sum_{i=2}^n u_i \alpha_iX_i
            \end{align}
            where we've used the fact that \(\ad_Y(X_1) = 0\) to get rid of the first term of the sum.
            We also have
            \begin{equation}
                \ad_X(Y) = aY = aX_1.
            \end{equation}
            Equating these we have that
            \begin{equation}
                aX_1 = -\sum_{i=1}^n \alpha_i u_i X_i \implies aX_1 + \sum_{i=2} \alpha_i u_i X_i = 0.
            \end{equation}
            Since the \(X_i\) form a basis this implies that \(a = \alpha_i u_i = 0\) for all \(i\) by linear independence.
            Thus, \(a = 0\) and \(\ad_X\) has only zero eigenvalues and is semisimple, so it is zero.
            Hence, \(\lie{h}\) is abelian.
        \end{proof}
    \end{lma}
    
    \begin{crl}{}{}
        Let \(\lie{g}\) be a Lie algebra and \(\lie{h} \subseteq \lie{g}\) a subalgebra consisting of only semisimple elements.
        Then if any subalgebra strictly containing \(\lie{h}\) is nonabelian \(\lie{h}\) is a Cartan subalgebra of \(\lie{g}\).
        \begin{proof}
            By the contrapositive of \cref{lma:subalgebra of semisimple elements is abelian} the subalgebras containing \(\lie{h}\) cannot contain only semisimple elements, and therefore \(\lie{h}\) is a maximal subalgebra containing only semisimple elements, and thus is a Cartan subalgebra of \(\lie{g}\).
        \end{proof}
    \end{crl}
    
    This corollary gives us a method of checking if a given subalgebra is Cartan that is often easier to check than directly checking that there is no larger subalgebra.
    
    \begin{dfn}{Normaliser}{}
        Let \(\lie{g}\) be a Lie algebra and let \(\lie{m} \subseteq\) be a subalgebra.
        The \defineindex{normaliser}, \(N_{\lie{g}}(\lie{m})\), of \(\lie{m}\) in \(\lie{g}\) is the largest subalgebra of \(\lie{g}\) such that \(\bracket{N_{\lie{g}}(\lie{m})}{\lie{m}} \subseteq \lie{m}\).
        That is,
        \begin{equation}
            N_{\lie{g}}(\lie{m}) = \{X \in \lie{g} \mid \bracket{X}{\lie{m}} \subseteq \lie{m}\}.
        \end{equation}
    \end{dfn}
    
    \begin{lma}{}{}
        The normaliser of a subalgebra is again a subalgebra.
        Further the normaliser of a subalgebra in \(\lie{g}\) is equal to \(\lie{g}\) if and only if the subalgebra is an ideal.
        \begin{proof}
            Let \(\lie{g}\) be a Lie algebra and \(\lie{m} \subseteq \lie{g}\) a subalgebra.
            To show that \(N_{\lie{g}}(\lie{m})\) is a subalgebra we need to show that it is a subspace which is closed under the bracket.
            First note that if \(X \in N_{\lie{g}}(\lie{m})\) then \(\bracket{X}{\lie{m}} \subseteq \lie{m}\), that is \(\bracket{X}{Z} \in \lie{m}\) for all \(Z \in \lie{m}\).
            Then it follows by linearity that \(\bracket{aX}{Z} = a\bracket{X}{Z} \in \lie{m}\) for any \(a \in \complex\).
            If \(Y \in N_{\lie{g}}(\lie{m})\) also then \(\bracket{Y}{Z} \in \lie{m}\) for all \(Z \in \lie{m}\) and again by linearity we have
            \begin{equation}
                \bracket{X + Y}{Z} = \bracket{X}{Z} + \bracket{Y}{Z} \in \lie{m}.
            \end{equation}
            So, \(N_{\lie{g}}(\lie{m})\) is a subspace.
            
            Now suppose that \(X, Y \in N_{\lie{g}}(\lie{m})\).
            Then for \(Z \in \lie{m}\) using the Jacobi identity we hvae
            \begin{equation}
                \bracket{\bracket{X}{Y}}{Z} = -\bracket{Z}{\bracket{X}{Y}} = -\bracket{\bracket{Z}{X}}{Y} - \bracket{X}{\bracket{Z}{Y}}
            \end{equation}
            and each of these brackets is in \(N_{\lie{g}}(\lie{m})\) since \(\bracket{Z}{X}\) and \(\bracket{Z}{Y}\) are in \(\lie{m}\) and so the bracket of these with \(Y\) and \(X\) respectively is again in \(\lie{m}\).
            Thus, \(N_{\lie{g}}(\lie{m})\) is closed under the bracket so it is a subalgebra.
            
            Now suppose that \(\lie{m}\) is an ideal.
            Then \(\bracket{X}{Z} \in \lie{m}\) for any \(X \in \lie{g}\) and \(Z \in \lie{m}\), so \(N_{\lie{g}}(\lie{m}) = \lie{g}\).
            Conversely, suppose that \(N_{\lie{g}}(\lie{m}) = \lie{g}\).
            Then \(\bracket{X}{Z} \in \lie{m}\) for any \(X \in \lie{g}\) and \(Z \in \lie{m}\), so \(N_{\lie{g}}(\lie{m})\) is all of \(\lie{g}\).
        \end{proof}
    \end{lma}
    
    From the last part of this proof we see that \(N_{\lie{g}}(\lie{m}) = \lie{g}\) is just a slightly more complicated way of writing the absorbing property of an ideal.
    
    An important property of Cartan subalgebras is that they are equal to their own normalisers.
    That is, if \(X \in \lie{g}\) such that \(\bracket{X}{\csa} \subset \csa\) then \(X \in \lie{h}\).
    This follows immediately from the maximality of Cartan subalgebras, since the normaliser of a subalgebra of semisimple elements consists only of semisimple elements.
    
    \subsubsection{Examples of Cartan Subalgebras}
    \paragraph{Cartan Subalgebras of \(\specialLinearLie_2\)}
    The simplest example of a Cartan subalgebra is the\footnote{a slight abuse of language, but one that is common} Cartan subalgebra of \(\specialLinearLie_2\), which is simply \(\csa = \complex H\).
    To prove this is a Cartan subalgebra we need to show that
    \begin{enumerate}[label=\alph*)]
        \item every element of \(\csa\) is semisimple; and
        \item \(\csa\) is maximal.
    \end{enumerate}
    
    First note that we need to show that all elements of \(\csa\) are \(\ad\)-semisimple, that is that \(\ad_X\) is diagonalisable, not that \(X\) is diagonalisable.
    An arbitrary element of \(\csa\) is of the form \(cH\) for some \(c \in \complex\).
    We then have
    \begin{align}
        \ad_{cH}(E) &= \bracket{cH}{E} = 2cE,\\
        \ad_{cH}(H) &= \bracket{cH}{H} = 0,\\
        \ad_{cH}(F) &= \bracket{cH}{F} = -2cF.\\
    \end{align}
    This shows that in the basis \(\{E, H, F\}\) we have
    \begin{equation}
        \ad_{cH} = 
        \begin{bmatrix}
            2c & 0 & 0\\
            0 & 0 & 0\\
            0 & 0 & -2c
        \end{bmatrix}
    \end{equation}
    which shows that \(\ad_{cH}\) is diagonalisable, and thus \(cH\) is semisimple.
    
    Now suppose that \(\tilde{\lie{h}}\) is some other subalgebra of \(\specialLinearLie_2\) such that \(\lie{h} \subseteq \tilde{\lie{h}}\) and \(\tilde{\lie{h}}\) is abelian.
    Then for all \(Y \in \tilde{\lie{h}}\) we have \(\ad_X(Y) = 0\), which implies that all \(Y \in \tilde{\lie{h}}\) lie in the zero eigenspace of \(\ad_X\) for all \(X \in \csa\).
    However, from our calculations above we see that the zero eigenspace of \(\ad_X\) is simply the span of \(H\), which is just \(\csa\).
    Thus, \(\tilde{\lie{h}} \subseteq \csa\), and so we have \(\tilde{\lie{h}} = \csa\).
    This proves maximality.
    
    It turns out that this is really the \emph{only} example of a Cartan subalgebra of \(\specialLinearLie_2\) up to a very specific type of isomorhpism.
    The Lie group \(\specialLinear_2\) consists of \(2 \times 2\) matrices over \(\complex\) with unit determinant.
    There is an action of \(\specialLinear_2\) on \(\specialLinearLie_2\) given by conjugation.
    For \(A \in \specialLinear_2\) denote this action by \(c_A\), that is \(c_A(X) = AXA^{-1}\) for \(X \in \specialLinearLie_2\).
    Since the bracket in \(\specialLinearLie_2\) is just the commutator it's easy to show that \(c_A\) preserves the bracket:
    \begin{align}
        \bracket{c_A(X)}{c_A(Y)} &= \bracket{AXA^{-1}}{AYA^{-1}}\\
        &= AXA^{-1}AYA^{-1} - AYA^{-1}AXA^{-1}\\
        &= AXYA^{-1} - AYXA^{-1}\\
        &= A\bracket{X}{Y}A^{-1}\\
        &= c_A(\bracket{X}{Y}).
    \end{align}
    Clearly conjugation is linear.
    Thus, \(c_A\) is an automorphism of \(\specialLinearLie_2\).
    
    We claim that \(\Span_{\complex}\{AHA^{-1}\}\) for some fixed \(A \in \specialLinear_2\) is also a Cartan subalgebra of \(\specialLinearLie_2\).
    This follows in exactly the same way as the proof that \(\Span_{\complex}\{H\}\) is a Cartan subalgebra.
    The conjugate \(A\)s just factor out at each step.
    In fact, one can show that all Cartan subalgebras of \(\specialLinearLie_2\) arise in this way.
    
    % TODO: are all CSAs conjugate under this action of the corresponding Lie group?
    
    \paragraph{Cartan Subalgebras of \(\specialLinearLie_n\)}
    The next simplest example is the generalisation of this to \(\specialLinearLie_n\).
    Note that \(\specialLinearLie_n\) consists of all \(n \times n\) traceless matrices over \(\complex\).
    The Cartan subalgebra, \(\lie{h}\), consists of all such matrices which are also diagonal.
    A basis for this is \(H_i = E_{ii} - E_{i+1,i+1}\) for \(i=1, \dotsc, n-1\) where \(E_{ij}\) is the elementary matrix with a 1 in the \(i\)th row and \(j\)th column and zero everywhere else.
    
    To show that this is indeed the Cartan subalgebra we need to show that each \(H_i\) is \(\ad\)-semisimple.
    Rather than computing the action of \(H_i\) on \(\specialLinearLie_n\) it's simpler to find a simultaneous eigenbasis of the \(H_i\) for \(\specialLinearLie_n\).
    This can be done because the adjoint action of the \(H_i\)s all commute with each other.
    The \(H_i\) give us the diagonal matrices in \(\specialLinearLie_n\).
    To get the other matrices we can simply take \(E_{ij}\) with \(i \ne j\).
    Then a basis for \(\specialLinearLie_n\) is
    \begin{equation}
        \{H_i \mid i = 1, \dotsc, n - 1\} \cup \{E_{ij} \mid i, j \in \{1, \dotsc, n\} \text{ and } i \ne j\}.
    \end{equation}
    This is an eigenbasis for the adjoint action of the \(H_i\)s which can be shown using the relation
    \begin{equation}
        E_{ij}E_{k\ell} = \delta_{jk}E_{i\ell}.
    \end{equation}
    To see this we show that \(\ad_{H_i}\) acts on each \(H_j\) or \(E_{jk}\) by scalar multiplication.
    First note that \(\ad_{H_i}(H_j) = 0\), since diagonal matrices commute, and thus \(\ad_{H_i}\) acts on \(H_j\) by scalar multiplication by \(0\).
    For the action on \(E_{ij}\) it's actually more efficient to consider the action of \(\ad_{X}\) on \(E_{ij}\) where \(X\) is an arbitrary element of \(\csa\), that is \(X\) is a diagonal traceless matrix.
    Then we can write
    \begin{equation}
        X = 
        \begin{bmatrix}
            x_1 \\
            & x_2\\
            && \ddots\\
            &&& x_n
        \end{bmatrix}
        = \sum_{k=1}^{n} x_k E_{kk}
    \end{equation}
    where \(\sum_{k=1}^n x_k = 0\).
    This then gives, for \(i \ne j\),
    \begin{align}
        \ad_X(E_{ij}) &= \bracket*{\sum_{k=1}^n x_k E_{kk}}{E_{ij}}\\
        &= \sum_{k=1}^n x_k \bracket{E_{kk}}{E_{ij}}\\
        &= \sum_{k=1}^n x_k (E_{kk}E_{ij} - E_{ij}E_{kk})\\
        &= \sum_{k=1}^n x_k (\delta_{ki}E_{kj} - \delta_{jk}E_{ik})\\
        &= x_i E_{ij} - x_j E_{ij}\\
        &= (x_i - x_j) E_{ij},
    \end{align}
    which shows that \(E_{ij}\) is an eigenvector of \(\ad_X\) with eigenvalue \(x_i - x_j\).
    
    Following what we did for the \(\specialLinearLie_2\) case we would now consider the intersection of all of the eigenspaces of eigenvalue \(0\) as the Cartan subalgebra, which is exactly the span of the \(H_i\)s, and then we can show that no larger abelian subalgebra exists, and hence this is indeed a Cartan subalgebra.
    
    For \(i \ne j\) we have an eigenvalue of \(X \in \csa\) acting on \(E_{ij}\) given by \(x_i - x_j\).
    We can alternatively phrase this as each \(E_{ij}\) having an associated linear map,
    \begin{align}
        \alpha_{ij} \colon \csa &\to \complex\\
        X &\mapsto x_i - x_j.
    \end{align}
    This allows us to interpret a collection of \(\ad_X\) eigenvalues for eigenvectors \(E_{ij}\) as an element of \(\csa^*\).
    This is an example of something we'll make lots of use of later, known as a root.
    
    \paragraph{Cartan Subalgebras of \(\symplecticLie(2n, \complex)\)}
    Recall that \(\symplecticLie(2n, \complex)\) is defined to consist of all matrices of the form
    \begin{equation}
        \left[
        \begin{array}{c|c}
            X & Y\\ \hline
            Z & W
        \end{array}
        \right]
    \end{equation}
    with \(X, Y, Z, W \in \generalLinearLie(2n, \complex)\) subject to the conditions \(X = -W^{\trans}\), \(Y = Y^{\trans}\), and \(Z = Z^{\trans}\).
    A Cartan subalgebra of \(\symplecticLie(2n, \complex)\) consists of all matrices of the form
    \begin{equation}
        \left[
        \begin{array}{c|c}
            X & 0\\ \hline
            0 & -X
        \end{array}
        \right]
    \end{equation}
    where
    \begin{equation}
        X =
        \begin{bmatrix}
            x_1 \\
            & \ddots \\
            && x_n
        \end{bmatrix}
    \end{equation}
    for arbitrary \(x_i \in \complex\).
    Note that \(\csa = \Span\{E_{ii - E_{i+n, i+n}} \mid i = 1, \dotsc, n\}\).
    
    \paragraph{Cartan Subalgebras of \(\specialOrthogonalLie(2n, \complex)\)}
    Recall that \(\specialOrthogonalLie(2n, \complex)\) consists of all \(A \in \generalLinearLie(2n, \complex)\) subject to the condition that \(A + A^{\trans} = 0\).
    A Cartan subalgebra of \(\specialOrthogonalLie(2n, \complex)\) consists of all matrices of the form
    \begin{equation}
        \begin{bmatrix}
            0 & x_1\\
            -x_1 & 0\\
            && \ddots\\
            &&& 0 & x_n\\
            &&& -x_n & 0
        \end{bmatrix}
    \end{equation}
    for arbitrary \(x_i \in \complex\).
    
    \paragraph{Cartan Subalgebras of \(\specialOrthogonalLie(2n + 1, \complex)\)}
    Recall that \(\specialOrthogonalLie(2n + 1, \complex)\) consists of all \(A \in \generalLinearLie(2n + 1, \complex)\) subject to the condition that \(A + A^{\trans} = 0\).
    A Cartan subalgebra of \(\specialOrthogonalLie(2n + 1, \complex)\) consists of all matrices of the form
    \begin{equation}
        \begin{bmatrix}{}
            0 & x_1\\
            -x_1 & 0\\
            && \ddots\\
            &&& 0 & x_n\\
            &&& -x_n & 0\\
            &&&&& 0
        \end{bmatrix}
    \end{equation}
    for arbitrary \(x_i \in \complex\).
    
    The fact that \(\specialOrthogonalLie(2n, \complex)\) and \(\specialOrthogonalLie(2n + 1, \complex)\) have slightly different Cartan subalgebras propagates through to the classification of simple Lie algebras, in which we separate out \(\specialOrthogonalLie(2n, \complex)\) and \(\specialOrthogonalLie(2n + 1, \complex)\).
    Note that the dimension of the Cartan subalgebra of both \(\specialOrthogonalLie(2n, \complex)\) and \(\specialOrthogonalLie(2n + 1, C\gamma)\) is \(n\), so they are both rank \(n\).
    
    \section{Roots}
    When constructing a Cartan subalgebra of \(\specialLinearLie_n\) we defined a linear map \(\alpha_{ij} \in \csa^*\).
    It turns out that these maps are very important in the classification of simple Lie algebras.
    
    \begin{dfn}{Root Space}{}
        Let \(\lie{g}\) be a simple Lie algebra and \(\csa\) a Cartan subalgebra of \(\lie{g}\).
        For \(\alpha \in \csa^*\) define
        \begin{equation}
            \lie{g}_\alpha \coloneq \{Y \in \lie{g} \mid \forall X \in \csa \, \ad_X(Y) = \alpha(X)Y\}.
        \end{equation}
        We call this the \define{\(\symbf{\alpha}\)-root space}\index{root space} of \(\lie{g}\) associated with \(\csa\).
        
        For \(\alpha \in \csa^*\setminus\{0\}\) if \(\lie{g}_{\alpha} \ne 0\) we call \(\alpha\) a \defineindex{root} of \(\lie{g}\).
        The set of roots is usually denoted \(R \subseteq \csa^*\).
    \end{dfn}
    
    Note that for \(\alpha = 0\) we have \(\csa \subseteq \lie{g}_0\) since \(\csa\) is abelian so for \(Y \in \csa\) we have \(\ad_X(Y) = 0\) for all \(X \in \csa\).
    Actually, because \(\csa\) is its own normaliser it turns out that \(\lie{g}_0 = \csa\).
    Further, since every element of \(\csa\) is semisimple and \(\csa\) is abelian we have the decomposition
    \begin{equation}
        \lie{g} = \lie{g}_0 \oplus \bigoplus_{\alpha \in \csa^*\setminus\{0\}} \lie{g}_{\alpha} = \csa \oplus \bigoplus_{\alpha \in R} \lie{g}_\alpha.
    \end{equation}
    This follows because the elements, \(X\), of \(\lie{h}\) give simultaneously commuting operators, \(\ad_X\), on \(\lie{g}\), which means that they're simultaneously diagonalisable, so they act on each subspace by scalar multiplication, which is exactly the same as saying that we have this decomposition.
    Note that summing the \(\lie{g}_\alpha\) over \(\csa^*\setminus\{0\}\) or \(R\) is the same (up to isomorphism) since for \(\alpha \notin R\) \(\lie{g}_\alpha = 0\).
    Since \(\lie{g}\) is finite-dimensional it must be that only finitely many of the \(\lie{g}_\alpha\) are non-zero, so \(R\) is a finite set.
    
    It turns out that for a given rank the possible root sets, \(R\), are rather limited.
    This limitation arises from looking at how elements of \(R\) pair under the Killing form.
    This is ultimately a generalisation of the hidden integrality of \(\specialLinearLie_2\).
    
    \subsection{\texorpdfstring{\(\specialLinearLie_n\)}{sln} Roots}
    For now, let's look at \(\lie{g} = \specialLinearLie_n\).
    Then our Cartan subalgebra, \(\csa\), consists of all diagonal matrices with trace zero and we've already seen that the roots are \(\alpha_{ij}\) for \(i \ne j\) defined by \(\alpha_{ij}(X) = x_i - x_j\) where \(X\) is a diagonal matrix with \(x_i\) on the \(i\)th row.
    
    For each root, \(\alpha = \alpha_{ij}\), we can construct a copy of \(\specialLinearLie_2\), which gives us a subalgebra of \(\specialLinearLie_n\).
    To do so we need to find three generators, \(E_\alpha\), \(H_\alpha\), and \(F_\alpha\), such that \(\Span\{E_\alpha, H_\alpha, F_\alpha\} \isomorphic \specialLinearLie_2\).
    To do this we take \(E = E_{ij}\), \(F_\alpha = E_{ji}\), and using \(\specialLinearLie_2\) commutator relations we then know that we should have \(H_\alpha = \bracket{E_\alpha}{F_\alpha} = E_{ij}E_{ji} - E_{ji}E_{ij} = \delta_{jj}E_{ii} - \delta_{ii}E_{jj} = E_{ii} - E_{jj}\).
    It is then straightforward to check that
    \begin{equation}
        \bracket{H_\alpha}{E_\alpha} = 2E_\alpha, \qqand \bracket{H_\alpha}{F_\alpha} = -2F_\alpha,
    \end{equation}
    and so this really is a copy of \(\specialLinearLie_2\).
    We define \(\lie{s}_\alpha = \Span\{E_\alpha, H_\alpha, F_\alpha\}\) to be this copy of \(\specialLinearLie_2\).
    
    Note that \(E_{\alpha_{ij}} = E_{ij} \in \lie{g}_{\alpha_{ij}}\), \(F_{\alpha_{ij}} = E_{ji} \in \lie{g}_{\alpha_{ji}} = \lie{g}_{-\alpha_{ij}}\) and \(H_{\alpha_{ij}} \in \lie{g}_0 = \csa\).
    This gives us a good idea of where to look for copies of \(\specialLinearLie_2\) in a general simple Lie algebra.
    
    Notice that \(\lie{s}_\alpha\) has a natural action on \(\lie{g} = \specialLinearLie_n\) by the restriction of the adjoint action.
    Suppose that \(Y \in \lie{g}_\beta\) for some root \(\beta \in R\).
    Then \(\ad_{H_\alpha}(Y) = \beta(H_\alpha)Y\) by definition of the root space.
    By \(\specialLinearLie_2\) representation theory we know that \(\beta(H_\alpha) \in \integers\) because \(\beta(H_\alpha)\) is a weight of a finite dimensional \(\specialLinearLie_2\) representation, namely the \(\lie{s}_\alpha\)-module \(\lie{g} = \specialLinearLie_n\).
    
    \subsection{Roots for Simple Lie Algebra}
    Let \(\lie{g}\) be a simple Lie algebra.
    Recall that the Killing form is given by
    \begin{equation}
        \kappa(X, Y) = \tr(\ad_X \circ \ad_Y).
    \end{equation}
    
    \begin{lma}{}{lma:killing form vanishes on ga gb with a not -b}
        If \(\lie{g}\) is a simple Lie algebra with Cartan decomposition
        \begin{equation}
            \lie{g} = \csa \oplus \bigoplus_{\alpha \in R} \lie{g}_{\alpha}
        \end{equation}
        then if \(\alpha \ne -\beta\) we have \(\kappa(\lie{g}_\alpha, \lie{g}_\beta) = 0\).
        \begin{proof}
            We must show that \(\kappa(X, Y) = 0\) for all \(X \in \lie{g}_\alpha\) and \(Y \in \lie{g}_\beta\).
            Recall that this means that \(\ad_H(X) = \alpha(H)X\) and \(\ad_H(Y) = \beta(Y)\) for all \(H \in \csa\).
            We then have
            \begin{align}
                \alpha(H) \kappa(X, Y) &= \kappa(\alpha(H) X, Y)\\
                &= \kappa(\ad_H(X), Y)\\
                &= \kappa(\bracket{H}{X}, Y)\\
                &= \kappa(-\bracket{X}{H}, Y)\\
                &= -\kappa(\bracket{X}{H}, Y)\\
                &= -\kappa(X, \bracket{H}{Y})\\
                &= -\kappa(X, \ad_H(Y))\\
                &= -\kappa(X, \beta(H)Y)\\
                &= -\beta(H)\kappa(X, Y).
            \end{align}
            Now, since \(\alpha \ne -\beta\) we know that there is some \(H \in \csa\) such that \(\alpha(H) \ne -\beta(H)\) this simply cannot be true unless \(\kappa(X, Y) = 0\).
        \end{proof}
    \end{lma}
    
    It turns out that when \(\alpha = -\beta\) not only is the Killing form not zero, but it's actually nondegenerate.
    In particular, this applies when we choose \(\alpha = \beta = 0\), so the Killing form restricted to \(\lie{h}\) provides a nondegenerate pairing on \(\lie{h}\).
    
    \begin{lma}{}{lma:killing form nondegen pairing ga g-a}
        If \(\lie{g}\) is a simple Lie algebra with Cartan decomposition
        \begin{equation}
            \lie{g} = \csa \oplus \bigoplus_{\alpha \in R} \lie{g}_{\alpha}
        \end{equation}
        then the Killing form provides a non-degenerate pairing between \(\lie{g}_\alpha\) and \(\lie{g}_{-\alpha}\).
        \begin{proof}
            This simply follows from the fact that the Killing form is nondegenerate on \(\lie{g}\), and so for each \(X \in \lie{g}_\alpha\) there must be some \(Y \in \lie{g}\) such that \(\kappa(X, Y) \ne 0\), and by \cref{lma:killing form vanishes on ga gb with a not -b} we know that if \(Y \in \lie{g} \setminus \lie{g}_{-\alpha}\) then \(\kappa(X, Y) = 0\), so it must be that \(Y \in \lie{g}_{-\alpha}\).
            Thus, if we restrict the Killing form to a pairing \(\lie{g}_\alpha \times \lie{g}_{-\alpha} \to \complex\) it is still nondegenerate.
        \end{proof}
    \end{lma}
    
    Note that we implicitly assume here that if \(\alpha \in R\) then \(-\alpha \in R\).
    That is, we assume that if \(\lie{g}_\alpha \ne 0\) then \(\lie{g}_{-\alpha} \ne 0\).
    Actually, this proof of nondegeneracy gives us this fact, as requiring \(\kappa\) be nondegenerate for \(\lie{g}\) forces there to be a nonzero element of \(\lie{g}_{-\alpha}\) pairing with some given nonzero element of \(\lie{g}_\alpha\) to give a nonzero value under the Killing form.
    
    \begin{crl}{}{}
        Let \(R\) be the set of roots of some simple Lie algebra.
        If \(\alpha \in R\) then \(-\alpha \in R\).
    \end{crl}
    
    Since the Killing form gives a symmetric nondegenerate pairing on \(\csa\) we know from the theory of bilinear forms that it induces a canonical identification \(\iota_\kappa \colon \csa \to \csa^*\) given by \(\iota_\kappa(H) = \kappa(H, -)\).
    
    \begin{dfn}{}{}
        Let \(\lie{g}\) be a simple Lie algebra with Cartan subalgebra \(\csa\).
        Write \(\eta \colon \csa \to \csa^*\) for the induced canonical identification \(\eta = \iota_\kappa\).
        Write \(t_\alpha \in \csa\) for the element of \(\csa\) corresponding to \(\alpha \in \csa^*\) under this canonical identification.
        That is, \(t_\alpha = \eta^{-1}(\alpha)\).
    \end{dfn}
    
    Note that by definition of the identification \(\eta\) we have that
    \begin{equation}
        \eta(H)(\alpha) = \kappa(H, \eta^{-1}(\alpha)) = \alpha(H)
    \end{equation}
    for all \(\alpha \in \csa^*\) and \(H \in \csa\).
    In particular, \(\kappa(t_\alpha, H) = \kappa(\eta^{-1}(\alpha), H) = \alpha(H)\).
    
    \begin{lma}{}{lma:roots are nozero}
        Let \(\lie{g}\) be a simple Lie algebra and \(\csa\) a Cartan subalgebra with roots \(R\).
        If \(H \in \csa\) and \(H \ne 0\) then there exists a root \(\alpha \in R\) such that \(\alpha(H) \ne 0\).
        \begin{proof}
            Suppose \(\alpha(H) = 0\) for all roots.
            Then \(\bracket{H}{X} = \alpha(H)X = 0\) for all \(X \in \lie{g}_\alpha\).
            Since \(\csa\) is abelian it follows that \(H \in \centre(\lie{g})\), which implies \(H = 0\) since \(\lie{g}\) is simple and so has trivial centre.
        \end{proof}
    \end{lma}
    
    \begin{crl}{}{}
        The set of roots, \(R\), spans \(\csa^*\).
        \begin{proof}
            Suppose \(R\) doesn't span \(\csa^*\).
            Let \(W \subsetneq \csa^*\) be the span of \(R\).
            Then we may define the annihilator of \(W\) in \(H\) to be the subset
            \begin{equation}
                W^\circ = \{H \in \csa \mid \alpha(H) = 0 \forall \alpha \in W\}.
            \end{equation}
            This has dimension \(\dim \csa - \dim W \ne 0\), and therefore there is some nonzero \(H \in \csa\) such that \(\alpha(H) = 0\) for all \(\alpha \in W\), and in particular for all \(\alpha \in R\), which contradicts \cref{lma:roots are nozero}.
            Thus, we must have \(W = \csa\) so \(\dim \csa - \dim W = 0\).
        \end{proof}
    \end{crl}
    
    It turns out that \(t_\alpha\) is \emph{almost} the correct choice for \(H_\alpha\), the Cartan element of the \(\specialLinearLie_2\) subalgebra that we're looking for.
    In fact, all we need to do is scale it by a factor of \(2/\kappa(t_\alpha, t_\alpha)\).
    
    \begin{lma}{}{lma:bracket of ga and g-a is multiple of ta}
        Let \(\lie{g}\) be a simple Lie algebra with Cartan subalgebra \(\csa\) and roots \(R\).
        If \(X \in \lie{g}_\alpha\) and \(Y \in \lie{g}_{-\alpha}\) then \(\bracket{X}{Y} = \kappa(X, Y) t_\alpha\).
        \begin{proof}
            We will show that \(\bracket{X}{Y} - \kappa(X, Y)t_\alpha = 0\).
            First note that \(\bracket{X}{Y}, t_\alpha \in \csa\), so \(\bracket{X}{Y} - \kappa(X, Y) t_\alpha \in \csa\).
            Then by nondegeneracy of \(\kappa\) restricted to \(\csa\) it is sufficient to show that
            \begin{equation}
                \kappa(\bracket{X}{Y} - \kappa(X, Y)t_\alpha, H) = 0
            \end{equation}
            for all \(H \in \csa\).
            We have
            \begin{align}
                \kappa(\bracket{X}{Y}, H) &= \kappa(X, \bracket{Y}{H})\\
                &= -\kappa(X, \bracket{H}{Y})\\
                &= -\kappa(X, -\alpha(H)Y)\\
                &= \alpha(H) \kappa(X, Y)\\
                &= \kappa(t_\alpha, H) \kappa(X, Y)
            \end{align}
            so by linearity
            \begin{equation}
                \kappa(\bracket{X}{Y} - \kappa(X, Y)t_\alpha, H) = 0,
            \end{equation}
            and hence \(\bracket{X}{Y} - \kappa(X, Y)t_\alpha = 0\).
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{lma:killing form of ta ta nonzero}
        Let \(\lie{g}\) be a simple Lie algebra with Cartan subalgebra \(\csa\) and corresponding roots \(R\).
        Then for all \(\alpha \in R\) the quantity \(\kappa(t_\alpha, t_\alpha)\) is nonzero.
        \begin{proof}
            Suppose that \(\kappa(t_\alpha, t_\alpha) = 0\) for some \(\alpha \in R\).
            Then we have
            \begin{equation}
                \bracket{t_\alpha}{X} = \alpha(t_\alpha)X = \kappa(t_\alpha, t_\alpha)X = 0
            \end{equation}
            for \(X \in \lie{g}_\alpha\) and similarly
            \begin{equation}
                \bracket{t_\alpha}{Y} = -\alpha(t_\alpha)Y = -\kappa(t_\alpha, t_\alpha)Y = 0
            \end{equation}
            for \(Y \in \lie{g}_{-\alpha}\).
            
            Since the Killing form provides a nondegenerate pairing \(\lie{g}_\alpha \times \lie{g}_{-\alpha} \to \complex\) (\cref{lma:killing form nondegen pairing ga g-a}) for a choice of \(X \in \lie{g}_{\alpha}\) there must be some \(Y \in \lie{g}_{-\alpha}\) such that \(\kappa(X, Y) \ne 0\).
            The Killing form is bilinear, so we can always rescale \(X\) or \(Y\) so that \(\kappa(X, Y) = 1\).
            Then we have \(\bracket{X}{Y} = \kappa(X, Y)t_\alpha = t_\alpha\).
            Consider the subalgebra \(\lie{s} = \Span\{X, Y, t_\alpha\}\).
            This is a subalgebra as it is closed under the commutator since \cref{lma:bracket of ga and g-a is multiple of ta} shows that \(\bracket{X}{Y}\) is just a scalar multiple of \(t_\alpha\).
            Further, we have \(\bracket{\lie{s}}{\lie{s}} = \Span\{t_\alpha\}\), since \(t_\alpha\) brackets with anything to give zero, and \(\bracket{X}{Y} = t_\alpha\).
            Then we have \(\bracket{\bracket{\lie{s}}{\lie{s}}}{\bracket{\lie{s}}{\lie{s}}} = \bracket{\complex t_\alpha}{\complex t_\alpha} = 0\).
            Thus, the derived series of \(\lie{s}\) terminates, so this is a solvable subalgebra.
            We know from \cref{crl:solvable subalgebra consists of simple endomorphisms} that if \(\lie{s}\) is a solvable subalgebra then in some basis \(\lie{s}\) is a subalgebra of the algebra \(\borelLie(n, \complex)\).
            Then since \(\bracket{\borelLie(n, \complex)}{\borelLie(n, \complex)} = \nilpotentLie(n, \complex)\) we have that in some basis \(\bracket{\lie{s}}{\lie{s}}\) is a subalgebra of \(\nilpotentLie(n, \complex)\).
            Thus, \(\bracket{\lie{s}}{\lie{s}}\) consists of \(\ad\)-nilpotent endomorphisms of \(\lie{g}\).
            We've shown that \(\bracket{\lie{s}}{\lie{s}}\), so this means \(t_\alpha\) is \(\ad\)-nilpotent.
            Since \(t_\alpha \in \csa\) we also know that \(t_\alpha\) is semisimple, and being both nilpotent and semisimple implies that \(t_\alpha = 0\), a contradiction.
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{}
        Let \(\lie{g}\) be a simple Lie algebra with Cartan subalgebra \(\csa\) and corresponding roots \(R\).
        For each \(\alpha \in R\) we have
        \begin{itemize}
            \item an \(\specialLinearLie_2\) triple, \(\lie{s}_\alpha = \Span\{E_\alpha, H_\alpha, F_\alpha\}\) where \(E_\alpha \in \lie{g}_\alpha\), \(F_\alpha \in \lie{g}_{-\alpha}\), and \(H_\alpha = \bracket{E_\alpha}{F_\alpha}\); and
            \item \(H_\alpha = \displaystyle \frac{2t_\alpha}{\kappa(t_\alpha, t_\alpha)}\).
        \end{itemize}
        \begin{proof}
            Choose some \(E_\alpha \in \lie{g}_\alpha\).
            The Killing form provides a nondegenerate pairing \(\lie{g}_\alpha \times \lie{g}_{-\alpha} \to \complex\), so there exists some \(F_\alpha \in \lie{g}_{-\alpha}\) such that \(\kappa(E_\alpha, F_\alpha) \ne 0\).
            Since the Killing form is bilinear we can always rescale \(E_\alpha\) or \(F_\alpha\) so that \(\kappa(E_\alpha, F_\alpha) = 2/\kappa(t_\alpha, t_\alpha)\), where we've used the fact that \(\kappa(t_\alpha, t_\alpha) \ne 0\) (\cref{lma:killing form of ta ta nonzero}).
            Now set \(H_\alpha = \bracket{E_\alpha}{F_\alpha} = \kappa(X, Y)t_\alpha = 2t_\alpha/\kappa(t_\alpha, t_\alpha)\).
            
            The fact that this results in an \(\specialLinearLie_2\)-triple then follows from a direct calculation of brackets:
            \begin{itemize}
                \item \(\bracket{E_\alpha}{F_\alpha} = H_\alpha\) as required;
                \item \(\bracket{H_\alpha}{E_\alpha} = \alpha(H_\alpha) E_\alpha\) and \(\alpha(H_\alpha) = \alpha(2t_\alpha/\kappa(t_\alpha, t_\alpha)) = 2\alpha(t_\alpha)/\kappa(t_\alpha, t_\alpha) = 2\kappa(t_\alpha, t_\alpha)/\kappa(t_\alpha, t_\alpha) = 2\) as required;
                \item \(\bracket{H_\alpha}{F_\alpha} = -\alpha(H_\alpha) F_\alpha = -2F_\alpha\) as required.
            \end{itemize}
        \end{proof}
    \end{lma}
    
    \begin{lma}{}{}
        Let \(\lie{g}\) be a simple Lie algebra with Cartan subalgebra \(\csa\) and corresponding roots \(R\).
        For each \(\alpha \in R\) we have
        \begin{itemize}
            \item \(\dim \lie{g}_\alpha = 1\);
            \item \(c\alpha \in R\) if and only if \(c = \pm 1\).
        \end{itemize}
        \begin{proof}
            Consider \(\lie{g}\) as an \(\specialLinearLie_2\)-module where \(\specialLinearLie_2\) is taken to be the \(\specialLinearLie_2\) triple \(\lie{s}_\alpha = \Span\{E_\alpha, H_\alpha, F_\alpha\}\) acting on \(\lie{g}\) through the restriction of the adjoint representation.
            Let \(M\) be the space spanned by \(\csa\) and all \(\lie{g}_{c\alpha}\) for \(c \in \complex\).
            That is
            \begin{equation}
                M = \csa \oplus \bigoplus_{c\alpha \in R} \lie{g}_{c\alpha}.
            \end{equation}
            Note that we can restrict our sum to values of \(c\) such that \(c\alpha \in R\), since if \(c\alpha \notin R\) then \(\lie{g}_{c\alpha} = 0\).
            This is an \(\lie{s}_\alpha\)-submodule of \(\lie{g}\).
            By the classification of simple \(\specialLinearLie_2\)-modules and Weyl's complete reducibility theorem we know that the weights of \(H_\alpha\) on \(M\) are integers.
            Now, if \(c\alpha \in R\) then
            \begin{equation}
                c\alpha(H_\alpha) = c\alpha\left( \frac{2t_\alpha}{\kappa(t_\alpha, t_\alpha)} \right) = \frac{2c}{\kappa(t_\alpha, t_\alpha)} \alpha(t_\alpha) = \frac{2c}{\kappa(t_\alpha, t_\alpha)} \kappa(t_\alpha, t_\alpha) = 2c
            \end{equation}
            and so we have \(2c \in \integers\).
            
            We can decompose \(\csa\) as \(\csa = \ker \alpha \oplus \complex H_\alpha\).
            This is possible because if \(H \in \csa\) and \(\alpha(H) \ne 0\) then \(H\) acts nontrivially on \(\lie{g}_\alpha\), specifically, \(\bracket{H}{X} = \alpha(H)X\), but we also know that \(\bracket{H_\alpha}{X} = \alpha(H_\alpha)X = 2X\), so \(H\) acts as a multiple, \(\alpha(H)/2\), of \(H_\alpha\).
            This tells us that \(\ker \alpha\) is an \(\lie{s}_\alpha\)-submodule of \(\csa\) on which \(\lie{s}_\alpha\) acts trivially.
            We can therefore decompose \(M\) as \(M = \lie{s}_\alpha \oplus \ker \alpha \oplus M'\) for some \(\lie{s}_\alpha\)-submodule \(M'\).
            Since \(M_0 = \csa\) in the original decomposition of \(\lie{g}\) we must have that \(M'_0 = 0\), since there are no semisimple elements remaining for \(M'\), they're all in \(\lie{s}_\alpha\) (if they're a multiple of \(H_\alpha\)) or \(\ker \alpha\) (otherwise).
            This means that all of the \(H_\alpha\) weights of \(M'\) are odd integers.
            The reason for this is that if they were not then by acting with \(E_\alpha\) or \(F_\alpha\), which shift the weight by \(\pm 2\), we could get to \(M_0' = 0\), but then everything must be zero, so any even-weight subspace is trivial.
            
            This tells us that \(\dim \lie{g}_\alpha = 1\), since otherwise we could take two linearly independent elements of \(\lie{g}_\alpha\) and combine them to form an even weight element.
            
            We can also see that \(2\alpha \notin R\), since if it were then it would necessarily have even weights, that is, twice a root is never a root.
            Now, suppose that \(\frac{1}{2}\alpha\) is a root, then \(2 \frac{1}{2}\alpha = \alpha\) cannot be a root, as it's twice a root, so \(\frac{1}{2}\alpha\) isn't a root because we're assuming \(\alpha\) is.
            From this it follows that \(\pm \alpha\) are the only multiples of \(\alpha\) which are in \(R\).
        \end{proof}
    \end{lma}
    
    
    
    
    
    
    
    
    
    
    
	% Appdendix
%	\appendixpage
%	\begin{appendices}
%	
%	\end{appendices}
	
	\backmatter
	\renewcommand{\glossaryname}{Acronyms}
	\printglossary[acronym]
	\printindex
\end{document}
